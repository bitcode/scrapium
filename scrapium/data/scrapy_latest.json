[
{"url": "https://docs.scrapy.org/en/latest/", "content": "                              Scrapy 2.11 documentation Scrapy is a fast high-level web crawling and web scraping framework, used to crawl websites and extract structured data from their pages. It can be used for a wide range of purposes, from data mining to monitoring and automated testing.  Getting help Having trouble? We’d like to help!  Try the FAQ – it’s got answers to some common questions. Looking for specific information? Try the Index or Module Index. Ask or search questions in StackOverflow using the scrapy tag. Ask or search questions in the Scrapy subreddit. Search for questions on the archives of the scrapy-users mailing list. Ask a question in the #scrapy IRC channel, Report bugs with Scrapy in our issue tracker. Join the Discord community Scrapy Discord.    First steps    Scrapy at a glanceUnderstand what Scrapy is and how it can help you.  Installation guideGet Scrapy installed on your computer.  Scrapy TutorialWrite your first Scrapy project.  ExamplesLearn more by playing with a pre-made Scrapy project.     Basic concepts    Command line toolLearn about the command-line tool used to manage your Scrapy project.  SpidersWrite the rules to crawl your websites.  SelectorsExtract the data from web pages using XPath.  Scrapy shellTest your extraction code in an interactive environment.  ItemsDefine the data you want to scrape.  Item LoadersPopulate your items with the extracted data.  Item PipelinePost-process and store your scraped data.  Feed exportsOutput your scraped data using different formats and storages.  Requests and ResponsesUnderstand the classes used to represent HTTP requests and responses.  Link ExtractorsConvenient classes to extract links to follow from pages.  SettingsLearn how to configure Scrapy and see all available settings.  ExceptionsSee all available exceptions and their meaning.     Built-in services    LoggingLearn how to use Python’s builtin logging on Scrapy.  Stats CollectionCollect statistics about your scraping crawler.  Sending e-mailSend email notifications when certain events occur.  Telnet ConsoleInspect a running crawler using a built-in Python console.     Solving specific problems    Frequently Asked QuestionsGet answers to most frequently asked questions.  Debugging SpidersLearn how to debug common problems of your Scrapy spider.  Spiders ContractsLearn how to use contracts for testing your spiders.  Common PracticesGet familiar with some Scrapy common practices.  Broad CrawlsTune Scrapy for crawling a lot domains in parallel.  Using your browser’s Developer Tools for scrapingLearn how to scrape with your browser’s developer tools.  Selecting dynamically-loaded contentRead webpage data that is loaded dynamically.  Debugging memory leaksLearn how to find and get rid of memory leaks in your crawler.  Downloading and processing files and imagesDownload files and/or images associated with your scraped items.  Deploying SpidersDeploying your Scrapy spiders and run them in a remote server.  AutoThrottle extensionAdjust crawl rate dynamically based on load.  BenchmarkingCheck how Scrapy performs on your hardware.  Jobs: pausing and resuming crawlsLearn how to pause and resume crawls for large spiders.  CoroutinesUse the coroutine syntax.  asyncioUse asyncio and asyncio-powered libraries.     Extending Scrapy    Architecture overviewUnderstand the Scrapy architecture.  Add-onsEnable and configure third-party extensions.  Downloader MiddlewareCustomize how pages get requested and downloaded.  Spider MiddlewareCustomize the input and output of your spiders.  ExtensionsExtend Scrapy with your custom functionality  SignalsSee all available signals and how to work with them.  SchedulerUnderstand the scheduler component.  Item ExportersQuickly export your scraped items to a file (XML, CSV, etc).  ComponentsLearn the common API and some good practices when building custom Scrapy components.  Core APIUse it on extensions and middlewares to extend Scrapy functionality.     All the rest    Release notesSee what has changed in recent Scrapy versions.  Contributing to ScrapyLearn how to contribute to the Scrapy project.  Versioning and API stabilityUnderstand Scrapy versioning and API stability.                             ", "code_blocks": [], "links": [{"text": "web crawling", "href": "https://en.wikipedia.org/wiki/Web_crawler"}, {"text": "web scraping", "href": "https://en.wikipedia.org/wiki/Web_scraping"}, {"text": "FAQ", "href": "faq.html"}, {"text": "Index", "href": "genindex.html"}, {"text": "Module Index", "href": "py-modindex.html"}, {"text": "StackOverflow using the scrapy tag", "href": "https://stackoverflow.com/tags/scrapy"}, {"text": "Scrapy subreddit", "href": "https://www.reddit.com/r/scrapy/"}, {"text": "scrapy-users mailing list", "href": "https://groups.google.com/forum/#!forum/scrapy-users"}, {"text": "#scrapy IRC channel", "href": "irc://irc.freenode.net/scrapy"}, {"text": "issue tracker", "href": "https://github.com/scrapy/scrapy/issues"}, {"text": "Scrapy Discord", "href": "https://discord.gg/mv3yErfpvq"}, {"text": "Scrapy at a glance", "href": "intro/overview.html"}, {"text": "Installation guide", "href": "intro/install.html"}, {"text": "Scrapy Tutorial", "href": "intro/tutorial.html"}, {"text": "Examples", "href": "intro/examples.html"}, {"text": "Command line tool", "href": "topics/commands.html"}, {"text": "Spiders", "href": "topics/spiders.html"}, {"text": "Selectors", "href": "topics/selectors.html"}, {"text": "Scrapy shell", "href": "topics/shell.html"}, {"text": "Items", "href": "topics/items.html"}, {"text": "Item Loaders", "href": "topics/loaders.html"}, {"text": "Item Pipeline", "href": "topics/item-pipeline.html"}, {"text": "Feed exports", "href": "topics/feed-exports.html"}, {"text": "Requests and Responses", "href": "topics/request-response.html"}, {"text": "Link Extractors", "href": "topics/link-extractors.html"}, {"text": "Settings", "href": "topics/settings.html"}, {"text": "available settings", "href": "topics/settings.html#topics-settings-ref"}, {"text": "Exceptions", "href": "topics/exceptions.html"}, {"text": "Logging", "href": "topics/logging.html"}, {"text": "Stats Collection", "href": "topics/stats.html"}, {"text": "Sending e-mail", "href": "topics/email.html"}, {"text": "Telnet Console", "href": "topics/telnetconsole.html"}, {"text": "Frequently Asked Questions", "href": "faq.html"}, {"text": "Debugging Spiders", "href": "topics/debug.html"}, {"text": "Spiders Contracts", "href": "topics/contracts.html"}, {"text": "Common Practices", "href": "topics/practices.html"}, {"text": "Broad Crawls", "href": "topics/broad-crawls.html"}, {"text": "Using your browser’s Developer Tools for scraping", "href": "topics/developer-tools.html"}, {"text": "Selecting dynamically-loaded content", "href": "topics/dynamic-content.html"}, {"text": "Debugging memory leaks", "href": "topics/leaks.html"}, {"text": "Downloading and processing files and images", "href": "topics/media-pipeline.html"}, {"text": "Deploying Spiders", "href": "topics/deploy.html"}, {"text": "AutoThrottle extension", "href": "topics/autothrottle.html"}, {"text": "Benchmarking", "href": "topics/benchmarking.html"}, {"text": "Jobs: pausing and resuming crawls", "href": "topics/jobs.html"}, {"text": "Coroutines", "href": "topics/coroutines.html"}, {"text": "coroutine syntax", "href": "https://docs.python.org/3/reference/compound_stmts.html#async"}, {"text": "asyncio", "href": "topics/asyncio.html"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "Architecture overview", "href": "topics/architecture.html"}, {"text": "Add-ons", "href": "topics/addons.html"}, {"text": "Downloader Middleware", "href": "topics/downloader-middleware.html"}, {"text": "Spider Middleware", "href": "topics/spider-middleware.html"}, {"text": "Extensions", "href": "topics/extensions.html"}, {"text": "Signals", "href": "topics/signals.html"}, {"text": "Scheduler", "href": "topics/scheduler.html"}, {"text": "Item Exporters", "href": "topics/exporters.html"}, {"text": "Components", "href": "topics/components.html"}, {"text": "Core API", "href": "topics/api.html"}, {"text": "Release notes", "href": "news.html"}, {"text": "Contributing to Scrapy", "href": "contributing.html"}, {"text": "Versioning and API stability", "href": "versioning.html"}], "timestamp": "2023-10-12T00:10:18.252691", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/versioning.html", "content": "                              Versioning and API stability  Versioning There are 3 numbers in a Scrapy version: A.B.C  A is the major version. This will rarely change and will signify very large changes. B is the release number. This will include many changes including features and things that possibly break backward compatibility, although we strive to keep these cases at a minimum. C is the bugfix release number.  Backward-incompatibilities are explicitly mentioned in the release notes, and may require special attention before upgrading. Development releases do not follow 3-numbers version and are generally released as dev suffixed versions, e.g. 1.3dev.  Note With Scrapy 0.* series, Scrapy used odd-numbered versions for development releases. This is not the case anymore from Scrapy 1.0 onwards. Starting with Scrapy 1.0, all releases should be considered production-ready.  For example:  1.1.1 is the first bugfix release of the 1.1 series (safe to use in production)    API stability API stability was one of the major goals for the 1.0 release. Methods or functions that start with a single dash (_) are private and should never be relied as stable. Also, keep in mind that stable doesn’t mean complete: stable APIs could grow new methods or functionality but the existing methods should keep working the same way.   Deprecation policy We aim to maintain support for deprecated Scrapy features for at least 1 year. For example, if a feature is deprecated in a Scrapy version released on June 15th 2020, that feature should continue to work in versions released on June 14th 2021 or before that. Any new Scrapy release after a year may remove support for that deprecated feature. All deprecated features removed in a Scrapy release are explicitly mentioned in the release notes.                           ", "code_blocks": [], "links": [{"text": "release notes", "href": "news.html#news"}, {"text": "odd-numbered versions for development releases", "href": "https://en.wikipedia.org/wiki/Software_versioning#Odd-numbered_versions_for_development_releases"}, {"text": "release notes", "href": "news.html#news"}], "timestamp": "2023-10-12T00:10:22.639738", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/contributing.html", "content": "                              Contributing to Scrapy  Important Double check that you are reading the most recent version of this document at https://docs.scrapy.org/en/master/contributing.html  There are many ways to contribute to Scrapy. Here are some of them:  Report bugs and request features in the issue tracker, trying to follow the guidelines detailed in Reporting bugs below. Submit patches for new functionalities and/or bug fixes. Please read Writing patches and Submitting patches below for details on how to write and submit a patch. Blog about Scrapy. Tell the world how you’re using Scrapy. This will help newcomers with more examples and will help the Scrapy project to increase its visibility. Join the Scrapy subreddit and share your ideas on how to improve Scrapy. We’re always open to suggestions. Answer Scrapy questions at Stack Overflow.   Reporting bugs  Note Please report security issues only to scrapy-security@googlegroups.com. This is a private list only open to trusted Scrapy developers, and its archives are not public.  Well-written bug reports are very helpful, so keep in mind the following guidelines when you’re going to report a new bug.  check the FAQ first to see if your issue is addressed in a well-known question if you have a general question about Scrapy usage, please ask it at Stack Overflow (use “scrapy” tag). check the open issues to see if the issue has already been reported. If it has, don’t dismiss the report, but check the ticket history and comments. If you have additional useful information, please leave a comment, or consider sending a pull request with a fix. search the scrapy-users list and Scrapy subreddit to see if it has been discussed there, or if you’re not sure if what you’re seeing is a bug. You can also ask in the #scrapy IRC channel. write complete, reproducible, specific bug reports. The smaller the test case, the better. Remember that other developers won’t have your project to reproduce the bug, so please include all relevant files required to reproduce it. See for example StackOverflow’s guide on creating a Minimal, Complete, and Verifiable example exhibiting the issue. the most awesome way to provide a complete reproducible example is to send a pull request which adds a failing test case to the Scrapy testing suite (see Submitting patches). This is helpful even if you don’t have an intention to fix the issue yourselves. include the output of scrapy version -v so developers working on your bug know exactly which version and platform it occurred on, which is often very helpful for reproducing it, or knowing if it was already fixed.    Writing patches Scrapy has a list of good first issues and help wanted issues that you can work on. These issues are a great way to get started with contributing to Scrapy. If you’re new to the codebase, you may want to focus on documentation or testing-related issues, as they are always useful and can help you get more familiar with the project. You can also check Scrapy’s test coverage to see which areas may benefit from more tests. The better a patch is written, the higher the chances that it’ll get accepted and the sooner it will be merged. Well-written patches should:  contain the minimum amount of code required for the specific change. Small patches are easier to review and merge. So, if you’re doing more than one change (or bug fix), please consider submitting one patch per change. Do not collapse multiple changes into a single patch. For big changes consider using a patch queue. pass all unit-tests. See Running tests below. include one (or more) test cases that check the bug fixed or the new functionality added. See Writing tests below. if you’re adding or changing a public (documented) API, please include the documentation changes in the same patch.  See Documentation policies below. if you’re adding a private API, please add a regular expression to the coverage_ignore_pyobjects variable of docs/conf.py to exclude the new private API from documentation coverage checks. To see if your private API is skipped properly, generate a documentation coverage report as follows: tox -e docs-coverage    if you are removing deprecated code, first make sure that at least 1 year (12 months) has passed since the release that introduced the deprecation. See Deprecation policy.    Submitting patches The best way to submit a patch is to issue a pull request on GitHub, optionally creating a new issue first. Remember to explain what was fixed or the new functionality (what it is, why it’s needed, etc). The more info you include, the easier will be for core developers to understand and accept your patch. You can also discuss the new functionality (or bug fix) before creating the patch, but it’s always good to have a patch ready to illustrate your arguments and show that you have put some additional thought into the subject. A good starting point is to send a pull request on GitHub. It can be simple enough to illustrate your idea, and leave documentation/tests for later, after the idea has been validated and proven useful. Alternatively, you can start a conversation in the Scrapy subreddit to discuss your idea first. Sometimes there is an existing pull request for the problem you’d like to solve, which is stalled for some reason. Often the pull request is in a right direction, but changes are requested by Scrapy maintainers, and the original pull request author hasn’t had time to address them. In this case consider picking up this pull request: open a new pull request with all commits from the original pull request, as well as additional changes to address the raised issues. Doing so helps a lot; it is not considered rude as long as the original author is acknowledged by keeping his/her commits. You can pull an existing pull request to a local branch by running git fetch upstream pull/$PR_NUMBER/head:$BRANCH_NAME_TO_CREATE (replace ‘upstream’ with a remote name for scrapy repository, $PR_NUMBER with an ID of the pull request, and $BRANCH_NAME_TO_CREATE with a name of the branch you want to create locally). See also: https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/checking-out-pull-requests-locally#modifying-an-inactive-pull-request-locally. When writing GitHub pull requests, try to keep titles short but descriptive. E.g. For bug #411: “Scrapy hangs if an exception raises in start_requests” prefer “Fix hanging when exception occurs in start_requests (#411)” instead of “Fix for #411”. Complete titles make it easy to skim through the issue tracker. Finally, try to keep aesthetic changes (PEP 8 compliance, unused imports removal, etc) in separate commits from functional changes. This will make pull requests easier to review and more likely to get merged.   Coding style Please follow these coding conventions when writing code for inclusion in Scrapy:  We use black for code formatting. There is a hook in the pre-commit config that will automatically format your code before every commit. You can also run black manually with tox -e black. Don’t put your name in the code you contribute; git provides enough metadata to identify author of the code. See https://help.github.com/en/github/using-git/setting-your-username-in-git for setup instructions.    Pre-commit We use pre-commit to automatically address simple code issues before every commit. After your create a local clone of your fork of the Scrapy repository:  Install pre-commit. On the root of your local clone of the Scrapy repository, run the following command: pre-commit install     Now pre-commit will check your changes every time you create a Git commit. Upon finding issues, pre-commit aborts your commit, and either fixes those issues automatically, or only reports them to you. If it fixes those issues automatically, creating your commit again should succeed. Otherwise, you may need to address the corresponding issues manually first.   Documentation policies For reference documentation of API members (classes, methods, etc.) use docstrings and make sure that the Sphinx documentation uses the autodoc extension to pull the docstrings. API reference documentation should follow docstring conventions (PEP 257) and be IDE-friendly: short, to the point, and it may provide short examples. Other types of documentation, such as tutorials or topics, should be covered in files within the docs/ directory. This includes documentation that is specific to an API member, but goes beyond API reference documentation. In any case, if something is covered in a docstring, use the autodoc extension to pull the docstring into the documentation instead of duplicating the docstring in files within the docs/ directory. Documentation updates that cover new or modified features must use Sphinx’s versionadded and versionchanged directives. Use VERSION as version, we will replace it with the actual version right before the corresponding release. When we release a new major or minor version of Scrapy, we remove these directives if they are older than 3 years. Documentation about deprecated features must be removed as those features are deprecated, so that new readers do not run into it. New deprecations and deprecation removals are documented in the release notes.   Tests Tests are implemented using the Twisted unit-testing framework. Running tests requires tox.  Running tests To run all tests: tox   To run a specific test (say tests/test_loader.py) use:  tox -- tests/test_loader.py  To run the tests on a specific tox environment, use -e <name> with an environment name from tox.ini. For example, to run the tests with Python 3.10 use: tox -e py310   You can also specify a comma-separated list of environments, and use tox’s parallel mode to run the tests on multiple environments in parallel: tox -e py39,py310 -p auto   To pass command-line options to pytest, add them after -- in your call to tox. Using -- overrides the default positional arguments defined in tox.ini, so you must include those default positional arguments (scrapy tests) after -- as well: tox -- scrapy tests -x  # stop after first failure   You can also use the pytest-xdist plugin. For example, to run all tests on the Python 3.10 tox environment using all your CPU cores: tox -e py310 -- scrapy tests -n auto   To see coverage report install coverage (pip install coverage) and run:  coverage report  see output of coverage --help for more options like html or xml report.   Writing tests All functionality (including new features and bug fixes) must include a test case to check that it works as expected, so please include tests for your patches if you want them to get accepted sooner. Scrapy uses unit-tests, which are located in the tests/ directory. Their module name typically resembles the full path of the module they’re testing. For example, the item loaders code is in: scrapy.loader   And their unit-tests are in: tests/test_loader.py                              ", "code_blocks": ["tox -e docs-coverage\n</pre>", "pre-commit install\n</pre>", "tox\n</pre>", "tox -e py310\n</pre>", "tox -e py39,py310 -p auto\n</pre>", "tox -- scrapy tests -x  # stop after first failure\n</pre>", "tox -e py310 -- scrapy tests -n auto\n</pre>", "scrapy.loader\n</pre>", "tests/test_loader.py\n</pre>"], "links": [{"text": "https://docs.scrapy.org/en/master/contributing.html", "href": "https://docs.scrapy.org/en/master/contributing.html"}, {"text": "issue tracker", "href": "https://github.com/scrapy/scrapy/issues"}, {"text": "Scrapy subreddit", "href": "https://reddit.com/r/scrapy"}, {"text": "Stack Overflow", "href": "https://stackoverflow.com/questions/tagged/scrapy"}, {"text": "FAQ", "href": "faq.html#faq"}, {"text": "Stack Overflow", "href": "https://stackoverflow.com/questions/tagged/scrapy"}, {"text": "open issues", "href": "https://github.com/scrapy/scrapy/issues"}, {"text": "scrapy-users", "href": "https://groups.google.com/forum/#!forum/scrapy-users"}, {"text": "Scrapy subreddit", "href": "https://reddit.com/r/scrapy"}, {"text": "Minimal, Complete, and Verifiable example", "href": "https://stackoverflow.com/help/mcve"}, {"text": "good first issues", "href": "https://github.com/scrapy/scrapy/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"}, {"text": "help wanted issues", "href": "https://github.com/scrapy/scrapy/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22"}, {"text": "test coverage", "href": "https://app.codecov.io/gh/scrapy/scrapy"}, {"text": "Deprecation policy", "href": "versioning.html#deprecation-policy"}, {"text": "pull request", "href": "https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request"}, {"text": "Scrapy subreddit", "href": "https://reddit.com/r/scrapy"}, {"text": "https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/checking-out-pull-requests-locally#modifying-an-inactive-pull-request-locally", "href": "https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/checking-out-pull-requests-locally#modifying-an-inactive-pull-request-locally"}, {"text": "PEP 8", "href": "https://peps.python.org/pep-0008/"}, {"text": "black", "href": "https://black.readthedocs.io/en/stable/"}, {"text": "https://help.github.com/en/github/using-git/setting-your-username-in-git", "href": "https://help.github.com/en/github/using-git/setting-your-username-in-git"}, {"text": "pre-commit", "href": "https://pre-commit.com/"}, {"text": "Install pre-commit", "href": "https://pre-commit.com/#installation"}, {"text": "autodoc", "href": "https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#module-sphinx.ext.autodoc"}, {"text": "PEP 257", "href": "https://www.python.org/dev/peps/pep-0257/"}, {"text": "autodoc", "href": "https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#module-sphinx.ext.autodoc"}, {"text": "versionadded", "href": "https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionadded"}, {"text": "versionchanged", "href": "https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionchanged"}, {"text": "release notes", "href": "news.html#news"}, {"text": "Twisted unit-testing framework", "href": "https://docs.twisted.org/en/stable/development/test-standard.html"}, {"text": "tox", "href": "https://tox.wiki/en/latest/index.html"}, {"text": "tox", "href": "https://tox.wiki/en/latest/index.html"}, {"text": "tox’s\nparallel mode", "href": "https://tox.wiki/en/latest/user_guide.html#parallel-mode"}, {"text": "pytest", "href": "https://docs.pytest.org/en/latest/index.html"}, {"text": "tox", "href": "https://tox.wiki/en/latest/index.html"}, {"text": "pytest-xdist", "href": "https://github.com/pytest-dev/pytest-xdist"}, {"text": "tox", "href": "https://tox.wiki/en/latest/index.html"}, {"text": "coverage", "href": "https://coverage.readthedocs.io/en/stable/index.html"}, {"text": "tests/", "href": "https://github.com/scrapy/scrapy/tree/master/tests"}], "timestamp": "2023-10-12T00:10:26.396675", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/news.html", "content": "                              Release notes  Scrapy 2.11.0 (2023-09-18) Highlights:  Spiders can now modify settings in their from_crawler() methods, e.g. based on spider arguments. Periodic logging of stats.   Backward-incompatible changes  Most of the initialization of scrapy.crawler.Crawler instances is now done in crawl(), so the state of instances before that method is called is now different compared to older Scrapy versions. We do not recommend using the Crawler instances before crawl() is called. (issue 6038) scrapy.Spider.from_crawler() is now called before the initialization of various components previously initialized in scrapy.crawler.Crawler.__init__() and before the settings are finalized and frozen. This change was needed to allow changing the settings in scrapy.Spider.from_crawler(). If you want to access the final setting values in the spider code as early as possible you can do this in start_requests(). (issue 6038) The TextResponse.json method now requires the response to be in a valid JSON encoding (UTF-8, UTF-16, or UTF-32). If you need to deal with JSON documents in an invalid encoding, use json.loads(response.text) instead. (issue 6016)    Deprecation removals  Removed the binary export mode of PythonItemExporter, deprecated in Scrapy 1.1.0. (issue 6006, issue 6007)  Note If you are using this Scrapy version on Scrapy Cloud with a stack that includes an older Scrapy version and get a “TypeError: Unexpected options: binary” error, you may need to add scrapinghub-entrypoint-scrapy >= 0.14.1 to your project requirements or switch to a stack that includes Scrapy 2.11.   Removed the CrawlerRunner.spiders attribute, deprecated in Scrapy 1.0.0, use CrawlerRunner.spider_loader instead. (issue 6010)    Deprecations  Running crawl() more than once on the same scrapy.crawler.Crawler instance is now deprecated. (issue 1587, issue 6040)    New features  Spiders can now modify settings in their from_crawler() method, e.g. based on spider arguments. (issue 1305, issue 1580, issue 2392, issue 3663, issue 6038) Added the PeriodicLog extension which can be enabled to log stats and/or their differences periodically. (issue 5926) Optimized the memory usage in TextResponse.json by removing unnecessary body decoding. (issue 5968, issue 6016) Links to .webp files are now ignored by link extractors. (issue 6021)    Bug fixes  Fixed logging enabled add-ons. (issue 6036) Fixed MailSender producing invalid message bodies when the charset argument is passed to send(). (issue 5096, issue 5118) Fixed an exception when accessing self.EXCEPTIONS_TO_RETRY from a subclass of RetryMiddleware. (issue 6049, issue 6050) scrapy.settings.BaseSettings.getdictorlist(), used to parse FEED_EXPORT_FIELDS, now handles tuple values. (issue 6011, issue 6013) Calls to datetime.utcnow(), no longer recommended to be used, have been replaced with calls to datetime.now() with a timezone. (issue 6014)    Documentation  Updated a deprecated function call in a pipeline example. (issue 6008, issue 6009)    Quality assurance  Extended typing hints. (issue 6003, issue 6005, issue 6031, issue 6034) Pinned brotli to 1.0.9 for the PyPy tests as 1.1.0 breaks them. (issue 6044, issue 6045) Other CI and pre-commit improvements. (issue 6002, issue 6013, issue 6046)     Scrapy 2.10.1 (2023-08-30) Marked Twisted >= 23.8.0 as unsupported. (issue 6024, issue 6026)   Scrapy 2.10.0 (2023-08-04) Highlights:  Added Python 3.12 support, dropped Python 3.7 support. The new add-ons framework simplifies configuring 3rd-party components that support it. Exceptions to retry can now be configured. Many fixes and improvements for feed exports.   Modified requirements  Dropped support for Python 3.7. (issue 5953) Added support for the upcoming Python 3.12. (issue 5984) Minimum versions increased for these dependencies:  lxml: 4.3.0 → 4.4.1 cryptography: 3.4.6 → 36.0.0   pkg_resources is no longer used. (issue 5956, issue 5958) boto3 is now recommended instead of botocore for exporting to S3. (issue 5833).    Backward-incompatible changes  The value of the FEED_STORE_EMPTY setting is now True instead of False. In earlier Scrapy versions empty files were created even when this setting was False (which was a bug that is now fixed), so the new default should keep the old behavior. (issue 872, issue 5847)    Deprecation removals  When a function is assigned to the FEED_URI_PARAMS setting, returning None or modifying the params input parameter, deprecated in Scrapy 2.6, is no longer supported. (issue 5994, issue 5996) The scrapy.utils.reqser module, deprecated in Scrapy 2.6, is removed. (issue 5994, issue 5996) The scrapy.squeues classes PickleFifoDiskQueueNonRequest, PickleLifoDiskQueueNonRequest, MarshalFifoDiskQueueNonRequest, and MarshalLifoDiskQueueNonRequest, deprecated in Scrapy 2.6, are removed. (issue 5994, issue 5996) The property open_spiders and the methods has_capacity and schedule of scrapy.core.engine.ExecutionEngine, deprecated in Scrapy 2.6, are removed. (issue 5994, issue 5998) Passing a spider argument to the spider_is_idle(), crawl() and download() methods of scrapy.core.engine.ExecutionEngine, deprecated in Scrapy 2.6, is no longer supported. (issue 5994, issue 5998)    Deprecations  scrapy.utils.datatypes.CaselessDict is deprecated, use scrapy.utils.datatypes.CaseInsensitiveDict instead. (issue 5146) Passing the custom argument to scrapy.utils.conf.build_component_list() is deprecated, it was used in the past to merge FOO and FOO_BASE setting values but now Scrapy uses scrapy.settings.BaseSettings.getwithbase() to do the same. Code that uses this argument and cannot be switched to getwithbase() can be switched to merging the values explicitly. (issue 5726, issue 5923)    New features  Added support for Scrapy add-ons. (issue 5950) Added the RETRY_EXCEPTIONS setting that configures which exceptions will be retried by RetryMiddleware. (issue 2701, issue 5929) Added the possiiblity to close the spider if no items were produced in the specified time, configured by CLOSESPIDER_TIMEOUT_NO_ITEM. (issue 5979) Added support for the AWS_REGION_NAME setting to feed exports. (issue 5980) Added support for using pathlib.Path objects that refer to absolute Windows paths in the FEEDS setting. (issue 5939)    Bug fixes  Fixed creating empty feeds even with FEED_STORE_EMPTY=False. (issue 872, issue 5847) Fixed using absolute Windows paths when specifying output files. (issue 5969, issue 5971) Fixed problems with uploading large files to S3 by switching to multipart uploads (requires boto3). (issue 960, issue 5735, issue 5833) Fixed the JSON exporter writing extra commas when some exceptions occur. (issue 3090, issue 5952) Fixed the “read of closed file” error in the CSV exporter. (issue 5043, issue 5705) Fixed an error when a component added by the class object throws NotConfigured with a message. (issue 5950, issue 5992) Added the missing scrapy.settings.BaseSettings.pop() method. (issue 5959, issue 5960, issue 5963) Added CaseInsensitiveDict as a replacement for CaselessDict that fixes some API inconsistencies. (issue 5146)    Documentation  Documented scrapy.Spider.update_settings(). (issue 5745, issue 5846) Documented possible problems with early Twisted reactor installation and their solutions. (issue 5981, issue 6000) Added examples of making additional requests in callbacks. (issue 5927) Improved the feed export docs. (issue 5579, issue 5931) Clarified the docs about request objects on redirection. (issue 5707, issue 5937)    Quality assurance  Added support for running tests against the installed Scrapy version. (issue 4914, issue 5949) Extended typing hints. (issue 5925, issue 5977) Fixed the test_utils_asyncio.AsyncioTest.test_set_asyncio_event_loop test. (issue 5951) Fixed the test_feedexport.BatchDeliveriesTest.test_batch_path_differ test on Windows. (issue 5847) Enabled CI runs for Python 3.11 on Windows. (issue 5999) Simplified skipping tests that depend on uvloop. (issue 5984) Fixed the extra-deps-pinned tox env. (issue 5948) Implemented cleanups. (issue 5965, issue 5986)     Scrapy 2.9.0 (2023-05-08) Highlights:  Per-domain download settings. Compatibility with new cryptography and new parsel. JMESPath selectors from the new parsel. Bug fixes.   Deprecations  scrapy.extensions.feedexport._FeedSlot is renamed to scrapy.extensions.feedexport.FeedSlot and the old name is deprecated. (issue 5876)    New features  Settings correponding to DOWNLOAD_DELAY, CONCURRENT_REQUESTS_PER_DOMAIN and RANDOMIZE_DOWNLOAD_DELAY can now be set on a per-domain basis via the new DOWNLOAD_SLOTS setting. (issue 5328) Added TextResponse.jmespath(), a shortcut for JMESPath selectors available since parsel 1.8.1. (issue 5894, issue 5915) Added feed_slot_closed and feed_exporter_closed signals. (issue 5876) Added scrapy.utils.request.request_to_curl(), a function to produce a curl command from a Request object. (issue 5892) Values of FILES_STORE and IMAGES_STORE can now be pathlib.Path instances. (issue 5801)    Bug fixes  Fixed a warning with Parsel 1.8.1+. (issue 5903, issue 5918) Fixed an error when using feed postprocessing with S3 storage. (issue 5500, issue 5581) Added the missing scrapy.settings.BaseSettings.setdefault() method. (issue 5811, issue 5821) Fixed an error when using cryptography 40.0.0+ and DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING is enabled. (issue 5857, issue 5858) The checksums returned by FilesPipeline for files on Google Cloud Storage are no longer Base64-encoded. (issue 5874, issue 5891) scrapy.utils.request.request_from_curl() now supports $-prefixed string values for the curl --data-raw argument, which are produced by browsers for data that includes certain symbols. (issue 5899, issue 5901) The parse command now also works with async generator callbacks. (issue 5819, issue 5824) The genspider command now properly works with HTTPS URLs. (issue 3553, issue 5808) Improved handling of asyncio loops. (issue 5831, issue 5832) LinkExtractor now skips certain malformed URLs instead of raising an exception. (issue 5881) scrapy.utils.python.get_func_args() now supports more types of callables. (issue 5872, issue 5885) Fixed an error when processing non-UTF8 values of Content-Type headers. (issue 5914, issue 5917) Fixed an error breaking user handling of send failures in scrapy.mail.MailSender.send(). (issue 1611, issue 5880)    Documentation  Expanded contributing docs. (issue 5109, issue 5851) Added blacken-docs to pre-commit and reformatted the docs with it. (issue 5813, issue 5816) Fixed a JS issue. (issue 5875, issue 5877) Fixed make htmlview. (issue 5878, issue 5879) Fixed typos and other small errors. (issue 5827, issue 5839, issue 5883, issue 5890, issue 5895, issue 5904)    Quality assurance  Extended typing hints. (issue 5805, issue 5889, issue 5896) Tests for most of the examples in the docs are now run as a part of CI, found problems were fixed. (issue 5816, issue 5826, issue 5919) Removed usage of deprecated Python classes. (issue 5849) Silenced include-ignored warnings from coverage. (issue 5820) Fixed a random failure of the test_feedexport.test_batch_path_differ test. (issue 5855, issue 5898) Updated docstrings to match output produced by parsel 1.8.1 so that they don’t cause test failures. (issue 5902, issue 5919) Other CI and pre-commit improvements. (issue 5802, issue 5823, issue 5908)     Scrapy 2.8.0 (2023-02-02) This is a maintenance release, with minor features, bug fixes, and cleanups.  Deprecation removals  The scrapy.utils.gz.read1 function, deprecated in Scrapy 2.0, has now been removed. Use the read1() method of GzipFile instead. (issue 5719) The scrapy.utils.python.to_native_str function, deprecated in Scrapy 2.0, has now been removed. Use scrapy.utils.python.to_unicode() instead. (issue 5719) The scrapy.utils.python.MutableChain.next method, deprecated in Scrapy 2.0, has now been removed. Use __next__() instead. (issue 5719) The scrapy.linkextractors.FilteringLinkExtractor class, deprecated in Scrapy 2.0, has now been removed. Use LinkExtractor instead. (issue 5720) Support for using environment variables prefixed with SCRAPY_ to override settings, deprecated in Scrapy 2.0, has now been removed. (issue 5724) Support for the noconnect query string argument in proxy URLs, deprecated in Scrapy 2.0, has now been removed. We expect proxies that used to need it to work fine without it. (issue 5731) The scrapy.utils.python.retry_on_eintr function, deprecated in Scrapy 2.3, has now been removed. (issue 5719) The scrapy.utils.python.WeakKeyCache class, deprecated in Scrapy 2.4, has now been removed. (issue 5719) The scrapy.utils.boto.is_botocore() function, deprecated in Scrapy 2.4, has now been removed. (issue 5719)    Deprecations  scrapy.pipelines.images.NoimagesDrop is now deprecated. (issue 5368, issue 5489) ImagesPipeline.convert_image must now accept a response_body parameter. (issue 3055, issue 3689, issue 4753)    New features  Applied black coding style to files generated with the genspider and startproject commands. (issue 5809, issue 5814)  FEED_EXPORT_ENCODING is now set to \"utf-8\" in the settings.py file that the startproject command generates. With this value, JSON exports won’t force the use of escape sequences for non-ASCII characters. (issue 5797, issue 5800) The MemoryUsage extension now logs the peak memory usage during checks, and the binary unit MiB is now used to avoid confusion. (issue 5717, issue 5722, issue 5727) The callback parameter of Request can now be set to scrapy.http.request.NO_CALLBACK(), to distinguish it from None, as the latter indicates that the default spider callback (parse()) is to be used. (issue 5798)    Bug fixes  Enabled unsafe legacy SSL renegotiation to fix access to some outdated websites. (issue 5491, issue 5790) Fixed STARTTLS-based email delivery not working with Twisted 21.2.0 and better. (issue 5386, issue 5406) Fixed the finish_exporting() method of item exporters not being called for empty files. (issue 5537, issue 5758) Fixed HTTP/2 responses getting only the last value for a header when multiple headers with the same name are received. (issue 5777) Fixed an exception raised by the shell command on some cases when using asyncio. (issue 5740, issue 5742, issue 5748, issue 5759, issue 5760, issue 5771) When using CrawlSpider, callback keyword arguments (cb_kwargs) added to a request in the process_request callback of a Rule will no longer be ignored. (issue 5699) The images pipeline no longer re-encodes JPEG files. (issue 3055, issue 3689, issue 4753) Fixed the handling of transparent WebP images by the images pipeline. (issue 3072, issue 5766, issue 5767) scrapy.shell.inspect_response() no longer inhibits SIGINT (Ctrl+C). (issue 2918) LinkExtractor with unique=False no longer filters out links that have identical URL and text. (issue 3798, issue 3799, issue 4695, issue 5458) RobotsTxtMiddleware now ignores URL protocols that do not support robots.txt (data://, file://). (issue 5807) Silenced the filelock debug log messages introduced in Scrapy 2.6. (issue 5753, issue 5754) Fixed the output of scrapy -h showing an unintended **commands** line. (issue 5709, issue 5711, issue 5712) Made the active project indication in the output of commands more clear. (issue 5715)    Documentation  Documented how to debug spiders from Visual Studio Code. (issue 5721) Documented how DOWNLOAD_DELAY affects per-domain concurrency. (issue 5083, issue 5540) Improved consistency. (issue 5761) Fixed typos. (issue 5714, issue 5744, issue 5764)    Quality assurance  Applied black coding style, sorted import statements, and introduced pre-commit. (issue 4654, issue 4658, issue 5734, issue 5737, issue 5806, issue 5810) Switched from os.path to pathlib. (issue 4916, issue 4497, issue 5682) Addressed many issues reported by Pylint. (issue 5677) Improved code readability. (issue 5736) Improved package metadata. (issue 5768) Removed direct invocations of setup.py. (issue 5774, issue 5776) Removed unnecessary OrderedDict usages. (issue 5795) Removed unnecessary __str__ definitions. (issue 5150) Removed obsolete code and comments. (issue 5725, issue 5729, issue 5730, issue 5732) Fixed test and CI issues. (issue 5749, issue 5750, issue 5756, issue 5762, issue 5765, issue 5780, issue 5781, issue 5782, issue 5783, issue 5785, issue 5786)     Scrapy 2.7.1 (2022-11-02)  New features  Relaxed the restriction introduced in 2.6.2 so that the Proxy-Authorization header can again be set explicitly, as long as the proxy URL in the proxy metadata has no other credentials, and for as long as that proxy URL remains the same; this restores compatibility with scrapy-zyte-smartproxy 2.1.0 and older (issue 5626).    Bug fixes  Using -O/--overwrite-output and -t/--output-format options together now produces an error instead of ignoring the former option (issue 5516, issue 5605). Replaced deprecated asyncio APIs that implicitly use the current event loop with code that explicitly requests a loop from the event loop policy (issue 5685, issue 5689). Fixed uses of deprecated Scrapy APIs in Scrapy itself (issue 5588, issue 5589). Fixed uses of a deprecated Pillow API (issue 5684, issue 5692). Improved code that checks if generators return values, so that it no longer fails on decorated methods and partial methods (issue 5323, issue 5592, issue 5599, issue 5691).    Documentation  Upgraded the Code of Conduct to Contributor Covenant v2.1 (issue 5698). Fixed typos (issue 5681, issue 5694).    Quality assurance  Re-enabled some erroneously disabled flake8 checks (issue 5688). Ignored harmless deprecation warnings from typing in tests (issue 5686, issue 5697). Modernized our CI configuration (issue 5695, issue 5696).     Scrapy 2.7.0 (2022-10-17) Highlights:  Added Python 3.11 support, dropped Python 3.6 support Improved support for asynchronous callbacks Asyncio support is enabled by default on new projects Output names of item fields can now be arbitrary strings Centralized request fingerprinting configuration is now possible   Modified requirements Python 3.7 or greater is now required; support for Python 3.6 has been dropped. Support for the upcoming Python 3.11 has been added. The minimum required version of some dependencies has changed as well:  lxml: 3.5.0 → 4.3.0 Pillow (images pipeline): 4.0.0 → 7.1.0 zope.interface: 5.0.0 → 5.1.0  (issue 5512, issue 5514, issue 5524, issue 5563, issue 5664, issue 5670, issue 5678)   Deprecations  ImagesPipeline.thumb_path must now accept an item parameter (issue 5504, issue 5508). The scrapy.downloadermiddlewares.decompression module is now deprecated (issue 5546, issue 5547).    New features  The process_spider_output() method of spider middlewares can now be defined as an asynchronous generator (issue 4978). The output of Request callbacks defined as coroutines is now processed asynchronously (issue 4978). CrawlSpider now supports asynchronous callbacks (issue 5657). New projects created with the startproject command have asyncio support enabled by default (issue 5590, issue 5679). The FEED_EXPORT_FIELDS setting can now be defined as a dictionary to customize the output name of item fields, lifting the restriction that required output names to be valid Python identifiers, e.g. preventing them to have whitespace (issue 1008, issue 3266, issue 3696). You can now customize request fingerprinting through the new REQUEST_FINGERPRINTER_CLASS setting, instead of having to change it on every Scrapy component that relies on request fingerprinting (issue 900, issue 3420, issue 4113, issue 4762, issue 4524). jsonl is now supported and encouraged as a file extension for JSON Lines files (issue 4848).  ImagesPipeline.thumb_path now receives the source item (issue 5504, issue 5508).    Bug fixes  When using Google Cloud Storage with a media pipeline, FILES_EXPIRES now also works when FILES_STORE does not point at the root of your Google Cloud Storage bucket (issue 5317, issue 5318). The parse command now supports asynchronous callbacks (issue 5424, issue 5577). When using the parse command with a URL for which there is no available spider, an exception is no longer raised (issue 3264, issue 3265, issue 5375, issue 5376, issue 5497). TextResponse now gives higher priority to the byte order mark when determining the text encoding of the response body, following the HTML living standard (issue 5601, issue 5611).  MIME sniffing takes the response body into account in FTP and HTTP/1.0 requests, as well as in cached requests (issue 4873). MIME sniffing now detects valid HTML 5 documents even if the html tag is missing (issue 4873). An exception is now raised if ASYNCIO_EVENT_LOOP has a value that does not match the asyncio event loop actually installed (issue 5529). Fixed Headers.getlist returning only the last header (issue 5515, issue 5526). Fixed LinkExtractor not ignoring the tar.gz file extension by default (issue 1837, issue 2067, issue 4066)    Documentation  Clarified the return type of Spider.parse (issue 5602, issue 5608). To enable HttpCompressionMiddleware to do brotli compression, installing brotli is now recommended instead of installing brotlipy, as the former provides a more recent version of brotli.  Signal documentation now mentions coroutine support and uses it in code examples (issue 4852, issue 5358). Avoiding getting banned now recommends Common Crawl instead of Google cache (issue 3582, issue 5432).  The new Components topic covers enforcing requirements on Scrapy components, like downloader middlewares, extensions, item pipelines, spider middlewares, and more; Enforcing asyncio as a requirement has also been added (issue 4978). Settings now indicates that setting values must be picklable (issue 5607, issue 5629). Removed outdated documentation (issue 5446, issue 5373, issue 5369, issue 5370, issue 5554). Fixed typos (issue 5442, issue 5455, issue 5457, issue 5461, issue 5538, issue 5553, issue 5558, issue 5624, issue 5631). Fixed other issues (issue 5283, issue 5284, issue 5559, issue 5567, issue 5648, issue 5659, issue 5665).    Quality assurance  Added a continuous integration job to run twine check (issue 5655, issue 5656).  Addressed test issues and warnings (issue 5560, issue 5561, issue 5612, issue 5617, issue 5639, issue 5645, issue 5662, issue 5671, issue 5675). Cleaned up code (issue 4991, issue 4995, issue 5451, issue 5487, issue 5542, issue 5667, issue 5668, issue 5672). Applied minor code improvements (issue 5661).     Scrapy 2.6.3 (2022-09-27)  Added support for pyOpenSSL 22.1.0, removing support for SSLv3 (issue 5634, issue 5635, issue 5636). Upgraded the minimum versions of the following dependencies:  cryptography: 2.0 → 3.3 pyOpenSSL: 16.2.0 → 21.0.0 service_identity: 16.0.0 → 18.1.0 Twisted: 17.9.0 → 18.9.0 zope.interface: 4.1.3 → 5.0.0  (issue 5621, issue 5632)  Fixes test and documentation issues (issue 5612, issue 5617, issue 5631).    Scrapy 2.6.2 (2022-07-25) Security bug fix:  When HttpProxyMiddleware processes a request with proxy metadata, and that proxy metadata includes proxy credentials, HttpProxyMiddleware sets the Proxy-Authorization header, but only if that header is not already set. There are third-party proxy-rotation downloader middlewares that set different proxy metadata every time they process a request. Because of request retries and redirects, the same request can be processed by downloader middlewares more than once, including both HttpProxyMiddleware and any third-party proxy-rotation downloader middleware. These third-party proxy-rotation downloader middlewares could change the proxy metadata of a request to a new value, but fail to remove the Proxy-Authorization header from the previous value of the proxy metadata, causing the credentials of one proxy to be sent to a different proxy. To prevent the unintended leaking of proxy credentials, the behavior of HttpProxyMiddleware is now as follows when processing a request:  If the request being processed defines proxy metadata that includes credentials, the Proxy-Authorization header is always updated to feature those credentials. If the request being processed defines proxy metadata without credentials, the Proxy-Authorization header is removed unless it was originally defined for the same proxy URL. To remove proxy credentials while keeping the same proxy URL, remove the Proxy-Authorization header.  If the request has no proxy metadata, or that metadata is a falsy value (e.g. None), the Proxy-Authorization header is removed. It is no longer possible to set a proxy URL through the proxy metadata but set the credentials through the Proxy-Authorization header. Set proxy credentials through the proxy metadata instead.     Also fixes the following regressions introduced in 2.6.0:  CrawlerProcess supports again crawling multiple spiders (issue 5435, issue 5436) Installing a Twisted reactor before Scrapy does (e.g. importing twisted.internet.reactor somewhere at the module level) no longer prevents Scrapy from starting, as long as a different reactor is not specified in TWISTED_REACTOR (issue 5525, issue 5528) Fixed an exception that was being logged after the spider finished under certain conditions (issue 5437, issue 5440) The --output/-o command-line parameter supports again a value starting with a hyphen (issue 5444, issue 5445) The scrapy parse -h command no longer throws an error (issue 5481, issue 5482)    Scrapy 2.6.1 (2022-03-01) Fixes a regression introduced in 2.6.0 that would unset the request method when following redirects.   Scrapy 2.6.0 (2022-03-01) Highlights:  Security fixes for cookie handling Python 3.10 support asyncio support is no longer considered experimental, and works out-of-the-box on Windows regardless of your Python version Feed exports now support pathlib.Path output paths and per-feed item filtering and post-processing   Security bug fixes  When a Request object with cookies defined gets a redirect response causing a new Request object to be scheduled, the cookies defined in the original Request object are no longer copied into the new Request object. If you manually set the Cookie header on a Request object and the domain name of the redirect URL is not an exact match for the domain of the URL of the original Request object, your Cookie header is now dropped from the new Request object. The old behavior could be exploited by an attacker to gain access to your cookies. Please, see the cjvr-mfj7-j4j8 security advisory for more information.  Note It is still possible to enable the sharing of cookies between different domains with a shared domain suffix (e.g. example.com and any subdomain) by defining the shared domain suffix (e.g. example.com) as the cookie domain when defining your cookies. See the documentation of the Request class for more information.   When the domain of a cookie, either received in the Set-Cookie header of a response or defined in a Request object, is set to a public suffix, the cookie is now ignored unless the cookie domain is the same as the request domain. The old behavior could be exploited by an attacker to inject cookies from a controlled domain into your cookiejar that could be sent to other domains not controlled by the attacker. Please, see the mfjm-vh54-3f96 security advisory for more information.     Modified requirements  The h2 dependency is now optional, only needed to enable HTTP/2 support. (issue 5113)     Backward-incompatible changes  The formdata parameter of FormRequest, if specified for a non-POST request, now overrides the URL query string, instead of being appended to it. (issue 2919, issue 3579) When a function is assigned to the FEED_URI_PARAMS setting, now the return value of that function, and not the params input parameter, will determine the feed URI parameters, unless that return value is None. (issue 4962, issue 4966) In scrapy.core.engine.ExecutionEngine, methods crawl(), download(), schedule(), and spider_is_idle() now raise RuntimeError if called before open_spider(). (issue 5090) These methods used to assume that ExecutionEngine.slot had been defined by a prior call to open_spider(), so they were raising AttributeError instead.  If the API of the configured scheduler does not meet expectations, TypeError is now raised at startup time. Before, other exceptions would be raised at run time. (issue 3559) The _encoding field of serialized Request objects is now named encoding, in line with all other fields (issue 5130)    Deprecation removals  scrapy.http.TextResponse.body_as_unicode, deprecated in Scrapy 2.2, has now been removed. (issue 5393) scrapy.item.BaseItem, deprecated in Scrapy 2.2, has now been removed. (issue 5398) scrapy.item.DictItem, deprecated in Scrapy 1.8, has now been removed. (issue 5398) scrapy.Spider.make_requests_from_url, deprecated in Scrapy 1.4, has now been removed. (issue 4178, issue 4356)    Deprecations  When a function is assigned to the FEED_URI_PARAMS setting, returning None or modifying the params input parameter is now deprecated. Return a new dictionary instead. (issue 4962, issue 4966) scrapy.utils.reqser is deprecated. (issue 5130)  Instead of request_to_dict(), use the new Request.to_dict method. Instead of request_from_dict(), use the new scrapy.utils.request.request_from_dict() function.   In scrapy.squeues, the following queue classes are deprecated: PickleFifoDiskQueueNonRequest, PickleLifoDiskQueueNonRequest, MarshalFifoDiskQueueNonRequest, and MarshalLifoDiskQueueNonRequest. You should instead use: PickleFifoDiskQueue, PickleLifoDiskQueue, MarshalFifoDiskQueue, and MarshalLifoDiskQueue. (issue 5117) Many aspects of scrapy.core.engine.ExecutionEngine that come from a time when this class could handle multiple Spider objects at a time have been deprecated. (issue 5090)  The has_capacity() method is deprecated. The schedule() method is deprecated, use crawl() or download() instead. The open_spiders attribute is deprecated, use spider instead. The spider parameter is deprecated for the following methods:  spider_is_idle() crawl() download()  Instead, call open_spider() first to set the Spider object.       New features  You can now use item filtering to control which items are exported to each output feed. (issue 4575, issue 5178, issue 5161, issue 5203) You can now apply post-processing to feeds, and built-in post-processing plugins are provided for output file compression. (issue 2174, issue 5168, issue 5190) The FEEDS setting now supports pathlib.Path objects as keys. (issue 5383, issue 5384) Enabling asyncio while using Windows and Python 3.8 or later will automatically switch the asyncio event loop to one that allows Scrapy to work. See Windows-specific notes. (issue 4976, issue 5315) The genspider command now supports a start URL instead of a domain name. (issue 4439) scrapy.utils.defer gained 2 new functions, deferred_to_future() and maybe_deferred_to_future(), to help await on Deferreds when using the asyncio reactor. (issue 5288) Amazon S3 feed export storage gained support for temporary security credentials (AWS_SESSION_TOKEN) and endpoint customization (AWS_ENDPOINT_URL). (issue 4998, issue 5210)  New LOG_FILE_APPEND setting to allow truncating the log file. (issue 5279) Request.cookies values that are bool, float or int are cast to str. (issue 5252, issue 5253) You may now raise CloseSpider from a handler of the spider_idle signal to customize the reason why the spider is stopping. (issue 5191) When using HttpProxyMiddleware, the proxy URL for non-HTTPS HTTP/1.1 requests no longer needs to include a URL scheme. (issue 4505, issue 4649) All built-in queues now expose a peek method that returns the next queue object (like pop) but does not remove the returned object from the queue. (issue 5112) If the underlying queue does not support peeking (e.g. because you are not using queuelib 1.6.1 or later), the peek method raises NotImplementedError.  Request and Response now have an attributes attribute that makes subclassing easier. For Request, it also allows subclasses to work with scrapy.utils.request.request_from_dict(). (issue 1877, issue 5130, issue 5218) The open() and close() methods of the scheduler are now optional. (issue 3559) HTTP/1.1 TunnelError exceptions now only truncate response bodies longer than 1000 characters, instead of those longer than 32 characters, making it easier to debug such errors. (issue 4881, issue 5007) ItemLoader now supports non-text responses. (issue 5145, issue 5269)    Bug fixes  The TWISTED_REACTOR and ASYNCIO_EVENT_LOOP settings are no longer ignored if defined in custom_settings. (issue 4485, issue 5352) Removed a module-level Twisted reactor import that could prevent using the asyncio reactor. (issue 5357) The startproject command works with existing folders again. (issue 4665, issue 4676) The FEED_URI_PARAMS setting now behaves as documented. (issue 4962, issue 4966) Request.cb_kwargs once again allows the callback keyword. (issue 5237, issue 5251, issue 5264) Made scrapy.utils.response.open_in_browser() support more complex HTML. (issue 5319, issue 5320) Fixed CSVFeedSpider.quotechar being interpreted as the CSV file encoding. (issue 5391, issue 5394) Added missing setuptools to the list of dependencies. (issue 5122)  LinkExtractor now also works as expected with links that have comma-separated rel attribute values including nofollow. (issue 5225) Fixed a TypeError that could be raised during feed export parameter parsing. (issue 5359)    Documentation  asyncio support is no longer considered experimental. (issue 5332) Included Windows-specific help for asyncio usage. (issue 4976, issue 5315) Rewrote Using a headless browser with up-to-date best practices. (issue 4484, issue 4613) Documented local file naming in media pipelines. (issue 5069, issue 5152) Frequently Asked Questions now covers spider file name collision issues. (issue 2680, issue 3669) Provided better context and instructions to disable the URLLENGTH_LIMIT setting. (issue 5135, issue 5250) Documented that Reppy parser does not support Python 3.9+. (issue 5226, issue 5231) Documented the scheduler component. (issue 3537, issue 3559) Documented the method used by media pipelines to determine if a file has expired. (issue 5120, issue 5254) Running multiple spiders in the same process now features scrapy.utils.project.get_project_settings() usage. (issue 5070) Running multiple spiders in the same process now covers what happens when you define different per-spider values for some settings that cannot differ at run time. (issue 4485, issue 5352) Extended the documentation of the StatsMailer extension. (issue 5199, issue 5217) Added JOBDIR to Settings. (issue 5173, issue 5224) Documented Spider.attribute. (issue 5174, issue 5244) Documented TextResponse.urljoin. (issue 1582) Added the body_length parameter to the documented signature of the headers_received signal. (issue 5270) Clarified SelectorList.get usage in the tutorial. (issue 5256) The documentation now features the shortest import path of classes with multiple import paths. (issue 2733, issue 5099) quotes.toscrape.com references now use HTTPS instead of HTTP. (issue 5395, issue 5396) Added a link to our Discord server to Getting help. (issue 5421, issue 5422) The pronunciation of the project name is now officially /ˈskreɪpaɪ/. (issue 5280, issue 5281) Added the Scrapy logo to the README. (issue 5255, issue 5258) Fixed issues and implemented minor improvements. (issue 3155, issue 4335, issue 5074, issue 5098, issue 5134, issue 5180, issue 5194, issue 5239, issue 5266, issue 5271, issue 5273, issue 5274, issue 5276, issue 5347, issue 5356, issue 5414, issue 5415, issue 5416, issue 5419, issue 5420)    Quality Assurance  Added support for Python 3.10. (issue 5212, issue 5221, issue 5265) Significantly reduced memory usage by scrapy.utils.response.response_httprepr(), used by the DownloaderStats downloader middleware, which is enabled by default. (issue 4964, issue 4972) Removed uses of the deprecated optparse module. (issue 5366, issue 5374) Extended typing hints. (issue 5077, issue 5090, issue 5100, issue 5108, issue 5171, issue 5215, issue 5334) Improved tests, fixed CI issues, removed unused code. (issue 5094, issue 5157, issue 5162, issue 5198, issue 5207, issue 5208, issue 5229, issue 5298, issue 5299, issue 5310, issue 5316, issue 5333, issue 5388, issue 5389, issue 5400, issue 5401, issue 5404, issue 5405, issue 5407, issue 5410, issue 5412, issue 5425, issue 5427) Implemented improvements for contributors. (issue 5080, issue 5082, issue 5177, issue 5200) Implemented cleanups. (issue 5095, issue 5106, issue 5209, issue 5228, issue 5235, issue 5245, issue 5246, issue 5292, issue 5314, issue 5322)     Scrapy 2.5.1 (2021-10-05)  Security bug fix: If you use HttpAuthMiddleware (i.e. the http_user and http_pass spider attributes) for HTTP authentication, any request exposes your credentials to the request target. To prevent unintended exposure of authentication credentials to unintended domains, you must now additionally set a new, additional spider attribute, http_auth_domain, and point it to the specific domain to which the authentication credentials must be sent. If the http_auth_domain spider attribute is not set, the domain of the first request will be considered the HTTP authentication target, and authentication credentials will only be sent in requests targeting that domain. If you need to send the same HTTP authentication credentials to multiple domains, you can use w3lib.http.basic_auth_header() instead to set the value of the Authorization header of your requests. If you really want your spider to send the same HTTP authentication credentials to any domain, set the http_auth_domain spider attribute to None. Finally, if you are a user of scrapy-splash, know that this version of Scrapy breaks compatibility with scrapy-splash 0.7.2 and earlier. You will need to upgrade scrapy-splash to a greater version for it to continue to work.     Scrapy 2.5.0 (2021-04-06) Highlights:  Official Python 3.9 support Experimental HTTP/2 support New get_retry_request() function to retry requests from spider callbacks New headers_received signal that allows stopping downloads early New Response.protocol attribute   Deprecation removals  Removed all code that was deprecated in 1.7.0 and had not already been removed in 2.4.0. (issue 4901) Removed support for the SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE environment variable, deprecated in 1.8.0. (issue 4912)    Deprecations  The scrapy.utils.py36 module is now deprecated in favor of scrapy.utils.asyncgen. (issue 4900)    New features  Experimental HTTP/2 support through a new download handler that can be assigned to the https protocol in the DOWNLOAD_HANDLERS setting. (issue 1854, issue 4769, issue 5058, issue 5059, issue 5066) The new scrapy.downloadermiddlewares.retry.get_retry_request() function may be used from spider callbacks or middlewares to handle the retrying of a request beyond the scenarios that RetryMiddleware supports. (issue 3590, issue 3685, issue 4902) The new headers_received signal gives early access to response headers and allows stopping downloads. (issue 1772, issue 4897) The new Response.protocol attribute gives access to the string that identifies the protocol used to download a response. (issue 4878) Stats now include the following entries that indicate the number of successes and failures in storing feeds: feedexport/success_count/<storage type> feedexport/failed_count/<storage type>   Where <storage type> is the feed storage backend class name, such as FileFeedStorage or FTPFeedStorage. (issue 3947, issue 4850)  The UrlLengthMiddleware spider middleware now logs ignored URLs with INFO logging level instead of DEBUG, and it now includes the following entry into stats to keep track of the number of ignored URLs: urllength/request_ignored_count   (issue 5036)  The HttpCompressionMiddleware downloader middleware now logs the number of decompressed responses and the total count of resulting bytes: httpcompression/response_bytes httpcompression/response_count   (issue 4797, issue 4799)     Bug fixes  Fixed installation on PyPy installing PyDispatcher in addition to PyPyDispatcher, which could prevent Scrapy from working depending on which package got imported. (issue 4710, issue 4814) When inspecting a callback to check if it is a generator that also returns a value, an exception is no longer raised if the callback has a docstring with lower indentation than the following code. (issue 4477, issue 4935) The Content-Length header is no longer omitted from responses when using the default, HTTP/1.1 download handler (see DOWNLOAD_HANDLERS). (issue 5009, issue 5034, issue 5045, issue 5057, issue 5062) Setting the handle_httpstatus_all request meta key to False now has the same effect as not setting it at all, instead of having the same effect as setting it to True. (issue 3851, issue 4694)    Documentation  Added instructions to install Scrapy in Windows using pip. (issue 4715, issue 4736) Logging documentation now includes additional ways to filter logs. (issue 4216, issue 4257, issue 4965) Covered how to deal with long lists of allowed domains in the FAQ. (issue 2263, issue 3667) Covered scrapy-bench in Benchmarking. (issue 4996, issue 5016) Clarified that one extension instance is created per crawler. (issue 5014) Fixed some errors in examples. (issue 4829, issue 4830, issue 4907, issue 4909, issue 5008) Fixed some external links, typos, and so on. (issue 4892, issue 4899, issue 4936, issue 4942, issue 5005, issue 5063) The list of Request.meta keys is now sorted alphabetically. (issue 5061, issue 5065) Updated references to Scrapinghub, which is now called Zyte. (issue 4973, issue 5072) Added a mention to contributors in the README. (issue 4956) Reduced the top margin of lists. (issue 4974)    Quality Assurance  Made Python 3.9 support official (issue 4757, issue 4759) Extended typing hints (issue 4895) Fixed deprecated uses of the Twisted API. (issue 4940, issue 4950, issue 5073) Made our tests run with the new pip resolver. (issue 4710, issue 4814) Added tests to ensure that coroutine support is tested. (issue 4987) Migrated from Travis CI to GitHub Actions. (issue 4924) Fixed CI issues. (issue 4986, issue 5020, issue 5022, issue 5027, issue 5052, issue 5053) Implemented code refactorings, style fixes and cleanups. (issue 4911, issue 4982, issue 5001, issue 5002, issue 5076)     Scrapy 2.4.1 (2020-11-17)  Fixed feed exports overwrite support (issue 4845, issue 4857, issue 4859) Fixed the AsyncIO event loop handling, which could make code hang (issue 4855, issue 4872) Fixed the IPv6-capable DNS resolver CachingHostnameResolver for download handlers that call reactor.resolve (issue 4802, issue 4803) Fixed the output of the genspider command showing placeholders instead of the import path of the generated spider module (issue 4874) Migrated Windows CI from Azure Pipelines to GitHub Actions (issue 4869, issue 4876)    Scrapy 2.4.0 (2020-10-11) Highlights:  Python 3.5 support has been dropped. The file_path method of media pipelines can now access the source item. This allows you to set a download file path based on item data.  The new item_export_kwargs key of the FEEDS setting allows to define keyword parameters to pass to item exporter classes You can now choose whether feed exports overwrite or append to the output file. For example, when using the crawl or runspider commands, you can use the -O option instead of -o to overwrite the output file.  Zstd-compressed responses are now supported if zstandard is installed. In settings, where the import path of a class is required, it is now possible to pass a class object instead.   Modified requirements  Python 3.6 or greater is now required; support for Python 3.5 has been dropped As a result:  When using PyPy, PyPy 7.2.0 or greater is now required For Amazon S3 storage support in feed exports or media pipelines, botocore 1.4.87 or greater is now required To use the images pipeline, Pillow 4.0.0 or greater is now required  (issue 4718, issue 4732, issue 4733, issue 4742, issue 4743, issue 4764)     Backward-incompatible changes  CookiesMiddleware once again discards cookies defined in Request.headers. We decided to revert this bug fix, introduced in Scrapy 2.2.0, because it was reported that the current implementation could break existing code. If you need to set cookies for a request, use the Request.cookies parameter. A future version of Scrapy will include a new, better implementation of the reverted bug fix. (issue 4717, issue 4823)     Deprecation removals  scrapy.extensions.feedexport.S3FeedStorage no longer reads the values of access_key and secret_key from the running project settings when they are not passed to its __init__ method; you must either pass those parameters to its __init__ method or use S3FeedStorage.from_crawler (issue 4356, issue 4411, issue 4688) Rule.process_request no longer admits callables which expect a single request parameter, rather than both request and response (issue 4818)    Deprecations  In custom media pipelines, signatures that do not accept a keyword-only item parameter in any of the  methods that now support this parameter are now deprecated (issue 4628, issue 4686) In custom feed storage backend classes, __init__ method signatures that do not accept a keyword-only feed_options parameter are now deprecated (issue 547, issue 716, issue 4512) The scrapy.utils.python.WeakKeyCache class is now deprecated (issue 4684, issue 4701) The scrapy.utils.boto.is_botocore() function is now deprecated, use scrapy.utils.boto.is_botocore_available() instead (issue 4734, issue 4776)    New features  The following methods of media pipelines now accept an item keyword-only parameter containing the source item:  In scrapy.pipelines.files.FilesPipeline:  file_downloaded() file_path() media_downloaded() media_to_download()   In scrapy.pipelines.images.ImagesPipeline:  file_downloaded() file_path() get_images() image_downloaded() media_downloaded() media_to_download()    (issue 4628, issue 4686)  The new item_export_kwargs key of the FEEDS setting allows to define keyword parameters to pass to item exporter classes (issue 4606, issue 4768) Feed exports gained overwrite support:  When using the crawl or runspider commands, you can use the -O option instead of -o to overwrite the output file You can use the overwrite key in the FEEDS setting to configure whether to overwrite the output file (True) or append to its content (False) The __init__ and from_crawler methods of feed storage backend classes now receive a new keyword-only parameter, feed_options, which is a dictionary of feed options  (issue 547, issue 716, issue 4512)  Zstd-compressed responses are now supported if zstandard is installed (issue 4831) In settings, where the import path of a class is required, it is now possible to pass a class object instead (issue 3870, issue 3873). This includes also settings where only part of its value is made of an import path, such as DOWNLOADER_MIDDLEWARES or DOWNLOAD_HANDLERS.  Downloader middlewares can now override response.request. If a downloader middleware returns a Response object from process_response() or process_exception() with a custom Request object assigned to response.request:  The response is handled by the callback of that custom Request object, instead of being handled by the callback of the original Request object That custom Request object is now sent as the request argument to the response_received signal, instead of the original Request object  (issue 4529, issue 4632)  When using the FTP feed storage backend:  It is now possible to set the new overwrite feed option to False to append to an existing file instead of overwriting it The FTP password can now be omitted if it is not necessary  (issue 547, issue 716, issue 4512)  The __init__ method of CsvItemExporter now supports an errors parameter to indicate how to handle encoding errors (issue 4755) When using asyncio, it is now possible to set a custom asyncio loop (issue 4306, issue 4414) Serialized requests (see Jobs: pausing and resuming crawls) now support callbacks that are spider methods that delegate on other callable (issue 4756) When a response is larger than DOWNLOAD_MAXSIZE, the logged message is now a warning, instead of an error (issue 3874, issue 3886, issue 4752)    Bug fixes  The genspider command no longer overwrites existing files unless the --force option is used (issue 4561, issue 4616, issue 4623) Cookies with an empty value are no longer considered invalid cookies (issue 4772) The runspider command now supports files with the .pyw file extension (issue 4643, issue 4646) The HttpProxyMiddleware middleware now simply ignores unsupported proxy values (issue 3331, issue 4778) Checks for generator callbacks with a return statement no longer warn about return statements in nested functions (issue 4720, issue 4721) The system file mode creation mask no longer affects the permissions of files generated using the startproject command (issue 4722) scrapy.utils.iterators.xmliter() now supports namespaced node names (issue 861, issue 4746) Request objects can now have about: URLs, which can work when using a headless browser (issue 4835)    Documentation  The FEED_URI_PARAMS setting is now documented (issue 4671, issue 4724) Improved the documentation of link extractors with an usage example from a spider callback and reference documentation for the Link class (issue 4751, issue 4775) Clarified the impact of CONCURRENT_REQUESTS when using the CloseSpider extension (issue 4836) Removed references to Python 2’s unicode type (issue 4547, issue 4703) We now have an official deprecation policy (issue 4705) Our documentation policies now cover usage of Sphinx’s versionadded and versionchanged directives, and we have removed usages referencing Scrapy 1.4.0 and earlier versions (issue 3971, issue 4310) Other documentation cleanups (issue 4090, issue 4782, issue 4800, issue 4801, issue 4809, issue 4816, issue 4825)    Quality assurance  Extended typing hints (issue 4243, issue 4691) Added tests for the check command (issue 4663) Fixed test failures on Debian (issue 4726, issue 4727, issue 4735) Improved Windows test coverage (issue 4723) Switched to formatted string literals where possible (issue 4307, issue 4324, issue 4672) Modernized super() usage (issue 4707) Other code and test cleanups (issue 1790, issue 3288, issue 4165, issue 4564, issue 4651, issue 4714, issue 4738, issue 4745, issue 4747, issue 4761, issue 4765, issue 4804, issue 4817, issue 4820, issue 4822, issue 4839)     Scrapy 2.3.0 (2020-08-04) Highlights:  Feed exports now support Google Cloud Storage as a storage backend The new FEED_EXPORT_BATCH_ITEM_COUNT setting allows to deliver output items in batches of up to the specified number of items. It also serves as a workaround for delayed file delivery, which causes Scrapy to only start item delivery after the crawl has finished when using certain storage backends (S3, FTP, and now GCS).  The base implementation of item loaders has been moved into a separate library, itemloaders, allowing usage from outside Scrapy and a separate release schedule   Deprecation removals  Removed the following classes and their parent modules from scrapy.linkextractors:  htmlparser.HtmlParserLinkExtractor regex.RegexLinkExtractor sgml.BaseSgmlLinkExtractor sgml.SgmlLinkExtractor  Use LinkExtractor instead (issue 4356, issue 4679)     Deprecations  The scrapy.utils.python.retry_on_eintr function is now deprecated (issue 4683)    New features  Feed exports support Google Cloud Storage (issue 685, issue 3608) New FEED_EXPORT_BATCH_ITEM_COUNT setting for batch deliveries (issue 4250, issue 4434) The parse command now allows specifying an output file (issue 4317, issue 4377) Request.from_curl and curl_to_request_kwargs() now also support --data-raw (issue 4612) A parse callback may now be used in built-in spider subclasses, such as CrawlSpider (issue 712, issue 732, issue 781, issue 4254 )    Bug fixes  Fixed the CSV exporting of dataclass items and attr.s items (issue 4667, issue 4668) Request.from_curl and curl_to_request_kwargs() now set the request method to POST when a request body is specified and no request method is specified (issue 4612) The processing of ANSI escape sequences in enabled in Windows 10.0.14393 and later, where it is required for colored output (issue 4393, issue 4403)    Documentation  Updated the OpenSSL cipher list format link in the documentation about the DOWNLOADER_CLIENT_TLS_CIPHERS setting (issue 4653) Simplified the code example in Working with dataclass items (issue 4652)    Quality assurance  The base implementation of item loaders has been moved into itemloaders (issue 4005, issue 4516) Fixed a silenced error in some scheduler tests (issue 4644, issue 4645) Renewed the localhost certificate used for SSL tests (issue 4650) Removed cookie-handling code specific to Python 2 (issue 4682) Stopped using Python 2 unicode literal syntax (issue 4704) Stopped using a backlash for line continuation (issue 4673) Removed unneeded entries from the MyPy exception list (issue 4690) Automated tests now pass on Windows as part of our continuous integration system (issue 4458) Automated tests now pass on the latest PyPy version for supported Python versions in our continuous integration system (issue 4504)     Scrapy 2.2.1 (2020-07-17)  The startproject command no longer makes unintended changes to the permissions of files in the destination folder, such as removing execution permissions (issue 4662, issue 4666)    Scrapy 2.2.0 (2020-06-24) Highlights:  Python 3.5.2+ is required now dataclass objects and attrs objects are now valid item types New TextResponse.json method New bytes_received signal that allows canceling response download CookiesMiddleware fixes   Backward-incompatible changes  Support for Python 3.5.0 and 3.5.1 has been dropped; Scrapy now refuses to run with a Python version lower than 3.5.2, which introduced typing.Type (issue 4615)    Deprecations  TextResponse.body_as_unicode is now deprecated, use TextResponse.text instead (issue 4546, issue 4555, issue 4579) scrapy.item.BaseItem is now deprecated, use scrapy.item.Item instead (issue 4534)    New features  dataclass objects and attrs objects are now valid item types, and a new itemadapter library makes it easy to write code that supports any item type (issue 2749, issue 2807, issue 3761, issue 3881, issue 4642) A new TextResponse.json method allows to deserialize JSON responses (issue 2444, issue 4460, issue 4574) A new bytes_received signal allows monitoring response download progress and stopping downloads (issue 4205, issue 4559) The dictionaries in the result list of a media pipeline now include a new key, status, which indicates if the file was downloaded or, if the file was not downloaded, why it was not downloaded; see FilesPipeline.get_media_requests for more information (issue 2893, issue 4486) When using Google Cloud Storage for a media pipeline, a warning is now logged if the configured credentials do not grant the required permissions (issue 4346, issue 4508) Link extractors are now serializable, as long as you do not use lambdas for parameters; for example, you can now pass link extractors in Request.cb_kwargs or Request.meta when persisting scheduled requests (issue 4554) Upgraded the pickle protocol that Scrapy uses from protocol 2 to protocol 4, improving serialization capabilities and performance (issue 4135, issue 4541) scrapy.utils.misc.create_instance() now raises a TypeError exception if the resulting instance is None (issue 4528, issue 4532)    Bug fixes  CookiesMiddleware no longer discards cookies defined in Request.headers (issue 1992, issue 2400) CookiesMiddleware no longer re-encodes cookies defined as bytes in the cookies parameter of the __init__ method of Request (issue 2400, issue 3575) When FEEDS defines multiple URIs, FEED_STORE_EMPTY is False and the crawl yields no items, Scrapy no longer stops feed exports after the first URI (issue 4621, issue 4626) Spider callbacks defined using coroutine syntax no longer need to return an iterable, and may instead return a Request object, an item, or None (issue 4609) The startproject command now ensures that the generated project folders and files have the right permissions (issue 4604) Fix a KeyError exception being sometimes raised from scrapy.utils.datatypes.LocalWeakReferencedCache (issue 4597, issue 4599) When FEEDS defines multiple URIs, log messages about items being stored now contain information from the corresponding feed, instead of always containing information about only one of the feeds (issue 4619, issue 4629)    Documentation  Added a new section about accessing cb_kwargs from errbacks (issue 4598, issue 4634) Covered chompjs in Parsing JavaScript code (issue 4556, issue 4562) Removed from Coroutines the warning about the API being experimental (issue 4511, issue 4513) Removed references to unsupported versions of Twisted (issue 4533) Updated the description of the screenshot pipeline example, which now uses coroutine syntax instead of returning a Deferred (issue 4514, issue 4593) Removed a misleading import line from the scrapy.utils.log.configure_logging() code example (issue 4510, issue 4587) The display-on-hover behavior of internal documentation references now also covers links to commands, Request.meta keys, settings and signals (issue 4495, issue 4563) It is again possible to download the documentation for offline reading (issue 4578, issue 4585) Removed backslashes preceding *args and **kwargs in some function and method signatures (issue 4592, issue 4596)    Quality assurance  Adjusted the code base further to our style guidelines (issue 4237, issue 4525, issue 4538, issue 4539, issue 4540, issue 4542, issue 4543, issue 4544, issue 4545, issue 4557, issue 4558, issue 4566, issue 4568, issue 4572) Removed remnants of Python 2 support (issue 4550, issue 4553, issue 4568) Improved code sharing between the crawl and runspider commands (issue 4548, issue 4552) Replaced chain(*iterable) with chain.from_iterable(iterable) (issue 4635) You may now run the asyncio tests with Tox on any Python version (issue 4521) Updated test requirements to reflect an incompatibility with pytest 5.4 and 5.4.1 (issue 4588) Improved SpiderLoader test coverage for scenarios involving duplicate spider names (issue 4549, issue 4560) Configured Travis CI to also run the tests with Python 3.5.2 (issue 4518, issue 4615) Added a Pylint job to Travis CI (issue 3727) Added a Mypy job to Travis CI (issue 4637) Made use of set literals in tests (issue 4573) Cleaned up the Travis CI configuration (issue 4517, issue 4519, issue 4522, issue 4537)     Scrapy 2.1.0 (2020-04-24) Highlights:  New FEEDS setting to export to multiple feeds New Response.ip_address attribute   Backward-incompatible changes  AssertionError exceptions triggered by assert statements have been replaced by new exception types, to support running Python in optimized mode (see -O) without changing Scrapy’s behavior in any unexpected ways. If you catch an AssertionError exception from Scrapy, update your code to catch the corresponding new exception. (issue 4440)     Deprecation removals  The LOG_UNSERIALIZABLE_REQUESTS setting is no longer supported, use SCHEDULER_DEBUG instead (issue 4385) The REDIRECT_MAX_METAREFRESH_DELAY setting is no longer supported, use METAREFRESH_MAXDELAY instead (issue 4385) The ChunkedTransferMiddleware middleware has been removed, including the entire scrapy.downloadermiddlewares.chunked module; chunked transfers work out of the box (issue 4431) The spiders property has been removed from Crawler, use CrawlerRunner.spider_loader or instantiate SPIDER_LOADER_CLASS with your settings instead (issue 4398) The MultiValueDict, MultiValueDictKeyError, and SiteNode classes have been removed from scrapy.utils.datatypes (issue 4400)    Deprecations  The FEED_FORMAT and FEED_URI settings have been deprecated in favor of the new FEEDS setting (issue 1336, issue 3858, issue 4507)    New features  A new setting, FEEDS, allows configuring multiple output feeds with different settings each (issue 1336, issue 3858, issue 4507) The crawl and runspider commands now support multiple -o parameters (issue 1336, issue 3858, issue 4507) The crawl and runspider commands now support specifying an output format by appending :<format> to the output file (issue 1336, issue 3858, issue 4507) The new Response.ip_address attribute gives access to the IP address that originated a response (issue 3903, issue 3940) A warning is now issued when a value in allowed_domains includes a port (issue 50, issue 3198, issue 4413) Zsh completion now excludes used option aliases from the completion list (issue 4438)    Bug fixes  Request serialization no longer breaks for callbacks that are spider attributes which are assigned a function with a different name (issue 4500) None values in allowed_domains no longer cause a TypeError exception (issue 4410) Zsh completion no longer allows options after arguments (issue 4438) zope.interface 5.0.0 and later versions are now supported (issue 4447, issue 4448) Spider.make_requests_from_url, deprecated in Scrapy 1.4.0, now issues a warning when used (issue 4412)    Documentation  Improved the documentation about signals that allow their handlers to return a Deferred (issue 4295, issue 4390) Our PyPI entry now includes links for our documentation, our source code repository and our issue tracker (issue 4456) Covered the curl2scrapy service in the documentation (issue 4206, issue 4455) Removed references to the Guppy library, which only works in Python 2 (issue 4285, issue 4343) Extended use of InterSphinx to link to Python 3 documentation (issue 4444, issue 4445) Added support for Sphinx 3.0 and later (issue 4475, issue 4480, issue 4496, issue 4503)    Quality assurance  Removed warnings about using old, removed settings (issue 4404) Removed a warning about importing StringTransport from twisted.test.proto_helpers in Twisted 19.7.0 or newer (issue 4409) Removed outdated Debian package build files (issue 4384) Removed object usage as a base class (issue 4430) Removed code that added support for old versions of Twisted that we no longer support (issue 4472) Fixed code style issues (issue 4468, issue 4469, issue 4471, issue 4481) Removed twisted.internet.defer.returnValue() calls (issue 4443, issue 4446, issue 4489)     Scrapy 2.0.1 (2020-03-18)  Response.follow_all now supports an empty URL iterable as input (issue 4408, issue 4420) Removed top-level reactor imports to prevent errors about the wrong Twisted reactor being installed when setting a different Twisted reactor using TWISTED_REACTOR (issue 4401, issue 4406) Fixed tests (issue 4422)    Scrapy 2.0.0 (2020-03-03) Highlights:  Python 2 support has been removed Partial coroutine syntax support and experimental asyncio support New Response.follow_all method FTP support for media pipelines New Response.certificate attribute IPv6 support through DNS_RESOLVER   Backward-incompatible changes  Python 2 support has been removed, following Python 2 end-of-life on January 1, 2020 (issue 4091, issue 4114, issue 4115, issue 4121, issue 4138, issue 4231, issue 4242, issue 4304, issue 4309, issue 4373) Retry gaveups (see RETRY_TIMES) are now logged as errors instead of as debug information (issue 3171, issue 3566) File extensions that LinkExtractor ignores by default now also include 7z, 7zip, apk, bz2, cdr, dmg, ico, iso, tar, tar.gz, webm, and xz (issue 1837, issue 2067, issue 4066) The METAREFRESH_IGNORE_TAGS setting is now an empty list by default, following web browser behavior (issue 3844, issue 4311) The HttpCompressionMiddleware now includes spaces after commas in the value of the Accept-Encoding header that it sets, following web browser behavior (issue 4293) The __init__ method of custom download handlers (see DOWNLOAD_HANDLERS) or subclasses of the following downloader handlers  no longer receives a settings parameter:  scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler scrapy.core.downloader.handlers.file.FileDownloadHandler  Use the from_settings or from_crawler class methods to expose such a parameter to your custom download handlers. (issue 4126)  We have refactored the scrapy.core.scheduler.Scheduler class and related queue classes (see SCHEDULER_PRIORITY_QUEUE, SCHEDULER_DISK_QUEUE and SCHEDULER_MEMORY_QUEUE) to make it easier to implement custom scheduler queue classes. See Changes to scheduler queue classes below for details. Overridden settings are now logged in a different format. This is more in line with similar information logged at startup (issue 4199)    Deprecation removals  The Scrapy shell no longer provides a sel proxy object, use response.selector instead (issue 4347) LevelDB support has been removed (issue 4112) The following functions have been removed from scrapy.utils.python: isbinarytext, is_writable, setattr_default, stringify_dict (issue 4362)    Deprecations  Using environment variables prefixed with SCRAPY_ to override settings is deprecated (issue 4300, issue 4374, issue 4375) scrapy.linkextractors.FilteringLinkExtractor is deprecated, use scrapy.linkextractors.LinkExtractor instead (issue 4045) The noconnect query string argument of proxy URLs is deprecated and should be removed from proxy URLs (issue 4198) The next method of scrapy.utils.python.MutableChain is deprecated, use the global next() function or MutableChain.__next__ instead (issue 4153)    New features  Added partial support for Python’s coroutine syntax and experimental support for asyncio and asyncio-powered libraries (issue 4010, issue 4259, issue 4269, issue 4270, issue 4271, issue 4316, issue 4318) The new Response.follow_all method offers the same functionality as Response.follow but supports an iterable of URLs as input and returns an iterable of requests (issue 2582, issue 4057, issue 4286) Media pipelines now support FTP storage (issue 3928, issue 3961) The new Response.certificate attribute exposes the SSL certificate of the server as a twisted.internet.ssl.Certificate object for HTTPS responses (issue 2726, issue 4054) A new DNS_RESOLVER setting allows enabling IPv6 support (issue 1031, issue 4227) A new SCRAPER_SLOT_MAX_ACTIVE_SIZE setting allows configuring the existing soft limit that pauses request downloads when the total response data being processed is too high (issue 1410, issue 3551) A new TWISTED_REACTOR setting allows customizing the reactor that Scrapy uses, allowing to enable asyncio support or deal with a common macOS issue (issue 2905, issue 4294) Scheduler disk and memory queues may now use the class methods from_crawler or from_settings (issue 3884) The new Response.cb_kwargs attribute serves as a shortcut for Response.request.cb_kwargs (issue 4331) Response.follow now supports a flags parameter, for consistency with Request (issue 4277, issue 4279) Item loader processors can now be regular functions, they no longer need to be methods (issue 3899) Rule now accepts an errback parameter (issue 4000) Request no longer requires a callback parameter when an errback parameter is specified (issue 3586, issue 4008) LogFormatter now supports some additional methods:  download_error for download errors item_error for exceptions raised during item processing by item pipelines spider_error for exceptions raised from spider callbacks  (issue 374, issue 3986, issue 3989, issue 4176, issue 4188)  The FEED_URI setting now supports pathlib.Path values (issue 3731, issue 4074) A new request_left_downloader signal is sent when a request leaves the downloader (issue 4303) Scrapy logs a warning when it detects a request callback or errback that uses yield but also returns a value, since the returned value would be lost (issue 3484, issue 3869) Spider objects now raise an AttributeError exception if they do not have a start_urls attribute nor reimplement start_requests, but have a start_url attribute (issue 4133, issue 4170) BaseItemExporter subclasses may now use super().__init__(**kwargs) instead of self._configure(kwargs) in their __init__ method, passing dont_fail=True to the parent __init__ method if needed, and accessing kwargs at self._kwargs after calling their parent __init__ method (issue 4193, issue 4370) A new keep_fragments parameter of scrapy.utils.request.request_fingerprint allows to generate different fingerprints for requests with different fragments in their URL (issue 4104) Download handlers (see DOWNLOAD_HANDLERS) may now use the from_settings and from_crawler class methods that other Scrapy components already supported (issue 4126) scrapy.utils.python.MutableChain.__iter__ now returns self, allowing it to be used as a sequence (issue 4153)    Bug fixes  The crawl command now also exits with exit code 1 when an exception happens before the crawling starts (issue 4175, issue 4207) LinkExtractor.extract_links no longer re-encodes the query string or URLs from non-UTF-8 responses in UTF-8 (issue 998, issue 1403, issue 1949, issue 4321) The first spider middleware (see SPIDER_MIDDLEWARES) now also processes exceptions raised from callbacks that are generators (issue 4260, issue 4272) Redirects to URLs starting with 3 slashes (///) are now supported (issue 4032, issue 4042) Request no longer accepts strings as url simply because they have a colon (issue 2552, issue 4094) The correct encoding is now used for attach names in MailSender (issue 4229, issue 4239) RFPDupeFilter, the default DUPEFILTER_CLASS, no longer writes an extra \\r character on each line in Windows, which made the size of the requests.seen file unnecessarily large on that platform (issue 4283) Z shell auto-completion now looks for .html files, not .http files, and covers the -h command-line switch (issue 4122, issue 4291) Adding items to a scrapy.utils.datatypes.LocalCache object without a limit defined no longer raises a TypeError exception (issue 4123) Fixed a typo in the message of the ValueError exception raised when scrapy.utils.misc.create_instance() gets both settings and crawler set to None (issue 4128)    Documentation  API documentation now links to an online, syntax-highlighted view of the corresponding source code (issue 4148) Links to unexisting documentation pages now allow access to the sidebar (issue 4152, issue 4169) Cross-references within our documentation now display a tooltip when hovered (issue 4173, issue 4183) Improved the documentation about LinkExtractor.extract_links and simplified Link Extractors (issue 4045) Clarified how ItemLoader.item works (issue 3574, issue 4099) Clarified that logging.basicConfig() should not be used when also using CrawlerProcess (issue 2149, issue 2352, issue 3146, issue 3960) Clarified the requirements for Request objects when using persistence (issue 4124, issue 4139) Clarified how to install a custom image pipeline (issue 4034, issue 4252) Fixed the signatures of the file_path method in media pipeline examples (issue 4290) Covered a backward-incompatible change in Scrapy 1.7.0 affecting custom scrapy.core.scheduler.Scheduler subclasses (issue 4274) Improved the README.rst and CODE_OF_CONDUCT.md files (issue 4059) Documentation examples are now checked as part of our test suite and we have fixed some of the issues detected (issue 4142, issue 4146, issue 4171, issue 4184, issue 4190) Fixed logic issues, broken links and typos (issue 4247, issue 4258, issue 4282, issue 4288, issue 4305, issue 4308, issue 4323, issue 4338, issue 4359, issue 4361) Improved consistency when referring to the __init__ method of an object (issue 4086, issue 4088) Fixed an inconsistency between code and output in Scrapy at a glance (issue 4213) Extended intersphinx usage (issue 4147, issue 4172, issue 4185, issue 4194, issue 4197) We now use a recent version of Python to build the documentation (issue 4140, issue 4249) Cleaned up documentation (issue 4143, issue 4275)    Quality assurance  Re-enabled proxy CONNECT tests (issue 2545, issue 4114) Added Bandit security checks to our test suite (issue 4162, issue 4181) Added Flake8 style checks to our test suite and applied many of the corresponding changes (issue 3944, issue 3945, issue 4137, issue 4157, issue 4167, issue 4174, issue 4186, issue 4195, issue 4238, issue 4246, issue 4355, issue 4360, issue 4365) Improved test coverage (issue 4097, issue 4218, issue 4236) Started reporting slowest tests, and improved the performance of some of them (issue 4163, issue 4164) Fixed broken tests and refactored some tests (issue 4014, issue 4095, issue 4244, issue 4268, issue 4372) Modified the tox configuration to allow running tests with any Python version, run Bandit and Flake8 tests by default, and enforce a minimum tox version programmatically (issue 4179) Cleaned up code (issue 3937, issue 4208, issue 4209, issue 4210, issue 4212, issue 4369, issue 4376, issue 4378)    Changes to scheduler queue classes The following changes may impact any custom queue classes of all types:  The push method no longer receives a second positional parameter containing request.priority * -1. If you need that value, get it from the first positional parameter, request, instead, or use the new priority() method in scrapy.core.scheduler.ScrapyPriorityQueue subclasses.  The following changes may impact custom priority queue classes:  In the __init__ method or the from_crawler or from_settings class methods:  The parameter that used to contain a factory function, qfactory, is now passed as a keyword parameter named downstream_queue_cls. A new keyword parameter has been added: key. It is a string that is always an empty string for memory queues and indicates the JOB_DIR value for disk queues. The parameter for disk queues that contains data from the previous crawl, startprios or slot_startprios, is now passed as a keyword parameter named startprios. The serialize parameter is no longer passed. The disk queue class must take care of request serialization on its own before writing to disk, using the request_to_dict() and request_from_dict() functions from the scrapy.utils.reqser module.    The following changes may impact custom disk and memory queue classes:  The signature of the __init__ method is now __init__(self, crawler, key).  The following changes affect specifically the ScrapyPriorityQueue and DownloaderAwarePriorityQueue classes from scrapy.core.scheduler and may affect subclasses:  In the __init__ method, most of the changes described above apply. __init__ may still receive all parameters as positional parameters, however:  downstream_queue_cls, which replaced qfactory, must be instantiated differently. qfactory was instantiated with a priority value (integer). Instances of downstream_queue_cls should be created using the new ScrapyPriorityQueue.qfactory or DownloaderAwarePriorityQueue.pqfactory methods.  The new key parameter displaced the startprios parameter 1 position to the right.   The following class attributes have been added:  crawler downstream_queue_cls (details above) key (details above)   The serialize attribute has been removed (details above)  The following changes affect specifically the ScrapyPriorityQueue class and may affect subclasses:  A new priority() method has been added which, given a request, returns request.priority * -1. It is used in push() to make up for the removal of its priority parameter.  The spider attribute has been removed. Use crawler.spider instead.  The following changes affect specifically the DownloaderAwarePriorityQueue class and may affect subclasses:  A new pqueues attribute offers a mapping of downloader slot names to the corresponding instances of downstream_queue_cls.  (issue 3884)    Scrapy 1.8.3 (2022-07-25) Security bug fix:  When HttpProxyMiddleware processes a request with proxy metadata, and that proxy metadata includes proxy credentials, HttpProxyMiddleware sets the Proxy-Authorization header, but only if that header is not already set. There are third-party proxy-rotation downloader middlewares that set different proxy metadata every time they process a request. Because of request retries and redirects, the same request can be processed by downloader middlewares more than once, including both HttpProxyMiddleware and any third-party proxy-rotation downloader middleware. These third-party proxy-rotation downloader middlewares could change the proxy metadata of a request to a new value, but fail to remove the Proxy-Authorization header from the previous value of the proxy metadata, causing the credentials of one proxy to be sent to a different proxy. To prevent the unintended leaking of proxy credentials, the behavior of HttpProxyMiddleware is now as follows when processing a request:  If the request being processed defines proxy metadata that includes credentials, the Proxy-Authorization header is always updated to feature those credentials. If the request being processed defines proxy metadata without credentials, the Proxy-Authorization header is removed unless it was originally defined for the same proxy URL. To remove proxy credentials while keeping the same proxy URL, remove the Proxy-Authorization header.  If the request has no proxy metadata, or that metadata is a falsy value (e.g. None), the Proxy-Authorization header is removed. It is no longer possible to set a proxy URL through the proxy metadata but set the credentials through the Proxy-Authorization header. Set proxy credentials through the proxy metadata instead.       Scrapy 1.8.2 (2022-03-01) Security bug fixes:  When a Request object with cookies defined gets a redirect response causing a new Request object to be scheduled, the cookies defined in the original Request object are no longer copied into the new Request object. If you manually set the Cookie header on a Request object and the domain name of the redirect URL is not an exact match for the domain of the URL of the original Request object, your Cookie header is now dropped from the new Request object. The old behavior could be exploited by an attacker to gain access to your cookies. Please, see the cjvr-mfj7-j4j8 security advisory for more information.  Note It is still possible to enable the sharing of cookies between different domains with a shared domain suffix (e.g. example.com and any subdomain) by defining the shared domain suffix (e.g. example.com) as the cookie domain when defining your cookies. See the documentation of the Request class for more information.   When the domain of a cookie, either received in the Set-Cookie header of a response or defined in a Request object, is set to a public suffix, the cookie is now ignored unless the cookie domain is the same as the request domain. The old behavior could be exploited by an attacker to inject cookies into your requests to some other domains. Please, see the mfjm-vh54-3f96 security advisory for more information.     Scrapy 1.8.1 (2021-10-05)  Security bug fix: If you use HttpAuthMiddleware (i.e. the http_user and http_pass spider attributes) for HTTP authentication, any request exposes your credentials to the request target. To prevent unintended exposure of authentication credentials to unintended domains, you must now additionally set a new, additional spider attribute, http_auth_domain, and point it to the specific domain to which the authentication credentials must be sent. If the http_auth_domain spider attribute is not set, the domain of the first request will be considered the HTTP authentication target, and authentication credentials will only be sent in requests targeting that domain. If you need to send the same HTTP authentication credentials to multiple domains, you can use w3lib.http.basic_auth_header() instead to set the value of the Authorization header of your requests. If you really want your spider to send the same HTTP authentication credentials to any domain, set the http_auth_domain spider attribute to None. Finally, if you are a user of scrapy-splash, know that this version of Scrapy breaks compatibility with scrapy-splash 0.7.2 and earlier. You will need to upgrade scrapy-splash to a greater version for it to continue to work.     Scrapy 1.8.0 (2019-10-28) Highlights:  Dropped Python 3.4 support and updated minimum requirements; made Python 3.8 support official New Request.from_curl class method New ROBOTSTXT_PARSER and ROBOTSTXT_USER_AGENT settings New DOWNLOADER_CLIENT_TLS_CIPHERS and DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING settings   Backward-incompatible changes  Python 3.4 is no longer supported, and some of the minimum requirements of Scrapy have also changed:  cssselect 0.9.1 cryptography 2.0 lxml 3.5.0 pyOpenSSL 16.2.0 queuelib 1.4.2 service_identity 16.0.0 six 1.10.0 Twisted 17.9.0 (16.0.0 with Python 2) zope.interface 4.1.3  (issue 3892)  JSONRequest is now called JsonRequest for consistency with similar classes (issue 3929, issue 3982) If you are using a custom context factory (DOWNLOADER_CLIENTCONTEXTFACTORY), its __init__ method must accept two new parameters: tls_verbose_logging and tls_ciphers (issue 2111, issue 3392, issue 3442, issue 3450) ItemLoader now turns the values of its input item into lists: >>> item = MyItem() >>> item[\"field\"] = \"value1\" >>> loader = ItemLoader(item=item) >>> item[\"field\"] ['value1']   This is needed to allow adding values to existing fields (loader.add_value('field', 'value2')). (issue 3804, issue 3819, issue 3897, issue 3976, issue 3998, issue 4036)   See also Deprecation removals below.   New features  A new Request.from_curl class method allows creating a request from a cURL command (issue 2985, issue 3862) A new ROBOTSTXT_PARSER setting allows choosing which robots.txt parser to use. It includes built-in support for RobotFileParser, Protego (default), Reppy, and Robotexclusionrulesparser, and allows you to implement support for additional parsers (issue 754, issue 2669, issue 3796, issue 3935, issue 3969, issue 4006) A new ROBOTSTXT_USER_AGENT setting allows defining a separate user agent string to use for robots.txt parsing (issue 3931, issue 3966) Rule no longer requires a LinkExtractor parameter (issue 781, issue 4016) Use the new DOWNLOADER_CLIENT_TLS_CIPHERS setting to customize the TLS/SSL ciphers used by the default HTTP/1.1 downloader (issue 3392, issue 3442) Set the new DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING setting to True to enable debug-level messages about TLS connection parameters after establishing HTTPS connections (issue 2111, issue 3450) Callbacks that receive keyword arguments (see Request.cb_kwargs) can now be tested using the new @cb_kwargs spider contract (issue 3985, issue 3988) When a @scrapes spider contract fails, all missing fields are now reported (issue 766, issue 3939) Custom log formats can now drop messages by having the corresponding methods of the configured LOG_FORMATTER return None (issue 3984, issue 3987) A much improved completion definition is now available for Zsh (issue 4069)    Bug fixes  ItemLoader.load_item() no longer makes later calls to ItemLoader.get_output_value() or ItemLoader.load_item() return empty data (issue 3804, issue 3819, issue 3897, issue 3976, issue 3998, issue 4036) Fixed DummyStatsCollector raising a TypeError exception (issue 4007, issue 4052) FilesPipeline.file_path and ImagesPipeline.file_path no longer choose file extensions that are not registered with IANA (issue 1287, issue 3953, issue 3954) When using botocore to persist files in S3, all botocore-supported headers are properly mapped now (issue 3904, issue 3905) FTP passwords in FEED_URI containing percent-escaped characters are now properly decoded (issue 3941) A memory-handling and error-handling issue in scrapy.utils.ssl.get_temp_key_info() has been fixed (issue 3920)    Documentation  The documentation now covers how to define and configure a custom log format (issue 3616, issue 3660) API documentation added for MarshalItemExporter and PythonItemExporter (issue 3973) API documentation added for BaseItem and ItemMeta (issue 3999) Minor documentation fixes (issue 2998, issue 3398, issue 3597, issue 3894, issue 3934, issue 3978, issue 3993, issue 4022, issue 4028, issue 4033, issue 4046, issue 4050, issue 4055, issue 4056, issue 4061, issue 4072, issue 4071, issue 4079, issue 4081, issue 4089, issue 4093)    Deprecation removals  scrapy.xlib has been removed (issue 4015)    Deprecations  The LevelDB storage backend (scrapy.extensions.httpcache.LeveldbCacheStorage) of HttpCacheMiddleware is deprecated (issue 4085, issue 4092) Use of the undocumented SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE environment variable is deprecated (issue 3910) scrapy.item.DictItem is deprecated, use Item instead (issue 3999)    Other changes  Minimum versions of optional Scrapy requirements that are covered by continuous integration tests have been updated:  botocore 1.3.23 Pillow 3.4.2  Lower versions of these optional requirements may work, but it is not guaranteed (issue 3892)  GitHub templates for bug reports and feature requests (issue 3126, issue 3471, issue 3749, issue 3754) Continuous integration fixes (issue 3923) Code cleanup (issue 3391, issue 3907, issue 3946, issue 3950, issue 4023, issue 4031)     Scrapy 1.7.4 (2019-10-21) Revert the fix for issue 3804 (issue 3819), which has a few undesired side effects (issue 3897, issue 3976). As a result, when an item loader is initialized with an item, ItemLoader.load_item() once again makes later calls to ItemLoader.get_output_value() or ItemLoader.load_item() return empty data.   Scrapy 1.7.3 (2019-08-01) Enforce lxml 4.3.5 or lower for Python 3.4 (issue 3912, issue 3918).   Scrapy 1.7.2 (2019-07-23) Fix Python 2 support (issue 3889, issue 3893, issue 3896).   Scrapy 1.7.1 (2019-07-18) Re-packaging of Scrapy 1.7.0, which was missing some changes in PyPI.   Scrapy 1.7.0 (2019-07-18)  Note Make sure you install Scrapy 1.7.1. The Scrapy 1.7.0 package in PyPI is the result of an erroneous commit tagging and does not include all the changes described below.  Highlights:  Improvements for crawls targeting multiple domains A cleaner way to pass arguments to callbacks A new class for JSON requests Improvements for rule-based spiders New features for feed exports   Backward-incompatible changes  429 is now part of the RETRY_HTTP_CODES setting by default This change is backward incompatible. If you don’t want to retry 429, you must override RETRY_HTTP_CODES accordingly.  Crawler, CrawlerRunner.crawl and CrawlerRunner.create_crawler no longer accept a Spider subclass instance, they only accept a Spider subclass now. Spider subclass instances were never meant to work, and they were not working as one would expect: instead of using the passed Spider subclass instance, their from_crawler method was called to generate a new instance.  Non-default values for the SCHEDULER_PRIORITY_QUEUE setting may stop working. Scheduler priority queue classes now need to handle Request objects instead of arbitrary Python data structures. An additional crawler parameter has been added to the __init__ method of the Scheduler class. Custom scheduler subclasses which don’t accept arbitrary parameters in their __init__ method might break because of this change. For more information, see SCHEDULER.   See also Deprecation removals below.   New features  A new scheduler priority queue, scrapy.pqueues.DownloaderAwarePriorityQueue, may be enabled for a significant scheduling improvement on crawls targeting multiple web domains, at the cost of no CONCURRENT_REQUESTS_PER_IP support (issue 3520) A new Request.cb_kwargs attribute provides a cleaner way to pass keyword arguments to callback methods (issue 1138, issue 3563) A new JSONRequest class offers a more convenient way to build JSON requests (issue 3504, issue 3505) A process_request callback passed to the Rule __init__ method now receives the Response object that originated the request as its second argument (issue 3682) A new restrict_text parameter for the LinkExtractor __init__ method allows filtering links by linking text (issue 3622, issue 3635) A new FEED_STORAGE_S3_ACL setting allows defining a custom ACL for feeds exported to Amazon S3 (issue 3607) A new FEED_STORAGE_FTP_ACTIVE setting allows using FTP’s active connection mode for feeds exported to FTP servers (issue 3829) A new METAREFRESH_IGNORE_TAGS setting allows overriding which HTML tags are ignored when searching a response for HTML meta tags that trigger a redirect (issue 1422, issue 3768) A new redirect_reasons request meta key exposes the reason (status code, meta refresh) behind every followed redirect (issue 3581, issue 3687) The SCRAPY_CHECK variable is now set to the true string during runs of the check command, which allows detecting contract check runs from code (issue 3704, issue 3739) A new Item.deepcopy() method makes it easier to deep-copy items (issue 1493, issue 3671) CoreStats also logs elapsed_time_seconds now (issue 3638) Exceptions from ItemLoader input and output processors are now more verbose (issue 3836, issue 3840) Crawler, CrawlerRunner.crawl and CrawlerRunner.create_crawler now fail gracefully if they receive a Spider subclass instance instead of the subclass itself (issue 2283, issue 3610, issue 3872)    Bug fixes  process_spider_exception() is now also invoked for generators (issue 220, issue 2061) System exceptions like KeyboardInterrupt are no longer caught (issue 3726) ItemLoader.load_item() no longer makes later calls to ItemLoader.get_output_value() or ItemLoader.load_item() return empty data (issue 3804, issue 3819) The images pipeline (ImagesPipeline) no longer ignores these Amazon S3 settings: AWS_ENDPOINT_URL, AWS_REGION_NAME, AWS_USE_SSL, AWS_VERIFY (issue 3625) Fixed a memory leak in scrapy.pipelines.media.MediaPipeline affecting, for example, non-200 responses and exceptions from custom middlewares (issue 3813) Requests with private callbacks are now correctly unserialized from disk (issue 3790) FormRequest.from_response() now handles invalid methods like major web browsers (issue 3777, issue 3794)    Documentation  A new topic, Selecting dynamically-loaded content, covers recommended approaches to read dynamically-loaded data (issue 3703) Broad Crawls now features information about memory usage (issue 1264, issue 3866) The documentation of Rule now covers how to access the text of a link when using CrawlSpider (issue 3711, issue 3712) A new section, Writing your own storage backend, covers writing a custom cache storage backend for HttpCacheMiddleware (issue 3683, issue 3692) A new FAQ entry, How to split an item into multiple items in an item pipeline?, explains what to do when you want to split an item into multiple items from an item pipeline (issue 2240, issue 3672) Updated the FAQ entry about crawl order to explain why the first few requests rarely follow the desired order (issue 1739, issue 3621) The LOGSTATS_INTERVAL setting (issue 3730), the FilesPipeline.file_path and ImagesPipeline.file_path methods (issue 2253, issue 3609) and the Crawler.stop() method (issue 3842) are now documented Some parts of the documentation that were confusing or misleading are now clearer (issue 1347, issue 1789, issue 2289, issue 3069, issue 3615, issue 3626, issue 3668, issue 3670, issue 3673, issue 3728, issue 3762, issue 3861, issue 3882) Minor documentation fixes (issue 3648, issue 3649, issue 3662, issue 3674, issue 3676, issue 3694, issue 3724, issue 3764, issue 3767, issue 3791, issue 3797, issue 3806, issue 3812)    Deprecation removals The following deprecated APIs have been removed (issue 3578):  scrapy.conf (use Crawler.settings) From scrapy.core.downloader.handlers:  http.HttpDownloadHandler (use http10.HTTP10DownloadHandler)   scrapy.loader.ItemLoader._get_values (use _get_xpathvalues) scrapy.loader.XPathItemLoader (use ItemLoader) scrapy.log (see Logging) From scrapy.pipelines:  files.FilesPipeline.file_key (use file_path) images.ImagesPipeline.file_key (use file_path) images.ImagesPipeline.image_key (use file_path) images.ImagesPipeline.thumb_key (use thumb_path)   From both scrapy.selector and scrapy.selector.lxmlsel:  HtmlXPathSelector (use Selector) XmlXPathSelector (use Selector) XPathSelector (use Selector) XPathSelectorList (use Selector)   From scrapy.selector.csstranslator:  ScrapyGenericTranslator (use parsel.csstranslator.GenericTranslator) ScrapyHTMLTranslator (use parsel.csstranslator.HTMLTranslator) ScrapyXPathExpr (use parsel.csstranslator.XPathExpr)   From Selector:  _root (both the __init__ method argument and the object property, use root) extract_unquoted (use getall) select (use xpath)   From SelectorList:  extract_unquoted (use getall) select (use xpath) x (use xpath)   scrapy.spiders.BaseSpider (use Spider) From Spider (and subclasses):  DOWNLOAD_DELAY (use download_delay) set_crawler (use from_crawler())   scrapy.spiders.spiders (use SpiderLoader) scrapy.telnet (use scrapy.extensions.telnet) From scrapy.utils.python:  str_to_unicode (use to_unicode) unicode_to_str (use to_bytes)   scrapy.utils.response.body_or_str  The following deprecated settings have also been removed (issue 3578):  SPIDER_MANAGER_CLASS (use SPIDER_LOADER_CLASS)    Deprecations  The queuelib.PriorityQueue value for the SCHEDULER_PRIORITY_QUEUE setting is deprecated. Use scrapy.pqueues.ScrapyPriorityQueue instead. process_request callbacks passed to Rule that do not accept two arguments are deprecated. The following modules are deprecated:  scrapy.utils.http (use w3lib.http) scrapy.utils.markup (use w3lib.html) scrapy.utils.multipart (use urllib3)   The scrapy.utils.datatypes.MergeDict class is deprecated for Python 3 code bases. Use ChainMap instead. (issue 3878) The scrapy.utils.gz.is_gzipped function is deprecated. Use scrapy.utils.gz.gzip_magic_number instead.    Other changes  It is now possible to run all tests from the same tox environment in parallel; the documentation now covers this and other ways to run tests (issue 3707) It is now possible to generate an API documentation coverage report (issue 3806, issue 3810, issue 3860) The documentation policies now require docstrings (issue 3701) that follow PEP 257 (issue 3748) Internal fixes and cleanup (issue 3629, issue 3643, issue 3684, issue 3698, issue 3734, issue 3735, issue 3736, issue 3737, issue 3809, issue 3821, issue 3825, issue 3827, issue 3833, issue 3857, issue 3877)     Scrapy 1.6.0 (2019-01-30) Highlights:  better Windows support; Python 3.7 compatibility; big documentation improvements, including a switch from .extract_first() + .extract() API to .get() + .getall() API; feed exports, FilePipeline and MediaPipeline improvements; better extensibility: item_error and request_reached_downloader signals; from_crawler support for feed exporters, feed storages and dupefilters. scrapy.contracts fixes and new features; telnet console security improvements, first released as a backport in Scrapy 1.5.2 (2019-01-22); clean-up of the deprecated code; various bug fixes, small new features and usability improvements across the codebase.   Selector API changes While these are not changes in Scrapy itself, but rather in the parsel library which Scrapy uses for xpath/css selectors, these changes are worth mentioning here. Scrapy now depends on parsel >= 1.5, and Scrapy documentation is updated to follow recent parsel API conventions. Most visible change is that .get() and .getall() selector methods are now preferred over .extract_first() and .extract(). We feel that these new methods result in a more concise and readable code. See extract() and extract_first() for more details.  Note There are currently no plans to deprecate .extract() and .extract_first() methods.  Another useful new feature is the introduction of Selector.attrib and SelectorList.attrib properties, which make it easier to get attributes of HTML elements. See Selecting element attributes. CSS selectors are cached in parsel >= 1.5, which makes them faster when the same CSS path is used many times. This is very common in case of Scrapy spiders: callbacks are usually called several times, on different pages. If you’re using custom Selector or SelectorList subclasses, a backward incompatible change in parsel may affect your code. See parsel changelog for a detailed description, as well as for the full list of improvements.   Telnet console Backward incompatible: Scrapy’s telnet console now requires username and password. See Telnet Console for more details. This change fixes a security issue; see Scrapy 1.5.2 (2019-01-22) release notes for details.   New extensibility features  from_crawler support is added to feed exporters and feed storages. This, among other things, allows to access Scrapy settings from custom feed storages and exporters (issue 1605, issue 3348). from_crawler support is added to dupefilters (issue 2956); this allows to access e.g. settings or a spider from a dupefilter. item_error is fired when an error happens in a pipeline (issue 3256); request_reached_downloader is fired when Downloader gets a new Request; this signal can be useful e.g. for custom Schedulers (issue 3393). new SitemapSpider sitemap_filter() method which allows to select sitemap entries based on their attributes in SitemapSpider subclasses (issue 3512). Lazy loading of Downloader Handlers is now optional; this enables better initialization error handling in custom Downloader Handlers (issue 3394).    New FilePipeline and MediaPipeline features  Expose more options for S3FilesStore: AWS_ENDPOINT_URL, AWS_USE_SSL, AWS_VERIFY, AWS_REGION_NAME. For example, this allows to use alternative or self-hosted AWS-compatible providers (issue 2609, issue 3548). ACL support for Google Cloud Storage: FILES_STORE_GCS_ACL and IMAGES_STORE_GCS_ACL (issue 3199).    scrapy.contracts improvements  Exceptions in contracts code are handled better (issue 3377); dont_filter=True is used for contract requests, which allows to test different callbacks with the same URL (issue 3381); request_cls attribute in Contract subclasses allow to use different Request classes in contracts, for example FormRequest (issue 3383). Fixed errback handling in contracts, e.g. for cases where a contract is executed for URL which returns non-200 response (issue 3371).    Usability improvements  more stats for RobotsTxtMiddleware (issue 3100) INFO log level is used to show telnet host/port (issue 3115) a message is added to IgnoreRequest in RobotsTxtMiddleware (issue 3113) better validation of url argument in Response.follow (issue 3131) non-zero exit code is returned from Scrapy commands when error happens on spider initialization (issue 3226) Link extraction improvements: “ftp” is added to scheme list (issue 3152); “flv” is added to common video extensions (issue 3165) better error message when an exporter is disabled (issue 3358); scrapy shell --help mentions syntax required for local files (./file.html) - issue 3496. Referer header value is added to RFPDupeFilter log messages (issue 3588)    Bug fixes  fixed issue with extra blank lines in .csv exports under Windows (issue 3039); proper handling of pickling errors in Python 3 when serializing objects for disk queues (issue 3082) flags are now preserved when copying Requests (issue 3342); FormRequest.from_response clickdata shouldn’t ignore elements with input[type=image] (issue 3153). FormRequest.from_response should preserve duplicate keys (issue 3247)    Documentation improvements  Docs are re-written to suggest .get/.getall API instead of .extract/.extract_first. Also, Selectors docs are updated and re-structured to match latest parsel docs; they now contain more topics, such as Selecting element attributes or Extensions to CSS Selectors (issue 3390). Using your browser’s Developer Tools for scraping is a new tutorial which replaces old Firefox and Firebug tutorials (issue 3400). SCRAPY_PROJECT environment variable is documented (issue 3518); troubleshooting section is added to install instructions (issue 3517); improved links to beginner resources in the tutorial (issue 3367, issue 3468); fixed RETRY_HTTP_CODES default values in docs (issue 3335); remove unused DEPTH_STATS option from docs (issue 3245); other cleanups (issue 3347, issue 3350, issue 3445, issue 3544, issue 3605).    Deprecation removals Compatibility shims for pre-1.0 Scrapy module names are removed (issue 3318):  scrapy.command scrapy.contrib (with all submodules) scrapy.contrib_exp (with all submodules) scrapy.dupefilter scrapy.linkextractor scrapy.project scrapy.spider scrapy.spidermanager scrapy.squeue scrapy.stats scrapy.statscol scrapy.utils.decorator  See Module Relocations for more information, or use suggestions from Scrapy 1.5.x deprecation warnings to update your code. Other deprecation removals:  Deprecated scrapy.interfaces.ISpiderManager is removed; please use scrapy.interfaces.ISpiderLoader. Deprecated CrawlerSettings class is removed (issue 3327). Deprecated Settings.overrides and Settings.defaults attributes are removed (issue 3327, issue 3359).    Other improvements, cleanups  All Scrapy tests now pass on Windows; Scrapy testing suite is executed in a Windows environment on CI (issue 3315). Python 3.7 support (issue 3326, issue 3150, issue 3547). Testing and CI fixes (issue 3526, issue 3538, issue 3308, issue 3311, issue 3309, issue 3305, issue 3210, issue 3299) scrapy.http.cookies.CookieJar.clear accepts “domain”, “path” and “name” optional arguments (issue 3231). additional files are included to sdist (issue 3495); code style fixes (issue 3405, issue 3304); unneeded .strip() call is removed (issue 3519); collections.deque is used to store MiddlewareManager methods instead of a list (issue 3476)     Scrapy 1.5.2 (2019-01-22)  Security bugfix: Telnet console extension can be easily exploited by rogue websites POSTing content to http://localhost:6023, we haven’t found a way to exploit it from Scrapy, but it is very easy to trick a browser to do so and elevates the risk for local development environment. The fix is backward incompatible, it enables telnet user-password authentication by default with a random generated password. If you can’t upgrade right away, please consider setting TELNETCONSOLE_PORT out of its default value. See telnet console documentation for more info  Backport CI build failure under GCE environment due to boto import error.    Scrapy 1.5.1 (2018-07-12) This is a maintenance release with important bug fixes, but no new features:  O(N^2) gzip decompression issue which affected Python 3 and PyPy is fixed (issue 3281); skipping of TLS validation errors is improved (issue 3166); Ctrl-C handling is fixed in Python 3.5+ (issue 3096); testing fixes (issue 3092, issue 3263); documentation improvements (issue 3058, issue 3059, issue 3089, issue 3123, issue 3127, issue 3189, issue 3224, issue 3280, issue 3279, issue 3201, issue 3260, issue 3284, issue 3298, issue 3294).    Scrapy 1.5.0 (2017-12-29) This release brings small new features and improvements across the codebase. Some highlights:  Google Cloud Storage is supported in FilesPipeline and ImagesPipeline. Crawling with proxy servers becomes more efficient, as connections to proxies can be reused now. Warnings, exception and logging messages are improved to make debugging easier. scrapy parse command now allows to set custom request meta via --meta argument. Compatibility with Python 3.6, PyPy and PyPy3 is improved; PyPy and PyPy3 are now supported officially, by running tests on CI. Better default handling of HTTP 308, 522 and 524 status codes. Documentation is improved, as usual.   Backward Incompatible Changes  Scrapy 1.5 drops support for Python 3.3. Default Scrapy User-Agent now uses https link to scrapy.org (issue 2983). This is technically backward-incompatible; override USER_AGENT if you relied on old value. Logging of settings overridden by custom_settings is fixed; this is technically backward-incompatible because the logger changes from [scrapy.utils.log] to [scrapy.crawler]. If you’re parsing Scrapy logs, please update your log parsers (issue 1343). LinkExtractor now ignores m4v extension by default, this is change in behavior. 522 and 524 status codes are added to RETRY_HTTP_CODES (issue 2851)    New features  Support <link> tags in Response.follow (issue 2785) Support for ptpython REPL (issue 2654) Google Cloud Storage support for FilesPipeline and ImagesPipeline (issue 2923). New --meta option of the “scrapy parse” command allows to pass additional request.meta (issue 2883) Populate spider variable when using shell.inspect_response (issue 2812) Handle HTTP 308 Permanent Redirect (issue 2844) Add 522 and 524 to RETRY_HTTP_CODES (issue 2851) Log versions information at startup (issue 2857) scrapy.mail.MailSender now works in Python 3 (it requires Twisted 17.9.0) Connections to proxy servers are reused (issue 2743) Add template for a downloader middleware (issue 2755) Explicit message for NotImplementedError when parse callback not defined (issue 2831) CrawlerProcess got an option to disable installation of root log handler (issue 2921) LinkExtractor now ignores m4v extension by default Better log messages for responses over DOWNLOAD_WARNSIZE and DOWNLOAD_MAXSIZE limits (issue 2927) Show warning when a URL is put to Spider.allowed_domains instead of a domain (issue 2250).    Bug fixes  Fix logging of settings overridden by custom_settings; this is technically backward-incompatible because the logger changes from [scrapy.utils.log] to [scrapy.crawler], so please update your log parsers if needed (issue 1343) Default Scrapy User-Agent now uses https link to scrapy.org (issue 2983). This is technically backward-incompatible; override USER_AGENT if you relied on old value. Fix PyPy and PyPy3 test failures, support them officially (issue 2793, issue 2935, issue 2990, issue 3050, issue 2213, issue 3048) Fix DNS resolver when DNSCACHE_ENABLED=False (issue 2811) Add cryptography for Debian Jessie tox test env (issue 2848) Add verification to check if Request callback is callable (issue 2766) Port extras/qpsclient.py to Python 3 (issue 2849) Use getfullargspec under the scenes for Python 3 to stop DeprecationWarning (issue 2862) Update deprecated test aliases (issue 2876) Fix SitemapSpider support for alternate links (issue 2853)    Docs  Added missing bullet point for the AUTOTHROTTLE_TARGET_CONCURRENCY setting. (issue 2756) Update Contributing docs, document new support channels (issue 2762, issue:3038) Include references to Scrapy subreddit in the docs Fix broken links; use https:// for external links (issue 2978, issue 2982, issue 2958) Document CloseSpider extension better (issue 2759) Use pymongo.collection.Collection.insert_one() in MongoDB example (issue 2781) Spelling mistake and typos (issue 2828, issue 2837, issue 2884, issue 2924) Clarify CSVFeedSpider.headers documentation (issue 2826) Document DontCloseSpider exception and clarify spider_idle (issue 2791) Update “Releases” section in README (issue 2764) Fix rst syntax in DOWNLOAD_FAIL_ON_DATALOSS docs (issue 2763) Small fix in description of startproject arguments (issue 2866) Clarify data types in Response.body docs (issue 2922) Add a note about request.meta['depth'] to DepthMiddleware docs (issue 2374) Add a note about request.meta['dont_merge_cookies'] to CookiesMiddleware docs (issue 2999) Up-to-date example of project structure (issue 2964, issue 2976) A better example of ItemExporters usage (issue 2989) Document from_crawler methods for spider and downloader middlewares (issue 3019)     Scrapy 1.4.0 (2017-05-18) Scrapy 1.4 does not bring that many breathtaking new features but quite a few handy improvements nonetheless. Scrapy now supports anonymous FTP sessions with customizable user and password via the new FTP_USER and FTP_PASSWORD settings. And if you’re using Twisted version 17.1.0 or above, FTP is now available with Python 3. There’s a new response.follow method for creating requests; it is now a recommended way to create Requests in Scrapy spiders. This method makes it easier to write correct spiders; response.follow has several advantages over creating scrapy.Request objects directly:  it handles relative URLs; it works properly with non-ascii URLs on non-UTF8 pages; in addition to absolute and relative URLs it supports Selectors; for <a> elements it can also extract their href values.  For example, instead of this: for href in response.css('li.page a::attr(href)').extract():     url = response.urljoin(href)     yield scrapy.Request(url, self.parse, encoding=response.encoding)   One can now write this: for a in response.css('li.page a'):     yield response.follow(a, self.parse)   Link extractors are also improved. They work similarly to what a regular modern browser would do: leading and trailing whitespace are removed from attributes (think href=\"   http://example.com\") when building Link objects. This whitespace-stripping also happens for action attributes with FormRequest. Please also note that link extractors do not canonicalize URLs by default anymore. This was puzzling users every now and then, and it’s not what browsers do in fact, so we removed that extra transformation on extracted links. For those of you wanting more control on the Referer: header that Scrapy sends when following links, you can set your own Referrer Policy. Prior to Scrapy 1.4, the default RefererMiddleware would simply and blindly set it to the URL of the response that generated the HTTP request (which could leak information on your URL seeds). By default, Scrapy now behaves much like your regular browser does. And this policy is fully customizable with W3C standard values (or with something really custom of your own if you wish). See REFERRER_POLICY for details. To make Scrapy spiders easier to debug, Scrapy logs more stats by default in 1.4: memory usage stats, detailed retry stats, detailed HTTP error code stats. A similar change is that HTTP cache path is also visible in logs now. Last but not least, Scrapy now has the option to make JSON and XML items more human-readable, with newlines between items and even custom indenting offset, using the new FEED_EXPORT_INDENT setting. Enjoy! (Or read on for the rest of changes in this release.)  Deprecations and Backward Incompatible Changes  Default to canonicalize=False in scrapy.linkextractors.LinkExtractor (issue 2537, fixes issue 1941 and issue 1982): warning, this is technically backward-incompatible Enable memusage extension by default (issue 2539, fixes issue 2187); this is technically backward-incompatible so please check if you have any non-default MEMUSAGE_*** options set. EDITOR environment variable now takes precedence over EDITOR option defined in settings.py (issue 1829); Scrapy default settings no longer depend on environment variables. This is technically a backward incompatible change. Spider.make_requests_from_url is deprecated (issue 1728, fixes issue 1495).    New Features  Accept proxy credentials in proxy request meta key (issue 2526) Support brotli-compressed content; requires optional brotlipy (issue 2535) New response.follow shortcut for creating requests (issue 1940) Added flags argument and attribute to Request objects (issue 2047) Support Anonymous FTP (issue 2342) Added retry/count, retry/max_reached and retry/reason_count/<reason> stats to RetryMiddleware (issue 2543) Added httperror/response_ignored_count and httperror/response_ignored_status_count/<status> stats to HttpErrorMiddleware (issue 2566) Customizable Referrer policy in RefererMiddleware (issue 2306) New data: URI download handler (issue 2334, fixes issue 2156) Log cache directory when HTTP Cache is used (issue 2611, fixes issue 2604) Warn users when project contains duplicate spider names (fixes issue 2181) scrapy.utils.datatypes.CaselessDict now accepts Mapping instances and not only dicts (issue 2646) Media downloads, with FilesPipeline or ImagesPipeline, can now optionally handle HTTP redirects using the new MEDIA_ALLOW_REDIRECTS setting (issue 2616, fixes issue 2004) Accept non-complete responses from websites using a new DOWNLOAD_FAIL_ON_DATALOSS setting (issue 2590, fixes issue 2586) Optional pretty-printing of JSON and XML items via FEED_EXPORT_INDENT setting (issue 2456, fixes issue 1327) Allow dropping fields in FormRequest.from_response formdata when None value is passed (issue 667) Per-request retry times with the new max_retry_times meta key (issue 2642) python -m scrapy as a more explicit alternative to scrapy command (issue 2740)    Bug fixes  LinkExtractor now strips leading and trailing whitespaces from attributes (issue 2547, fixes issue 1614) Properly handle whitespaces in action attribute in FormRequest (issue 2548) Buffer CONNECT response bytes from proxy until all HTTP headers are received (issue 2495, fixes issue 2491) FTP downloader now works on Python 3, provided you use Twisted>=17.1 (issue 2599) Use body to choose response type after decompressing content (issue 2393, fixes issue 2145) Always decompress Content-Encoding: gzip at HttpCompressionMiddleware stage (issue 2391) Respect custom log level in Spider.custom_settings (issue 2581, fixes issue 1612) ‘make htmlview’ fix for macOS (issue 2661) Remove “commands” from the command list  (issue 2695) Fix duplicate Content-Length header for POST requests with empty body (issue 2677) Properly cancel large downloads, i.e. above DOWNLOAD_MAXSIZE (issue 1616) ImagesPipeline: fixed processing of transparent PNG images with palette (issue 2675)    Cleanups & Refactoring  Tests: remove temp files and folders (issue 2570), fixed ProjectUtilsTest on macOS (issue 2569), use portable pypy for Linux on Travis CI (issue 2710) Separate building request from _requests_to_follow in CrawlSpider (issue 2562) Remove “Python 3 progress” badge (issue 2567) Add a couple more lines to .gitignore (issue 2557) Remove bumpversion prerelease configuration (issue 2159) Add codecov.yml file (issue 2750) Set context factory implementation based on Twisted version (issue 2577, fixes issue 2560) Add omitted self arguments in default project middleware template (issue 2595) Remove redundant slot.add_request() call in ExecutionEngine (issue 2617) Catch more specific os.error exception in scrapy.pipelines.files.FSFilesStore (issue 2644) Change “localhost” test server certificate (issue 2720) Remove unused MEMUSAGE_REPORT setting (issue 2576)    Documentation  Binary mode is required for exporters (issue 2564, fixes issue 2553) Mention issue with FormRequest.from_response due to bug in lxml (issue 2572) Use single quotes uniformly in templates (issue 2596) Document ftp_user and ftp_password meta keys (issue 2587) Removed section on deprecated contrib/ (issue 2636) Recommend Anaconda when installing Scrapy on Windows (issue 2477, fixes issue 2475) FAQ: rewrite note on Python 3 support on Windows (issue 2690) Rearrange selector sections (issue 2705) Remove __nonzero__ from SelectorList docs (issue 2683) Mention how to disable request filtering in documentation of DUPEFILTER_CLASS setting (issue 2714) Add sphinx_rtd_theme to docs setup readme (issue 2668) Open file in text mode in JSON item writer example (issue 2729) Clarify allowed_domains example (issue 2670)     Scrapy 1.3.3 (2017-03-10)  Bug fixes  Make SpiderLoader raise ImportError again by default for missing dependencies and wrong SPIDER_MODULES. These exceptions were silenced as warnings since 1.3.0. A new setting is introduced to toggle between warning or exception if needed ; see SPIDER_LOADER_WARN_ONLY for details.     Scrapy 1.3.2 (2017-02-13)  Bug fixes  Preserve request class when converting to/from dicts (utils.reqser) (issue 2510). Use consistent selectors for author field in tutorial (issue 2551). Fix TLS compatibility in Twisted 17+ (issue 2558)     Scrapy 1.3.1 (2017-02-08)  New features  Support 'True' and 'False' string values for boolean settings (issue 2519); you can now do something like scrapy crawl myspider -s REDIRECT_ENABLED=False. Support kwargs with response.xpath() to use XPath variables and ad-hoc namespaces declarations ; this requires at least Parsel v1.1 (issue 2457). Add support for Python 3.6 (issue 2485). Run tests on PyPy (warning: some tests still fail, so PyPy is not supported yet).    Bug fixes  Enforce DNS_TIMEOUT setting (issue 2496). Fix view command ; it was a regression in v1.3.0 (issue 2503). Fix tests regarding *_EXPIRES settings with Files/Images pipelines (issue 2460). Fix name of generated pipeline class when using basic project template (issue 2466). Fix compatibility with Twisted 17+ (issue 2496, issue 2528). Fix scrapy.Item inheritance on Python 3.6 (issue 2511). Enforce numeric values for components order in SPIDER_MIDDLEWARES, DOWNLOADER_MIDDLEWARES, EXTENSIONS and SPIDER_CONTRACTS (issue 2420).    Documentation  Reword Code of Conduct section and upgrade to Contributor Covenant v1.4 (issue 2469). Clarify that passing spider arguments converts them to spider attributes (issue 2483). Document formid argument on FormRequest.from_response() (issue 2497). Add .rst extension to README files (issue 2507). Mention LevelDB cache storage backend (issue 2525). Use yield in sample callback code (issue 2533). Add note about HTML entities decoding with .re()/.re_first() (issue 1704). Typos (issue 2512, issue 2534, issue 2531).    Cleanups  Remove redundant check in MetaRefreshMiddleware (issue 2542). Faster checks in LinkExtractor for allow/deny patterns (issue 2538). Remove dead code supporting old Twisted versions (issue 2544).     Scrapy 1.3.0 (2016-12-21) This release comes rather soon after 1.2.2 for one main reason: it was found out that releases since 0.18 up to 1.2.2 (included) use some backported code from Twisted (scrapy.xlib.tx.*), even if newer Twisted modules are available. Scrapy now uses twisted.web.client and twisted.internet.endpoints directly. (See also cleanups below.) As it is a major change, we wanted to get the bug fix out quickly while not breaking any projects using the 1.2 series.  New Features  MailSender now accepts single strings as values for to and cc arguments (issue 2272) scrapy fetch url, scrapy shell url and fetch(url) inside Scrapy shell now follow HTTP redirections by default (issue 2290); See fetch and shell for details. HttpErrorMiddleware now logs errors with INFO level instead of DEBUG; this is technically backward incompatible so please check your log parsers. By default, logger names now use a long-form path, e.g. [scrapy.extensions.logstats], instead of the shorter “top-level” variant of prior releases (e.g. [scrapy]); this is backward incompatible if you have log parsers expecting the short logger name part. You can switch back to short logger names using LOG_SHORT_NAMES set to True.    Dependencies & Cleanups  Scrapy now requires Twisted >= 13.1 which is the case for many Linux distributions already. As a consequence, we got rid of scrapy.xlib.tx.* modules, which copied some of Twisted code for users stuck with an “old” Twisted version ChunkedTransferMiddleware is deprecated and removed from the default downloader middlewares.     Scrapy 1.2.3 (2017-03-03)  Packaging fix: disallow unsupported Twisted versions in setup.py    Scrapy 1.2.2 (2016-12-06)  Bug fixes  Fix a cryptic traceback when a pipeline fails on open_spider() (issue 2011) Fix embedded IPython shell variables (fixing issue 396 that re-appeared in 1.2.0, fixed in issue 2418) A couple of patches when dealing with robots.txt:  handle (non-standard) relative sitemap URLs (issue 2390) handle non-ASCII URLs and User-Agents in Python 2 (issue 2373)      Documentation  Document \"download_latency\" key in Request’s meta dict (issue 2033) Remove page on (deprecated & unsupported) Ubuntu packages from ToC (issue 2335) A few fixed typos (issue 2346, issue 2369, issue 2369, issue 2380) and clarifications (issue 2354, issue 2325, issue 2414)    Other changes  Advertize conda-forge as Scrapy’s official conda channel (issue 2387) More helpful error messages when trying to use .css() or .xpath() on non-Text Responses (issue 2264) startproject command now generates a sample middlewares.py file (issue 2335) Add more dependencies’ version info in scrapy version verbose output (issue 2404) Remove all *.pyc files from source distribution (issue 2386)     Scrapy 1.2.1 (2016-10-21)  Bug fixes  Include OpenSSL’s more permissive default ciphers when establishing TLS/SSL connections (issue 2314). Fix “Location” HTTP header decoding on non-ASCII URL redirects (issue 2321).    Documentation  Fix JsonWriterPipeline example (issue 2302). Various notes: issue 2330 on spider names, issue 2329 on middleware methods processing order, issue 2327 on getting multi-valued HTTP headers as lists.    Other changes  Removed www. from start_urls in built-in spider templates (issue 2299).     Scrapy 1.2.0 (2016-10-03)  New Features  New FEED_EXPORT_ENCODING setting to customize the encoding used when writing items to a file. This can be used to turn off \\uXXXX escapes in JSON output. This is also useful for those wanting something else than UTF-8 for XML or CSV output (issue 2034). startproject command now supports an optional destination directory to override the default one based on the project name (issue 2005). New SCHEDULER_DEBUG setting to log requests serialization failures (issue 1610). JSON encoder now supports serialization of set instances (issue 2058). Interpret application/json-amazonui-streaming as TextResponse (issue 1503). scrapy is imported by default when using shell tools (shell, inspect_response) (issue 2248).    Bug fixes  DefaultRequestHeaders middleware now runs before UserAgent middleware (issue 2088). Warning: this is technically backward incompatible, though we consider this a bug fix. HTTP cache extension and plugins that use the .scrapy data directory now work outside projects (issue 1581).  Warning: this is technically backward incompatible, though we consider this a bug fix. Selector does not allow passing both response and text anymore (issue 2153). Fixed logging of wrong callback name with scrapy parse (issue 2169). Fix for an odd gzip decompression bug (issue 1606). Fix for selected callbacks when using CrawlSpider with scrapy parse (issue 2225). Fix for invalid JSON and XML files when spider yields no items (issue 872). Implement flush() for StreamLogger avoiding a warning in logs (issue 2125).    Refactoring  canonicalize_url has been moved to w3lib.url (issue 2168).    Tests & Requirements Scrapy’s new requirements baseline is Debian 8 “Jessie”. It was previously Ubuntu 12.04 Precise. What this means in practice is that we run continuous integration tests with these (main) packages versions at a minimum: Twisted 14.0, pyOpenSSL 0.14, lxml 3.4. Scrapy may very well work with older versions of these packages (the code base still has switches for older Twisted versions for example) but it is not guaranteed (because it’s not tested anymore).   Documentation  Grammar fixes: issue 2128, issue 1566. Download stats badge removed from README (issue 2160). New Scrapy architecture diagram (issue 2165). Updated Response parameters documentation (issue 2197). Reworded misleading RANDOMIZE_DOWNLOAD_DELAY description (issue 2190). Add StackOverflow as a support channel (issue 2257).     Scrapy 1.1.4 (2017-03-03)  Packaging fix: disallow unsupported Twisted versions in setup.py    Scrapy 1.1.3 (2016-09-22)  Bug fixes  Class attributes for subclasses of ImagesPipeline and FilesPipeline work as they did before 1.1.1 (issue 2243, fixes issue 2198)    Documentation  Overview and tutorial rewritten to use http://toscrape.com websites (issue 2236, issue 2249, issue 2252).     Scrapy 1.1.2 (2016-08-18)  Bug fixes  Introduce a missing IMAGES_STORE_S3_ACL setting to override the default ACL policy in ImagesPipeline when uploading images to S3 (note that default ACL policy is “private” – instead of “public-read” – since Scrapy 1.1.0) IMAGES_EXPIRES default value set back to 90 (the regression was introduced in 1.1.1)     Scrapy 1.1.1 (2016-07-13)  Bug fixes  Add “Host” header in CONNECT requests to HTTPS proxies (issue 2069) Use response body when choosing response class (issue 2001, fixes issue 2000) Do not fail on canonicalizing URLs with wrong netlocs (issue 2038, fixes issue 2010) a few fixes for HttpCompressionMiddleware (and SitemapSpider):  Do not decode HEAD responses (issue 2008, fixes issue 1899) Handle charset parameter in gzip Content-Type header (issue 2050, fixes issue 2049) Do not decompress gzip octet-stream responses (issue 2065, fixes issue 2063)   Catch (and ignore with a warning) exception when verifying certificate against IP-address hosts (issue 2094, fixes issue 2092) Make FilesPipeline and ImagesPipeline backward compatible again regarding the use of legacy class attributes for customization (issue 1989, fixes issue 1985)    New features  Enable genspider command outside project folder (issue 2052) Retry HTTPS CONNECT TunnelError by default (issue 1974)    Documentation  FEED_TEMPDIR setting at lexicographical position (commit 9b3c72c) Use idiomatic .extract_first() in overview (issue 1994) Update years in copyright notice (commit c2c8036) Add information and example on errbacks (issue 1995) Use “url” variable in downloader middleware example (issue 2015) Grammar fixes (issue 2054, issue 2120) New FAQ entry on using BeautifulSoup in spider callbacks (issue 2048) Add notes about Scrapy not working on Windows with Python 3 (issue 2060) Encourage complete titles in pull requests (issue 2026)    Tests  Upgrade py.test requirement on Travis CI and Pin pytest-cov to 2.2.1 (issue 2095)     Scrapy 1.1.0 (2016-05-11) This 1.1 release brings a lot of interesting features and bug fixes:  Scrapy 1.1 has beta Python 3 support (requires Twisted >= 15.5). See Beta Python 3 Support for more details and some limitations. Hot new features:  Item loaders now support nested loaders (issue 1467). FormRequest.from_response improvements (issue 1382, issue 1137). Added setting AUTOTHROTTLE_TARGET_CONCURRENCY and improved AutoThrottle docs (issue 1324). Added response.text to get body as unicode (issue 1730). Anonymous S3 connections (issue 1358). Deferreds in downloader middlewares (issue 1473). This enables better robots.txt handling (issue 1471). HTTP caching now follows RFC2616 more closely, added settings HTTPCACHE_ALWAYS_STORE and HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS (issue 1151). Selectors were extracted to the parsel library (issue 1409). This means you can use Scrapy Selectors without Scrapy and also upgrade the selectors engine without needing to upgrade Scrapy. HTTPS downloader now does TLS protocol negotiation by default, instead of forcing TLS 1.0. You can also set the SSL/TLS method using the new DOWNLOADER_CLIENT_TLS_METHOD.   These bug fixes may require your attention:  Don’t retry bad requests (HTTP 400) by default (issue 1289). If you need the old behavior, add 400 to RETRY_HTTP_CODES. Fix shell files argument handling (issue 1710, issue 1550). If you try scrapy shell index.html it will try to load the URL http://index.html, use scrapy shell ./index.html to load a local file. Robots.txt compliance is now enabled by default for newly-created projects (issue 1724). Scrapy will also wait for robots.txt to be downloaded before proceeding with the crawl (issue 1735). If you want to disable this behavior, update ROBOTSTXT_OBEY in settings.py file after creating a new project. Exporters now work on unicode, instead of bytes by default (issue 1080). If you use PythonItemExporter, you may want to update your code to disable binary mode which is now deprecated. Accept XML node names containing dots as valid (issue 1533). When uploading files or images to S3 (with FilesPipeline or ImagesPipeline), the default ACL policy is now “private” instead of “public” Warning: backward incompatible!. You can use FILES_STORE_S3_ACL to change it. We’ve reimplemented canonicalize_url() for more correct output, especially for URLs with non-ASCII characters (issue 1947). This could change link extractors output compared to previous Scrapy versions. This may also invalidate some cache entries you could still have from pre-1.1 runs. Warning: backward incompatible!.    Keep reading for more details on other improvements and bug fixes.  Beta Python 3 Support We have been hard at work to make Scrapy run on Python 3. As a result, now you can run spiders on Python 3.3, 3.4 and 3.5 (Twisted >= 15.5 required). Some features are still missing (and some may never be ported). Almost all builtin extensions/middlewares are expected to work. However, we are aware of some limitations in Python 3:  Scrapy does not work on Windows with Python 3 Sending emails is not supported FTP download handler is not supported Telnet console is not supported    Additional New Features and Enhancements  Scrapy now has a Code of Conduct (issue 1681). Command line tool now has completion for zsh (issue 934). Improvements to scrapy shell:  Support for bpython and configure preferred Python shell via SCRAPY_PYTHON_SHELL (issue 1100, issue 1444). Support URLs without scheme (issue 1498) Warning: backward incompatible! Bring back support for relative file path (issue 1710, issue 1550).   Added MEMUSAGE_CHECK_INTERVAL_SECONDS setting to change default check interval (issue 1282). Download handlers are now lazy-loaded on first request using their scheme (issue 1390, issue 1421). HTTPS download handlers do not force TLS 1.0 anymore; instead, OpenSSL’s SSLv23_method()/TLS_method() is used allowing to try negotiating with the remote hosts the highest TLS protocol version it can (issue 1794, issue 1629). RedirectMiddleware now skips the status codes from handle_httpstatus_list on spider attribute or in Request’s meta key (issue 1334, issue 1364, issue 1447). Form submission:  now works with <button> elements too (issue 1469). an empty string is now used for submit buttons without a value (issue 1472)   Dict-like settings now have per-key priorities (issue 1135, issue 1149 and issue 1586). Sending non-ASCII emails (issue 1662) CloseSpider and SpiderState extensions now get disabled if no relevant setting is set (issue 1723, issue 1725). Added method ExecutionEngine.close (issue 1423). Added method CrawlerRunner.create_crawler (issue 1528). Scheduler priority queue can now be customized via SCHEDULER_PRIORITY_QUEUE (issue 1822). .pps links are now ignored by default in link extractors (issue 1835). temporary data folder for FTP and S3 feed storages can be customized using a new FEED_TEMPDIR setting (issue 1847). FilesPipeline and ImagesPipeline settings are now instance attributes instead of class attributes, enabling spider-specific behaviors (issue 1891). JsonItemExporter now formats opening and closing square brackets on their own line (first and last lines of output file) (issue 1950). If available, botocore is used for S3FeedStorage, S3DownloadHandler and S3FilesStore (issue 1761, issue 1883). Tons of documentation updates and related fixes (issue 1291, issue 1302, issue 1335, issue 1683, issue 1660, issue 1642, issue 1721, issue 1727, issue 1879). Other refactoring, optimizations and cleanup (issue 1476, issue 1481, issue 1477, issue 1315, issue 1290, issue 1750, issue 1881).    Deprecations and Removals  Added to_bytes and to_unicode, deprecated str_to_unicode and unicode_to_str functions (issue 778). binary_is_text is introduced, to replace use of isbinarytext (but with inverse return value) (issue 1851) The optional_features set has been removed (issue 1359). The --lsprof command line option has been removed (issue 1689). Warning: backward incompatible, but doesn’t break user code. The following datatypes were deprecated (issue 1720):  scrapy.utils.datatypes.MultiValueDictKeyError scrapy.utils.datatypes.MultiValueDict scrapy.utils.datatypes.SiteNode   The previously bundled scrapy.xlib.pydispatch library was deprecated and replaced by pydispatcher.    Relocations  telnetconsole was relocated to extensions/ (issue 1524).  Note: telnet is not enabled on Python 3 (https://github.com/scrapy/scrapy/pull/1524#issuecomment-146985595)      Bugfixes  Scrapy does not retry requests that got a HTTP 400 Bad Request response anymore (issue 1289). Warning: backward incompatible! Support empty password for http_proxy config (issue 1274). Interpret application/x-json as TextResponse (issue 1333). Support link rel attribute with multiple values (issue 1201). Fixed scrapy.http.FormRequest.from_response when there is a <base> tag (issue 1564). Fixed TEMPLATES_DIR handling (issue 1575). Various FormRequest fixes (issue 1595, issue 1596, issue 1597). Makes _monkeypatches more robust (issue 1634). Fixed bug on XMLItemExporter with non-string fields in items (issue 1738). Fixed startproject command in macOS (issue 1635). Fixed PythonItemExporter and CSVExporter for non-string item types (issue 1737). Various logging related fixes (issue 1294, issue 1419, issue 1263, issue 1624, issue 1654, issue 1722, issue 1726 and issue 1303). Fixed bug in utils.template.render_templatefile() (issue 1212). sitemaps extraction from robots.txt is now case-insensitive (issue 1902). HTTPS+CONNECT tunnels could get mixed up when using multiple proxies to same remote host (issue 1912).     Scrapy 1.0.7 (2017-03-03)  Packaging fix: disallow unsupported Twisted versions in setup.py    Scrapy 1.0.6 (2016-05-04)  FIX: RetryMiddleware is now robust to non-standard HTTP status codes (issue 1857) FIX: Filestorage HTTP cache was checking wrong modified time (issue 1875) DOC: Support for Sphinx 1.4+ (issue 1893) DOC: Consistency in selectors examples (issue 1869)    Scrapy 1.0.5 (2016-02-04)  FIX: [Backport] Ignore bogus links in LinkExtractors (fixes issue 907, commit 108195e) TST: Changed buildbot makefile to use ‘pytest’ (commit 1f3d90a) DOC: Fixed typos in tutorial and media-pipeline (commit 808a9ea and commit 803bd87) DOC: Add AjaxCrawlMiddleware to DOWNLOADER_MIDDLEWARES_BASE in settings docs (commit aa94121)    Scrapy 1.0.4 (2015-12-30)  Ignoring xlib/tx folder, depending on Twisted version. (commit 7dfa979) Run on new travis-ci infra (commit 6e42f0b) Spelling fixes (commit 823a1cc) escape nodename in xmliter regex (commit da3c155) test xml nodename with dots (commit 4418fc3) TST don’t use broken Pillow version in tests (commit a55078c) disable log on version command. closes #1426 (commit 86fc330) disable log on startproject command (commit db4c9fe) Add PyPI download stats badge (commit df2b944) don’t run tests twice on Travis if a PR is made from a scrapy/scrapy branch (commit a83ab41) Add Python 3 porting status badge to the README (commit 73ac80d) fixed RFPDupeFilter persistence (commit 97d080e) TST a test to show that dupefilter persistence is not working (commit 97f2fb3) explicit close file on file:// scheme handler (commit d9b4850) Disable dupefilter in shell (commit c0d0734) DOC: Add captions to toctrees which appear in sidebar (commit aa239ad) DOC Removed pywin32 from install instructions as it’s already declared as dependency. (commit 10eb400) Added installation notes about using Conda for Windows and other OSes. (commit 1c3600a) Fixed minor grammar issues. (commit 7f4ddd5) fixed a typo in the documentation. (commit b71f677) Version 1 now exists (commit 5456c0e) fix another invalid xpath error (commit 0a1366e) fix ValueError: Invalid XPath: //div/[id=”not-exists”]/text() on selectors.rst (commit ca8d60f) Typos corrections (commit 7067117) fix typos in downloader-middleware.rst and exceptions.rst, middlware -> middleware (commit 32f115c) Add note to Ubuntu install section about Debian compatibility (commit 23fda69) Replace alternative macOS install workaround with virtualenv (commit 98b63ee) Reference Homebrew’s homepage for installation instructions (commit 1925db1) Add oldest supported tox version to contributing docs (commit 5d10d6d) Note in install docs about pip being already included in python>=2.7.9 (commit 85c980e) Add non-python dependencies to Ubuntu install section in the docs (commit fbd010d) Add macOS installation section to docs (commit d8f4cba) DOC(ENH): specify path to rtd theme explicitly (commit de73b1a) minor: scrapy.Spider docs grammar (commit 1ddcc7b) Make common practices sample code match the comments (commit 1b85bcf) nextcall repetitive calls (heartbeats). (commit 55f7104) Backport fix compatibility with Twisted 15.4.0 (commit b262411) pin pytest to 2.7.3 (commit a6535c2) Merge pull request #1512 from mgedmin/patch-1 (commit 8876111) Merge pull request #1513 from mgedmin/patch-2 (commit 5d4daf8) Typo (commit f8d0682) Fix list formatting (commit 5f83a93) fix Scrapy squeue tests after recent changes to queuelib (commit 3365c01) Merge pull request #1475 from rweindl/patch-1 (commit 2d688cd) Update tutorial.rst (commit fbc1f25) Merge pull request #1449 from rhoekman/patch-1 (commit 7d6538c) Small grammatical change (commit 8752294) Add openssl version to version command (commit 13c45ac)    Scrapy 1.0.3 (2015-08-11)  add service_identity to Scrapy install_requires (commit cbc2501) Workaround for travis#296 (commit 66af9cd)    Scrapy 1.0.2 (2015-08-06)  Twisted 15.3.0 does not raises PicklingError serializing lambda functions (commit b04dd7d) Minor method name fix (commit 6f85c7f) minor: scrapy.Spider grammar and clarity (commit 9c9d2e0) Put a blurb about support channels in CONTRIBUTING (commit c63882b) Fixed typos (commit a9ae7b0) Fix doc reference. (commit 7c8a4fe)    Scrapy 1.0.1 (2015-07-01)  Unquote request path before passing to FTPClient, it already escape paths (commit cc00ad2) include tests/ to source distribution in MANIFEST.in (commit eca227e) DOC Fix SelectJmes documentation (commit b8567bc) DOC Bring Ubuntu and Archlinux outside of Windows subsection (commit 392233f) DOC remove version suffix from Ubuntu package (commit 5303c66) DOC Update release date for 1.0 (commit c89fa29)    Scrapy 1.0.0 (2015-06-19) You will find a lot of new features and bugfixes in this major release.  Make sure to check our updated overview to get a glance of some of the changes, along with our brushed tutorial.  Support for returning dictionaries in spiders Declaring and returning Scrapy Items is no longer necessary to collect the scraped data from your spider, you can now return explicit dictionaries instead. Classic version class MyItem(scrapy.Item):     url = scrapy.Field()  class MySpider(scrapy.Spider):     def parse(self, response):         return MyItem(url=response.url)   New version class MySpider(scrapy.Spider):     def parse(self, response):         return {'url': response.url}     Per-spider settings (GSoC 2014) Last Google Summer of Code project accomplished an important redesign of the mechanism used for populating settings, introducing explicit priorities to override any given setting. As an extension of that goal, we included a new level of priority for settings that act exclusively for a single spider, allowing them to redefine project settings. Start using it by defining a custom_settings class variable in your spider: class MySpider(scrapy.Spider):     custom_settings = {         \"DOWNLOAD_DELAY\": 5.0,         \"RETRY_ENABLED\": False,     }   Read more about settings population: Settings   Python Logging Scrapy 1.0 has moved away from Twisted logging to support Python built in’s as default logging system. We’re maintaining backward compatibility for most of the old custom interface to call logging functions, but you’ll get warnings to switch to the Python logging API entirely. Old version from scrapy import log log.msg('MESSAGE', log.INFO)   New version import logging logging.info('MESSAGE')   Logging with spiders remains the same, but on top of the log() method you’ll have access to a custom logger created for the spider to issue log events: class MySpider(scrapy.Spider):     def parse(self, response):         self.logger.info('Response received')   Read more in the logging documentation: Logging   Crawler API refactoring (GSoC 2014) Another milestone for last Google Summer of Code was a refactoring of the internal API, seeking a simpler and easier usage. Check new core interface in: Core API A common situation where you will face these changes is while running Scrapy from scripts. Here’s a quick example of how to run a Spider manually with the new API: from scrapy.crawler import CrawlerProcess  process = CrawlerProcess({     'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)' }) process.crawl(MySpider) process.start()   Bear in mind this feature is still under development and its API may change until it reaches a stable status. See more examples for scripts running Scrapy: Common Practices   Module Relocations There’s been a large rearrangement of modules trying to improve the general structure of Scrapy. Main changes were separating various subpackages into new projects and dissolving both scrapy.contrib and scrapy.contrib_exp into top level packages. Backward compatibility was kept among internal relocations, while importing deprecated modules expect warnings indicating their new place.  Full list of relocations Outsourced packages  Note These extensions went through some minor changes, e.g. some setting names were changed. Please check the documentation in each new repository to get familiar with the new usage.        Old location New location    scrapy.commands.deploy scrapyd-client (See other alternatives here: Deploying Spiders)  scrapy.contrib.djangoitem scrapy-djangoitem  scrapy.webservice scrapy-jsonrpc    scrapy.contrib_exp and scrapy.contrib dissolutions       Old location New location    scrapy.contrib_exp.downloadermiddleware.decompression scrapy.downloadermiddlewares.decompression  scrapy.contrib_exp.iterators scrapy.utils.iterators  scrapy.contrib.downloadermiddleware scrapy.downloadermiddlewares  scrapy.contrib.exporter scrapy.exporters  scrapy.contrib.linkextractors scrapy.linkextractors  scrapy.contrib.loader scrapy.loader  scrapy.contrib.loader.processor scrapy.loader.processors  scrapy.contrib.pipeline scrapy.pipelines  scrapy.contrib.spidermiddleware scrapy.spidermiddlewares  scrapy.contrib.spiders scrapy.spiders   scrapy.contrib.closespider scrapy.contrib.corestats scrapy.contrib.debug scrapy.contrib.feedexport scrapy.contrib.httpcache scrapy.contrib.logstats scrapy.contrib.memdebug scrapy.contrib.memusage scrapy.contrib.spiderstate scrapy.contrib.statsmailer scrapy.contrib.throttle   scrapy.extensions.*    Plural renames and Modules unification       Old location New location    scrapy.command scrapy.commands  scrapy.dupefilter scrapy.dupefilters  scrapy.linkextractor scrapy.linkextractors  scrapy.spider scrapy.spiders  scrapy.squeue scrapy.squeues  scrapy.statscol scrapy.statscollectors  scrapy.utils.decorator scrapy.utils.decorators    Class renames       Old location New location    scrapy.spidermanager.SpiderManager scrapy.spiderloader.SpiderLoader    Settings renames       Old location New location    SPIDER_MANAGER_CLASS SPIDER_LOADER_CLASS       Changelog New Features and Enhancements  Python logging (issue 1060, issue 1235, issue 1236, issue 1240, issue 1259, issue 1278, issue 1286) FEED_EXPORT_FIELDS option (issue 1159, issue 1224) Dns cache size and timeout options (issue 1132) support namespace prefix in xmliter_lxml (issue 963) Reactor threadpool max size setting (issue 1123) Allow spiders to return dicts. (issue 1081) Add Response.urljoin() helper (issue 1086) look in ~/.config/scrapy.cfg for user config (issue 1098) handle TLS SNI (issue 1101) Selectorlist extract first (issue 624, issue 1145) Added JmesSelect (issue 1016) add gzip compression to filesystem http cache backend (issue 1020) CSS support in link extractors (issue 983) httpcache dont_cache meta #19 #689 (issue 821) add signal to be sent when request is dropped by the scheduler (issue 961) avoid download large response (issue 946) Allow to specify the quotechar in CSVFeedSpider (issue 882) Add referer to “Spider error processing” log message (issue 795) process robots.txt once (issue 896) GSoC Per-spider settings (issue 854) Add project name validation (issue 817) GSoC API cleanup (issue 816, issue 1128, issue 1147, issue 1148, issue 1156, issue 1185, issue 1187, issue 1258, issue 1268, issue 1276, issue 1285, issue 1284) Be more responsive with IO operations (issue 1074 and issue 1075) Do leveldb compaction for httpcache on closing (issue 1297)  Deprecations and Removals  Deprecate htmlparser link extractor (issue 1205) remove deprecated code from FeedExporter (issue 1155) a leftover for.15 compatibility (issue 925) drop support for CONCURRENT_REQUESTS_PER_SPIDER (issue 895) Drop old engine code (issue 911) Deprecate SgmlLinkExtractor (issue 777)  Relocations  Move exporters/__init__.py to exporters.py (issue 1242) Move base classes to their packages (issue 1218, issue 1233) Module relocation (issue 1181, issue 1210) rename SpiderManager to SpiderLoader (issue 1166) Remove djangoitem (issue 1177) remove scrapy deploy command (issue 1102) dissolve contrib_exp (issue 1134) Deleted bin folder from root, fixes #913 (issue 914) Remove jsonrpc based webservice (issue 859) Move Test cases under project root dir (issue 827, issue 841) Fix backward incompatibility for relocated paths in settings (issue 1267)  Documentation  CrawlerProcess documentation (issue 1190) Favoring web scraping over screen scraping in the descriptions (issue 1188) Some improvements for Scrapy tutorial (issue 1180) Documenting Files Pipeline together with Images Pipeline (issue 1150) deployment docs tweaks (issue 1164) Added deployment section covering scrapyd-deploy and shub (issue 1124) Adding more settings to project template (issue 1073) some improvements to overview page (issue 1106) Updated link in docs/topics/architecture.rst (issue 647) DOC reorder topics (issue 1022) updating list of Request.meta special keys (issue 1071) DOC document download_timeout (issue 898) DOC simplify extension docs (issue 893) Leaks docs (issue 894) DOC document from_crawler method for item pipelines (issue 904) Spider_error doesn’t support deferreds (issue 1292) Corrections & Sphinx related fixes (issue 1220, issue 1219, issue 1196, issue 1172, issue 1171, issue 1169, issue 1160, issue 1154, issue 1127, issue 1112, issue 1105, issue 1041, issue 1082, issue 1033, issue 944, issue 866, issue 864, issue 796, issue 1260, issue 1271, issue 1293, issue 1298)  Bugfixes  Item multi inheritance fix (issue 353, issue 1228) ItemLoader.load_item: iterate over copy of fields (issue 722) Fix Unhandled error in Deferred (RobotsTxtMiddleware) (issue 1131, issue 1197) Force to read DOWNLOAD_TIMEOUT as int (issue 954) scrapy.utils.misc.load_object should print full traceback (issue 902) Fix bug for “.local” host name (issue 878) Fix for Enabled extensions, middlewares, pipelines info not printed anymore (issue 879) fix dont_merge_cookies bad behaviour when set to false on meta (issue 846)  Python 3 In Progress Support  disable scrapy.telnet if twisted.conch is not available (issue 1161) fix Python 3 syntax errors in ajaxcrawl.py (issue 1162) more python3 compatibility changes for urllib (issue 1121) assertItemsEqual was renamed to assertCountEqual in Python 3. (issue 1070) Import unittest.mock if available. (issue 1066) updated deprecated cgi.parse_qsl to use six’s parse_qsl (issue 909) Prevent Python 3 port regressions (issue 830) PY3: use MutableMapping for python 3 (issue 810) PY3: use six.BytesIO and six.moves.cStringIO (issue 803) PY3: fix xmlrpclib and email imports (issue 801) PY3: use six for robotparser and urlparse (issue 800) PY3: use six.iterkeys, six.iteritems, and tempfile (issue 799) PY3: fix has_key and use six.moves.configparser (issue 798) PY3: use six.moves.cPickle (issue 797) PY3 make it possible to run some tests in Python3 (issue 776)  Tests  remove unnecessary lines from py3-ignores (issue 1243) Fix remaining warnings from pytest while collecting tests (issue 1206) Add docs build to travis (issue 1234) TST don’t collect tests from deprecated modules. (issue 1165) install service_identity package in tests to prevent warnings (issue 1168) Fix deprecated settings API in tests (issue 1152) Add test for webclient with POST method and no body given (issue 1089) py3-ignores.txt supports comments (issue 1044) modernize some of the asserts (issue 835) selector.__repr__ test (issue 779)  Code refactoring  CSVFeedSpider cleanup: use iterate_spider_output (issue 1079) remove unnecessary check from scrapy.utils.spider.iter_spider_output (issue 1078) Pydispatch pep8 (issue 992) Removed unused ‘load=False’ parameter from walk_modules() (issue 871) For consistency, use job_dir helper in SpiderState extension. (issue 805) rename “sflo” local variables to less cryptic “log_observer” (issue 775)     Scrapy 0.24.6 (2015-04-20)  encode invalid xpath with unicode_escape under PY2 (commit 07cb3e5) fix IPython shell scope issue and load IPython user config (commit 2c8e573) Fix small typo in the docs (commit d694019) Fix small typo (commit f92fa83) Converted sel.xpath() calls to response.xpath() in Extracting the data (commit c2c6d15)    Scrapy 0.24.5 (2015-02-25)  Support new _getEndpoint Agent signatures on Twisted 15.0.0 (commit 540b9bc) DOC a couple more references are fixed (commit b4c454b) DOC fix a reference (commit e3c1260) t.i.b.ThreadedResolver is now a new-style class (commit 9e13f42) S3DownloadHandler: fix auth for requests with quoted paths/query params (commit cdb9a0b) fixed the variable types in mailsender documentation (commit bb3a848) Reset items_scraped instead of item_count (commit edb07a4) Tentative attention message about what document to read for contributions (commit 7ee6f7a) mitmproxy 0.10.1 needs netlib 0.10.1 too (commit 874fcdd) pin mitmproxy 0.10.1 as >0.11 does not work with tests (commit c6b21f0) Test the parse command locally instead of against an external url (commit c3a6628) Patches Twisted issue while closing the connection pool on HTTPDownloadHandler (commit d0bf957) Updates documentation on dynamic item classes. (commit eeb589a) Merge pull request #943 from Lazar-T/patch-3 (commit 5fdab02) typo (commit b0ae199) pywin32 is required by Twisted. closes #937 (commit 5cb0cfb) Update install.rst (commit 781286b) Merge pull request #928 from Lazar-T/patch-1 (commit b415d04) comma instead of fullstop (commit 627b9ba) Merge pull request #885 from jsma/patch-1 (commit de909ad) Update request-response.rst (commit 3f3263d) SgmlLinkExtractor - fix for parsing <area> tag with Unicode present (commit 49b40f0)    Scrapy 0.24.4 (2014-08-09)  pem file is used by mockserver and required by scrapy bench (commit 5eddc68) scrapy bench needs scrapy.tests* (commit d6cb999)    Scrapy 0.24.3 (2014-08-09)  no need to waste travis-ci time on py3 for 0.24 (commit 8e080c1) Update installation docs (commit 1d0c096) There is a trove classifier for Scrapy framework! (commit 4c701d7) update other places where w3lib version is mentioned (commit d109c13) Update w3lib requirement to 1.8.0 (commit 39d2ce5) Use w3lib.html.replace_entities() (remove_entities() is deprecated) (commit 180d3ad) set zip_safe=False (commit a51ee8b) do not ship tests package (commit ee3b371) scrapy.bat is not needed anymore (commit c3861cf) Modernize setup.py (commit 362e322) headers can not handle non-string values (commit 94a5c65) fix ftp test cases (commit a274a7f) The sum up of travis-ci builds are taking like 50min to complete (commit ae1e2cc) Update shell.rst typo (commit e49c96a) removes weird indentation in the shell results (commit 1ca489d) improved explanations, clarified blog post as source, added link for XPath string functions in the spec (commit 65c8f05) renamed UserTimeoutError and ServerTimeouterror #583 (commit 037f6ab) adding some xpath tips to selectors docs (commit 2d103e0) fix tests to account for https://github.com/scrapy/w3lib/pull/23 (commit f8d366a) get_func_args maximum recursion fix #728 (commit 81344ea) Updated input/output processor example according to #560. (commit f7c4ea8) Fixed Python syntax in tutorial. (commit db59ed9) Add test case for tunneling proxy (commit f090260) Bugfix for leaking Proxy-Authorization header to remote host when using tunneling (commit d8793af) Extract links from XHTML documents with MIME-Type “application/xml” (commit ed1f376) Merge pull request #793 from roysc/patch-1 (commit 91a1106) Fix typo in commands.rst (commit 743e1e2) better testcase for settings.overrides.setdefault (commit e22daaf) Using CRLF as line marker according to http 1.1 definition (commit 5ec430b)    Scrapy 0.24.2 (2014-07-08)  Use a mutable mapping to proxy deprecated settings.overrides and settings.defaults attribute (commit e5e8133) there is not support for python3 yet (commit 3cd6146) Update python compatible version set to Debian packages (commit fa5d76b) DOC fix formatting in release notes (commit c6a9e20)    Scrapy 0.24.1 (2014-06-27)  Fix deprecated CrawlerSettings and increase backward compatibility with .defaults attribute (commit 8e3f20a)    Scrapy 0.24.0 (2014-06-26)  Enhancements  Improve Scrapy top-level namespace (issue 494, issue 684) Add selector shortcuts to responses (issue 554, issue 690) Add new lxml based LinkExtractor to replace unmaintained SgmlLinkExtractor (issue 559, issue 761, issue 763) Cleanup settings API - part of per-spider settings GSoC project (issue 737) Add UTF8 encoding header to templates (issue 688, issue 762) Telnet console now binds to 127.0.0.1 by default (issue 699) Update Debian/Ubuntu install instructions (issue 509, issue 549) Disable smart strings in lxml XPath evaluations (issue 535) Restore filesystem based cache as default for http cache middleware (issue 541, issue 500, issue 571) Expose current crawler in Scrapy shell (issue 557) Improve testsuite comparing CSV and XML exporters (issue 570) New offsite/filtered and offsite/domains stats (issue 566) Support process_links as generator in CrawlSpider (issue 555) Verbose logging and new stats counters for DupeFilter (issue 553) Add a mimetype parameter to MailSender.send() (issue 602) Generalize file pipeline log messages (issue 622) Replace unencodeable codepoints with html entities in SGMLLinkExtractor (issue 565) Converted SEP documents to rst format (issue 629, issue 630, issue 638, issue 632, issue 636, issue 640, issue 635, issue 634, issue 639, issue 637, issue 631, issue 633, issue 641, issue 642) Tests and docs for clickdata’s nr index in FormRequest (issue 646, issue 645) Allow to disable a downloader handler just like any other component (issue 650) Log when a request is discarded after too many redirections (issue 654) Log error responses if they are not handled by spider callbacks (issue 612, issue 656) Add content-type check to http compression mw (issue 193, issue 660) Run pypy tests using latest pypi from ppa (issue 674) Run test suite using pytest instead of trial (issue 679) Build docs and check for dead links in tox environment (issue 687) Make scrapy.version_info a tuple of integers (issue 681, issue 692) Infer exporter’s output format from filename extensions (issue 546, issue 659, issue 760) Support case-insensitive domains in url_is_from_any_domain() (issue 693) Remove pep8 warnings in project and spider templates (issue 698) Tests and docs for request_fingerprint function (issue 597) Update SEP-19 for GSoC project per-spider settings (issue 705) Set exit code to non-zero when contracts fails (issue 727) Add a setting to control what class is instantiated as Downloader component (issue 738) Pass response in item_dropped signal (issue 724) Improve scrapy check contracts command (issue 733, issue 752) Document spider.closed() shortcut (issue 719) Document request_scheduled signal (issue 746) Add a note about reporting security issues (issue 697) Add LevelDB http cache storage backend (issue 626, issue 500) Sort spider list output of scrapy list command (issue 742) Multiple documentation enhancements and fixes (issue 575, issue 587, issue 590, issue 596, issue 610, issue 617, issue 618, issue 627, issue 613, issue 643, issue 654, issue 675, issue 663, issue 711, issue 714)    Bugfixes  Encode unicode URL value when creating Links in RegexLinkExtractor (issue 561) Ignore None values in ItemLoader processors (issue 556) Fix link text when there is an inner tag in SGMLLinkExtractor and HtmlParserLinkExtractor (issue 485, issue 574) Fix wrong checks on subclassing of deprecated classes (issue 581, issue 584) Handle errors caused by inspect.stack() failures (issue 582) Fix a reference to unexistent engine attribute (issue 593, issue 594) Fix dynamic itemclass example usage of type() (issue 603) Use lucasdemarchi/codespell to fix typos (issue 628) Fix default value of attrs argument in SgmlLinkExtractor to be tuple (issue 661) Fix XXE flaw in sitemap reader (issue 676) Fix engine to support filtered start requests (issue 707) Fix offsite middleware case on urls with no hostnames (issue 745) Testsuite doesn’t require PIL anymore (issue 585)     Scrapy 0.22.2 (released 2014-02-14)  fix a reference to unexistent engine.slots. closes #593 (commit 13c099a) downloaderMW doc typo (spiderMW doc copy remnant) (commit 8ae11bf) Correct typos (commit 1346037)    Scrapy 0.22.1 (released 2014-02-08)  localhost666 can resolve under certain circumstances (commit 2ec2279) test inspect.stack failure (commit cc3eda3) Handle cases when inspect.stack() fails (commit 8cb44f9) Fix wrong checks on subclassing of deprecated classes. closes #581 (commit 46d98d6) Docs: 4-space indent for final spider example (commit 13846de) Fix HtmlParserLinkExtractor and tests after #485 merge (commit 368a946) BaseSgmlLinkExtractor: Fixed the missing space when the link has an inner tag (commit b566388) BaseSgmlLinkExtractor: Added unit test of a link with an inner tag (commit c1cb418) BaseSgmlLinkExtractor: Fixed unknown_endtag() so that it only set current_link=None when the end tag match the opening tag (commit 7e4d627) Fix tests for Travis-CI build (commit 76c7e20) replace unencodeable codepoints with html entities. fixes #562 and #285 (commit 5f87b17) RegexLinkExtractor: encode URL unicode value when creating Links (commit d0ee545) Updated the tutorial crawl output with latest output. (commit 8da65de) Updated shell docs with the crawler reference and fixed the actual shell output. (commit 875b9ab) PEP8 minor edits. (commit f89efaf) Expose current crawler in the Scrapy shell. (commit 5349cec) Unused re import and PEP8 minor edits. (commit 387f414) Ignore None’s values when using the ItemLoader. (commit 0632546) DOC Fixed HTTPCACHE_STORAGE typo in the default value which is now Filesystem instead Dbm. (commit cde9a8c) show Ubuntu setup instructions as literal code (commit fb5c9c5) Update Ubuntu installation instructions (commit 70fb105) Merge pull request #550 from stray-leone/patch-1 (commit 6f70b6a) modify the version of Scrapy Ubuntu package (commit 725900d) fix 0.22.0 release date (commit af0219a) fix typos in news.rst and remove (not released yet) header (commit b7f58f4)    Scrapy 0.22.0 (released 2014-01-17)  Enhancements  [Backward incompatible] Switched HTTPCacheMiddleware backend to filesystem (issue 541) To restore old backend set HTTPCACHE_STORAGE to scrapy.contrib.httpcache.DbmCacheStorage Proxy https:// urls using CONNECT method (issue 392, issue 397) Add a middleware to crawl ajax crawlable pages as defined by google (issue 343) Rename scrapy.spider.BaseSpider to scrapy.spider.Spider (issue 510, issue 519) Selectors register EXSLT namespaces by default (issue 472) Unify item loaders similar to selectors renaming (issue 461) Make RFPDupeFilter class easily subclassable (issue 533) Improve test coverage and forthcoming Python 3 support (issue 525) Promote startup info on settings and middleware to INFO level (issue 520) Support partials in get_func_args util (issue 506, issue:504) Allow running individual tests via tox (issue 503) Update extensions ignored by link extractors (issue 498) Add middleware methods to get files/images/thumbs paths (issue 490) Improve offsite middleware tests (issue 478) Add a way to skip default Referer header set by RefererMiddleware (issue 475) Do not send x-gzip in default Accept-Encoding header (issue 469) Support defining http error handling using settings (issue 466) Use modern python idioms wherever you find legacies (issue 497) Improve and correct documentation (issue 527, issue 524, issue 521, issue 517, issue 512, issue 505, issue 502, issue 489, issue 465, issue 460, issue 425, issue 536)    Fixes  Update Selector class imports in CrawlSpider template (issue 484) Fix unexistent reference to engine.slots (issue 464) Do not try to call body_as_unicode() on a non-TextResponse instance (issue 462) Warn when subclassing XPathItemLoader, previously it only warned on instantiation. (issue 523) Warn when subclassing XPathSelector, previously it only warned on instantiation. (issue 537) Multiple fixes to memory stats (issue 531, issue 530, issue 529) Fix overriding url in FormRequest.from_response() (issue 507) Fix tests runner under pip 1.5 (issue 513) Fix logging error when spider name is unicode (issue 479)     Scrapy 0.20.2 (released 2013-12-09)  Update CrawlSpider Template with Selector changes (commit 6d1457d) fix method name in tutorial. closes GH-480 (commit b4fc359    Scrapy 0.20.1 (released 2013-11-28)  include_package_data is required to build wheels from published sources (commit 5ba1ad5) process_parallel was leaking the failures on its internal deferreds.  closes #458 (commit 419a780)    Scrapy 0.20.0 (released 2013-11-08)  Enhancements  New Selector’s API including CSS selectors (issue 395 and issue 426), Request/Response url/body attributes are now immutable (modifying them had been deprecated for a long time) ITEM_PIPELINES is now defined as a dict (instead of a list) Sitemap spider can fetch alternate URLs (issue 360) Selector.remove_namespaces() now remove namespaces from element’s attributes. (issue 416) Paved the road for Python 3.3+ (issue 435, issue 436, issue 431, issue 452) New item exporter using native python types with nesting support (issue 366) Tune HTTP1.1 pool size so it matches concurrency defined by settings (commit b43b5f575) scrapy.mail.MailSender now can connect over TLS or upgrade using STARTTLS (issue 327) New FilesPipeline with functionality factored out from ImagesPipeline (issue 370, issue 409) Recommend Pillow instead of PIL for image handling (issue 317) Added Debian packages for Ubuntu Quantal and Raring (commit 86230c0) Mock server (used for tests) can listen for HTTPS requests (issue 410) Remove multi spider support from multiple core components (issue 422, issue 421, issue 420, issue 419, issue 423, issue 418) Travis-CI now tests Scrapy changes against development versions of w3lib and queuelib python packages. Add pypy 2.1 to continuous integration tests (commit ecfa7431) Pylinted, pep8 and removed old-style exceptions from source (issue 430, issue 432) Use importlib for parametric imports (issue 445) Handle a regression introduced in Python 2.7.5 that affects XmlItemExporter (issue 372) Bugfix crawling shutdown on SIGINT (issue 450) Do not submit reset type inputs in FormRequest.from_response (commit b326b87) Do not silence download errors when request errback raises an exception (commit 684cfc0)    Bugfixes  Fix tests under Django 1.6 (commit b6bed44c) Lot of bugfixes to retry middleware under disconnections using HTTP 1.1 download handler Fix inconsistencies among Twisted releases (issue 406) Fix Scrapy shell bugs (issue 418, issue 407) Fix invalid variable name in setup.py (issue 429) Fix tutorial references (issue 387) Improve request-response docs (issue 391) Improve best practices docs (issue 399, issue 400, issue 401, issue 402) Improve django integration docs (issue 404) Document bindaddress request meta (commit 37c24e01d7) Improve Request class documentation (issue 226)    Other  Dropped Python 2.6 support (issue 448) Add cssselect python package as install dependency Drop libxml2 and multi selector’s backend support, lxml is required from now on. Minimum Twisted version increased to 10.0.0, dropped Twisted 8.0 support. Running test suite now requires mock python library (issue 390)    Thanks Thanks to everyone who contribute to this release! List of contributors sorted by number of commits: 69 Daniel Graña <dangra@...> 37 Pablo Hoffman <pablo@...> 13 Mikhail Korobov <kmike84@...>  9 Alex Cepoi <alex.cepoi@...>  9 alexanderlukanin13 <alexander.lukanin.13@...>  8 Rolando Espinoza La fuente <darkrho@...>  8 Lukasz Biedrycki <lukasz.biedrycki@...>  6 Nicolas Ramirez <nramirez.uy@...>  3 Paul Tremberth <paul.tremberth@...>  2 Martin Olveyra <molveyra@...>  2 Stefan <misc@...>  2 Rolando Espinoza <darkrho@...>  2 Loren Davie <loren@...>  2 irgmedeiros <irgmedeiros@...>  1 Stefan Koch <taikano@...>  1 Stefan <cct@...>  1 scraperdragon <dragon@...>  1 Kumara Tharmalingam <ktharmal@...>  1 Francesco Piccinno <stack.box@...>  1 Marcos Campal <duendex@...>  1 Dragon Dave <dragon@...>  1 Capi Etheriel <barraponto@...>  1 cacovsky <amarquesferraz@...>  1 Berend Iwema <berend@...>      Scrapy 0.18.4 (released 2013-10-10)  IPython refuses to update the namespace. fix #396 (commit 3d32c4f) Fix AlreadyCalledError replacing a request in shell command. closes #407 (commit b1d8919) Fix start_requests laziness and early hangs (commit 89faf52)    Scrapy 0.18.3 (released 2013-10-03)  fix regression on lazy evaluation of start requests (commit 12693a5) forms: do not submit reset inputs (commit e429f63) increase unittest timeouts to decrease travis false positive failures (commit 912202e) backport master fixes to json exporter (commit cfc2d46) Fix permission and set umask before generating sdist tarball (commit 06149e0)    Scrapy 0.18.2 (released 2013-09-03)  Backport scrapy check command fixes and backward compatible multi crawler process(issue 339)    Scrapy 0.18.1 (released 2013-08-27)  remove extra import added by cherry picked changes (commit d20304e) fix crawling tests under twisted pre 11.0.0 (commit 1994f38) py26 can not format zero length fields {} (commit abf756f) test PotentiaDataLoss errors on unbound responses (commit b15470d) Treat responses without content-length or Transfer-Encoding as good responses (commit c4bf324) do no include ResponseFailed if http11 handler is not enabled (commit 6cbe684) New HTTP client wraps connection lost in ResponseFailed exception. fix #373 (commit 1a20bba) limit travis-ci build matrix (commit 3b01bb8) Merge pull request #375 from peterarenot/patch-1 (commit fa766d7) Fixed so it refers to the correct folder (commit 3283809) added Quantal & Raring to support Ubuntu releases (commit 1411923) fix retry middleware which didn’t retry certain connection errors after the upgrade to http1 client, closes GH-373 (commit bb35ed0) fix XmlItemExporter in Python 2.7.4 and 2.7.5 (commit de3e451) minor updates to 0.18 release notes (commit c45e5f1) fix contributors list format (commit 0b60031)    Scrapy 0.18.0 (released 2013-08-09)  Lot of improvements to testsuite run using Tox, including a way to test on pypi Handle GET parameters for AJAX crawlable urls (commit 3fe2a32) Use lxml recover option to parse sitemaps (issue 347) Bugfix cookie merging by hostname and not by netloc (issue 352) Support disabling HttpCompressionMiddleware using a flag setting (issue 359) Support xml namespaces using iternodes parser in XMLFeedSpider (issue 12) Support dont_cache request meta flag (issue 19) Bugfix scrapy.utils.gz.gunzip broken by changes in python 2.7.4 (commit 4dc76e) Bugfix url encoding on SgmlLinkExtractor (issue 24) Bugfix TakeFirst processor shouldn’t discard zero (0) value (issue 59) Support nested items in xml exporter (issue 66) Improve cookies handling performance (issue 77) Log dupe filtered requests once (issue 105) Split redirection middleware into status and meta based middlewares (issue 78) Use HTTP1.1 as default downloader handler (issue 109 and issue 318) Support xpath form selection on FormRequest.from_response (issue 185) Bugfix unicode decoding error on SgmlLinkExtractor (issue 199) Bugfix signal dispatching on pypi interpreter (issue 205) Improve request delay and concurrency handling (issue 206) Add RFC2616 cache policy to HttpCacheMiddleware (issue 212) Allow customization of messages logged by engine (issue 214) Multiples improvements to DjangoItem (issue 217, issue 218, issue 221) Extend Scrapy commands using setuptools entry points (issue 260) Allow spider allowed_domains value to be set/tuple (issue 261) Support settings.getdict (issue 269) Simplify internal scrapy.core.scraper slot handling (issue 271) Added Item.copy (issue 290) Collect idle downloader slots (issue 297) Add ftp:// scheme downloader handler (issue 329) Added downloader benchmark webserver and spider tools Benchmarking Moved persistent (on disk) queues to a separate project (queuelib) which Scrapy now depends on Add Scrapy commands using external libraries (issue 260) Added --pdb option to scrapy command line tool Added XPathSelector.remove_namespaces which allows to remove all namespaces from XML documents for convenience (to work with namespace-less XPaths). Documented in Selectors. Several improvements to spider contracts New default middleware named MetaRefreshMiddleware that handles meta-refresh html tag redirections, MetaRefreshMiddleware and RedirectMiddleware have different priorities to address #62 added from_crawler method to spiders added system tests with mock server more improvements to macOS compatibility (thanks Alex Cepoi) several more cleanups to singletons and multi-spider support (thanks Nicolas Ramirez) support custom download slots added –spider option to “shell” command. log overridden settings when Scrapy starts  Thanks to everyone who contribute to this release. Here is a list of contributors sorted by number of commits: 130 Pablo Hoffman <pablo@...>  97 Daniel Graña <dangra@...>  20 Nicolás Ramírez <nramirez.uy@...>  13 Mikhail Korobov <kmike84@...>  12 Pedro Faustino <pedrobandim@...>  11 Steven Almeroth <sroth77@...>   5 Rolando Espinoza La fuente <darkrho@...>   4 Michal Danilak <mimino.coder@...>   4 Alex Cepoi <alex.cepoi@...>   4 Alexandr N Zamaraev (aka tonal) <tonal@...>   3 paul <paul.tremberth@...>   3 Martin Olveyra <molveyra@...>   3 Jordi Llonch <llonchj@...>   3 arijitchakraborty <myself.arijit@...>   2 Shane Evans <shane.evans@...>   2 joehillen <joehillen@...>   2 Hart <HartSimha@...>   2 Dan <ellisd23@...>   1 Zuhao Wan <wanzuhao@...>   1 whodatninja <blake@...>   1 vkrest <v.krestiannykov@...>   1 tpeng <pengtaoo@...>   1 Tom Mortimer-Jones <tom@...>   1 Rocio Aramberri <roschegel@...>   1 Pedro <pedro@...>   1 notsobad <wangxiaohugg@...>   1 Natan L <kuyanatan.nlao@...>   1 Mark Grey <mark.grey@...>   1 Luan <luanpab@...>   1 Libor Nenadál <libor.nenadal@...>   1 Juan M Uys <opyate@...>   1 Jonas Brunsgaard <jonas.brunsgaard@...>   1 Ilya Baryshev <baryshev@...>   1 Hasnain Lakhani <m.hasnain.lakhani@...>   1 Emanuel Schorsch <emschorsch@...>   1 Chris Tilden <chris.tilden@...>   1 Capi Etheriel <barraponto@...>   1 cacovsky <amarquesferraz@...>   1 Berend Iwema <berend@...>     Scrapy 0.16.5 (released 2013-05-30)  obey request method when Scrapy deploy is redirected to a new endpoint (commit 8c4fcee) fix inaccurate downloader middleware documentation. refs #280 (commit 40667cb) doc: remove links to diveintopython.org, which is no longer available. closes #246 (commit bd58bfa) Find form nodes in invalid html5 documents (commit e3d6945) Fix typo labeling attrs type bool instead of list (commit a274276)    Scrapy 0.16.4 (released 2013-01-23)  fixes spelling errors in documentation (commit 6d2b3aa) add doc about disabling an extension. refs #132 (commit c90de33) Fixed error message formatting. log.err() doesn’t support cool formatting and when error occurred, the message was:    “ERROR: Error processing %(item)s” (commit c16150c) lint and improve images pipeline error logging (commit 56b45fc) fixed doc typos (commit 243be84) add documentation topics: Broad Crawls & Common Practices (commit 1fbb715) fix bug in Scrapy parse command when spider is not specified explicitly. closes #209 (commit c72e682) Update docs/topics/commands.rst (commit 28eac7a)    Scrapy 0.16.3 (released 2012-12-07)  Remove concurrency limitation when using download delays and still ensure inter-request delays are enforced (commit 487b9b5) add error details when image pipeline fails (commit 8232569) improve macOS compatibility (commit 8dcf8aa) setup.py: use README.rst to populate long_description (commit 7b5310d) doc: removed obsolete references to ClientForm (commit 80f9bb6) correct docs for default storage backend (commit 2aa491b) doc: removed broken proxyhub link from FAQ (commit bdf61c4) Fixed docs typo in SpiderOpenCloseLogging example (commit 7184094)    Scrapy 0.16.2 (released 2012-11-09)  Scrapy contracts: python2.6 compat (commit a4a9199) Scrapy contracts verbose option (commit ec41673) proper unittest-like output for Scrapy contracts (commit 86635e4) added open_in_browser to debugging doc (commit c9b690d) removed reference to global Scrapy stats from settings doc (commit dd55067) Fix SpiderState bug in Windows platforms (commit 58998f4)    Scrapy 0.16.1 (released 2012-10-26)  fixed LogStats extension, which got broken after a wrong merge before the 0.16 release (commit 8c780fd) better backward compatibility for scrapy.conf.settings (commit 3403089) extended documentation on how to access crawler stats from extensions (commit c4da0b5) removed .hgtags (no longer needed now that Scrapy uses git) (commit d52c188) fix dashes under rst headers (commit fa4f7f9) set release date for 0.16.0 in news (commit e292246)    Scrapy 0.16.0 (released 2012-10-18) Scrapy changes:  added Spiders Contracts, a mechanism for testing spiders in a formal/reproducible way added options -o and -t to the runspider command documented AutoThrottle extension and added to extensions installed by default. You still need to enable it with AUTOTHROTTLE_ENABLED major Stats Collection refactoring: removed separation of global/per-spider stats, removed stats-related signals (stats_spider_opened, etc). Stats are much simpler now, backward compatibility is kept on the Stats Collector API and signals. added process_start_requests() method to spider middlewares dropped Signals singleton. Signals should now be accessed through the Crawler.signals attribute. See the signals documentation for more info. dropped Stats Collector singleton. Stats can now be accessed through the Crawler.stats attribute. See the stats collection documentation for more info. documented Core API lxml is now the default selectors backend instead of libxml2 ported FormRequest.from_response() to use lxml instead of ClientForm removed modules: scrapy.xlib.BeautifulSoup and scrapy.xlib.ClientForm SitemapSpider: added support for sitemap urls ending in .xml and .xml.gz, even if they advertise a wrong content type (commit 10ed28b) StackTraceDump extension: also dump trackref live references (commit fe2ce93) nested items now fully supported in JSON and JSONLines exporters added cookiejar Request meta key to support multiple cookie sessions per spider decoupled encoding detection code to w3lib.encoding, and ported Scrapy code to use that module dropped support for Python 2.5. See https://blog.scrapinghub.com/2012/02/27/scrapy-0-15-dropping-support-for-python-2-5/ dropped support for Twisted 2.5 added REFERER_ENABLED setting, to control referer middleware changed default user agent to: Scrapy/VERSION (+http://scrapy.org) removed (undocumented) HTMLImageLinkExtractor class from scrapy.contrib.linkextractors.image removed per-spider settings (to be replaced by instantiating multiple crawler objects) USER_AGENT spider attribute will no longer work, use user_agent attribute instead DOWNLOAD_TIMEOUT spider attribute will no longer work, use download_timeout attribute instead removed ENCODING_ALIASES setting, as encoding auto-detection has been moved to the w3lib library promoted DjangoItem to main contrib LogFormatter method now return dicts(instead of strings) to support lazy formatting (issue 164, commit dcef7b0) downloader handlers (DOWNLOAD_HANDLERS setting) now receive settings as the first argument of the __init__ method replaced memory usage accounting with (more portable) resource module, removed scrapy.utils.memory module removed signal: scrapy.mail.mail_sent removed TRACK_REFS setting, now trackrefs is always enabled DBM is now the default storage backend for HTTP cache middleware number of log messages (per level) are now tracked through Scrapy stats (stat name: log_count/LEVEL) number received responses are now tracked through Scrapy stats (stat name: response_received_count) removed scrapy.log.started attribute    Scrapy 0.14.4  added precise to supported Ubuntu distros (commit b7e46df) fixed bug in json-rpc webservice reported in https://groups.google.com/forum/#!topic/scrapy-users/qgVBmFybNAQ/discussion. also removed no longer supported ‘run’ command from extras/scrapy-ws.py (commit 340fbdb) meta tag attributes for content-type http equiv can be in any order. #123 (commit 0cb68af) replace “import Image” by more standard “from PIL import Image”. closes #88 (commit 4d17048) return trial status as bin/runtests.sh exit value. #118 (commit b7b2e7f)    Scrapy 0.14.3  forgot to include pydispatch license. #118 (commit fd85f9c) include egg files used by testsuite in source distribution. #118 (commit c897793) update docstring in project template to avoid confusion with genspider command, which may be considered as an advanced feature. refs #107 (commit 2548dcc) added note to docs/topics/firebug.rst about google directory being shut down (commit 668e352) don’t discard slot when empty, just save in another dict in order to recycle if needed again. (commit 8e9f607) do not fail handling unicode xpaths in libxml2 backed selectors (commit b830e95) fixed minor mistake in Request objects documentation (commit bf3c9ee) fixed minor defect in link extractors documentation (commit ba14f38) removed some obsolete remaining code related to sqlite support in Scrapy (commit 0665175)    Scrapy 0.14.2  move buffer pointing to start of file before computing checksum. refs #92 (commit 6a5bef2) Compute image checksum before persisting images. closes #92 (commit 9817df1) remove leaking references in cached failures (commit 673a120) fixed bug in MemoryUsage extension: get_engine_status() takes exactly 1 argument (0 given) (commit 11133e9) fixed struct.error on http compression middleware. closes #87 (commit 1423140) ajax crawling wasn’t expanding for unicode urls (commit 0de3fb4) Catch start_requests iterator errors. refs #83 (commit 454a21d) Speed-up libxml2 XPathSelector (commit 2fbd662) updated versioning doc according to recent changes (commit 0a070f5) scrapyd: fixed documentation link (commit 2b4e4c3) extras/makedeb.py: no longer obtaining version from git (commit caffe0e)    Scrapy 0.14.1  extras/makedeb.py: no longer obtaining version from git (commit caffe0e) bumped version to 0.14.1 (commit 6cb9e1c) fixed reference to tutorial directory (commit 4b86bd6) doc: removed duplicated callback argument from Request.replace() (commit 1aeccdd) fixed formatting of scrapyd doc (commit 8bf19e6) Dump stacks for all running threads and fix engine status dumped by StackTraceDump extension (commit 14a8e6e) added comment about why we disable ssl on boto images upload (commit 5223575) SSL handshaking hangs when doing too many parallel connections to S3 (commit 63d583d) change tutorial to follow changes on dmoz site (commit bcb3198) Avoid _disconnectedDeferred AttributeError exception in Twisted>=11.1.0 (commit 98f3f87) allow spider to set autothrottle max concurrency (commit 175a4b5)    Scrapy 0.14  New features and settings  Support for AJAX crawlable urls New persistent scheduler that stores requests on disk, allowing to suspend and resume crawls (r2737) added -o option to scrapy crawl, a shortcut for dumping scraped items into a file (or standard output using -) Added support for passing custom settings to Scrapyd schedule.json api (r2779, r2783) New ChunkedTransferMiddleware (enabled by default) to support chunked transfer encoding (r2769) Add boto 2.0 support for S3 downloader handler (r2763) Added marshal to formats supported by feed exports (r2744) In request errbacks, offending requests are now received in failure.request attribute (r2738)  Big downloader refactoring to support per domain/ip concurrency limits (r2732)  CONCURRENT_REQUESTS_PER_SPIDER setting has been deprecated and replaced by: CONCURRENT_REQUESTS, CONCURRENT_REQUESTS_PER_DOMAIN, CONCURRENT_REQUESTS_PER_IP     check the documentation for more details     Added builtin caching DNS resolver (r2728) Moved Amazon AWS-related components/extensions (SQS spider queue, SimpleDB stats collector) to a separate project: [scaws](https://github.com/scrapinghub/scaws) (r2706, r2714) Moved spider queues to scrapyd: scrapy.spiderqueue -> scrapyd.spiderqueue (r2708) Moved sqlite utils to scrapyd: scrapy.utils.sqlite -> scrapyd.sqlite (r2781) Real support for returning iterators on start_requests() method. The iterator is now consumed during the crawl when the spider is getting idle (r2704) Added REDIRECT_ENABLED setting to quickly enable/disable the redirect middleware (r2697) Added RETRY_ENABLED setting to quickly enable/disable the retry middleware (r2694) Added CloseSpider exception to manually close spiders (r2691) Improved encoding detection by adding support for HTML5 meta charset declaration (r2690) Refactored close spider behavior to wait for all downloads to finish and be processed by spiders, before closing the spider (r2688) Added SitemapSpider (see documentation in Spiders page) (r2658) Added LogStats extension for periodically logging basic stats (like crawled pages and scraped items) (r2657) Make handling of gzipped responses more robust (#319, r2643). Now Scrapy will try and decompress as much as possible from a gzipped response, instead of failing with an IOError. Simplified !MemoryDebugger extension to use stats for dumping memory debugging info (r2639) Added new command to edit spiders: scrapy edit (r2636) and -e flag to genspider command that uses it (r2653) Changed default representation of items to pretty-printed dicts. (r2631). This improves default logging by making log more readable in the default case, for both Scraped and Dropped lines. Added spider_error signal (r2628) Added COOKIES_ENABLED setting (r2625) Stats are now dumped to Scrapy log (default value of STATS_DUMP setting has been changed to True). This is to make Scrapy users more aware of Scrapy stats and the data that is collected there. Added support for dynamically adjusting download delay and maximum concurrent requests (r2599) Added new DBM HTTP cache storage backend (r2576) Added listjobs.json API to Scrapyd (r2571) CsvItemExporter: added join_multivalued parameter (r2578) Added namespace support to xmliter_lxml (r2552) Improved cookies middleware by making COOKIES_DEBUG nicer and documenting it (r2579) Several improvements to Scrapyd and Link extractors    Code rearranged and removed   Merged item passed and item scraped concepts, as they have often proved confusing in the past. This means: (r2630) original item_scraped signal was removed original item_passed signal was renamed to item_scraped old log lines Scraped Item... were removed old log lines Passed Item... were renamed to Scraped Item... lines and downgraded to DEBUG level      Reduced Scrapy codebase by striping part of Scrapy code into two new libraries: w3lib (several functions from scrapy.utils.{http,markup,multipart,response,url}, done in r2584) scrapely (was scrapy.contrib.ibl, done in r2586)     Removed unused function: scrapy.utils.request.request_info() (r2577) Removed googledir project from examples/googledir. There’s now a new example project called dirbot available on GitHub: https://github.com/scrapy/dirbot Removed support for default field values in Scrapy items (r2616) Removed experimental crawlspider v2 (r2632) Removed scheduler middleware to simplify architecture. Duplicates filter is now done in the scheduler itself, using the same dupe filtering class as before (DUPEFILTER_CLASS setting) (r2640) Removed support for passing urls to scrapy crawl command (use scrapy parse instead) (r2704) Removed deprecated Execution Queue (r2704) Removed (undocumented) spider context extension (from scrapy.contrib.spidercontext) (r2780) removed CONCURRENT_SPIDERS setting (use scrapyd maxproc instead) (r2789) Renamed attributes of core components: downloader.sites -> downloader.slots, scraper.sites -> scraper.slots (r2717, r2718) Renamed setting CLOSESPIDER_ITEMPASSED to CLOSESPIDER_ITEMCOUNT (r2655). Backward compatibility kept.     Scrapy 0.12 The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.  New features and improvements  Passed item is now sent in the item argument of the item_passed (#273) Added verbose option to scrapy version command, useful for bug reports (#298) HTTP cache now stored by default in the project data dir (#279) Added project data storage directory (#276, #277) Documented file structure of Scrapy projects (see command-line tool doc) New lxml backend for XPath selectors (#147) Per-spider settings (#245) Support exit codes to signal errors in Scrapy commands (#248) Added -c argument to scrapy shell command Made libxml2 optional (#260) New deploy command (#261) Added CLOSESPIDER_PAGECOUNT setting (#253) Added CLOSESPIDER_ERRORCOUNT setting (#254)    Scrapyd changes  Scrapyd now uses one process per spider It stores one log file per spider run, and rotate them keeping the latest 5 logs per spider (by default) A minimal web ui was added, available at http://localhost:6800 by default There is now a scrapy server command to start a Scrapyd server of the current project    Changes to settings  added HTTPCACHE_ENABLED setting (False by default) to enable HTTP cache middleware changed HTTPCACHE_EXPIRATION_SECS semantics: now zero means “never expire”.    Deprecated/obsoleted functionality  Deprecated runserver command in favor of server command which starts a Scrapyd server. See also: Scrapyd changes Deprecated queue command in favor of using Scrapyd schedule.json API. See also: Scrapyd changes Removed the !LxmlItemLoader (experimental contrib which never graduated to main contrib)     Scrapy 0.10 The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.  New features and improvements  New Scrapy service called scrapyd for deploying Scrapy crawlers in production (#218) (documentation available) Simplified Images pipeline usage which doesn’t require subclassing your own images pipeline now (#217) Scrapy shell now shows the Scrapy log by default (#206) Refactored execution queue in a common base code and pluggable backends called “spider queues” (#220) New persistent spider queue (based on SQLite) (#198), available by default, which allows to start Scrapy in server mode and then schedule spiders to run. Added documentation for Scrapy command-line tool and all its available sub-commands. (documentation available) Feed exporters with pluggable backends (#197) (documentation available) Deferred signals (#193) Added two new methods to item pipeline open_spider(), close_spider() with deferred support (#195) Support for overriding default request headers per spider (#181) Replaced default Spider Manager with one with similar functionality but not depending on Twisted Plugins (#186) Split Debian package into two packages - the library and the service (#187) Scrapy log refactoring (#188) New extension for keeping persistent spider contexts among different runs (#203) Added dont_redirect request.meta key for avoiding redirects (#233) Added dont_retry request.meta key for avoiding retries (#234)    Command-line tool changes  New scrapy command which replaces the old scrapy-ctl.py (#199) - there is only one global scrapy command now, instead of one scrapy-ctl.py per project - Added scrapy.bat script for running more conveniently from Windows Added bash completion to command-line tool (#210) Renamed command start to runserver (#209)    API changes  url and body attributes of Request objects are now read-only (#230) Request.copy() and Request.replace() now also copies their callback and errback attributes (#231) Removed UrlFilterMiddleware from scrapy.contrib (already disabled by default) Offsite middleware doesn’t filter out any request coming from a spider that doesn’t have a allowed_domains attribute (#225) Removed Spider Manager load() method. Now spiders are loaded in the __init__ method itself.  Changes to Scrapy Manager (now called “Crawler”): scrapy.core.manager.ScrapyManager class renamed to scrapy.crawler.Crawler scrapy.core.manager.scrapymanager singleton moved to scrapy.project.crawler     Moved module: scrapy.contrib.spidermanager to scrapy.spidermanager Spider Manager singleton moved from scrapy.spider.spiders to the spiders` attribute of ``scrapy.project.crawler singleton.  moved Stats Collector classes: (#204) scrapy.stats.collector.StatsCollector to scrapy.statscol.StatsCollector scrapy.stats.collector.SimpledbStatsCollector to scrapy.contrib.statscol.SimpledbStatsCollector     default per-command settings are now specified in the default_settings attribute of command object class (#201)  changed arguments of Item pipeline process_item() method from (spider, item) to (item, spider) backward compatibility kept (with deprecation warning)      moved scrapy.core.signals module to scrapy.signals backward compatibility kept (with deprecation warning)      moved scrapy.core.exceptions module to scrapy.exceptions backward compatibility kept (with deprecation warning)     added handles_request() class method to BaseSpider dropped scrapy.log.exc() function (use scrapy.log.err() instead) dropped component argument of scrapy.log.msg() function dropped scrapy.log.log_level attribute Added from_settings() class methods to Spider Manager, and Item Pipeline Manager    Changes to settings  Added HTTPCACHE_IGNORE_SCHEMES setting to ignore certain schemes on !HttpCacheMiddleware (#225) Added SPIDER_QUEUE_CLASS setting which defines the spider queue to use (#220) Added KEEP_ALIVE setting (#220) Removed SERVICE_QUEUE setting (#220) Removed COMMANDS_SETTINGS_MODULE setting (#201) Renamed REQUEST_HANDLERS to DOWNLOAD_HANDLERS and make download handlers classes (instead of functions)     Scrapy 0.9 The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.  New features and improvements  Added SMTP-AUTH support to scrapy.mail New settings added: MAIL_USER, MAIL_PASS (r2065 | #149) Added new scrapy-ctl view command - To view URL in the browser, as seen by Scrapy (r2039) Added web service for controlling Scrapy process (this also deprecates the web console. (r2053 | #167) Support for running Scrapy as a service, for production systems (r1988, r2054, r2055, r2056, r2057 | #168) Added wrapper induction library (documentation only available in source code for now). (r2011) Simplified and improved response encoding support (r1961, r1969) Added LOG_ENCODING setting (r1956, documentation available) Added RANDOMIZE_DOWNLOAD_DELAY setting (enabled by default) (r1923, doc available) MailSender is no longer IO-blocking (r1955 | #146) Linkextractors and new Crawlspider now handle relative base tag urls (r1960 | #148) Several improvements to Item Loaders and processors (r2022, r2023, r2024, r2025, r2026, r2027, r2028, r2029, r2030) Added support for adding variables to telnet console (r2047 | #165) Support for requests without callbacks (r2050 | #166)    API changes  Change Spider.domain_name to Spider.name (SEP-012, r1975) Response.encoding is now the detected encoding (r1961) HttpErrorMiddleware now returns None or raises an exception (r2006 | #157) scrapy.command modules relocation (r2035, r2036, r2037) Added ExecutionQueue for feeding spiders to scrape (r2034) Removed ExecutionEngine singleton (r2039) Ported S3ImagesStore (images pipeline) to use boto and threads (r2033) Moved module: scrapy.management.telnet to scrapy.telnet (r2047)    Changes to default settings  Changed default SCHEDULER_ORDER to DFO (r1939)     Scrapy 0.8 The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.  New features  Added DEFAULT_RESPONSE_ENCODING setting (r1809) Added dont_click argument to FormRequest.from_response() method (r1813, r1816) Added clickdata argument to FormRequest.from_response() method (r1802, r1803) Added support for HTTP proxies (HttpProxyMiddleware) (r1781, r1785) Offsite spider middleware now logs messages when filtering out requests (r1841)    Backward-incompatible changes  Changed scrapy.utils.response.get_meta_refresh() signature (r1804) Removed deprecated scrapy.item.ScrapedItem class - use scrapy.item.Item instead (r1838) Removed deprecated scrapy.xpath module - use scrapy.selector instead. (r1836) Removed deprecated core.signals.domain_open signal - use core.signals.domain_opened instead (r1822)  log.msg() now receives a spider argument (r1822) Old domain argument has been deprecated and will be removed in 0.9. For spiders, you should always use the spider argument and pass spider references. If you really want to pass a string, use the component argument instead.     Changed core signals domain_opened, domain_closed, domain_idle  Changed Item pipeline to use spiders instead of domains The domain argument of  process_item() item pipeline method was changed to  spider, the new signature is: process_item(spider, item) (r1827 | #105) To quickly port your code (to work with Scrapy 0.8) just use spider.domain_name where you previously used domain.      Changed Stats API to use spiders instead of domains (r1849 | #113) StatsCollector was changed to receive spider references (instead of domains) in its methods (set_value, inc_value, etc). added StatsCollector.iter_spider_stats() method removed StatsCollector.list_domains() method Also, Stats signals were renamed and now pass around spider references (instead of domains). Here’s a summary of the changes: To quickly port your code (to work with Scrapy 0.8) just use spider.domain_name where you previously used domain. spider_stats contains exactly the same data as domain_stats.      CloseDomain extension moved to scrapy.contrib.closespider.CloseSpider (r1833)  Its settings were also renamed: CLOSEDOMAIN_TIMEOUT to CLOSESPIDER_TIMEOUT CLOSEDOMAIN_ITEMCOUNT to CLOSESPIDER_ITEMCOUNT         Removed deprecated SCRAPYSETTINGS_MODULE environment variable - use SCRAPY_SETTINGS_MODULE instead (r1840) Renamed setting: REQUESTS_PER_DOMAIN to CONCURRENT_REQUESTS_PER_SPIDER (r1830, r1844) Renamed setting: CONCURRENT_DOMAINS to CONCURRENT_SPIDERS (r1830) Refactored HTTP Cache middleware HTTP Cache middleware has been heavily refactored, retaining the same functionality except for the domain sectorization which was removed. (r1843 ) Renamed exception: DontCloseDomain to DontCloseSpider (r1859 | #120) Renamed extension: DelayedCloseDomain to SpiderCloseDelay (r1861 | #121) Removed obsolete scrapy.utils.markup.remove_escape_chars function - use scrapy.utils.markup.replace_escape_chars instead (r1865)     Scrapy 0.7 First release of Scrapy.                           ", "code_blocks": ["feedexport/success_count/<storage type>\nfeedexport/failed_count/<storage type>\n</pre>", "urllength/request_ignored_count\n</pre>", "httpcompression/response_bytes\nhttpcompression/response_count\n</pre>", ">>> item = MyItem()\n>>> item[\"field\"] = \"value1\"\n>>> loader = ItemLoader(item=item)\n>>> item[\"field\"]\n['value1']\n</pre>", "for href in response.css('li.page a::attr(href)').extract():\n    url = response.urljoin(href)\n    yield scrapy.Request(url, self.parse, encoding=response.encoding)\n</pre>", "for a in response.css('li.page a'):\n    yield response.follow(a, self.parse)\n</pre>", "class MyItem(scrapy.Item):\n    url = scrapy.Field()\n\nclass MySpider(scrapy.Spider):\n    def parse(self, response):\n        return MyItem(url=response.url)\n</pre>", "class MySpider(scrapy.Spider):\n    def parse(self, response):\n        return {'url': response.url}\n</pre>", "class MySpider(scrapy.Spider):\n    custom_settings = {\n        \"DOWNLOAD_DELAY\": 5.0,\n        \"RETRY_ENABLED\": False,\n    }\n</pre>", "from scrapy import log\nlog.msg('MESSAGE', log.INFO)\n</pre>", "import logging\nlogging.info('MESSAGE')\n</pre>", "class MySpider(scrapy.Spider):\n    def parse(self, response):\n        self.logger.info('Response received')\n</pre>", "from scrapy.crawler import CrawlerProcess\n\nprocess = CrawlerProcess({\n    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n})\nprocess.crawl(MySpider)\nprocess.start()\n</pre>", "69 Daniel Graña <dangra@...>\n37 Pablo Hoffman <pablo@...>\n13 Mikhail Korobov <kmike84@...>\n 9 Alex Cepoi <alex.cepoi@...>\n 9 alexanderlukanin13 <alexander.lukanin.13@...>\n 8 Rolando Espinoza La fuente <darkrho@...>\n 8 Lukasz Biedrycki <lukasz.biedrycki@...>\n 6 Nicolas Ramirez <nramirez.uy@...>\n 3 Paul Tremberth <paul.tremberth@...>\n 2 Martin Olveyra <molveyra@...>\n 2 Stefan <misc@...>\n 2 Rolando Espinoza <darkrho@...>\n 2 Loren Davie <loren@...>\n 2 irgmedeiros <irgmedeiros@...>\n 1 Stefan Koch <taikano@...>\n 1 Stefan <cct@...>\n 1 scraperdragon <dragon@...>\n 1 Kumara Tharmalingam <ktharmal@...>\n 1 Francesco Piccinno <stack.box@...>\n 1 Marcos Campal <duendex@...>\n 1 Dragon Dave <dragon@...>\n 1 Capi Etheriel <barraponto@...>\n 1 cacovsky <amarquesferraz@...>\n 1 Berend Iwema <berend@...>\n</pre>", "130 Pablo Hoffman <pablo@...>\n 97 Daniel Graña <dangra@...>\n 20 Nicolás Ramírez <nramirez.uy@...>\n 13 Mikhail Korobov <kmike84@...>\n 12 Pedro Faustino <pedrobandim@...>\n 11 Steven Almeroth <sroth77@...>\n  5 Rolando Espinoza La fuente <darkrho@...>\n  4 Michal Danilak <mimino.coder@...>\n  4 Alex Cepoi <alex.cepoi@...>\n  4 Alexandr N Zamaraev (aka tonal) <tonal@...>\n  3 paul <paul.tremberth@...>\n  3 Martin Olveyra <molveyra@...>\n  3 Jordi Llonch <llonchj@...>\n  3 arijitchakraborty <myself.arijit@...>\n  2 Shane Evans <shane.evans@...>\n  2 joehillen <joehillen@...>\n  2 Hart <HartSimha@...>\n  2 Dan <ellisd23@...>\n  1 Zuhao Wan <wanzuhao@...>\n  1 whodatninja <blake@...>\n  1 vkrest <v.krestiannykov@...>\n  1 tpeng <pengtaoo@...>\n  1 Tom Mortimer-Jones <tom@...>\n  1 Rocio Aramberri <roschegel@...>\n  1 Pedro <pedro@...>\n  1 notsobad <wangxiaohugg@...>\n  1 Natan L <kuyanatan.nlao@...>\n  1 Mark Grey <mark.grey@...>\n  1 Luan <luanpab@...>\n  1 Libor Nenadál <libor.nenadal@...>\n  1 Juan M Uys <opyate@...>\n  1 Jonas Brunsgaard <jonas.brunsgaard@...>\n  1 Ilya Baryshev <baryshev@...>\n  1 Hasnain Lakhani <m.hasnain.lakhani@...>\n  1 Emanuel Schorsch <emschorsch@...>\n  1 Chris Tilden <chris.tilden@...>\n  1 Capi Etheriel <barraponto@...>\n  1 cacovsky <amarquesferraz@...>\n  1 Berend Iwema <berend@...>\n</pre>"], "links": [{"text": "settings", "href": "topics/settings.html#topics-settings"}, {"text": "from_crawler()", "href": "topics/spiders.html#scrapy.Spider.from_crawler"}, {"text": "spider\narguments", "href": "topics/spiders.html#spiderargs"}, {"text": "scrapy.crawler.Crawler", "href": "topics/api.html#scrapy.crawler.Crawler"}, {"text": "crawl()", "href": "topics/api.html#scrapy.crawler.Crawler.crawl"}, {"text": "Crawler", "href": "topics/api.html#scrapy.crawler.Crawler"}, {"text": "crawl()", "href": "topics/api.html#scrapy.crawler.Crawler.crawl"}, {"text": "issue 6038", "href": "https://github.com/scrapy/scrapy/issues/6038"}, {"text": "scrapy.Spider.from_crawler()", "href": "topics/spiders.html#scrapy.Spider.from_crawler"}, {"text": "scrapy.Spider.from_crawler()", "href": "topics/spiders.html#scrapy.Spider.from_crawler"}, {"text": "start_requests()", "href": "topics/spiders.html#scrapy.Spider.start_requests"}, {"text": "issue 6038", "href": "https://github.com/scrapy/scrapy/issues/6038"}, {"text": "TextResponse.json", "href": "topics/request-response.html#scrapy.http.TextResponse.json"}, {"text": "issue 6016", "href": "https://github.com/scrapy/scrapy/issues/6016"}, {"text": "PythonItemExporter", "href": "topics/exporters.html#scrapy.exporters.PythonItemExporter"}, {"text": "issue 6006", "href": "https://github.com/scrapy/scrapy/issues/6006"}, {"text": "issue 6007", "href": "https://github.com/scrapy/scrapy/issues/6007"}, {"text": "issue 6010", "href": "https://github.com/scrapy/scrapy/issues/6010"}, {"text": "crawl()", "href": "topics/api.html#scrapy.crawler.Crawler.crawl"}, {"text": "scrapy.crawler.Crawler", "href": "topics/api.html#scrapy.crawler.Crawler"}, {"text": "issue 1587", "href": "https://github.com/scrapy/scrapy/issues/1587"}, {"text": "issue 6040", "href": "https://github.com/scrapy/scrapy/issues/6040"}, {"text": "from_crawler()", "href": "topics/spiders.html#scrapy.Spider.from_crawler"}, {"text": "spider\narguments", "href": "topics/spiders.html#spiderargs"}, {"text": "issue 1305", "href": "https://github.com/scrapy/scrapy/issues/1305"}, {"text": "issue 1580", "href": "https://github.com/scrapy/scrapy/issues/1580"}, {"text": "issue 2392", "href": "https://github.com/scrapy/scrapy/issues/2392"}, {"text": "issue 3663", "href": "https://github.com/scrapy/scrapy/issues/3663"}, {"text": "issue 6038", "href": "https://github.com/scrapy/scrapy/issues/6038"}, {"text": "PeriodicLog", "href": "topics/extensions.html#scrapy.extensions.periodic_log.PeriodicLog"}, {"text": "issue 5926", "href": "https://github.com/scrapy/scrapy/issues/5926"}, {"text": "TextResponse.json", "href": "topics/request-response.html#scrapy.http.TextResponse.json"}, {"text": "issue 5968", "href": "https://github.com/scrapy/scrapy/issues/5968"}, {"text": "issue 6016", "href": "https://github.com/scrapy/scrapy/issues/6016"}, {"text": "link extractors", "href": "topics/link-extractors.html#topics-link-extractors"}, {"text": "issue 6021", "href": "https://github.com/scrapy/scrapy/issues/6021"}, {"text": "issue 6036", "href": "https://github.com/scrapy/scrapy/issues/6036"}, {"text": "MailSender", "href": "topics/email.html#scrapy.mail.MailSender"}, {"text": "send()", "href": "topics/email.html#scrapy.mail.MailSender.send"}, {"text": "issue 5096", "href": "https://github.com/scrapy/scrapy/issues/5096"}, {"text": "issue 5118", "href": "https://github.com/scrapy/scrapy/issues/5118"}, {"text": "RetryMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.retry.RetryMiddleware"}, {"text": "issue 6049", "href": "https://github.com/scrapy/scrapy/issues/6049"}, {"text": "issue 6050", "href": "https://github.com/scrapy/scrapy/issues/6050"}, {"text": "scrapy.settings.BaseSettings.getdictorlist()", "href": "topics/api.html#scrapy.settings.BaseSettings.getdictorlist"}, {"text": "FEED_EXPORT_FIELDS", "href": "topics/feed-exports.html#std-setting-FEED_EXPORT_FIELDS"}, {"text": "issue 6011", "href": "https://github.com/scrapy/scrapy/issues/6011"}, {"text": "issue 6013", "href": "https://github.com/scrapy/scrapy/issues/6013"}, {"text": "issue 6014", "href": "https://github.com/scrapy/scrapy/issues/6014"}, {"text": "issue 6008", "href": "https://github.com/scrapy/scrapy/issues/6008"}, {"text": "issue 6009", "href": "https://github.com/scrapy/scrapy/issues/6009"}, {"text": "issue 6003", "href": "https://github.com/scrapy/scrapy/issues/6003"}, {"text": "issue 6005", "href": "https://github.com/scrapy/scrapy/issues/6005"}, {"text": "issue 6031", "href": "https://github.com/scrapy/scrapy/issues/6031"}, {"text": "issue 6034", "href": "https://github.com/scrapy/scrapy/issues/6034"}, {"text": "brotli", "href": "https://github.com/google/brotli"}, {"text": "issue 6044", "href": "https://github.com/scrapy/scrapy/issues/6044"}, {"text": "issue 6045", "href": "https://github.com/scrapy/scrapy/issues/6045"}, {"text": "issue 6002", "href": "https://github.com/scrapy/scrapy/issues/6002"}, {"text": "issue 6013", "href": "https://github.com/scrapy/scrapy/issues/6013"}, {"text": "issue 6046", "href": "https://github.com/scrapy/scrapy/issues/6046"}, {"text": "issue 6024", "href": "https://github.com/scrapy/scrapy/issues/6024"}, {"text": "issue 6026", "href": "https://github.com/scrapy/scrapy/issues/6026"}, {"text": "issue 5953", "href": "https://github.com/scrapy/scrapy/issues/5953"}, {"text": "issue 5984", "href": "https://github.com/scrapy/scrapy/issues/5984"}, {"text": "lxml", "href": "https://lxml.de/"}, {"text": "cryptography", "href": "https://cryptography.io/en/latest/"}, {"text": "issue 5956", "href": "https://github.com/scrapy/scrapy/issues/5956"}, {"text": "issue 5958", "href": "https://github.com/scrapy/scrapy/issues/5958"}, {"text": "boto3", "href": "https://github.com/boto/boto3"}, {"text": "botocore", "href": "https://github.com/boto/botocore"}, {"text": "issue 5833", "href": "https://github.com/scrapy/scrapy/issues/5833"}, {"text": "FEED_STORE_EMPTY", "href": "topics/feed-exports.html#std-setting-FEED_STORE_EMPTY"}, {"text": "issue 872", "href": "https://github.com/scrapy/scrapy/issues/872"}, {"text": "issue 5847", "href": "https://github.com/scrapy/scrapy/issues/5847"}, {"text": "FEED_URI_PARAMS", "href": "topics/feed-exports.html#std-setting-FEED_URI_PARAMS"}, {"text": "issue 5994", "href": "https://github.com/scrapy/scrapy/issues/5994"}, {"text": "issue 5996", "href": "https://github.com/scrapy/scrapy/issues/5996"}, {"text": "issue 5994", "href": "https://github.com/scrapy/scrapy/issues/5994"}, {"text": "issue 5996", "href": "https://github.com/scrapy/scrapy/issues/5996"}, {"text": "issue 5994", "href": "https://github.com/scrapy/scrapy/issues/5994"}, {"text": "issue 5996", "href": "https://github.com/scrapy/scrapy/issues/5996"}, {"text": "issue 5994", "href": "https://github.com/scrapy/scrapy/issues/5994"}, {"text": "issue 5998", "href": "https://github.com/scrapy/scrapy/issues/5998"}, {"text": "issue 5994", "href": "https://github.com/scrapy/scrapy/issues/5994"}, {"text": "issue 5998", "href": "https://github.com/scrapy/scrapy/issues/5998"}, {"text": "issue 5146", "href": "https://github.com/scrapy/scrapy/issues/5146"}, {"text": "scrapy.settings.BaseSettings.getwithbase()", "href": "topics/api.html#scrapy.settings.BaseSettings.getwithbase"}, {"text": "issue 5726", "href": "https://github.com/scrapy/scrapy/issues/5726"}, {"text": "issue 5923", "href": "https://github.com/scrapy/scrapy/issues/5923"}, {"text": "Scrapy add-ons", "href": "topics/addons.html#topics-addons"}, {"text": "issue 5950", "href": "https://github.com/scrapy/scrapy/issues/5950"}, {"text": "RETRY_EXCEPTIONS", "href": "topics/downloader-middleware.html#std-setting-RETRY_EXCEPTIONS"}, {"text": "RetryMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.retry.RetryMiddleware"}, {"text": "issue 2701", "href": "https://github.com/scrapy/scrapy/issues/2701"}, {"text": "issue 5929", "href": "https://github.com/scrapy/scrapy/issues/5929"}, {"text": "CLOSESPIDER_TIMEOUT_NO_ITEM", "href": "topics/extensions.html#std-setting-CLOSESPIDER_TIMEOUT_NO_ITEM"}, {"text": "issue 5979", "href": "https://github.com/scrapy/scrapy/issues/5979"}, {"text": "AWS_REGION_NAME", "href": "topics/settings.html#std-setting-AWS_REGION_NAME"}, {"text": "issue 5980", "href": "https://github.com/scrapy/scrapy/issues/5980"}, {"text": "pathlib.Path", "href": "https://docs.python.org/3/library/pathlib.html#pathlib.Path"}, {"text": "FEEDS", "href": "topics/feed-exports.html#std-setting-FEEDS"}, {"text": "issue 5939", "href": "https://github.com/scrapy/scrapy/issues/5939"}, {"text": "issue 872", "href": "https://github.com/scrapy/scrapy/issues/872"}, {"text": "issue 5847", "href": "https://github.com/scrapy/scrapy/issues/5847"}, {"text": "issue 5969", "href": "https://github.com/scrapy/scrapy/issues/5969"}, {"text": "issue 5971", "href": "https://github.com/scrapy/scrapy/issues/5971"}, {"text": "boto3", "href": "https://github.com/boto/boto3"}, {"text": "issue 960", "href": "https://github.com/scrapy/scrapy/issues/960"}, {"text": "issue 5735", "href": "https://github.com/scrapy/scrapy/issues/5735"}, {"text": "issue 5833", "href": "https://github.com/scrapy/scrapy/issues/5833"}, {"text": "issue 3090", "href": "https://github.com/scrapy/scrapy/issues/3090"}, {"text": "issue 5952", "href": "https://github.com/scrapy/scrapy/issues/5952"}, {"text": "issue 5043", "href": "https://github.com/scrapy/scrapy/issues/5043"}, {"text": "issue 5705", "href": "https://github.com/scrapy/scrapy/issues/5705"}, {"text": "NotConfigured", "href": "topics/exceptions.html#scrapy.exceptions.NotConfigured"}, {"text": "issue 5950", "href": "https://github.com/scrapy/scrapy/issues/5950"}, {"text": "issue 5992", "href": "https://github.com/scrapy/scrapy/issues/5992"}, {"text": "scrapy.settings.BaseSettings.pop()", "href": "topics/api.html#scrapy.settings.BaseSettings.pop"}, {"text": "issue 5959", "href": "https://github.com/scrapy/scrapy/issues/5959"}, {"text": "issue 5960", "href": "https://github.com/scrapy/scrapy/issues/5960"}, {"text": "issue 5963", "href": "https://github.com/scrapy/scrapy/issues/5963"}, {"text": "issue 5146", "href": "https://github.com/scrapy/scrapy/issues/5146"}, {"text": "scrapy.Spider.update_settings()", "href": "topics/spiders.html#scrapy.Spider.update_settings"}, {"text": "issue 5745", "href": "https://github.com/scrapy/scrapy/issues/5745"}, {"text": "issue 5846", "href": "https://github.com/scrapy/scrapy/issues/5846"}, {"text": "issue 5981", "href": "https://github.com/scrapy/scrapy/issues/5981"}, {"text": "issue 6000", "href": "https://github.com/scrapy/scrapy/issues/6000"}, {"text": "issue 5927", "href": "https://github.com/scrapy/scrapy/issues/5927"}, {"text": "issue 5579", "href": "https://github.com/scrapy/scrapy/issues/5579"}, {"text": "issue 5931", "href": "https://github.com/scrapy/scrapy/issues/5931"}, {"text": "issue 5707", "href": "https://github.com/scrapy/scrapy/issues/5707"}, {"text": "issue 5937", "href": "https://github.com/scrapy/scrapy/issues/5937"}, {"text": "issue 4914", "href": "https://github.com/scrapy/scrapy/issues/4914"}, {"text": "issue 5949", "href": "https://github.com/scrapy/scrapy/issues/5949"}, {"text": "issue 5925", "href": "https://github.com/scrapy/scrapy/issues/5925"}, {"text": "issue 5977", "href": "https://github.com/scrapy/scrapy/issues/5977"}, {"text": "issue 5951", "href": "https://github.com/scrapy/scrapy/issues/5951"}, {"text": "issue 5847", "href": "https://github.com/scrapy/scrapy/issues/5847"}, {"text": "issue 5999", "href": "https://github.com/scrapy/scrapy/issues/5999"}, {"text": "issue 5984", "href": "https://github.com/scrapy/scrapy/issues/5984"}, {"text": "issue 5948", "href": "https://github.com/scrapy/scrapy/issues/5948"}, {"text": "issue 5965", "href": "https://github.com/scrapy/scrapy/issues/5965"}, {"text": "issue 5986", "href": "https://github.com/scrapy/scrapy/issues/5986"}, {"text": "cryptography", "href": "https://cryptography.io/en/latest/"}, {"text": "parsel", "href": "https://github.com/scrapy/parsel"}, {"text": "parsel", "href": "https://github.com/scrapy/parsel"}, {"text": "issue 5876", "href": "https://github.com/scrapy/scrapy/issues/5876"}, {"text": "DOWNLOAD_DELAY", "href": "topics/settings.html#std-setting-DOWNLOAD_DELAY"}, {"text": "CONCURRENT_REQUESTS_PER_DOMAIN", "href": "topics/settings.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"}, {"text": "RANDOMIZE_DOWNLOAD_DELAY", "href": "topics/settings.html#std-setting-RANDOMIZE_DOWNLOAD_DELAY"}, {"text": "DOWNLOAD_SLOTS", "href": "topics/settings.html#std-setting-DOWNLOAD_SLOTS"}, {"text": "issue 5328", "href": "https://github.com/scrapy/scrapy/issues/5328"}, {"text": "parsel", "href": "https://github.com/scrapy/parsel"}, {"text": "issue 5894", "href": "https://github.com/scrapy/scrapy/issues/5894"}, {"text": "issue 5915", "href": "https://github.com/scrapy/scrapy/issues/5915"}, {"text": "feed_slot_closed", "href": "topics/signals.html#std-signal-feed_slot_closed"}, {"text": "feed_exporter_closed", "href": "topics/signals.html#std-signal-feed_exporter_closed"}, {"text": "issue 5876", "href": "https://github.com/scrapy/scrapy/issues/5876"}, {"text": "issue 5892", "href": "https://github.com/scrapy/scrapy/issues/5892"}, {"text": "FILES_STORE", "href": "topics/media-pipeline.html#std-setting-FILES_STORE"}, {"text": "IMAGES_STORE", "href": "topics/media-pipeline.html#std-setting-IMAGES_STORE"}, {"text": "pathlib.Path", "href": "https://docs.python.org/3/library/pathlib.html#pathlib.Path"}, {"text": "issue 5801", "href": "https://github.com/scrapy/scrapy/issues/5801"}, {"text": "issue 5903", "href": "https://github.com/scrapy/scrapy/issues/5903"}, {"text": "issue 5918", "href": "https://github.com/scrapy/scrapy/issues/5918"}, {"text": "issue 5500", "href": "https://github.com/scrapy/scrapy/issues/5500"}, {"text": "issue 5581", "href": "https://github.com/scrapy/scrapy/issues/5581"}, {"text": "scrapy.settings.BaseSettings.setdefault()", "href": "topics/api.html#scrapy.settings.BaseSettings.setdefault"}, {"text": "issue 5811", "href": "https://github.com/scrapy/scrapy/issues/5811"}, {"text": "issue 5821", "href": "https://github.com/scrapy/scrapy/issues/5821"}, {"text": "cryptography", "href": "https://cryptography.io/en/latest/"}, {"text": "DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING", "href": "topics/settings.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING"}, {"text": "issue 5857", "href": "https://github.com/scrapy/scrapy/issues/5857"}, {"text": "issue 5858", "href": "https://github.com/scrapy/scrapy/issues/5858"}, {"text": "FilesPipeline", "href": "topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline"}, {"text": "issue 5874", "href": "https://github.com/scrapy/scrapy/issues/5874"}, {"text": "issue 5891", "href": "https://github.com/scrapy/scrapy/issues/5891"}, {"text": "issue 5899", "href": "https://github.com/scrapy/scrapy/issues/5899"}, {"text": "issue 5901", "href": "https://github.com/scrapy/scrapy/issues/5901"}, {"text": "parse", "href": "topics/commands.html#std-command-parse"}, {"text": "issue 5819", "href": "https://github.com/scrapy/scrapy/issues/5819"}, {"text": "issue 5824", "href": "https://github.com/scrapy/scrapy/issues/5824"}, {"text": "genspider", "href": "topics/commands.html#std-command-genspider"}, {"text": "issue 3553", "href": "https://github.com/scrapy/scrapy/issues/3553"}, {"text": "issue 5808", "href": "https://github.com/scrapy/scrapy/issues/5808"}, {"text": "issue 5831", "href": "https://github.com/scrapy/scrapy/issues/5831"}, {"text": "issue 5832", "href": "https://github.com/scrapy/scrapy/issues/5832"}, {"text": "LinkExtractor", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"}, {"text": "issue 5881", "href": "https://github.com/scrapy/scrapy/issues/5881"}, {"text": "issue 5872", "href": "https://github.com/scrapy/scrapy/issues/5872"}, {"text": "issue 5885", "href": "https://github.com/scrapy/scrapy/issues/5885"}, {"text": "issue 5914", "href": "https://github.com/scrapy/scrapy/issues/5914"}, {"text": "issue 5917", "href": "https://github.com/scrapy/scrapy/issues/5917"}, {"text": "scrapy.mail.MailSender.send()", "href": "topics/email.html#scrapy.mail.MailSender.send"}, {"text": "issue 1611", "href": "https://github.com/scrapy/scrapy/issues/1611"}, {"text": "issue 5880", "href": "https://github.com/scrapy/scrapy/issues/5880"}, {"text": "issue 5109", "href": "https://github.com/scrapy/scrapy/issues/5109"}, {"text": "issue 5851", "href": "https://github.com/scrapy/scrapy/issues/5851"}, {"text": "blacken-docs", "href": "https://github.com/adamchainz/blacken-docs"}, {"text": "issue 5813", "href": "https://github.com/scrapy/scrapy/issues/5813"}, {"text": "issue 5816", "href": "https://github.com/scrapy/scrapy/issues/5816"}, {"text": "issue 5875", "href": "https://github.com/scrapy/scrapy/issues/5875"}, {"text": "issue 5877", "href": "https://github.com/scrapy/scrapy/issues/5877"}, {"text": "issue 5878", "href": "https://github.com/scrapy/scrapy/issues/5878"}, {"text": "issue 5879", "href": "https://github.com/scrapy/scrapy/issues/5879"}, {"text": "issue 5827", "href": "https://github.com/scrapy/scrapy/issues/5827"}, {"text": "issue 5839", "href": "https://github.com/scrapy/scrapy/issues/5839"}, {"text": "issue 5883", "href": "https://github.com/scrapy/scrapy/issues/5883"}, {"text": "issue 5890", "href": "https://github.com/scrapy/scrapy/issues/5890"}, {"text": "issue 5895", "href": "https://github.com/scrapy/scrapy/issues/5895"}, {"text": "issue 5904", "href": "https://github.com/scrapy/scrapy/issues/5904"}, {"text": "issue 5805", "href": "https://github.com/scrapy/scrapy/issues/5805"}, {"text": "issue 5889", "href": "https://github.com/scrapy/scrapy/issues/5889"}, {"text": "issue 5896", "href": "https://github.com/scrapy/scrapy/issues/5896"}, {"text": "issue 5816", "href": "https://github.com/scrapy/scrapy/issues/5816"}, {"text": "issue 5826", "href": "https://github.com/scrapy/scrapy/issues/5826"}, {"text": "issue 5919", "href": "https://github.com/scrapy/scrapy/issues/5919"}, {"text": "issue 5849", "href": "https://github.com/scrapy/scrapy/issues/5849"}, {"text": "issue 5820", "href": "https://github.com/scrapy/scrapy/issues/5820"}, {"text": "issue 5855", "href": "https://github.com/scrapy/scrapy/issues/5855"}, {"text": "issue 5898", "href": "https://github.com/scrapy/scrapy/issues/5898"}, {"text": "parsel", "href": "https://github.com/scrapy/parsel"}, {"text": "issue 5902", "href": "https://github.com/scrapy/scrapy/issues/5902"}, {"text": "issue 5919", "href": "https://github.com/scrapy/scrapy/issues/5919"}, {"text": "issue 5802", "href": "https://github.com/scrapy/scrapy/issues/5802"}, {"text": "issue 5823", "href": "https://github.com/scrapy/scrapy/issues/5823"}, {"text": "issue 5908", "href": "https://github.com/scrapy/scrapy/issues/5908"}, {"text": "read1()", "href": "https://docs.python.org/3/library/io.html#io.BufferedIOBase.read1"}, {"text": "GzipFile", "href": "https://docs.python.org/3/library/gzip.html#gzip.GzipFile"}, {"text": "issue 5719", "href": "https://github.com/scrapy/scrapy/issues/5719"}, {"text": "issue 5719", "href": "https://github.com/scrapy/scrapy/issues/5719"}, {"text": "issue 5719", "href": "https://github.com/scrapy/scrapy/issues/5719"}, {"text": "LinkExtractor", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"}, {"text": "issue 5720", "href": "https://github.com/scrapy/scrapy/issues/5720"}, {"text": "issue 5724", "href": "https://github.com/scrapy/scrapy/issues/5724"}, {"text": "issue 5731", "href": "https://github.com/scrapy/scrapy/issues/5731"}, {"text": "issue 5719", "href": "https://github.com/scrapy/scrapy/issues/5719"}, {"text": "issue 5719", "href": "https://github.com/scrapy/scrapy/issues/5719"}, {"text": "issue 5719", "href": "https://github.com/scrapy/scrapy/issues/5719"}, {"text": "issue 5368", "href": "https://github.com/scrapy/scrapy/issues/5368"}, {"text": "issue 5489", "href": "https://github.com/scrapy/scrapy/issues/5489"}, {"text": "issue 3055", "href": "https://github.com/scrapy/scrapy/issues/3055"}, {"text": "issue 3689", "href": "https://github.com/scrapy/scrapy/issues/3689"}, {"text": "issue 4753", "href": "https://github.com/scrapy/scrapy/issues/4753"}, {"text": "black", "href": "https://black.readthedocs.io/en/stable/"}, {"text": "genspider", "href": "topics/commands.html#std-command-genspider"}, {"text": "startproject", "href": "topics/commands.html#std-command-startproject"}, {"text": "issue 5809", "href": "https://github.com/scrapy/scrapy/issues/5809"}, {"text": "issue 5814", "href": "https://github.com/scrapy/scrapy/issues/5814"}, {"text": "FEED_EXPORT_ENCODING", "href": "topics/feed-exports.html#std-setting-FEED_EXPORT_ENCODING"}, {"text": "startproject", "href": "topics/commands.html#std-command-startproject"}, {"text": "issue 5797", "href": "https://github.com/scrapy/scrapy/issues/5797"}, {"text": "issue 5800", "href": "https://github.com/scrapy/scrapy/issues/5800"}, {"text": "MemoryUsage", "href": "topics/extensions.html#scrapy.extensions.memusage.MemoryUsage"}, {"text": "issue 5717", "href": "https://github.com/scrapy/scrapy/issues/5717"}, {"text": "issue 5722", "href": "https://github.com/scrapy/scrapy/issues/5722"}, {"text": "issue 5727", "href": "https://github.com/scrapy/scrapy/issues/5727"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "scrapy.http.request.NO_CALLBACK()", "href": "topics/request-response.html#scrapy.http.request.NO_CALLBACK"}, {"text": "parse()", "href": "topics/spiders.html#scrapy.Spider.parse"}, {"text": "issue 5798", "href": "https://github.com/scrapy/scrapy/issues/5798"}, {"text": "issue 5491", "href": "https://github.com/scrapy/scrapy/issues/5491"}, {"text": "issue 5790", "href": "https://github.com/scrapy/scrapy/issues/5790"}, {"text": "issue 5386", "href": "https://github.com/scrapy/scrapy/issues/5386"}, {"text": "issue 5406", "href": "https://github.com/scrapy/scrapy/issues/5406"}, {"text": "item exporters", "href": "topics/exporters.html#topics-exporters"}, {"text": "issue 5537", "href": "https://github.com/scrapy/scrapy/issues/5537"}, {"text": "issue 5758", "href": "https://github.com/scrapy/scrapy/issues/5758"}, {"text": "issue 5777", "href": "https://github.com/scrapy/scrapy/issues/5777"}, {"text": "shell", "href": "topics/commands.html#std-command-shell"}, {"text": "using asyncio", "href": "topics/asyncio.html#using-asyncio"}, {"text": "issue 5740", "href": "https://github.com/scrapy/scrapy/issues/5740"}, {"text": "issue 5742", "href": "https://github.com/scrapy/scrapy/issues/5742"}, {"text": "issue 5748", "href": "https://github.com/scrapy/scrapy/issues/5748"}, {"text": "issue 5759", "href": "https://github.com/scrapy/scrapy/issues/5759"}, {"text": "issue 5760", "href": "https://github.com/scrapy/scrapy/issues/5760"}, {"text": "issue 5771", "href": "https://github.com/scrapy/scrapy/issues/5771"}, {"text": "CrawlSpider", "href": "topics/spiders.html#scrapy.spiders.CrawlSpider"}, {"text": "Rule", "href": "topics/spiders.html#scrapy.spiders.Rule"}, {"text": "issue 5699", "href": "https://github.com/scrapy/scrapy/issues/5699"}, {"text": "images pipeline", "href": "topics/media-pipeline.html#images-pipeline"}, {"text": "issue 3055", "href": "https://github.com/scrapy/scrapy/issues/3055"}, {"text": "issue 3689", "href": "https://github.com/scrapy/scrapy/issues/3689"}, {"text": "issue 4753", "href": "https://github.com/scrapy/scrapy/issues/4753"}, {"text": "images pipeline", "href": "topics/media-pipeline.html#images-pipeline"}, {"text": "issue 3072", "href": "https://github.com/scrapy/scrapy/issues/3072"}, {"text": "issue 5766", "href": "https://github.com/scrapy/scrapy/issues/5766"}, {"text": "issue 5767", "href": "https://github.com/scrapy/scrapy/issues/5767"}, {"text": "issue 2918", "href": "https://github.com/scrapy/scrapy/issues/2918"}, {"text": "LinkExtractor", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"}, {"text": "issue 3798", "href": "https://github.com/scrapy/scrapy/issues/3798"}, {"text": "issue 3799", "href": "https://github.com/scrapy/scrapy/issues/3799"}, {"text": "issue 4695", "href": "https://github.com/scrapy/scrapy/issues/4695"}, {"text": "issue 5458", "href": "https://github.com/scrapy/scrapy/issues/5458"}, {"text": "RobotsTxtMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware"}, {"text": "issue 5807", "href": "https://github.com/scrapy/scrapy/issues/5807"}, {"text": "issue 5753", "href": "https://github.com/scrapy/scrapy/issues/5753"}, {"text": "issue 5754", "href": "https://github.com/scrapy/scrapy/issues/5754"}, {"text": "issue 5709", "href": "https://github.com/scrapy/scrapy/issues/5709"}, {"text": "issue 5711", "href": "https://github.com/scrapy/scrapy/issues/5711"}, {"text": "issue 5712", "href": "https://github.com/scrapy/scrapy/issues/5712"}, {"text": "commands", "href": "topics/commands.html#topics-commands"}, {"text": "issue 5715", "href": "https://github.com/scrapy/scrapy/issues/5715"}, {"text": "debug spiders from Visual Studio Code", "href": "topics/debug.html#debug-vscode"}, {"text": "issue 5721", "href": "https://github.com/scrapy/scrapy/issues/5721"}, {"text": "DOWNLOAD_DELAY", "href": "topics/settings.html#std-setting-DOWNLOAD_DELAY"}, {"text": "issue 5083", "href": "https://github.com/scrapy/scrapy/issues/5083"}, {"text": "issue 5540", "href": "https://github.com/scrapy/scrapy/issues/5540"}, {"text": "issue 5761", "href": "https://github.com/scrapy/scrapy/issues/5761"}, {"text": "issue 5714", "href": "https://github.com/scrapy/scrapy/issues/5714"}, {"text": "issue 5744", "href": "https://github.com/scrapy/scrapy/issues/5744"}, {"text": "issue 5764", "href": "https://github.com/scrapy/scrapy/issues/5764"}, {"text": "black coding style", "href": "contributing.html#coding-style"}, {"text": "pre-commit", "href": "contributing.html#scrapy-pre-commit"}, {"text": "issue 4654", "href": "https://github.com/scrapy/scrapy/issues/4654"}, {"text": "issue 4658", "href": "https://github.com/scrapy/scrapy/issues/4658"}, {"text": "issue 5734", "href": "https://github.com/scrapy/scrapy/issues/5734"}, {"text": "issue 5737", "href": "https://github.com/scrapy/scrapy/issues/5737"}, {"text": "issue 5806", "href": "https://github.com/scrapy/scrapy/issues/5806"}, {"text": "issue 5810", "href": "https://github.com/scrapy/scrapy/issues/5810"}, {"text": "os.path", "href": "https://docs.python.org/3/library/os.path.html#module-os.path"}, {"text": "pathlib", "href": "https://docs.python.org/3/library/pathlib.html#module-pathlib"}, {"text": "issue 4916", "href": "https://github.com/scrapy/scrapy/issues/4916"}, {"text": "issue 4497", "href": "https://github.com/scrapy/scrapy/issues/4497"}, {"text": "issue 5682", "href": "https://github.com/scrapy/scrapy/issues/5682"}, {"text": "issue 5677", "href": "https://github.com/scrapy/scrapy/issues/5677"}, {"text": "issue 5736", "href": "https://github.com/scrapy/scrapy/issues/5736"}, {"text": "issue 5768", "href": "https://github.com/scrapy/scrapy/issues/5768"}, {"text": "issue 5774", "href": "https://github.com/scrapy/scrapy/issues/5774"}, {"text": "issue 5776", "href": "https://github.com/scrapy/scrapy/issues/5776"}, {"text": "OrderedDict", "href": "https://docs.python.org/3/library/collections.html#collections.OrderedDict"}, {"text": "issue 5795", "href": "https://github.com/scrapy/scrapy/issues/5795"}, {"text": "issue 5150", "href": "https://github.com/scrapy/scrapy/issues/5150"}, {"text": "issue 5725", "href": "https://github.com/scrapy/scrapy/issues/5725"}, {"text": "issue 5729", "href": "https://github.com/scrapy/scrapy/issues/5729"}, {"text": "issue 5730", "href": "https://github.com/scrapy/scrapy/issues/5730"}, {"text": "issue 5732", "href": "https://github.com/scrapy/scrapy/issues/5732"}, {"text": "issue 5749", "href": "https://github.com/scrapy/scrapy/issues/5749"}, {"text": "issue 5750", "href": "https://github.com/scrapy/scrapy/issues/5750"}, {"text": "issue 5756", "href": "https://github.com/scrapy/scrapy/issues/5756"}, {"text": "issue 5762", "href": "https://github.com/scrapy/scrapy/issues/5762"}, {"text": "issue 5765", "href": "https://github.com/scrapy/scrapy/issues/5765"}, {"text": "issue 5780", "href": "https://github.com/scrapy/scrapy/issues/5780"}, {"text": "issue 5781", "href": "https://github.com/scrapy/scrapy/issues/5781"}, {"text": "issue 5782", "href": "https://github.com/scrapy/scrapy/issues/5782"}, {"text": "issue 5783", "href": "https://github.com/scrapy/scrapy/issues/5783"}, {"text": "issue 5785", "href": "https://github.com/scrapy/scrapy/issues/5785"}, {"text": "issue 5786", "href": "https://github.com/scrapy/scrapy/issues/5786"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "issue 5626", "href": "https://github.com/scrapy/scrapy/issues/5626"}, {"text": "issue 5516", "href": "https://github.com/scrapy/scrapy/issues/5516"}, {"text": "issue 5605", "href": "https://github.com/scrapy/scrapy/issues/5605"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "issue 5685", "href": "https://github.com/scrapy/scrapy/issues/5685"}, {"text": "issue 5689", "href": "https://github.com/scrapy/scrapy/issues/5689"}, {"text": "issue 5588", "href": "https://github.com/scrapy/scrapy/issues/5588"}, {"text": "issue 5589", "href": "https://github.com/scrapy/scrapy/issues/5589"}, {"text": "issue 5684", "href": "https://github.com/scrapy/scrapy/issues/5684"}, {"text": "issue 5692", "href": "https://github.com/scrapy/scrapy/issues/5692"}, {"text": "issue 5323", "href": "https://github.com/scrapy/scrapy/issues/5323"}, {"text": "issue 5592", "href": "https://github.com/scrapy/scrapy/issues/5592"}, {"text": "issue 5599", "href": "https://github.com/scrapy/scrapy/issues/5599"}, {"text": "issue 5691", "href": "https://github.com/scrapy/scrapy/issues/5691"}, {"text": "issue 5698", "href": "https://github.com/scrapy/scrapy/issues/5698"}, {"text": "issue 5681", "href": "https://github.com/scrapy/scrapy/issues/5681"}, {"text": "issue 5694", "href": "https://github.com/scrapy/scrapy/issues/5694"}, {"text": "issue 5688", "href": "https://github.com/scrapy/scrapy/issues/5688"}, {"text": "typing", "href": "https://docs.python.org/3/library/typing.html#module-typing"}, {"text": "issue 5686", "href": "https://github.com/scrapy/scrapy/issues/5686"}, {"text": "issue 5697", "href": "https://github.com/scrapy/scrapy/issues/5697"}, {"text": "issue 5695", "href": "https://github.com/scrapy/scrapy/issues/5695"}, {"text": "issue 5696", "href": "https://github.com/scrapy/scrapy/issues/5696"}, {"text": "asynchronous callbacks", "href": "topics/coroutines.html#topics-coroutines"}, {"text": "Asyncio support", "href": "topics/asyncio.html#using-asyncio"}, {"text": "request fingerprinting", "href": "topics/request-response.html#request-fingerprints"}, {"text": "lxml", "href": "https://lxml.de/"}, {"text": "Pillow", "href": "https://python-pillow.org/"}, {"text": "images pipeline", "href": "topics/media-pipeline.html#images-pipeline"}, {"text": "zope.interface", "href": "https://zopeinterface.readthedocs.io/en/latest/"}, {"text": "issue 5512", "href": "https://github.com/scrapy/scrapy/issues/5512"}, {"text": "issue 5514", "href": "https://github.com/scrapy/scrapy/issues/5514"}, {"text": "issue 5524", "href": "https://github.com/scrapy/scrapy/issues/5524"}, {"text": "issue 5563", "href": "https://github.com/scrapy/scrapy/issues/5563"}, {"text": "issue 5664", "href": "https://github.com/scrapy/scrapy/issues/5664"}, {"text": "issue 5670", "href": "https://github.com/scrapy/scrapy/issues/5670"}, {"text": "issue 5678", "href": "https://github.com/scrapy/scrapy/issues/5678"}, {"text": "ImagesPipeline.thumb_path", "href": "topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline.thumb_path"}, {"text": "issue 5504", "href": "https://github.com/scrapy/scrapy/issues/5504"}, {"text": "issue 5508", "href": "https://github.com/scrapy/scrapy/issues/5508"}, {"text": "issue 5546", "href": "https://github.com/scrapy/scrapy/issues/5546"}, {"text": "issue 5547", "href": "https://github.com/scrapy/scrapy/issues/5547"}, {"text": "process_spider_output()", "href": "topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"}, {"text": "spider middlewares", "href": "topics/spider-middleware.html#topics-spider-middleware"}, {"text": "asynchronous generator", "href": "https://docs.python.org/3/glossary.html#term-asynchronous-generator"}, {"text": "issue 4978", "href": "https://github.com/scrapy/scrapy/issues/4978"}, {"text": "coroutines", "href": "topics/coroutines.html#topics-coroutines"}, {"text": "issue 4978", "href": "https://github.com/scrapy/scrapy/issues/4978"}, {"text": "asynchronous\ncallbacks", "href": "topics/coroutines.html#topics-coroutines"}, {"text": "issue 5657", "href": "https://github.com/scrapy/scrapy/issues/5657"}, {"text": "startproject", "href": "topics/commands.html#std-command-startproject"}, {"text": "asyncio support", "href": "topics/asyncio.html#using-asyncio"}, {"text": "issue 5590", "href": "https://github.com/scrapy/scrapy/issues/5590"}, {"text": "issue 5679", "href": "https://github.com/scrapy/scrapy/issues/5679"}, {"text": "FEED_EXPORT_FIELDS", "href": "topics/feed-exports.html#std-setting-FEED_EXPORT_FIELDS"}, {"text": "issue 1008", "href": "https://github.com/scrapy/scrapy/issues/1008"}, {"text": "issue 3266", "href": "https://github.com/scrapy/scrapy/issues/3266"}, {"text": "issue 3696", "href": "https://github.com/scrapy/scrapy/issues/3696"}, {"text": "request fingerprinting", "href": "topics/request-response.html#request-fingerprints"}, {"text": "REQUEST_FINGERPRINTER_CLASS", "href": "topics/request-response.html#std-setting-REQUEST_FINGERPRINTER_CLASS"}, {"text": "issue 900", "href": "https://github.com/scrapy/scrapy/issues/900"}, {"text": "issue 3420", "href": "https://github.com/scrapy/scrapy/issues/3420"}, {"text": "issue 4113", "href": "https://github.com/scrapy/scrapy/issues/4113"}, {"text": "issue 4762", "href": "https://github.com/scrapy/scrapy/issues/4762"}, {"text": "issue 4524", "href": "https://github.com/scrapy/scrapy/issues/4524"}, {"text": "JSON\nLines", "href": "https://jsonlines.org/"}, {"text": "issue 4848", "href": "https://github.com/scrapy/scrapy/issues/4848"}, {"text": "ImagesPipeline.thumb_path", "href": "topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline.thumb_path"}, {"text": "item", "href": "topics/items.html#topics-items"}, {"text": "issue 5504", "href": "https://github.com/scrapy/scrapy/issues/5504"}, {"text": "issue 5508", "href": "https://github.com/scrapy/scrapy/issues/5508"}, {"text": "media pipeline", "href": "topics/media-pipeline.html#topics-media-pipeline"}, {"text": "FILES_EXPIRES", "href": "topics/media-pipeline.html#std-setting-FILES_EXPIRES"}, {"text": "FILES_STORE", "href": "topics/media-pipeline.html#std-setting-FILES_STORE"}, {"text": "issue 5317", "href": "https://github.com/scrapy/scrapy/issues/5317"}, {"text": "issue 5318", "href": "https://github.com/scrapy/scrapy/issues/5318"}, {"text": "parse", "href": "topics/commands.html#std-command-parse"}, {"text": "asynchronous callbacks", "href": "topics/coroutines.html#topics-coroutines"}, {"text": "issue 5424", "href": "https://github.com/scrapy/scrapy/issues/5424"}, {"text": "issue 5577", "href": "https://github.com/scrapy/scrapy/issues/5577"}, {"text": "parse", "href": "topics/commands.html#std-command-parse"}, {"text": "issue 3264", "href": "https://github.com/scrapy/scrapy/issues/3264"}, {"text": "issue 3265", "href": "https://github.com/scrapy/scrapy/issues/3265"}, {"text": "issue 5375", "href": "https://github.com/scrapy/scrapy/issues/5375"}, {"text": "issue 5376", "href": "https://github.com/scrapy/scrapy/issues/5376"}, {"text": "issue 5497", "href": "https://github.com/scrapy/scrapy/issues/5497"}, {"text": "TextResponse", "href": "topics/request-response.html#scrapy.http.TextResponse"}, {"text": "byte\norder mark", "href": "https://en.wikipedia.org/wiki/Byte_order_mark"}, {"text": "HTML living standard", "href": "https://html.spec.whatwg.org/multipage/parsing.html#determining-the-character-encoding"}, {"text": "issue 5601", "href": "https://github.com/scrapy/scrapy/issues/5601"}, {"text": "issue 5611", "href": "https://github.com/scrapy/scrapy/issues/5611"}, {"text": "issue 4873", "href": "https://github.com/scrapy/scrapy/issues/4873"}, {"text": "issue 4873", "href": "https://github.com/scrapy/scrapy/issues/4873"}, {"text": "ASYNCIO_EVENT_LOOP", "href": "topics/settings.html#std-setting-ASYNCIO_EVENT_LOOP"}, {"text": "issue 5529", "href": "https://github.com/scrapy/scrapy/issues/5529"}, {"text": "issue 5515", "href": "https://github.com/scrapy/scrapy/issues/5515"}, {"text": "issue 5526", "href": "https://github.com/scrapy/scrapy/issues/5526"}, {"text": "LinkExtractor", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"}, {"text": "issue 1837", "href": "https://github.com/scrapy/scrapy/issues/1837"}, {"text": "issue 2067", "href": "https://github.com/scrapy/scrapy/issues/2067"}, {"text": "issue 4066", "href": "https://github.com/scrapy/scrapy/issues/4066"}, {"text": "Spider.parse", "href": "topics/spiders.html#scrapy.Spider.parse"}, {"text": "issue 5602", "href": "https://github.com/scrapy/scrapy/issues/5602"}, {"text": "issue 5608", "href": "https://github.com/scrapy/scrapy/issues/5608"}, {"text": "HttpCompressionMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware"}, {"text": "brotli compression", "href": "https://www.ietf.org/rfc/rfc7932.txt"}, {"text": "brotli", "href": "https://github.com/google/brotli"}, {"text": "brotlipy", "href": "https://github.com/python-hyper/brotlipy/"}, {"text": "Signal documentation", "href": "topics/signals.html#topics-signals"}, {"text": "coroutine\nsupport", "href": "topics/coroutines.html#topics-coroutines"}, {"text": "issue 4852", "href": "https://github.com/scrapy/scrapy/issues/4852"}, {"text": "issue 5358", "href": "https://github.com/scrapy/scrapy/issues/5358"}, {"text": "Avoiding getting banned", "href": "topics/practices.html#bans"}, {"text": "Common Crawl", "href": "https://commoncrawl.org/"}, {"text": "Google cache", "href": "http://www.googleguide.com/cached_pages.html"}, {"text": "issue 3582", "href": "https://github.com/scrapy/scrapy/issues/3582"}, {"text": "issue 5432", "href": "https://github.com/scrapy/scrapy/issues/5432"}, {"text": "Components", "href": "topics/components.html#topics-components"}, {"text": "downloader middlewares", "href": "topics/downloader-middleware.html#topics-downloader-middleware"}, {"text": "extensions", "href": "topics/extensions.html#topics-extensions"}, {"text": "item pipelines", "href": "topics/item-pipeline.html#topics-item-pipeline"}, {"text": "spider middlewares", "href": "topics/spider-middleware.html#topics-spider-middleware"}, {"text": "Enforcing asyncio as a requirement", "href": "topics/asyncio.html#enforce-asyncio-requirement"}, {"text": "issue 4978", "href": "https://github.com/scrapy/scrapy/issues/4978"}, {"text": "Settings", "href": "topics/settings.html#topics-settings"}, {"text": "picklable", "href": "https://docs.python.org/3/library/pickle.html#pickle-picklable"}, {"text": "issue 5607", "href": "https://github.com/scrapy/scrapy/issues/5607"}, {"text": "issue 5629", "href": "https://github.com/scrapy/scrapy/issues/5629"}, {"text": "issue 5446", "href": "https://github.com/scrapy/scrapy/issues/5446"}, {"text": "issue 5373", "href": "https://github.com/scrapy/scrapy/issues/5373"}, {"text": "issue 5369", "href": "https://github.com/scrapy/scrapy/issues/5369"}, {"text": "issue 5370", "href": "https://github.com/scrapy/scrapy/issues/5370"}, {"text": "issue 5554", "href": "https://github.com/scrapy/scrapy/issues/5554"}, {"text": "issue 5442", "href": "https://github.com/scrapy/scrapy/issues/5442"}, {"text": "issue 5455", "href": "https://github.com/scrapy/scrapy/issues/5455"}, {"text": "issue 5457", "href": "https://github.com/scrapy/scrapy/issues/5457"}, {"text": "issue 5461", "href": "https://github.com/scrapy/scrapy/issues/5461"}, {"text": "issue 5538", "href": "https://github.com/scrapy/scrapy/issues/5538"}, {"text": "issue 5553", "href": "https://github.com/scrapy/scrapy/issues/5553"}, {"text": "issue 5558", "href": "https://github.com/scrapy/scrapy/issues/5558"}, {"text": "issue 5624", "href": "https://github.com/scrapy/scrapy/issues/5624"}, {"text": "issue 5631", "href": "https://github.com/scrapy/scrapy/issues/5631"}, {"text": "issue 5283", "href": "https://github.com/scrapy/scrapy/issues/5283"}, {"text": "issue 5284", "href": "https://github.com/scrapy/scrapy/issues/5284"}, {"text": "issue 5559", "href": "https://github.com/scrapy/scrapy/issues/5559"}, {"text": "issue 5567", "href": "https://github.com/scrapy/scrapy/issues/5567"}, {"text": "issue 5648", "href": "https://github.com/scrapy/scrapy/issues/5648"}, {"text": "issue 5659", "href": "https://github.com/scrapy/scrapy/issues/5659"}, {"text": "issue 5665", "href": "https://github.com/scrapy/scrapy/issues/5665"}, {"text": "twine check", "href": "https://twine.readthedocs.io/en/stable/#twine-check"}, {"text": "issue 5655", "href": "https://github.com/scrapy/scrapy/issues/5655"}, {"text": "issue 5656", "href": "https://github.com/scrapy/scrapy/issues/5656"}, {"text": "issue 5560", "href": "https://github.com/scrapy/scrapy/issues/5560"}, {"text": "issue 5561", "href": "https://github.com/scrapy/scrapy/issues/5561"}, {"text": "issue 5612", "href": "https://github.com/scrapy/scrapy/issues/5612"}, {"text": "issue 5617", "href": "https://github.com/scrapy/scrapy/issues/5617"}, {"text": "issue 5639", "href": "https://github.com/scrapy/scrapy/issues/5639"}, {"text": "issue 5645", "href": "https://github.com/scrapy/scrapy/issues/5645"}, {"text": "issue 5662", "href": "https://github.com/scrapy/scrapy/issues/5662"}, {"text": "issue 5671", "href": "https://github.com/scrapy/scrapy/issues/5671"}, {"text": "issue 5675", "href": "https://github.com/scrapy/scrapy/issues/5675"}, {"text": "issue 4991", "href": "https://github.com/scrapy/scrapy/issues/4991"}, {"text": "issue 4995", "href": "https://github.com/scrapy/scrapy/issues/4995"}, {"text": "issue 5451", "href": "https://github.com/scrapy/scrapy/issues/5451"}, {"text": "issue 5487", "href": "https://github.com/scrapy/scrapy/issues/5487"}, {"text": "issue 5542", "href": "https://github.com/scrapy/scrapy/issues/5542"}, {"text": "issue 5667", "href": "https://github.com/scrapy/scrapy/issues/5667"}, {"text": "issue 5668", "href": "https://github.com/scrapy/scrapy/issues/5668"}, {"text": "issue 5672", "href": "https://github.com/scrapy/scrapy/issues/5672"}, {"text": "issue 5661", "href": "https://github.com/scrapy/scrapy/issues/5661"}, {"text": "pyOpenSSL", "href": "https://www.pyopenssl.org/en/stable/"}, {"text": "issue 5634", "href": "https://github.com/scrapy/scrapy/issues/5634"}, {"text": "issue 5635", "href": "https://github.com/scrapy/scrapy/issues/5635"}, {"text": "issue 5636", "href": "https://github.com/scrapy/scrapy/issues/5636"}, {"text": "cryptography", "href": "https://cryptography.io/en/latest/"}, {"text": "pyOpenSSL", "href": "https://www.pyopenssl.org/en/stable/"}, {"text": "service_identity", "href": "https://service-identity.readthedocs.io/en/stable/"}, {"text": "Twisted", "href": "https://twistedmatrix.com/trac/"}, {"text": "zope.interface", "href": "https://zopeinterface.readthedocs.io/en/latest/"}, {"text": "issue 5621", "href": "https://github.com/scrapy/scrapy/issues/5621"}, {"text": "issue 5632", "href": "https://github.com/scrapy/scrapy/issues/5632"}, {"text": "issue 5612", "href": "https://github.com/scrapy/scrapy/issues/5612"}, {"text": "issue 5617", "href": "https://github.com/scrapy/scrapy/issues/5617"}, {"text": "issue 5631", "href": "https://github.com/scrapy/scrapy/issues/5631"}, {"text": "HttpProxyMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "HttpProxyMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "HttpProxyMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "HttpProxyMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "CrawlerProcess", "href": "topics/api.html#scrapy.crawler.CrawlerProcess"}, {"text": "issue 5435", "href": "https://github.com/scrapy/scrapy/issues/5435"}, {"text": "issue 5436", "href": "https://github.com/scrapy/scrapy/issues/5436"}, {"text": "twisted.internet.reactor", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html"}, {"text": "TWISTED_REACTOR", "href": "topics/settings.html#std-setting-TWISTED_REACTOR"}, {"text": "issue 5525", "href": "https://github.com/scrapy/scrapy/issues/5525"}, {"text": "issue 5528", "href": "https://github.com/scrapy/scrapy/issues/5528"}, {"text": "issue 5437", "href": "https://github.com/scrapy/scrapy/issues/5437"}, {"text": "issue 5440", "href": "https://github.com/scrapy/scrapy/issues/5440"}, {"text": "issue 5444", "href": "https://github.com/scrapy/scrapy/issues/5444"}, {"text": "issue 5445", "href": "https://github.com/scrapy/scrapy/issues/5445"}, {"text": "issue 5481", "href": "https://github.com/scrapy/scrapy/issues/5481"}, {"text": "issue 5482", "href": "https://github.com/scrapy/scrapy/issues/5482"}, {"text": "asyncio support", "href": "topics/asyncio.html#using-asyncio"}, {"text": "pathlib.Path", "href": "https://docs.python.org/3/library/pathlib.html#pathlib.Path"}, {"text": "item filtering", "href": "topics/feed-exports.html#item-filter"}, {"text": "post-processing", "href": "topics/feed-exports.html#post-processing"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "cjvr-mfj7-j4j8 security advisory", "href": "https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "public suffix", "href": "https://publicsuffix.org/"}, {"text": "mfjm-vh54-3f96 security\nadvisory", "href": "https://github.com/scrapy/scrapy/security/advisories/GHSA-mfjm-vh54-3f96"}, {"text": "h2", "href": "https://pypi.org/project/h2/"}, {"text": "enable HTTP/2 support", "href": "topics/settings.html#http2"}, {"text": "issue 5113", "href": "https://github.com/scrapy/scrapy/issues/5113"}, {"text": "issue 2919", "href": "https://github.com/scrapy/scrapy/issues/2919"}, {"text": "issue 3579", "href": "https://github.com/scrapy/scrapy/issues/3579"}, {"text": "FEED_URI_PARAMS", "href": "topics/feed-exports.html#std-setting-FEED_URI_PARAMS"}, {"text": "issue 4962", "href": "https://github.com/scrapy/scrapy/issues/4962"}, {"text": "issue 4966", "href": "https://github.com/scrapy/scrapy/issues/4966"}, {"text": "RuntimeError", "href": "https://docs.python.org/3/library/exceptions.html#RuntimeError"}, {"text": "issue 5090", "href": "https://github.com/scrapy/scrapy/issues/5090"}, {"text": "AttributeError", "href": "https://docs.python.org/3/library/exceptions.html#AttributeError"}, {"text": "scheduler", "href": "topics/scheduler.html#topics-scheduler"}, {"text": "TypeError", "href": "https://docs.python.org/3/library/exceptions.html#TypeError"}, {"text": "issue 3559", "href": "https://github.com/scrapy/scrapy/issues/3559"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "issue 5130", "href": "https://github.com/scrapy/scrapy/issues/5130"}, {"text": "issue 5393", "href": "https://github.com/scrapy/scrapy/issues/5393"}, {"text": "issue 5398", "href": "https://github.com/scrapy/scrapy/issues/5398"}, {"text": "issue 5398", "href": "https://github.com/scrapy/scrapy/issues/5398"}, {"text": "issue 4178", "href": "https://github.com/scrapy/scrapy/issues/4178"}, {"text": "issue 4356", "href": "https://github.com/scrapy/scrapy/issues/4356"}, {"text": "FEED_URI_PARAMS", "href": "topics/feed-exports.html#std-setting-FEED_URI_PARAMS"}, {"text": "issue 4962", "href": "https://github.com/scrapy/scrapy/issues/4962"}, {"text": "issue 4966", "href": "https://github.com/scrapy/scrapy/issues/4966"}, {"text": "issue 5130", "href": "https://github.com/scrapy/scrapy/issues/5130"}, {"text": "Request.to_dict", "href": "topics/request-response.html#scrapy.http.Request.to_dict"}, {"text": "scrapy.utils.request.request_from_dict()", "href": "topics/request-response.html#scrapy.utils.request.request_from_dict"}, {"text": "issue 5117", "href": "https://github.com/scrapy/scrapy/issues/5117"}, {"text": "Spider", "href": "topics/spiders.html#scrapy.Spider"}, {"text": "issue 5090", "href": "https://github.com/scrapy/scrapy/issues/5090"}, {"text": "Spider", "href": "topics/spiders.html#scrapy.Spider"}, {"text": "item filtering", "href": "topics/feed-exports.html#item-filter"}, {"text": "issue 4575", "href": "https://github.com/scrapy/scrapy/issues/4575"}, {"text": "issue 5178", "href": "https://github.com/scrapy/scrapy/issues/5178"}, {"text": "issue 5161", "href": "https://github.com/scrapy/scrapy/issues/5161"}, {"text": "issue 5203", "href": "https://github.com/scrapy/scrapy/issues/5203"}, {"text": "post-processing", "href": "topics/feed-exports.html#post-processing"}, {"text": "built-in post-processing plugins", "href": "topics/feed-exports.html#builtin-plugins"}, {"text": "issue 2174", "href": "https://github.com/scrapy/scrapy/issues/2174"}, {"text": "issue 5168", "href": "https://github.com/scrapy/scrapy/issues/5168"}, {"text": "issue 5190", "href": "https://github.com/scrapy/scrapy/issues/5190"}, {"text": "FEEDS", "href": "topics/feed-exports.html#std-setting-FEEDS"}, {"text": "pathlib.Path", "href": "https://docs.python.org/3/library/pathlib.html#pathlib.Path"}, {"text": "issue 5383", "href": "https://github.com/scrapy/scrapy/issues/5383"}, {"text": "issue 5384", "href": "https://github.com/scrapy/scrapy/issues/5384"}, {"text": "asyncio", "href": "topics/asyncio.html#using-asyncio"}, {"text": "Windows-specific notes", "href": "topics/asyncio.html#asyncio-windows"}, {"text": "issue 4976", "href": "https://github.com/scrapy/scrapy/issues/4976"}, {"text": "issue 5315", "href": "https://github.com/scrapy/scrapy/issues/5315"}, {"text": "genspider", "href": "topics/commands.html#std-command-genspider"}, {"text": "issue 4439", "href": "https://github.com/scrapy/scrapy/issues/4439"}, {"text": "deferred_to_future()", "href": "topics/asyncio.html#scrapy.utils.defer.deferred_to_future"}, {"text": "maybe_deferred_to_future()", "href": "topics/asyncio.html#scrapy.utils.defer.maybe_deferred_to_future"}, {"text": "await\non Deferreds when using the asyncio reactor", "href": "topics/asyncio.html#asyncio-await-dfd"}, {"text": "issue 5288", "href": "https://github.com/scrapy/scrapy/issues/5288"}, {"text": "Amazon S3 feed export storage", "href": "topics/feed-exports.html#topics-feed-storage-s3"}, {"text": "temporary security credentials", "href": "https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#temporary-access-keys"}, {"text": "AWS_SESSION_TOKEN", "href": "topics/settings.html#std-setting-AWS_SESSION_TOKEN"}, {"text": "AWS_ENDPOINT_URL", "href": "topics/settings.html#std-setting-AWS_ENDPOINT_URL"}, {"text": "issue 4998", "href": "https://github.com/scrapy/scrapy/issues/4998"}, {"text": "issue 5210", "href": "https://github.com/scrapy/scrapy/issues/5210"}, {"text": "LOG_FILE_APPEND", "href": "topics/settings.html#std-setting-LOG_FILE_APPEND"}, {"text": "issue 5279", "href": "https://github.com/scrapy/scrapy/issues/5279"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "issue 5252", "href": "https://github.com/scrapy/scrapy/issues/5252"}, {"text": "issue 5253", "href": "https://github.com/scrapy/scrapy/issues/5253"}, {"text": "CloseSpider", "href": "topics/exceptions.html#scrapy.exceptions.CloseSpider"}, {"text": "spider_idle", "href": "topics/signals.html#std-signal-spider_idle"}, {"text": "issue 5191", "href": "https://github.com/scrapy/scrapy/issues/5191"}, {"text": "HttpProxyMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"}, {"text": "issue 4505", "href": "https://github.com/scrapy/scrapy/issues/4505"}, {"text": "issue 4649", "href": "https://github.com/scrapy/scrapy/issues/4649"}, {"text": "issue 5112", "href": "https://github.com/scrapy/scrapy/issues/5112"}, {"text": "NotImplementedError", "href": "https://docs.python.org/3/library/exceptions.html#NotImplementedError"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Response", "href": "topics/request-response.html#scrapy.http.Response"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "scrapy.utils.request.request_from_dict()", "href": "topics/request-response.html#scrapy.utils.request.request_from_dict"}, {"text": "issue 1877", "href": "https://github.com/scrapy/scrapy/issues/1877"}, {"text": "issue 5130", "href": "https://github.com/scrapy/scrapy/issues/5130"}, {"text": "issue 5218", "href": "https://github.com/scrapy/scrapy/issues/5218"}, {"text": "open()", "href": "topics/scheduler.html#scrapy.core.scheduler.BaseScheduler.open"}, {"text": "close()", "href": "topics/scheduler.html#scrapy.core.scheduler.BaseScheduler.close"}, {"text": "scheduler", "href": "topics/scheduler.html#topics-scheduler"}, {"text": "issue 3559", "href": "https://github.com/scrapy/scrapy/issues/3559"}, {"text": "issue 4881", "href": "https://github.com/scrapy/scrapy/issues/4881"}, {"text": "issue 5007", "href": "https://github.com/scrapy/scrapy/issues/5007"}, {"text": "ItemLoader", "href": "topics/loaders.html#scrapy.loader.ItemLoader"}, {"text": "issue 5145", "href": "https://github.com/scrapy/scrapy/issues/5145"}, {"text": "issue 5269", "href": "https://github.com/scrapy/scrapy/issues/5269"}, {"text": "TWISTED_REACTOR", "href": "topics/settings.html#std-setting-TWISTED_REACTOR"}, {"text": "ASYNCIO_EVENT_LOOP", "href": "topics/settings.html#std-setting-ASYNCIO_EVENT_LOOP"}, {"text": "custom_settings", "href": "topics/spiders.html#scrapy.Spider.custom_settings"}, {"text": "issue 4485", "href": "https://github.com/scrapy/scrapy/issues/4485"}, {"text": "issue 5352", "href": "https://github.com/scrapy/scrapy/issues/5352"}, {"text": "using the asyncio reactor", "href": "topics/asyncio.html#using-asyncio"}, {"text": "issue 5357", "href": "https://github.com/scrapy/scrapy/issues/5357"}, {"text": "startproject", "href": "topics/commands.html#std-command-startproject"}, {"text": "issue 4665", "href": "https://github.com/scrapy/scrapy/issues/4665"}, {"text": "issue 4676", "href": "https://github.com/scrapy/scrapy/issues/4676"}, {"text": "FEED_URI_PARAMS", "href": "topics/feed-exports.html#std-setting-FEED_URI_PARAMS"}, {"text": "issue 4962", "href": "https://github.com/scrapy/scrapy/issues/4962"}, {"text": "issue 4966", "href": "https://github.com/scrapy/scrapy/issues/4966"}, {"text": "issue 5237", "href": "https://github.com/scrapy/scrapy/issues/5237"}, {"text": "issue 5251", "href": "https://github.com/scrapy/scrapy/issues/5251"}, {"text": "issue 5264", "href": "https://github.com/scrapy/scrapy/issues/5264"}, {"text": "issue 5319", "href": "https://github.com/scrapy/scrapy/issues/5319"}, {"text": "issue 5320", "href": "https://github.com/scrapy/scrapy/issues/5320"}, {"text": "CSVFeedSpider.quotechar", "href": "topics/spiders.html#scrapy.spiders.CSVFeedSpider.quotechar"}, {"text": "issue 5391", "href": "https://github.com/scrapy/scrapy/issues/5391"}, {"text": "issue 5394", "href": "https://github.com/scrapy/scrapy/issues/5394"}, {"text": "setuptools", "href": "https://pypi.org/project/setuptools/"}, {"text": "issue 5122", "href": "https://github.com/scrapy/scrapy/issues/5122"}, {"text": "LinkExtractor", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"}, {"text": "issue 5225", "href": "https://github.com/scrapy/scrapy/issues/5225"}, {"text": "TypeError", "href": "https://docs.python.org/3/library/exceptions.html#TypeError"}, {"text": "feed export", "href": "topics/feed-exports.html#topics-feed-exports"}, {"text": "issue 5359", "href": "https://github.com/scrapy/scrapy/issues/5359"}, {"text": "asyncio support", "href": "topics/asyncio.html#using-asyncio"}, {"text": "issue 5332", "href": "https://github.com/scrapy/scrapy/issues/5332"}, {"text": "Windows-specific help for asyncio usage", "href": "topics/asyncio.html#asyncio-windows"}, {"text": "issue 4976", "href": "https://github.com/scrapy/scrapy/issues/4976"}, {"text": "issue 5315", "href": "https://github.com/scrapy/scrapy/issues/5315"}, {"text": "Using a headless browser", "href": "topics/dynamic-content.html#topics-headless-browsing"}, {"text": "issue 4484", "href": "https://github.com/scrapy/scrapy/issues/4484"}, {"text": "issue 4613", "href": "https://github.com/scrapy/scrapy/issues/4613"}, {"text": "local file naming in media pipelines", "href": "topics/media-pipeline.html#topics-file-naming"}, {"text": "issue 5069", "href": "https://github.com/scrapy/scrapy/issues/5069"}, {"text": "issue 5152", "href": "https://github.com/scrapy/scrapy/issues/5152"}, {"text": "Frequently Asked Questions", "href": "faq.html#faq"}, {"text": "issue 2680", "href": "https://github.com/scrapy/scrapy/issues/2680"}, {"text": "issue 3669", "href": "https://github.com/scrapy/scrapy/issues/3669"}, {"text": "URLLENGTH_LIMIT", "href": "topics/settings.html#std-setting-URLLENGTH_LIMIT"}, {"text": "issue 5135", "href": "https://github.com/scrapy/scrapy/issues/5135"}, {"text": "issue 5250", "href": "https://github.com/scrapy/scrapy/issues/5250"}, {"text": "Reppy parser", "href": "topics/downloader-middleware.html#reppy-parser"}, {"text": "issue 5226", "href": "https://github.com/scrapy/scrapy/issues/5226"}, {"text": "issue 5231", "href": "https://github.com/scrapy/scrapy/issues/5231"}, {"text": "the scheduler component", "href": "topics/scheduler.html#topics-scheduler"}, {"text": "issue 3537", "href": "https://github.com/scrapy/scrapy/issues/3537"}, {"text": "issue 3559", "href": "https://github.com/scrapy/scrapy/issues/3559"}, {"text": "media pipelines", "href": "topics/media-pipeline.html#topics-media-pipeline"}, {"text": "determine if a file has expired", "href": "topics/media-pipeline.html#file-expiration"}, {"text": "issue 5120", "href": "https://github.com/scrapy/scrapy/issues/5120"}, {"text": "issue 5254", "href": "https://github.com/scrapy/scrapy/issues/5254"}, {"text": "Running multiple spiders in the same process", "href": "topics/practices.html#run-multiple-spiders"}, {"text": "issue 5070", "href": "https://github.com/scrapy/scrapy/issues/5070"}, {"text": "Running multiple spiders in the same process", "href": "topics/practices.html#run-multiple-spiders"}, {"text": "issue 4485", "href": "https://github.com/scrapy/scrapy/issues/4485"}, {"text": "issue 5352", "href": "https://github.com/scrapy/scrapy/issues/5352"}, {"text": "StatsMailer", "href": "topics/extensions.html#scrapy.extensions.statsmailer.StatsMailer"}, {"text": "issue 5199", "href": "https://github.com/scrapy/scrapy/issues/5199"}, {"text": "issue 5217", "href": "https://github.com/scrapy/scrapy/issues/5217"}, {"text": "JOBDIR", "href": "topics/settings.html#std-setting-JOBDIR"}, {"text": "Settings", "href": "topics/settings.html#topics-settings"}, {"text": "issue 5173", "href": "https://github.com/scrapy/scrapy/issues/5173"}, {"text": "issue 5224", "href": "https://github.com/scrapy/scrapy/issues/5224"}, {"text": "issue 5174", "href": "https://github.com/scrapy/scrapy/issues/5174"}, {"text": "issue 5244", "href": "https://github.com/scrapy/scrapy/issues/5244"}, {"text": "TextResponse.urljoin", "href": "topics/request-response.html#scrapy.http.TextResponse.urljoin"}, {"text": "issue 1582", "href": "https://github.com/scrapy/scrapy/issues/1582"}, {"text": "headers_received", "href": "topics/signals.html#std-signal-headers_received"}, {"text": "issue 5270", "href": "https://github.com/scrapy/scrapy/issues/5270"}, {"text": "SelectorList.get", "href": "topics/selectors.html#scrapy.selector.SelectorList.get"}, {"text": "tutorial", "href": "intro/tutorial.html#intro-tutorial"}, {"text": "issue 5256", "href": "https://github.com/scrapy/scrapy/issues/5256"}, {"text": "issue 2733", "href": "https://github.com/scrapy/scrapy/issues/2733"}, {"text": "issue 5099", "href": "https://github.com/scrapy/scrapy/issues/5099"}, {"text": "issue 5395", "href": "https://github.com/scrapy/scrapy/issues/5395"}, {"text": "issue 5396", "href": "https://github.com/scrapy/scrapy/issues/5396"}, {"text": "our Discord server", "href": "https://discord.gg/mv3yErfpvq"}, {"text": "Getting help", "href": "index.html#getting-help"}, {"text": "issue 5421", "href": "https://github.com/scrapy/scrapy/issues/5421"}, {"text": "issue 5422", "href": "https://github.com/scrapy/scrapy/issues/5422"}, {"text": "officially", "href": "intro/overview.html#intro-overview"}, {"text": "issue 5280", "href": "https://github.com/scrapy/scrapy/issues/5280"}, {"text": "issue 5281", "href": "https://github.com/scrapy/scrapy/issues/5281"}, {"text": "issue 5255", "href": "https://github.com/scrapy/scrapy/issues/5255"}, {"text": "issue 5258", "href": "https://github.com/scrapy/scrapy/issues/5258"}, {"text": "issue 3155", "href": "https://github.com/scrapy/scrapy/issues/3155"}, {"text": "issue 4335", "href": "https://github.com/scrapy/scrapy/issues/4335"}, {"text": "issue 5074", "href": "https://github.com/scrapy/scrapy/issues/5074"}, {"text": "issue 5098", "href": "https://github.com/scrapy/scrapy/issues/5098"}, {"text": "issue 5134", "href": "https://github.com/scrapy/scrapy/issues/5134"}, {"text": "issue 5180", "href": "https://github.com/scrapy/scrapy/issues/5180"}, {"text": "issue 5194", "href": "https://github.com/scrapy/scrapy/issues/5194"}, {"text": "issue 5239", "href": "https://github.com/scrapy/scrapy/issues/5239"}, {"text": "issue 5266", "href": "https://github.com/scrapy/scrapy/issues/5266"}, {"text": "issue 5271", "href": "https://github.com/scrapy/scrapy/issues/5271"}, {"text": "issue 5273", "href": "https://github.com/scrapy/scrapy/issues/5273"}, {"text": "issue 5274", "href": "https://github.com/scrapy/scrapy/issues/5274"}, {"text": "issue 5276", "href": "https://github.com/scrapy/scrapy/issues/5276"}, {"text": "issue 5347", "href": "https://github.com/scrapy/scrapy/issues/5347"}, {"text": "issue 5356", "href": "https://github.com/scrapy/scrapy/issues/5356"}, {"text": "issue 5414", "href": "https://github.com/scrapy/scrapy/issues/5414"}, {"text": "issue 5415", "href": "https://github.com/scrapy/scrapy/issues/5415"}, {"text": "issue 5416", "href": "https://github.com/scrapy/scrapy/issues/5416"}, {"text": "issue 5419", "href": "https://github.com/scrapy/scrapy/issues/5419"}, {"text": "issue 5420", "href": "https://github.com/scrapy/scrapy/issues/5420"}, {"text": "issue 5212", "href": "https://github.com/scrapy/scrapy/issues/5212"}, {"text": "issue 5221", "href": "https://github.com/scrapy/scrapy/issues/5221"}, {"text": "issue 5265", "href": "https://github.com/scrapy/scrapy/issues/5265"}, {"text": "DownloaderStats", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.stats.DownloaderStats"}, {"text": "issue 4964", "href": "https://github.com/scrapy/scrapy/issues/4964"}, {"text": "issue 4972", "href": "https://github.com/scrapy/scrapy/issues/4972"}, {"text": "optparse", "href": "https://docs.python.org/3/library/optparse.html#module-optparse"}, {"text": "issue 5366", "href": "https://github.com/scrapy/scrapy/issues/5366"}, {"text": "issue 5374", "href": "https://github.com/scrapy/scrapy/issues/5374"}, {"text": "issue 5077", "href": "https://github.com/scrapy/scrapy/issues/5077"}, {"text": "issue 5090", "href": "https://github.com/scrapy/scrapy/issues/5090"}, {"text": "issue 5100", "href": "https://github.com/scrapy/scrapy/issues/5100"}, {"text": "issue 5108", "href": "https://github.com/scrapy/scrapy/issues/5108"}, {"text": "issue 5171", "href": "https://github.com/scrapy/scrapy/issues/5171"}, {"text": "issue 5215", "href": "https://github.com/scrapy/scrapy/issues/5215"}, {"text": "issue 5334", "href": "https://github.com/scrapy/scrapy/issues/5334"}, {"text": "issue 5094", "href": "https://github.com/scrapy/scrapy/issues/5094"}, {"text": "issue 5157", "href": "https://github.com/scrapy/scrapy/issues/5157"}, {"text": "issue 5162", "href": "https://github.com/scrapy/scrapy/issues/5162"}, {"text": "issue 5198", "href": "https://github.com/scrapy/scrapy/issues/5198"}, {"text": "issue 5207", "href": "https://github.com/scrapy/scrapy/issues/5207"}, {"text": "issue 5208", "href": "https://github.com/scrapy/scrapy/issues/5208"}, {"text": "issue 5229", "href": "https://github.com/scrapy/scrapy/issues/5229"}, {"text": "issue 5298", "href": "https://github.com/scrapy/scrapy/issues/5298"}, {"text": "issue 5299", "href": "https://github.com/scrapy/scrapy/issues/5299"}, {"text": "issue 5310", "href": "https://github.com/scrapy/scrapy/issues/5310"}, {"text": "issue 5316", "href": "https://github.com/scrapy/scrapy/issues/5316"}, {"text": "issue 5333", "href": "https://github.com/scrapy/scrapy/issues/5333"}, {"text": "issue 5388", "href": "https://github.com/scrapy/scrapy/issues/5388"}, {"text": "issue 5389", "href": "https://github.com/scrapy/scrapy/issues/5389"}, {"text": "issue 5400", "href": "https://github.com/scrapy/scrapy/issues/5400"}, {"text": "issue 5401", "href": "https://github.com/scrapy/scrapy/issues/5401"}, {"text": "issue 5404", "href": "https://github.com/scrapy/scrapy/issues/5404"}, {"text": "issue 5405", "href": "https://github.com/scrapy/scrapy/issues/5405"}, {"text": "issue 5407", "href": "https://github.com/scrapy/scrapy/issues/5407"}, {"text": "issue 5410", "href": "https://github.com/scrapy/scrapy/issues/5410"}, {"text": "issue 5412", "href": "https://github.com/scrapy/scrapy/issues/5412"}, {"text": "issue 5425", "href": "https://github.com/scrapy/scrapy/issues/5425"}, {"text": "issue 5427", "href": "https://github.com/scrapy/scrapy/issues/5427"}, {"text": "issue 5080", "href": "https://github.com/scrapy/scrapy/issues/5080"}, {"text": "issue 5082", "href": "https://github.com/scrapy/scrapy/issues/5082"}, {"text": "issue 5177", "href": "https://github.com/scrapy/scrapy/issues/5177"}, {"text": "issue 5200", "href": "https://github.com/scrapy/scrapy/issues/5200"}, {"text": "issue 5095", "href": "https://github.com/scrapy/scrapy/issues/5095"}, {"text": "issue 5106", "href": "https://github.com/scrapy/scrapy/issues/5106"}, {"text": "issue 5209", "href": "https://github.com/scrapy/scrapy/issues/5209"}, {"text": "issue 5228", "href": "https://github.com/scrapy/scrapy/issues/5228"}, {"text": "issue 5235", "href": "https://github.com/scrapy/scrapy/issues/5235"}, {"text": "issue 5245", "href": "https://github.com/scrapy/scrapy/issues/5245"}, {"text": "issue 5246", "href": "https://github.com/scrapy/scrapy/issues/5246"}, {"text": "issue 5292", "href": "https://github.com/scrapy/scrapy/issues/5292"}, {"text": "issue 5314", "href": "https://github.com/scrapy/scrapy/issues/5314"}, {"text": "issue 5322", "href": "https://github.com/scrapy/scrapy/issues/5322"}, {"text": "HttpAuthMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware"}, {"text": "w3lib.http.basic_auth_header()", "href": "https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header"}, {"text": "scrapy-splash", "href": "https://github.com/scrapy-plugins/scrapy-splash"}, {"text": "HTTP/2 support", "href": "topics/settings.html#http2"}, {"text": "get_retry_request()", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.retry.get_retry_request"}, {"text": "headers_received", "href": "topics/signals.html#scrapy.signals.headers_received"}, {"text": "Response.protocol", "href": "topics/request-response.html#scrapy.http.Response.protocol"}, {"text": "issue 4901", "href": "https://github.com/scrapy/scrapy/issues/4901"}, {"text": "issue 4912", "href": "https://github.com/scrapy/scrapy/issues/4912"}, {"text": "issue 4900", "href": "https://github.com/scrapy/scrapy/issues/4900"}, {"text": "HTTP/2 support", "href": "topics/settings.html#http2"}, {"text": "DOWNLOAD_HANDLERS", "href": "topics/settings.html#std-setting-DOWNLOAD_HANDLERS"}, {"text": "issue 1854", "href": "https://github.com/scrapy/scrapy/issues/1854"}, {"text": "issue 4769", "href": "https://github.com/scrapy/scrapy/issues/4769"}, {"text": "issue 5058", "href": "https://github.com/scrapy/scrapy/issues/5058"}, {"text": "issue 5059", "href": "https://github.com/scrapy/scrapy/issues/5059"}, {"text": "issue 5066", "href": "https://github.com/scrapy/scrapy/issues/5066"}, {"text": "scrapy.downloadermiddlewares.retry.get_retry_request()", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.retry.get_retry_request"}, {"text": "RetryMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.retry.RetryMiddleware"}, {"text": "issue 3590", "href": "https://github.com/scrapy/scrapy/issues/3590"}, {"text": "issue 3685", "href": "https://github.com/scrapy/scrapy/issues/3685"}, {"text": "issue 4902", "href": "https://github.com/scrapy/scrapy/issues/4902"}, {"text": "headers_received", "href": "topics/signals.html#scrapy.signals.headers_received"}, {"text": "stopping downloads", "href": "topics/request-response.html#topics-stop-response-download"}, {"text": "issue 1772", "href": "https://github.com/scrapy/scrapy/issues/1772"}, {"text": "issue 4897", "href": "https://github.com/scrapy/scrapy/issues/4897"}, {"text": "Response.protocol", "href": "topics/request-response.html#scrapy.http.Response.protocol"}, {"text": "issue 4878", "href": "https://github.com/scrapy/scrapy/issues/4878"}, {"text": "Stats", "href": "topics/stats.html#topics-stats"}, {"text": "feeds", "href": "topics/feed-exports.html#topics-feed-exports"}, {"text": "issue 3947", "href": "https://github.com/scrapy/scrapy/issues/3947"}, {"text": "issue 4850", "href": "https://github.com/scrapy/scrapy/issues/4850"}, {"text": "UrlLengthMiddleware", "href": "topics/spider-middleware.html#scrapy.spidermiddlewares.urllength.UrlLengthMiddleware"}, {"text": "logging level", "href": "https://docs.python.org/3/library/logging.html#levels"}, {"text": "stats", "href": "topics/stats.html#topics-stats"}, {"text": "issue 5036", "href": "https://github.com/scrapy/scrapy/issues/5036"}, {"text": "HttpCompressionMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware"}, {"text": "issue 4797", "href": "https://github.com/scrapy/scrapy/issues/4797"}, {"text": "issue 4799", "href": "https://github.com/scrapy/scrapy/issues/4799"}, {"text": "issue 4710", "href": "https://github.com/scrapy/scrapy/issues/4710"}, {"text": "issue 4814", "href": "https://github.com/scrapy/scrapy/issues/4814"}, {"text": "issue 4477", "href": "https://github.com/scrapy/scrapy/issues/4477"}, {"text": "issue 4935", "href": "https://github.com/scrapy/scrapy/issues/4935"}, {"text": "Content-Length", "href": "https://tools.ietf.org/html/rfc2616#section-14.13"}, {"text": "DOWNLOAD_HANDLERS", "href": "topics/settings.html#std-setting-DOWNLOAD_HANDLERS"}, {"text": "issue 5009", "href": "https://github.com/scrapy/scrapy/issues/5009"}, {"text": "issue 5034", "href": "https://github.com/scrapy/scrapy/issues/5034"}, {"text": "issue 5045", "href": "https://github.com/scrapy/scrapy/issues/5045"}, {"text": "issue 5057", "href": "https://github.com/scrapy/scrapy/issues/5057"}, {"text": "issue 5062", "href": "https://github.com/scrapy/scrapy/issues/5062"}, {"text": "handle_httpstatus_all", "href": "topics/spider-middleware.html#std-reqmeta-handle_httpstatus_all"}, {"text": "issue 3851", "href": "https://github.com/scrapy/scrapy/issues/3851"}, {"text": "issue 4694", "href": "https://github.com/scrapy/scrapy/issues/4694"}, {"text": "install Scrapy in Windows using pip", "href": "intro/install.html#intro-install-windows"}, {"text": "issue 4715", "href": "https://github.com/scrapy/scrapy/issues/4715"}, {"text": "issue 4736", "href": "https://github.com/scrapy/scrapy/issues/4736"}, {"text": "additional ways to filter logs", "href": "topics/logging.html#topics-logging-advanced-customization"}, {"text": "issue 4216", "href": "https://github.com/scrapy/scrapy/issues/4216"}, {"text": "issue 4257", "href": "https://github.com/scrapy/scrapy/issues/4257"}, {"text": "issue 4965", "href": "https://github.com/scrapy/scrapy/issues/4965"}, {"text": "FAQ", "href": "faq.html#faq"}, {"text": "issue 2263", "href": "https://github.com/scrapy/scrapy/issues/2263"}, {"text": "issue 3667", "href": "https://github.com/scrapy/scrapy/issues/3667"}, {"text": "scrapy-bench", "href": "https://github.com/scrapy/scrapy-bench"}, {"text": "Benchmarking", "href": "topics/benchmarking.html#benchmarking"}, {"text": "issue 4996", "href": "https://github.com/scrapy/scrapy/issues/4996"}, {"text": "issue 5016", "href": "https://github.com/scrapy/scrapy/issues/5016"}, {"text": "extension", "href": "topics/extensions.html#topics-extensions"}, {"text": "issue 5014", "href": "https://github.com/scrapy/scrapy/issues/5014"}, {"text": "issue 4829", "href": "https://github.com/scrapy/scrapy/issues/4829"}, {"text": "issue 4830", "href": "https://github.com/scrapy/scrapy/issues/4830"}, {"text": "issue 4907", "href": "https://github.com/scrapy/scrapy/issues/4907"}, {"text": "issue 4909", "href": "https://github.com/scrapy/scrapy/issues/4909"}, {"text": "issue 5008", "href": "https://github.com/scrapy/scrapy/issues/5008"}, {"text": "issue 4892", "href": "https://github.com/scrapy/scrapy/issues/4892"}, {"text": "issue 4899", "href": "https://github.com/scrapy/scrapy/issues/4899"}, {"text": "issue 4936", "href": "https://github.com/scrapy/scrapy/issues/4936"}, {"text": "issue 4942", "href": "https://github.com/scrapy/scrapy/issues/4942"}, {"text": "issue 5005", "href": "https://github.com/scrapy/scrapy/issues/5005"}, {"text": "issue 5063", "href": "https://github.com/scrapy/scrapy/issues/5063"}, {"text": "list of Request.meta keys", "href": "topics/request-response.html#topics-request-meta"}, {"text": "issue 5061", "href": "https://github.com/scrapy/scrapy/issues/5061"}, {"text": "issue 5065", "href": "https://github.com/scrapy/scrapy/issues/5065"}, {"text": "issue 4973", "href": "https://github.com/scrapy/scrapy/issues/4973"}, {"text": "issue 5072", "href": "https://github.com/scrapy/scrapy/issues/5072"}, {"text": "issue 4956", "href": "https://github.com/scrapy/scrapy/issues/4956"}, {"text": "issue 4974", "href": "https://github.com/scrapy/scrapy/issues/4974"}, {"text": "issue 4757", "href": "https://github.com/scrapy/scrapy/issues/4757"}, {"text": "issue 4759", "href": "https://github.com/scrapy/scrapy/issues/4759"}, {"text": "issue 4895", "href": "https://github.com/scrapy/scrapy/issues/4895"}, {"text": "issue 4940", "href": "https://github.com/scrapy/scrapy/issues/4940"}, {"text": "issue 4950", "href": "https://github.com/scrapy/scrapy/issues/4950"}, {"text": "issue 5073", "href": "https://github.com/scrapy/scrapy/issues/5073"}, {"text": "issue 4710", "href": "https://github.com/scrapy/scrapy/issues/4710"}, {"text": "issue 4814", "href": "https://github.com/scrapy/scrapy/issues/4814"}, {"text": "coroutine support", "href": "topics/coroutines.html#coroutine-support"}, {"text": "issue 4987", "href": "https://github.com/scrapy/scrapy/issues/4987"}, {"text": "issue 4924", "href": "https://github.com/scrapy/scrapy/issues/4924"}, {"text": "issue 4986", "href": "https://github.com/scrapy/scrapy/issues/4986"}, {"text": "issue 5020", "href": "https://github.com/scrapy/scrapy/issues/5020"}, {"text": "issue 5022", "href": "https://github.com/scrapy/scrapy/issues/5022"}, {"text": "issue 5027", "href": "https://github.com/scrapy/scrapy/issues/5027"}, {"text": "issue 5052", "href": "https://github.com/scrapy/scrapy/issues/5052"}, {"text": "issue 5053", "href": "https://github.com/scrapy/scrapy/issues/5053"}, {"text": "issue 4911", "href": "https://github.com/scrapy/scrapy/issues/4911"}, {"text": "issue 4982", "href": "https://github.com/scrapy/scrapy/issues/4982"}, {"text": "issue 5001", "href": "https://github.com/scrapy/scrapy/issues/5001"}, {"text": "issue 5002", "href": "https://github.com/scrapy/scrapy/issues/5002"}, {"text": "issue 5076", "href": "https://github.com/scrapy/scrapy/issues/5076"}, {"text": "feed exports", "href": "topics/feed-exports.html#topics-feed-exports"}, {"text": "issue 4845", "href": "https://github.com/scrapy/scrapy/issues/4845"}, {"text": "issue 4857", "href": "https://github.com/scrapy/scrapy/issues/4857"}, {"text": "issue 4859", "href": "https://github.com/scrapy/scrapy/issues/4859"}, {"text": "issue 4855", "href": "https://github.com/scrapy/scrapy/issues/4855"}, {"text": "issue 4872", "href": "https://github.com/scrapy/scrapy/issues/4872"}, {"text": "reactor.resolve", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.interfaces.IReactorCore.html#resolve"}, {"text": "issue 4802", "href": "https://github.com/scrapy/scrapy/issues/4802"}, {"text": "issue 4803", "href": "https://github.com/scrapy/scrapy/issues/4803"}, {"text": "genspider", "href": "topics/commands.html#std-command-genspider"}, {"text": "issue 4874", "href": "https://github.com/scrapy/scrapy/issues/4874"}, {"text": "issue 4869", "href": "https://github.com/scrapy/scrapy/issues/4869"}, {"text": "issue 4876", "href": "https://github.com/scrapy/scrapy/issues/4876"}, {"text": "media pipelines", "href": "topics/media-pipeline.html#topics-media-pipeline"}, {"text": "item", "href": "topics/items.html#topics-items"}, {"text": "FEEDS", "href": "topics/feed-exports.html#std-setting-FEEDS"}, {"text": "item exporter classes", "href": "topics/exporters.html#topics-exporters"}, {"text": "feed exports", "href": "topics/feed-exports.html#topics-feed-exports"}, {"text": "crawl", "href": "topics/commands.html#std-command-crawl"}, {"text": "runspider", "href": "topics/commands.html#std-command-runspider"}, {"text": "zstandard", "href": "https://pypi.org/project/zstandard/"}, {"text": "is now required", "href": "intro/install.html#faq-python-versions"}, {"text": "feed exports", "href": "topics/feed-exports.html#topics-feed-storage-s3"}, {"text": "media pipelines", "href": "topics/media-pipeline.html#media-pipelines-s3"}, {"text": "botocore", "href": "https://github.com/boto/botocore"}, {"text": "images pipeline", "href": "topics/media-pipeline.html#images-pipeline"}, {"text": "Pillow", "href": "https://python-pillow.org/"}, {"text": "issue 4718", "href": "https://github.com/scrapy/scrapy/issues/4718"}, {"text": "issue 4732", "href": "https://github.com/scrapy/scrapy/issues/4732"}, {"text": "issue 4733", "href": "https://github.com/scrapy/scrapy/issues/4733"}, {"text": "issue 4742", "href": "https://github.com/scrapy/scrapy/issues/4742"}, {"text": "issue 4743", "href": "https://github.com/scrapy/scrapy/issues/4743"}, {"text": "issue 4764", "href": "https://github.com/scrapy/scrapy/issues/4764"}, {"text": "CookiesMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware"}, {"text": "Request.headers", "href": "topics/request-response.html#scrapy.http.Request.headers"}, {"text": "Request.cookies", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "issue 4717", "href": "https://github.com/scrapy/scrapy/issues/4717"}, {"text": "issue 4823", "href": "https://github.com/scrapy/scrapy/issues/4823"}, {"text": "issue 4356", "href": "https://github.com/scrapy/scrapy/issues/4356"}, {"text": "issue 4411", "href": "https://github.com/scrapy/scrapy/issues/4411"}, {"text": "issue 4688", "href": "https://github.com/scrapy/scrapy/issues/4688"}, {"text": "issue 4818", "href": "https://github.com/scrapy/scrapy/issues/4818"}, {"text": "media pipelines", "href": "topics/media-pipeline.html#topics-media-pipeline"}, {"text": "issue 4628", "href": "https://github.com/scrapy/scrapy/issues/4628"}, {"text": "issue 4686", "href": "https://github.com/scrapy/scrapy/issues/4686"}, {"text": "feed storage backend classes", "href": "topics/feed-exports.html#topics-feed-storage"}, {"text": "issue 547", "href": "https://github.com/scrapy/scrapy/issues/547"}, {"text": "issue 716", "href": "https://github.com/scrapy/scrapy/issues/716"}, {"text": "issue 4512", "href": "https://github.com/scrapy/scrapy/issues/4512"}, {"text": "issue 4684", "href": "https://github.com/scrapy/scrapy/issues/4684"}, {"text": "issue 4701", "href": "https://github.com/scrapy/scrapy/issues/4701"}, {"text": "issue 4734", "href": "https://github.com/scrapy/scrapy/issues/4734"}, {"text": "issue 4776", "href": "https://github.com/scrapy/scrapy/issues/4776"}, {"text": "media pipelines", "href": "topics/media-pipeline.html#topics-media-pipeline"}, {"text": "item", "href": "topics/items.html#topics-items"}, {"text": "scrapy.pipelines.files.FilesPipeline", "href": "topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline"}, {"text": "file_path()", "href": "topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.file_path"}, {"text": "scrapy.pipelines.images.ImagesPipeline", "href": "topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline"}, {"text": "file_path()", "href": "topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline.file_path"}, {"text": "issue 4628", "href": "https://github.com/scrapy/scrapy/issues/4628"}, {"text": "issue 4686", "href": "https://github.com/scrapy/scrapy/issues/4686"}, {"text": "FEEDS", "href": "topics/feed-exports.html#std-setting-FEEDS"}, {"text": "item exporter classes", "href": "topics/exporters.html#topics-exporters"}, {"text": "issue 4606", "href": "https://github.com/scrapy/scrapy/issues/4606"}, {"text": "issue 4768", "href": "https://github.com/scrapy/scrapy/issues/4768"}, {"text": "Feed exports", "href": "topics/feed-exports.html#topics-feed-exports"}, {"text": "crawl", "href": "topics/commands.html#std-command-crawl"}, {"text": "runspider", "href": "topics/commands.html#std-command-runspider"}, {"text": "FEEDS", "href": "topics/feed-exports.html#std-setting-FEEDS"}, {"text": "feed storage\nbackend classes", "href": "topics/feed-exports.html#topics-feed-storage"}, {"text": "feed\noptions", "href": "topics/feed-exports.html#feed-options"}, {"text": "issue 547", "href": "https://github.com/scrapy/scrapy/issues/547"}, {"text": "issue 716", "href": "https://github.com/scrapy/scrapy/issues/716"}, {"text": "issue 4512", "href": "https://github.com/scrapy/scrapy/issues/4512"}, {"text": "zstandard", "href": "https://pypi.org/project/zstandard/"}, {"text": "issue 4831", "href": "https://github.com/scrapy/scrapy/issues/4831"}, {"text": "issue 3870", "href": "https://github.com/scrapy/scrapy/issues/3870"}, {"text": "issue 3873", "href": "https://github.com/scrapy/scrapy/issues/3873"}, {"text": "DOWNLOADER_MIDDLEWARES", "href": "topics/settings.html#std-setting-DOWNLOADER_MIDDLEWARES"}, {"text": "DOWNLOAD_HANDLERS", "href": "topics/settings.html#std-setting-DOWNLOAD_HANDLERS"}, {"text": "Downloader middlewares", "href": "topics/downloader-middleware.html#topics-downloader-middleware"}, {"text": "response.request", "href": "topics/request-response.html#scrapy.http.Response.request"}, {"text": "downloader middleware", "href": "topics/downloader-middleware.html#topics-downloader-middleware"}, {"text": "Response", "href": "topics/request-response.html#scrapy.http.Response"}, {"text": "process_response()", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"}, {"text": "process_exception()", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "response.request", "href": "topics/request-response.html#scrapy.http.Response.request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "response_received", "href": "topics/signals.html#std-signal-response_received"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "issue 4529", "href": "https://github.com/scrapy/scrapy/issues/4529"}, {"text": "issue 4632", "href": "https://github.com/scrapy/scrapy/issues/4632"}, {"text": "FTP feed storage backend", "href": "topics/feed-exports.html#topics-feed-storage-ftp"}, {"text": "feed option", "href": "topics/feed-exports.html#feed-options"}, {"text": "issue 547", "href": "https://github.com/scrapy/scrapy/issues/547"}, {"text": "issue 716", "href": "https://github.com/scrapy/scrapy/issues/716"}, {"text": "issue 4512", "href": "https://github.com/scrapy/scrapy/issues/4512"}, {"text": "CsvItemExporter", "href": "topics/exporters.html#scrapy.exporters.CsvItemExporter"}, {"text": "issue 4755", "href": "https://github.com/scrapy/scrapy/issues/4755"}, {"text": "using asyncio", "href": "topics/asyncio.html#using-asyncio"}, {"text": "set a custom asyncio loop", "href": "topics/asyncio.html#using-custom-loops"}, {"text": "issue 4306", "href": "https://github.com/scrapy/scrapy/issues/4306"}, {"text": "issue 4414", "href": "https://github.com/scrapy/scrapy/issues/4414"}, {"text": "Jobs: pausing and resuming crawls", "href": "topics/jobs.html#topics-jobs"}, {"text": "issue 4756", "href": "https://github.com/scrapy/scrapy/issues/4756"}, {"text": "DOWNLOAD_MAXSIZE", "href": "topics/settings.html#std-setting-DOWNLOAD_MAXSIZE"}, {"text": "issue 3874", "href": "https://github.com/scrapy/scrapy/issues/3874"}, {"text": "issue 3886", "href": "https://github.com/scrapy/scrapy/issues/3886"}, {"text": "issue 4752", "href": "https://github.com/scrapy/scrapy/issues/4752"}, {"text": "genspider", "href": "topics/commands.html#std-command-genspider"}, {"text": "issue 4561", "href": "https://github.com/scrapy/scrapy/issues/4561"}, {"text": "issue 4616", "href": "https://github.com/scrapy/scrapy/issues/4616"}, {"text": "issue 4623", "href": "https://github.com/scrapy/scrapy/issues/4623"}, {"text": "issue 4772", "href": "https://github.com/scrapy/scrapy/issues/4772"}, {"text": "runspider", "href": "topics/commands.html#std-command-runspider"}, {"text": "issue 4643", "href": "https://github.com/scrapy/scrapy/issues/4643"}, {"text": "issue 4646", "href": "https://github.com/scrapy/scrapy/issues/4646"}, {"text": "HttpProxyMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"}, {"text": "issue 3331", "href": "https://github.com/scrapy/scrapy/issues/3331"}, {"text": "issue 4778", "href": "https://github.com/scrapy/scrapy/issues/4778"}, {"text": "issue 4720", "href": "https://github.com/scrapy/scrapy/issues/4720"}, {"text": "issue 4721", "href": "https://github.com/scrapy/scrapy/issues/4721"}, {"text": "startproject", "href": "topics/commands.html#std-command-startproject"}, {"text": "issue 4722", "href": "https://github.com/scrapy/scrapy/issues/4722"}, {"text": "issue 861", "href": "https://github.com/scrapy/scrapy/issues/861"}, {"text": "issue 4746", "href": "https://github.com/scrapy/scrapy/issues/4746"}, {"text": "issue 4835", "href": "https://github.com/scrapy/scrapy/issues/4835"}, {"text": "FEED_URI_PARAMS", "href": "topics/feed-exports.html#std-setting-FEED_URI_PARAMS"}, {"text": "issue 4671", "href": "https://github.com/scrapy/scrapy/issues/4671"}, {"text": "issue 4724", "href": "https://github.com/scrapy/scrapy/issues/4724"}, {"text": "link extractors", "href": "topics/link-extractors.html#topics-link-extractors"}, {"text": "Link", "href": "topics/link-extractors.html#scrapy.link.Link"}, {"text": "issue 4751", "href": "https://github.com/scrapy/scrapy/issues/4751"}, {"text": "issue 4775", "href": "https://github.com/scrapy/scrapy/issues/4775"}, {"text": "CONCURRENT_REQUESTS", "href": "topics/settings.html#std-setting-CONCURRENT_REQUESTS"}, {"text": "CloseSpider", "href": "topics/extensions.html#scrapy.extensions.closespider.CloseSpider"}, {"text": "issue 4836", "href": "https://github.com/scrapy/scrapy/issues/4836"}, {"text": "issue 4547", "href": "https://github.com/scrapy/scrapy/issues/4547"}, {"text": "issue 4703", "href": "https://github.com/scrapy/scrapy/issues/4703"}, {"text": "official deprecation policy", "href": "versioning.html#deprecation-policy"}, {"text": "issue 4705", "href": "https://github.com/scrapy/scrapy/issues/4705"}, {"text": "documentation policies", "href": "contributing.html#documentation-policies"}, {"text": "versionadded", "href": "https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionadded"}, {"text": "versionchanged", "href": "https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionchanged"}, {"text": "issue 3971", "href": "https://github.com/scrapy/scrapy/issues/3971"}, {"text": "issue 4310", "href": "https://github.com/scrapy/scrapy/issues/4310"}, {"text": "issue 4090", "href": "https://github.com/scrapy/scrapy/issues/4090"}, {"text": "issue 4782", "href": "https://github.com/scrapy/scrapy/issues/4782"}, {"text": "issue 4800", "href": "https://github.com/scrapy/scrapy/issues/4800"}, {"text": "issue 4801", "href": "https://github.com/scrapy/scrapy/issues/4801"}, {"text": "issue 4809", "href": "https://github.com/scrapy/scrapy/issues/4809"}, {"text": "issue 4816", "href": "https://github.com/scrapy/scrapy/issues/4816"}, {"text": "issue 4825", "href": "https://github.com/scrapy/scrapy/issues/4825"}, {"text": "issue 4243", "href": "https://github.com/scrapy/scrapy/issues/4243"}, {"text": "issue 4691", "href": "https://github.com/scrapy/scrapy/issues/4691"}, {"text": "check", "href": "topics/commands.html#std-command-check"}, {"text": "issue 4663", "href": "https://github.com/scrapy/scrapy/issues/4663"}, {"text": "issue 4726", "href": "https://github.com/scrapy/scrapy/issues/4726"}, {"text": "issue 4727", "href": "https://github.com/scrapy/scrapy/issues/4727"}, {"text": "issue 4735", "href": "https://github.com/scrapy/scrapy/issues/4735"}, {"text": "issue 4723", "href": "https://github.com/scrapy/scrapy/issues/4723"}, {"text": "formatted string literals", "href": "https://docs.python.org/3/reference/lexical_analysis.html#f-strings"}, {"text": "issue 4307", "href": "https://github.com/scrapy/scrapy/issues/4307"}, {"text": "issue 4324", "href": "https://github.com/scrapy/scrapy/issues/4324"}, {"text": "issue 4672", "href": "https://github.com/scrapy/scrapy/issues/4672"}, {"text": "issue 4707", "href": "https://github.com/scrapy/scrapy/issues/4707"}, {"text": "issue 1790", "href": "https://github.com/scrapy/scrapy/issues/1790"}, {"text": "issue 3288", "href": "https://github.com/scrapy/scrapy/issues/3288"}, {"text": "issue 4165", "href": "https://github.com/scrapy/scrapy/issues/4165"}, {"text": "issue 4564", "href": "https://github.com/scrapy/scrapy/issues/4564"}, {"text": "issue 4651", "href": "https://github.com/scrapy/scrapy/issues/4651"}, {"text": "issue 4714", "href": "https://github.com/scrapy/scrapy/issues/4714"}, {"text": "issue 4738", "href": "https://github.com/scrapy/scrapy/issues/4738"}, {"text": "issue 4745", "href": "https://github.com/scrapy/scrapy/issues/4745"}, {"text": "issue 4747", "href": "https://github.com/scrapy/scrapy/issues/4747"}, {"text": "issue 4761", "href": "https://github.com/scrapy/scrapy/issues/4761"}, {"text": "issue 4765", "href": "https://github.com/scrapy/scrapy/issues/4765"}, {"text": "issue 4804", "href": "https://github.com/scrapy/scrapy/issues/4804"}, {"text": "issue 4817", "href": "https://github.com/scrapy/scrapy/issues/4817"}, {"text": "issue 4820", "href": "https://github.com/scrapy/scrapy/issues/4820"}, {"text": "issue 4822", "href": "https://github.com/scrapy/scrapy/issues/4822"}, {"text": "issue 4839", "href": "https://github.com/scrapy/scrapy/issues/4839"}, {"text": "Feed exports", "href": "topics/feed-exports.html#topics-feed-exports"}, {"text": "Google Cloud\nStorage", "href": "topics/feed-exports.html#topics-feed-storage-gcs"}, {"text": "FEED_EXPORT_BATCH_ITEM_COUNT", "href": "topics/feed-exports.html#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT"}, {"text": "delayed file delivery", "href": "topics/feed-exports.html#delayed-file-delivery"}, {"text": "S3", "href": "topics/feed-exports.html#topics-feed-storage-s3"}, {"text": "FTP", "href": "topics/feed-exports.html#topics-feed-storage-ftp"}, {"text": "GCS", "href": "topics/feed-exports.html#topics-feed-storage-gcs"}, {"text": "item loaders", "href": "topics/loaders.html#topics-loaders"}, {"text": "itemloaders", "href": "https://itemloaders.readthedocs.io/en/latest/index.html"}, {"text": "LinkExtractor", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"}, {"text": "issue 4356", "href": "https://github.com/scrapy/scrapy/issues/4356"}, {"text": "issue 4679", "href": "https://github.com/scrapy/scrapy/issues/4679"}, {"text": "issue 4683", "href": "https://github.com/scrapy/scrapy/issues/4683"}, {"text": "Feed exports", "href": "topics/feed-exports.html#topics-feed-exports"}, {"text": "Google Cloud\nStorage", "href": "topics/feed-exports.html#topics-feed-storage-gcs"}, {"text": "issue 685", "href": "https://github.com/scrapy/scrapy/issues/685"}, {"text": "issue 3608", "href": "https://github.com/scrapy/scrapy/issues/3608"}, {"text": "FEED_EXPORT_BATCH_ITEM_COUNT", "href": "topics/feed-exports.html#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT"}, {"text": "issue 4250", "href": "https://github.com/scrapy/scrapy/issues/4250"}, {"text": "issue 4434", "href": "https://github.com/scrapy/scrapy/issues/4434"}, {"text": "parse", "href": "topics/commands.html#std-command-parse"}, {"text": "issue 4317", "href": "https://github.com/scrapy/scrapy/issues/4317"}, {"text": "issue 4377", "href": "https://github.com/scrapy/scrapy/issues/4377"}, {"text": "Request.from_curl", "href": "topics/request-response.html#scrapy.http.Request.from_curl"}, {"text": "curl_to_request_kwargs()", "href": "topics/developer-tools.html#scrapy.utils.curl.curl_to_request_kwargs"}, {"text": "issue 4612", "href": "https://github.com/scrapy/scrapy/issues/4612"}, {"text": "CrawlSpider", "href": "topics/spiders.html#scrapy.spiders.CrawlSpider"}, {"text": "issue 712", "href": "https://github.com/scrapy/scrapy/issues/712"}, {"text": "issue 732", "href": "https://github.com/scrapy/scrapy/issues/732"}, {"text": "issue 781", "href": "https://github.com/scrapy/scrapy/issues/781"}, {"text": "issue 4254", "href": "https://github.com/scrapy/scrapy/issues/4254"}, {"text": "CSV exporting", "href": "topics/feed-exports.html#topics-feed-format-csv"}, {"text": "dataclass items", "href": "topics/items.html#dataclass-items"}, {"text": "attr.s items", "href": "topics/items.html#attrs-items"}, {"text": "issue 4667", "href": "https://github.com/scrapy/scrapy/issues/4667"}, {"text": "issue 4668", "href": "https://github.com/scrapy/scrapy/issues/4668"}, {"text": "Request.from_curl", "href": "topics/request-response.html#scrapy.http.Request.from_curl"}, {"text": "curl_to_request_kwargs()", "href": "topics/developer-tools.html#scrapy.utils.curl.curl_to_request_kwargs"}, {"text": "issue 4612", "href": "https://github.com/scrapy/scrapy/issues/4612"}, {"text": "issue 4393", "href": "https://github.com/scrapy/scrapy/issues/4393"}, {"text": "issue 4403", "href": "https://github.com/scrapy/scrapy/issues/4403"}, {"text": "OpenSSL cipher list format", "href": "https://www.openssl.org/docs/manmaster/man1/openssl-ciphers.html#CIPHER-LIST-FORMAT"}, {"text": "DOWNLOADER_CLIENT_TLS_CIPHERS", "href": "topics/settings.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS"}, {"text": "issue 4653", "href": "https://github.com/scrapy/scrapy/issues/4653"}, {"text": "Working with dataclass items", "href": "topics/loaders.html#topics-loaders-dataclass"}, {"text": "issue 4652", "href": "https://github.com/scrapy/scrapy/issues/4652"}, {"text": "item loaders", "href": "topics/loaders.html#topics-loaders"}, {"text": "itemloaders", "href": "https://itemloaders.readthedocs.io/en/latest/index.html"}, {"text": "issue 4005", "href": "https://github.com/scrapy/scrapy/issues/4005"}, {"text": "issue 4516", "href": "https://github.com/scrapy/scrapy/issues/4516"}, {"text": "issue 4644", "href": "https://github.com/scrapy/scrapy/issues/4644"}, {"text": "issue 4645", "href": "https://github.com/scrapy/scrapy/issues/4645"}, {"text": "issue 4650", "href": "https://github.com/scrapy/scrapy/issues/4650"}, {"text": "issue 4682", "href": "https://github.com/scrapy/scrapy/issues/4682"}, {"text": "issue 4704", "href": "https://github.com/scrapy/scrapy/issues/4704"}, {"text": "issue 4673", "href": "https://github.com/scrapy/scrapy/issues/4673"}, {"text": "issue 4690", "href": "https://github.com/scrapy/scrapy/issues/4690"}, {"text": "issue 4458", "href": "https://github.com/scrapy/scrapy/issues/4458"}, {"text": "issue 4504", "href": "https://github.com/scrapy/scrapy/issues/4504"}, {"text": "startproject", "href": "topics/commands.html#std-command-startproject"}, {"text": "issue 4662", "href": "https://github.com/scrapy/scrapy/issues/4662"}, {"text": "issue 4666", "href": "https://github.com/scrapy/scrapy/issues/4666"}, {"text": "dataclass objects", "href": "topics/items.html#dataclass-items"}, {"text": "attrs objects", "href": "topics/items.html#attrs-items"}, {"text": "item types", "href": "topics/items.html#item-types"}, {"text": "TextResponse.json", "href": "topics/request-response.html#scrapy.http.TextResponse.json"}, {"text": "bytes_received", "href": "topics/signals.html#std-signal-bytes_received"}, {"text": "CookiesMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware"}, {"text": "typing.Type", "href": "https://docs.python.org/3/library/typing.html#typing.Type"}, {"text": "issue 4615", "href": "https://github.com/scrapy/scrapy/issues/4615"}, {"text": "TextResponse.text", "href": "topics/request-response.html#scrapy.http.TextResponse.text"}, {"text": "issue 4546", "href": "https://github.com/scrapy/scrapy/issues/4546"}, {"text": "issue 4555", "href": "https://github.com/scrapy/scrapy/issues/4555"}, {"text": "issue 4579", "href": "https://github.com/scrapy/scrapy/issues/4579"}, {"text": "issue 4534", "href": "https://github.com/scrapy/scrapy/issues/4534"}, {"text": "dataclass objects", "href": "topics/items.html#dataclass-items"}, {"text": "attrs objects", "href": "topics/items.html#attrs-items"}, {"text": "item types", "href": "topics/items.html#item-types"}, {"text": "itemadapter", "href": "https://github.com/scrapy/itemadapter"}, {"text": "supports any item type", "href": "topics/items.html#supporting-item-types"}, {"text": "issue 2749", "href": "https://github.com/scrapy/scrapy/issues/2749"}, {"text": "issue 2807", "href": "https://github.com/scrapy/scrapy/issues/2807"}, {"text": "issue 3761", "href": "https://github.com/scrapy/scrapy/issues/3761"}, {"text": "issue 3881", "href": "https://github.com/scrapy/scrapy/issues/3881"}, {"text": "issue 4642", "href": "https://github.com/scrapy/scrapy/issues/4642"}, {"text": "TextResponse.json", "href": "topics/request-response.html#scrapy.http.TextResponse.json"}, {"text": "issue 2444", "href": "https://github.com/scrapy/scrapy/issues/2444"}, {"text": "issue 4460", "href": "https://github.com/scrapy/scrapy/issues/4460"}, {"text": "issue 4574", "href": "https://github.com/scrapy/scrapy/issues/4574"}, {"text": "bytes_received", "href": "topics/signals.html#std-signal-bytes_received"}, {"text": "stopping downloads", "href": "topics/request-response.html#topics-stop-response-download"}, {"text": "issue 4205", "href": "https://github.com/scrapy/scrapy/issues/4205"}, {"text": "issue 4559", "href": "https://github.com/scrapy/scrapy/issues/4559"}, {"text": "media pipeline", "href": "topics/media-pipeline.html#topics-media-pipeline"}, {"text": "FilesPipeline.get_media_requests", "href": "topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.get_media_requests"}, {"text": "issue 2893", "href": "https://github.com/scrapy/scrapy/issues/2893"}, {"text": "issue 4486", "href": "https://github.com/scrapy/scrapy/issues/4486"}, {"text": "Google Cloud Storage", "href": "topics/media-pipeline.html#media-pipeline-gcs"}, {"text": "media pipeline", "href": "topics/media-pipeline.html#topics-media-pipeline"}, {"text": "issue 4346", "href": "https://github.com/scrapy/scrapy/issues/4346"}, {"text": "issue 4508", "href": "https://github.com/scrapy/scrapy/issues/4508"}, {"text": "Link extractors", "href": "topics/link-extractors.html#topics-link-extractors"}, {"text": "lambdas", "href": "https://docs.python.org/3/reference/expressions.html#lambda"}, {"text": "Request.cb_kwargs", "href": "topics/request-response.html#scrapy.http.Request.cb_kwargs"}, {"text": "Request.meta", "href": "topics/request-response.html#scrapy.http.Request.meta"}, {"text": "persisting\nscheduled requests", "href": "topics/jobs.html#topics-jobs"}, {"text": "issue 4554", "href": "https://github.com/scrapy/scrapy/issues/4554"}, {"text": "pickle protocol", "href": "https://docs.python.org/3/library/pickle.html#pickle-protocols"}, {"text": "issue 4135", "href": "https://github.com/scrapy/scrapy/issues/4135"}, {"text": "issue 4541", "href": "https://github.com/scrapy/scrapy/issues/4541"}, {"text": "TypeError", "href": "https://docs.python.org/3/library/exceptions.html#TypeError"}, {"text": "issue 4528", "href": "https://github.com/scrapy/scrapy/issues/4528"}, {"text": "issue 4532", "href": "https://github.com/scrapy/scrapy/issues/4532"}, {"text": "CookiesMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware"}, {"text": "Request.headers", "href": "topics/request-response.html#scrapy.http.Request.headers"}, {"text": "issue 1992", "href": "https://github.com/scrapy/scrapy/issues/1992"}, {"text": "issue 2400", "href": "https://github.com/scrapy/scrapy/issues/2400"}, {"text": "CookiesMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware"}, {"text": "bytes", "href": "https://docs.python.org/3/library/stdtypes.html#bytes"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "issue 2400", "href": "https://github.com/scrapy/scrapy/issues/2400"}, {"text": "issue 3575", "href": "https://github.com/scrapy/scrapy/issues/3575"}, {"text": "FEEDS", "href": "topics/feed-exports.html#std-setting-FEEDS"}, {"text": "FEED_STORE_EMPTY", "href": "topics/feed-exports.html#std-setting-FEED_STORE_EMPTY"}, {"text": "issue 4621", "href": "https://github.com/scrapy/scrapy/issues/4621"}, {"text": "issue 4626", "href": "https://github.com/scrapy/scrapy/issues/4626"}, {"text": "Spider", "href": "topics/spiders.html#scrapy.spiders.Spider"}, {"text": "coroutine\nsyntax", "href": "topics/coroutines.html"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "item", "href": "topics/items.html#topics-items"}, {"text": "issue 4609", "href": "https://github.com/scrapy/scrapy/issues/4609"}, {"text": "startproject", "href": "topics/commands.html#std-command-startproject"}, {"text": "issue 4604", "href": "https://github.com/scrapy/scrapy/issues/4604"}, {"text": "KeyError", "href": "https://docs.python.org/3/library/exceptions.html#KeyError"}, {"text": "issue 4597", "href": "https://github.com/scrapy/scrapy/issues/4597"}, {"text": "issue 4599", "href": "https://github.com/scrapy/scrapy/issues/4599"}, {"text": "FEEDS", "href": "topics/feed-exports.html#std-setting-FEEDS"}, {"text": "issue 4619", "href": "https://github.com/scrapy/scrapy/issues/4619"}, {"text": "issue 4629", "href": "https://github.com/scrapy/scrapy/issues/4629"}, {"text": "accessing cb_kwargs from errbacks", "href": "topics/request-response.html#errback-cb-kwargs"}, {"text": "issue 4598", "href": "https://github.com/scrapy/scrapy/issues/4598"}, {"text": "issue 4634", "href": "https://github.com/scrapy/scrapy/issues/4634"}, {"text": "chompjs", "href": "https://github.com/Nykakin/chompjs"}, {"text": "Parsing JavaScript code", "href": "topics/dynamic-content.html#topics-parsing-javascript"}, {"text": "issue 4556", "href": "https://github.com/scrapy/scrapy/issues/4556"}, {"text": "issue 4562", "href": "https://github.com/scrapy/scrapy/issues/4562"}, {"text": "Coroutines", "href": "topics/coroutines.html"}, {"text": "issue 4511", "href": "https://github.com/scrapy/scrapy/issues/4511"}, {"text": "issue 4513", "href": "https://github.com/scrapy/scrapy/issues/4513"}, {"text": "Twisted", "href": "https://docs.twisted.org/en/stable/index.html"}, {"text": "issue 4533", "href": "https://github.com/scrapy/scrapy/issues/4533"}, {"text": "screenshot pipeline example", "href": "topics/item-pipeline.html#screenshotpipeline"}, {"text": "coroutine syntax", "href": "topics/coroutines.html"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "issue 4514", "href": "https://github.com/scrapy/scrapy/issues/4514"}, {"text": "issue 4593", "href": "https://github.com/scrapy/scrapy/issues/4593"}, {"text": "scrapy.utils.log.configure_logging()", "href": "topics/logging.html#scrapy.utils.log.configure_logging"}, {"text": "issue 4510", "href": "https://github.com/scrapy/scrapy/issues/4510"}, {"text": "issue 4587", "href": "https://github.com/scrapy/scrapy/issues/4587"}, {"text": "commands", "href": "topics/commands.html#topics-commands"}, {"text": "Request.meta", "href": "topics/request-response.html#scrapy.http.Request.meta"}, {"text": "settings", "href": "topics/settings.html#topics-settings"}, {"text": "signals", "href": "topics/signals.html#topics-signals"}, {"text": "issue 4495", "href": "https://github.com/scrapy/scrapy/issues/4495"}, {"text": "issue 4563", "href": "https://github.com/scrapy/scrapy/issues/4563"}, {"text": "issue 4578", "href": "https://github.com/scrapy/scrapy/issues/4578"}, {"text": "issue 4585", "href": "https://github.com/scrapy/scrapy/issues/4585"}, {"text": "issue 4592", "href": "https://github.com/scrapy/scrapy/issues/4592"}, {"text": "issue 4596", "href": "https://github.com/scrapy/scrapy/issues/4596"}, {"text": "style guidelines", "href": "contributing.html#coding-style"}, {"text": "issue 4237", "href": "https://github.com/scrapy/scrapy/issues/4237"}, {"text": "issue 4525", "href": "https://github.com/scrapy/scrapy/issues/4525"}, {"text": "issue 4538", "href": "https://github.com/scrapy/scrapy/issues/4538"}, {"text": "issue 4539", "href": "https://github.com/scrapy/scrapy/issues/4539"}, {"text": "issue 4540", "href": "https://github.com/scrapy/scrapy/issues/4540"}, {"text": "issue 4542", "href": "https://github.com/scrapy/scrapy/issues/4542"}, {"text": "issue 4543", "href": "https://github.com/scrapy/scrapy/issues/4543"}, {"text": "issue 4544", "href": "https://github.com/scrapy/scrapy/issues/4544"}, {"text": "issue 4545", "href": "https://github.com/scrapy/scrapy/issues/4545"}, {"text": "issue 4557", "href": "https://github.com/scrapy/scrapy/issues/4557"}, {"text": "issue 4558", "href": "https://github.com/scrapy/scrapy/issues/4558"}, {"text": "issue 4566", "href": "https://github.com/scrapy/scrapy/issues/4566"}, {"text": "issue 4568", "href": "https://github.com/scrapy/scrapy/issues/4568"}, {"text": "issue 4572", "href": "https://github.com/scrapy/scrapy/issues/4572"}, {"text": "issue 4550", "href": "https://github.com/scrapy/scrapy/issues/4550"}, {"text": "issue 4553", "href": "https://github.com/scrapy/scrapy/issues/4553"}, {"text": "issue 4568", "href": "https://github.com/scrapy/scrapy/issues/4568"}, {"text": "crawl", "href": "topics/commands.html#std-command-crawl"}, {"text": "runspider", "href": "topics/commands.html#std-command-runspider"}, {"text": "issue 4548", "href": "https://github.com/scrapy/scrapy/issues/4548"}, {"text": "issue 4552", "href": "https://github.com/scrapy/scrapy/issues/4552"}, {"text": "issue 4635", "href": "https://github.com/scrapy/scrapy/issues/4635"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "issue 4521", "href": "https://github.com/scrapy/scrapy/issues/4521"}, {"text": "issue 4588", "href": "https://github.com/scrapy/scrapy/issues/4588"}, {"text": "SpiderLoader", "href": "topics/api.html#scrapy.spiderloader.SpiderLoader"}, {"text": "issue 4549", "href": "https://github.com/scrapy/scrapy/issues/4549"}, {"text": "issue 4560", "href": "https://github.com/scrapy/scrapy/issues/4560"}, {"text": "issue 4518", "href": "https://github.com/scrapy/scrapy/issues/4518"}, {"text": "issue 4615", "href": "https://github.com/scrapy/scrapy/issues/4615"}, {"text": "Pylint", "href": "https://www.pylint.org/"}, {"text": "issue 3727", "href": "https://github.com/scrapy/scrapy/issues/3727"}, {"text": "Mypy", "href": "http://mypy-lang.org/"}, {"text": "issue 4637", "href": "https://github.com/scrapy/scrapy/issues/4637"}, {"text": "issue 4573", "href": "https://github.com/scrapy/scrapy/issues/4573"}, {"text": "issue 4517", "href": "https://github.com/scrapy/scrapy/issues/4517"}, {"text": "issue 4519", "href": "https://github.com/scrapy/scrapy/issues/4519"}, {"text": "issue 4522", "href": "https://github.com/scrapy/scrapy/issues/4522"}, {"text": "issue 4537", "href": "https://github.com/scrapy/scrapy/issues/4537"}, {"text": "FEEDS", "href": "topics/feed-exports.html#std-setting-FEEDS"}, {"text": "Response.ip_address", "href": "topics/request-response.html#scrapy.http.Response.ip_address"}, {"text": "AssertionError", "href": "https://docs.python.org/3/library/exceptions.html#AssertionError"}, {"text": "assert", "href": "https://docs.python.org/3/reference/simple_stmts.html#assert"}, {"text": "-O", "href": "https://docs.python.org/3/using/cmdline.html#cmdoption-O"}, {"text": "AssertionError", "href": "https://docs.python.org/3/library/exceptions.html#AssertionError"}, {"text": "issue 4440", "href": "https://github.com/scrapy/scrapy/issues/4440"}, {"text": "SCHEDULER_DEBUG", "href": "topics/settings.html#std-setting-SCHEDULER_DEBUG"}, {"text": "issue 4385", "href": "https://github.com/scrapy/scrapy/issues/4385"}, {"text": "METAREFRESH_MAXDELAY", "href": "topics/downloader-middleware.html#std-setting-METAREFRESH_MAXDELAY"}, {"text": "issue 4385", "href": "https://github.com/scrapy/scrapy/issues/4385"}, {"text": "issue 4431", "href": "https://github.com/scrapy/scrapy/issues/4431"}, {"text": "Crawler", "href": "topics/api.html#scrapy.crawler.Crawler"}, {"text": "SPIDER_LOADER_CLASS", "href": "topics/settings.html#std-setting-SPIDER_LOADER_CLASS"}, {"text": "issue 4398", "href": "https://github.com/scrapy/scrapy/issues/4398"}, {"text": "issue 4400", "href": "https://github.com/scrapy/scrapy/issues/4400"}, {"text": "FEEDS", "href": "topics/feed-exports.html#std-setting-FEEDS"}, {"text": "issue 1336", "href": "https://github.com/scrapy/scrapy/issues/1336"}, {"text": "issue 3858", "href": "https://github.com/scrapy/scrapy/issues/3858"}, {"text": "issue 4507", "href": "https://github.com/scrapy/scrapy/issues/4507"}, {"text": "FEEDS", "href": "topics/feed-exports.html#std-setting-FEEDS"}, {"text": "issue 1336", "href": "https://github.com/scrapy/scrapy/issues/1336"}, {"text": "issue 3858", "href": "https://github.com/scrapy/scrapy/issues/3858"}, {"text": "issue 4507", "href": "https://github.com/scrapy/scrapy/issues/4507"}, {"text": "crawl", "href": "topics/commands.html#std-command-crawl"}, {"text": "runspider", "href": "topics/commands.html#std-command-runspider"}, {"text": "issue 1336", "href": "https://github.com/scrapy/scrapy/issues/1336"}, {"text": "issue 3858", "href": "https://github.com/scrapy/scrapy/issues/3858"}, {"text": "issue 4507", "href": "https://github.com/scrapy/scrapy/issues/4507"}, {"text": "crawl", "href": "topics/commands.html#std-command-crawl"}, {"text": "runspider", "href": "topics/commands.html#std-command-runspider"}, {"text": "issue 1336", "href": "https://github.com/scrapy/scrapy/issues/1336"}, {"text": "issue 3858", "href": "https://github.com/scrapy/scrapy/issues/3858"}, {"text": "issue 4507", "href": "https://github.com/scrapy/scrapy/issues/4507"}, {"text": "Response.ip_address", "href": "topics/request-response.html#scrapy.http.Response.ip_address"}, {"text": "issue 3903", "href": "https://github.com/scrapy/scrapy/issues/3903"}, {"text": "issue 3940", "href": "https://github.com/scrapy/scrapy/issues/3940"}, {"text": "issue 50", "href": "https://github.com/scrapy/scrapy/issues/50"}, {"text": "issue 3198", "href": "https://github.com/scrapy/scrapy/issues/3198"}, {"text": "issue 4413", "href": "https://github.com/scrapy/scrapy/issues/4413"}, {"text": "issue 4438", "href": "https://github.com/scrapy/scrapy/issues/4438"}, {"text": "Request serialization", "href": "topics/jobs.html#request-serialization"}, {"text": "issue 4500", "href": "https://github.com/scrapy/scrapy/issues/4500"}, {"text": "TypeError", "href": "https://docs.python.org/3/library/exceptions.html#TypeError"}, {"text": "issue 4410", "href": "https://github.com/scrapy/scrapy/issues/4410"}, {"text": "issue 4438", "href": "https://github.com/scrapy/scrapy/issues/4438"}, {"text": "issue 4447", "href": "https://github.com/scrapy/scrapy/issues/4447"}, {"text": "issue 4448", "href": "https://github.com/scrapy/scrapy/issues/4448"}, {"text": "issue 4412", "href": "https://github.com/scrapy/scrapy/issues/4412"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "issue 4295", "href": "https://github.com/scrapy/scrapy/issues/4295"}, {"text": "issue 4390", "href": "https://github.com/scrapy/scrapy/issues/4390"}, {"text": "issue 4456", "href": "https://github.com/scrapy/scrapy/issues/4456"}, {"text": "curl2scrapy", "href": "https://michael-shub.github.io/curl2scrapy/"}, {"text": "issue 4206", "href": "https://github.com/scrapy/scrapy/issues/4206"}, {"text": "issue 4455", "href": "https://github.com/scrapy/scrapy/issues/4455"}, {"text": "issue 4285", "href": "https://github.com/scrapy/scrapy/issues/4285"}, {"text": "issue 4343", "href": "https://github.com/scrapy/scrapy/issues/4343"}, {"text": "issue 4444", "href": "https://github.com/scrapy/scrapy/issues/4444"}, {"text": "issue 4445", "href": "https://github.com/scrapy/scrapy/issues/4445"}, {"text": "issue 4475", "href": "https://github.com/scrapy/scrapy/issues/4475"}, {"text": "issue 4480", "href": "https://github.com/scrapy/scrapy/issues/4480"}, {"text": "issue 4496", "href": "https://github.com/scrapy/scrapy/issues/4496"}, {"text": "issue 4503", "href": "https://github.com/scrapy/scrapy/issues/4503"}, {"text": "issue 4404", "href": "https://github.com/scrapy/scrapy/issues/4404"}, {"text": "StringTransport", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.testing.StringTransport.html"}, {"text": "issue 4409", "href": "https://github.com/scrapy/scrapy/issues/4409"}, {"text": "issue 4384", "href": "https://github.com/scrapy/scrapy/issues/4384"}, {"text": "object", "href": "https://docs.python.org/3/library/functions.html#object"}, {"text": "issue 4430", "href": "https://github.com/scrapy/scrapy/issues/4430"}, {"text": "issue 4472", "href": "https://github.com/scrapy/scrapy/issues/4472"}, {"text": "issue 4468", "href": "https://github.com/scrapy/scrapy/issues/4468"}, {"text": "issue 4469", "href": "https://github.com/scrapy/scrapy/issues/4469"}, {"text": "issue 4471", "href": "https://github.com/scrapy/scrapy/issues/4471"}, {"text": "issue 4481", "href": "https://github.com/scrapy/scrapy/issues/4481"}, {"text": "twisted.internet.defer.returnValue()", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.html#returnValue"}, {"text": "issue 4443", "href": "https://github.com/scrapy/scrapy/issues/4443"}, {"text": "issue 4446", "href": "https://github.com/scrapy/scrapy/issues/4446"}, {"text": "issue 4489", "href": "https://github.com/scrapy/scrapy/issues/4489"}, {"text": "Response.follow_all", "href": "topics/request-response.html#scrapy.http.Response.follow_all"}, {"text": "issue 4408", "href": "https://github.com/scrapy/scrapy/issues/4408"}, {"text": "issue 4420", "href": "https://github.com/scrapy/scrapy/issues/4420"}, {"text": "reactor", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html"}, {"text": "TWISTED_REACTOR", "href": "topics/settings.html#std-setting-TWISTED_REACTOR"}, {"text": "issue 4401", "href": "https://github.com/scrapy/scrapy/issues/4401"}, {"text": "issue 4406", "href": "https://github.com/scrapy/scrapy/issues/4406"}, {"text": "issue 4422", "href": "https://github.com/scrapy/scrapy/issues/4422"}, {"text": "Partial", "href": "topics/coroutines.html"}, {"text": "coroutine syntax", "href": "https://docs.python.org/3/reference/compound_stmts.html#async"}, {"text": "experimental", "href": "topics/asyncio.html"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "Response.follow_all", "href": "topics/request-response.html#scrapy.http.Response.follow_all"}, {"text": "FTP support", "href": "topics/media-pipeline.html#media-pipeline-ftp"}, {"text": "Response.certificate", "href": "topics/request-response.html#scrapy.http.Response.certificate"}, {"text": "DNS_RESOLVER", "href": "topics/settings.html#std-setting-DNS_RESOLVER"}, {"text": "Python 2 end-of-life on\nJanuary 1, 2020", "href": "https://www.python.org/doc/sunset-python-2/"}, {"text": "issue 4091", "href": "https://github.com/scrapy/scrapy/issues/4091"}, {"text": "issue 4114", "href": "https://github.com/scrapy/scrapy/issues/4114"}, {"text": "issue 4115", "href": "https://github.com/scrapy/scrapy/issues/4115"}, {"text": "issue 4121", "href": "https://github.com/scrapy/scrapy/issues/4121"}, {"text": "issue 4138", "href": "https://github.com/scrapy/scrapy/issues/4138"}, {"text": "issue 4231", "href": "https://github.com/scrapy/scrapy/issues/4231"}, {"text": "issue 4242", "href": "https://github.com/scrapy/scrapy/issues/4242"}, {"text": "issue 4304", "href": "https://github.com/scrapy/scrapy/issues/4304"}, {"text": "issue 4309", "href": "https://github.com/scrapy/scrapy/issues/4309"}, {"text": "issue 4373", "href": "https://github.com/scrapy/scrapy/issues/4373"}, {"text": "RETRY_TIMES", "href": "topics/downloader-middleware.html#std-setting-RETRY_TIMES"}, {"text": "issue 3171", "href": "https://github.com/scrapy/scrapy/issues/3171"}, {"text": "issue 3566", "href": "https://github.com/scrapy/scrapy/issues/3566"}, {"text": "LinkExtractor", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"}, {"text": "issue 1837", "href": "https://github.com/scrapy/scrapy/issues/1837"}, {"text": "issue 2067", "href": "https://github.com/scrapy/scrapy/issues/2067"}, {"text": "issue 4066", "href": "https://github.com/scrapy/scrapy/issues/4066"}, {"text": "METAREFRESH_IGNORE_TAGS", "href": "topics/downloader-middleware.html#std-setting-METAREFRESH_IGNORE_TAGS"}, {"text": "issue 3844", "href": "https://github.com/scrapy/scrapy/issues/3844"}, {"text": "issue 4311", "href": "https://github.com/scrapy/scrapy/issues/4311"}, {"text": "HttpCompressionMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware"}, {"text": "issue 4293", "href": "https://github.com/scrapy/scrapy/issues/4293"}, {"text": "DOWNLOAD_HANDLERS", "href": "topics/settings.html#std-setting-DOWNLOAD_HANDLERS"}, {"text": "issue 4126", "href": "https://github.com/scrapy/scrapy/issues/4126"}, {"text": "scrapy.core.scheduler.Scheduler", "href": "topics/scheduler.html#scrapy.core.scheduler.Scheduler"}, {"text": "SCHEDULER_PRIORITY_QUEUE", "href": "topics/settings.html#std-setting-SCHEDULER_PRIORITY_QUEUE"}, {"text": "SCHEDULER_DISK_QUEUE", "href": "topics/settings.html#std-setting-SCHEDULER_DISK_QUEUE"}, {"text": "SCHEDULER_MEMORY_QUEUE", "href": "topics/settings.html#std-setting-SCHEDULER_MEMORY_QUEUE"}, {"text": "issue 4199", "href": "https://github.com/scrapy/scrapy/issues/4199"}, {"text": "Scrapy shell", "href": "topics/shell.html#topics-shell"}, {"text": "issue 4347", "href": "https://github.com/scrapy/scrapy/issues/4347"}, {"text": "issue 4112", "href": "https://github.com/scrapy/scrapy/issues/4112"}, {"text": "issue 4362", "href": "https://github.com/scrapy/scrapy/issues/4362"}, {"text": "issue 4300", "href": "https://github.com/scrapy/scrapy/issues/4300"}, {"text": "issue 4374", "href": "https://github.com/scrapy/scrapy/issues/4374"}, {"text": "issue 4375", "href": "https://github.com/scrapy/scrapy/issues/4375"}, {"text": "scrapy.linkextractors.LinkExtractor", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"}, {"text": "issue 4045", "href": "https://github.com/scrapy/scrapy/issues/4045"}, {"text": "issue 4198", "href": "https://github.com/scrapy/scrapy/issues/4198"}, {"text": "next()", "href": "https://docs.python.org/3/library/functions.html#next"}, {"text": "issue 4153", "href": "https://github.com/scrapy/scrapy/issues/4153"}, {"text": "partial support", "href": "topics/coroutines.html"}, {"text": "coroutine syntax", "href": "https://docs.python.org/3/reference/compound_stmts.html#async"}, {"text": "experimental support", "href": "topics/asyncio.html"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "issue 4010", "href": "https://github.com/scrapy/scrapy/issues/4010"}, {"text": "issue 4259", "href": "https://github.com/scrapy/scrapy/issues/4259"}, {"text": "issue 4269", "href": "https://github.com/scrapy/scrapy/issues/4269"}, {"text": "issue 4270", "href": "https://github.com/scrapy/scrapy/issues/4270"}, {"text": "issue 4271", "href": "https://github.com/scrapy/scrapy/issues/4271"}, {"text": "issue 4316", "href": "https://github.com/scrapy/scrapy/issues/4316"}, {"text": "issue 4318", "href": "https://github.com/scrapy/scrapy/issues/4318"}, {"text": "Response.follow_all", "href": "topics/request-response.html#scrapy.http.Response.follow_all"}, {"text": "Response.follow", "href": "topics/request-response.html#scrapy.http.Response.follow"}, {"text": "issue 2582", "href": "https://github.com/scrapy/scrapy/issues/2582"}, {"text": "issue 4057", "href": "https://github.com/scrapy/scrapy/issues/4057"}, {"text": "issue 4286", "href": "https://github.com/scrapy/scrapy/issues/4286"}, {"text": "Media pipelines", "href": "topics/media-pipeline.html#topics-media-pipeline"}, {"text": "FTP\nstorage", "href": "topics/media-pipeline.html#media-pipeline-ftp"}, {"text": "issue 3928", "href": "https://github.com/scrapy/scrapy/issues/3928"}, {"text": "issue 3961", "href": "https://github.com/scrapy/scrapy/issues/3961"}, {"text": "Response.certificate", "href": "topics/request-response.html#scrapy.http.Response.certificate"}, {"text": "twisted.internet.ssl.Certificate", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.ssl.Certificate.html"}, {"text": "issue 2726", "href": "https://github.com/scrapy/scrapy/issues/2726"}, {"text": "issue 4054", "href": "https://github.com/scrapy/scrapy/issues/4054"}, {"text": "DNS_RESOLVER", "href": "topics/settings.html#std-setting-DNS_RESOLVER"}, {"text": "issue 1031", "href": "https://github.com/scrapy/scrapy/issues/1031"}, {"text": "issue 4227", "href": "https://github.com/scrapy/scrapy/issues/4227"}, {"text": "SCRAPER_SLOT_MAX_ACTIVE_SIZE", "href": "topics/settings.html#std-setting-SCRAPER_SLOT_MAX_ACTIVE_SIZE"}, {"text": "issue 1410", "href": "https://github.com/scrapy/scrapy/issues/1410"}, {"text": "issue 3551", "href": "https://github.com/scrapy/scrapy/issues/3551"}, {"text": "TWISTED_REACTOR", "href": "topics/settings.html#std-setting-TWISTED_REACTOR"}, {"text": "reactor", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html"}, {"text": "enable asyncio support", "href": "topics/asyncio.html"}, {"text": "common macOS issue", "href": "faq.html#faq-specific-reactor"}, {"text": "issue 2905", "href": "https://github.com/scrapy/scrapy/issues/2905"}, {"text": "issue 4294", "href": "https://github.com/scrapy/scrapy/issues/4294"}, {"text": "issue 3884", "href": "https://github.com/scrapy/scrapy/issues/3884"}, {"text": "Response.cb_kwargs", "href": "topics/request-response.html#scrapy.http.Response.cb_kwargs"}, {"text": "Response.request.cb_kwargs", "href": "topics/request-response.html#scrapy.http.Request.cb_kwargs"}, {"text": "issue 4331", "href": "https://github.com/scrapy/scrapy/issues/4331"}, {"text": "Response.follow", "href": "topics/request-response.html#scrapy.http.Response.follow"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "issue 4277", "href": "https://github.com/scrapy/scrapy/issues/4277"}, {"text": "issue 4279", "href": "https://github.com/scrapy/scrapy/issues/4279"}, {"text": "Item loader processors", "href": "topics/loaders.html#topics-loaders-processors"}, {"text": "issue 3899", "href": "https://github.com/scrapy/scrapy/issues/3899"}, {"text": "Rule", "href": "topics/spiders.html#scrapy.spiders.Rule"}, {"text": "issue 4000", "href": "https://github.com/scrapy/scrapy/issues/4000"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "issue 3586", "href": "https://github.com/scrapy/scrapy/issues/3586"}, {"text": "issue 4008", "href": "https://github.com/scrapy/scrapy/issues/4008"}, {"text": "LogFormatter", "href": "topics/logging.html#scrapy.logformatter.LogFormatter"}, {"text": "download_error", "href": "topics/logging.html#scrapy.logformatter.LogFormatter.download_error"}, {"text": "item_error", "href": "topics/logging.html#scrapy.logformatter.LogFormatter.item_error"}, {"text": "item pipelines", "href": "topics/item-pipeline.html#topics-item-pipeline"}, {"text": "spider_error", "href": "topics/logging.html#scrapy.logformatter.LogFormatter.spider_error"}, {"text": "spider callbacks", "href": "topics/spiders.html#topics-spiders"}, {"text": "issue 374", "href": "https://github.com/scrapy/scrapy/issues/374"}, {"text": "issue 3986", "href": "https://github.com/scrapy/scrapy/issues/3986"}, {"text": "issue 3989", "href": "https://github.com/scrapy/scrapy/issues/3989"}, {"text": "issue 4176", "href": "https://github.com/scrapy/scrapy/issues/4176"}, {"text": "issue 4188", "href": "https://github.com/scrapy/scrapy/issues/4188"}, {"text": "pathlib.Path", "href": "https://docs.python.org/3/library/pathlib.html#pathlib.Path"}, {"text": "issue 3731", "href": "https://github.com/scrapy/scrapy/issues/3731"}, {"text": "issue 4074", "href": "https://github.com/scrapy/scrapy/issues/4074"}, {"text": "request_left_downloader", "href": "topics/signals.html#std-signal-request_left_downloader"}, {"text": "issue 4303", "href": "https://github.com/scrapy/scrapy/issues/4303"}, {"text": "issue 3484", "href": "https://github.com/scrapy/scrapy/issues/3484"}, {"text": "issue 3869", "href": "https://github.com/scrapy/scrapy/issues/3869"}, {"text": "Spider", "href": "topics/spiders.html#scrapy.spiders.Spider"}, {"text": "AttributeError", "href": "https://docs.python.org/3/library/exceptions.html#AttributeError"}, {"text": "issue 4133", "href": "https://github.com/scrapy/scrapy/issues/4133"}, {"text": "issue 4170", "href": "https://github.com/scrapy/scrapy/issues/4170"}, {"text": "BaseItemExporter", "href": "topics/exporters.html#scrapy.exporters.BaseItemExporter"}, {"text": "issue 4193", "href": "https://github.com/scrapy/scrapy/issues/4193"}, {"text": "issue 4370", "href": "https://github.com/scrapy/scrapy/issues/4370"}, {"text": "issue 4104", "href": "https://github.com/scrapy/scrapy/issues/4104"}, {"text": "DOWNLOAD_HANDLERS", "href": "topics/settings.html#std-setting-DOWNLOAD_HANDLERS"}, {"text": "issue 4126", "href": "https://github.com/scrapy/scrapy/issues/4126"}, {"text": "allowing it to be used as a sequence", "href": "https://lgtm.com/rules/4850080/"}, {"text": "issue 4153", "href": "https://github.com/scrapy/scrapy/issues/4153"}, {"text": "crawl", "href": "topics/commands.html#std-command-crawl"}, {"text": "issue 4175", "href": "https://github.com/scrapy/scrapy/issues/4175"}, {"text": "issue 4207", "href": "https://github.com/scrapy/scrapy/issues/4207"}, {"text": "LinkExtractor.extract_links", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links"}, {"text": "issue 998", "href": "https://github.com/scrapy/scrapy/issues/998"}, {"text": "issue 1403", "href": "https://github.com/scrapy/scrapy/issues/1403"}, {"text": "issue 1949", "href": "https://github.com/scrapy/scrapy/issues/1949"}, {"text": "issue 4321", "href": "https://github.com/scrapy/scrapy/issues/4321"}, {"text": "SPIDER_MIDDLEWARES", "href": "topics/settings.html#std-setting-SPIDER_MIDDLEWARES"}, {"text": "issue 4260", "href": "https://github.com/scrapy/scrapy/issues/4260"}, {"text": "issue 4272", "href": "https://github.com/scrapy/scrapy/issues/4272"}, {"text": "issue 4032", "href": "https://github.com/scrapy/scrapy/issues/4032"}, {"text": "issue 4042", "href": "https://github.com/scrapy/scrapy/issues/4042"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "issue 2552", "href": "https://github.com/scrapy/scrapy/issues/2552"}, {"text": "issue 4094", "href": "https://github.com/scrapy/scrapy/issues/4094"}, {"text": "MailSender", "href": "topics/email.html#scrapy.mail.MailSender"}, {"text": "issue 4229", "href": "https://github.com/scrapy/scrapy/issues/4229"}, {"text": "issue 4239", "href": "https://github.com/scrapy/scrapy/issues/4239"}, {"text": "DUPEFILTER_CLASS", "href": "topics/settings.html#std-setting-DUPEFILTER_CLASS"}, {"text": "issue 4283", "href": "https://github.com/scrapy/scrapy/issues/4283"}, {"text": "issue 4122", "href": "https://github.com/scrapy/scrapy/issues/4122"}, {"text": "issue 4291", "href": "https://github.com/scrapy/scrapy/issues/4291"}, {"text": "TypeError", "href": "https://docs.python.org/3/library/exceptions.html#TypeError"}, {"text": "issue 4123", "href": "https://github.com/scrapy/scrapy/issues/4123"}, {"text": "ValueError", "href": "https://docs.python.org/3/library/exceptions.html#ValueError"}, {"text": "issue 4128", "href": "https://github.com/scrapy/scrapy/issues/4128"}, {"text": "issue 4148", "href": "https://github.com/scrapy/scrapy/issues/4148"}, {"text": "issue 4152", "href": "https://github.com/scrapy/scrapy/issues/4152"}, {"text": "issue 4169", "href": "https://github.com/scrapy/scrapy/issues/4169"}, {"text": "issue 4173", "href": "https://github.com/scrapy/scrapy/issues/4173"}, {"text": "issue 4183", "href": "https://github.com/scrapy/scrapy/issues/4183"}, {"text": "LinkExtractor.extract_links", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links"}, {"text": "Link Extractors", "href": "topics/link-extractors.html#topics-link-extractors"}, {"text": "issue 4045", "href": "https://github.com/scrapy/scrapy/issues/4045"}, {"text": "ItemLoader.item", "href": "topics/loaders.html#scrapy.loader.ItemLoader.item"}, {"text": "issue 3574", "href": "https://github.com/scrapy/scrapy/issues/3574"}, {"text": "issue 4099", "href": "https://github.com/scrapy/scrapy/issues/4099"}, {"text": "logging.basicConfig()", "href": "https://docs.python.org/3/library/logging.html#logging.basicConfig"}, {"text": "CrawlerProcess", "href": "topics/api.html#scrapy.crawler.CrawlerProcess"}, {"text": "issue 2149", "href": "https://github.com/scrapy/scrapy/issues/2149"}, {"text": "issue 2352", "href": "https://github.com/scrapy/scrapy/issues/2352"}, {"text": "issue 3146", "href": "https://github.com/scrapy/scrapy/issues/3146"}, {"text": "issue 3960", "href": "https://github.com/scrapy/scrapy/issues/3960"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "when using persistence", "href": "topics/jobs.html#request-serialization"}, {"text": "issue 4124", "href": "https://github.com/scrapy/scrapy/issues/4124"}, {"text": "issue 4139", "href": "https://github.com/scrapy/scrapy/issues/4139"}, {"text": "custom image pipeline", "href": "topics/media-pipeline.html#media-pipeline-example"}, {"text": "issue 4034", "href": "https://github.com/scrapy/scrapy/issues/4034"}, {"text": "issue 4252", "href": "https://github.com/scrapy/scrapy/issues/4252"}, {"text": "media pipeline", "href": "topics/media-pipeline.html#topics-media-pipeline"}, {"text": "issue 4290", "href": "https://github.com/scrapy/scrapy/issues/4290"}, {"text": "scrapy.core.scheduler.Scheduler", "href": "topics/scheduler.html#scrapy.core.scheduler.Scheduler"}, {"text": "issue 4274", "href": "https://github.com/scrapy/scrapy/issues/4274"}, {"text": "issue 4059", "href": "https://github.com/scrapy/scrapy/issues/4059"}, {"text": "issue 4142", "href": "https://github.com/scrapy/scrapy/issues/4142"}, {"text": "issue 4146", "href": "https://github.com/scrapy/scrapy/issues/4146"}, {"text": "issue 4171", "href": "https://github.com/scrapy/scrapy/issues/4171"}, {"text": "issue 4184", "href": "https://github.com/scrapy/scrapy/issues/4184"}, {"text": "issue 4190", "href": "https://github.com/scrapy/scrapy/issues/4190"}, {"text": "issue 4247", "href": "https://github.com/scrapy/scrapy/issues/4247"}, {"text": "issue 4258", "href": "https://github.com/scrapy/scrapy/issues/4258"}, {"text": "issue 4282", "href": "https://github.com/scrapy/scrapy/issues/4282"}, {"text": "issue 4288", "href": "https://github.com/scrapy/scrapy/issues/4288"}, {"text": "issue 4305", "href": "https://github.com/scrapy/scrapy/issues/4305"}, {"text": "issue 4308", "href": "https://github.com/scrapy/scrapy/issues/4308"}, {"text": "issue 4323", "href": "https://github.com/scrapy/scrapy/issues/4323"}, {"text": "issue 4338", "href": "https://github.com/scrapy/scrapy/issues/4338"}, {"text": "issue 4359", "href": "https://github.com/scrapy/scrapy/issues/4359"}, {"text": "issue 4361", "href": "https://github.com/scrapy/scrapy/issues/4361"}, {"text": "issue 4086", "href": "https://github.com/scrapy/scrapy/issues/4086"}, {"text": "issue 4088", "href": "https://github.com/scrapy/scrapy/issues/4088"}, {"text": "Scrapy at a glance", "href": "intro/overview.html#intro-overview"}, {"text": "issue 4213", "href": "https://github.com/scrapy/scrapy/issues/4213"}, {"text": "intersphinx", "href": "https://www.sphinx-doc.org/en/master/usage/extensions/intersphinx.html#module-sphinx.ext.intersphinx"}, {"text": "issue 4147", "href": "https://github.com/scrapy/scrapy/issues/4147"}, {"text": "issue 4172", "href": "https://github.com/scrapy/scrapy/issues/4172"}, {"text": "issue 4185", "href": "https://github.com/scrapy/scrapy/issues/4185"}, {"text": "issue 4194", "href": "https://github.com/scrapy/scrapy/issues/4194"}, {"text": "issue 4197", "href": "https://github.com/scrapy/scrapy/issues/4197"}, {"text": "issue 4140", "href": "https://github.com/scrapy/scrapy/issues/4140"}, {"text": "issue 4249", "href": "https://github.com/scrapy/scrapy/issues/4249"}, {"text": "issue 4143", "href": "https://github.com/scrapy/scrapy/issues/4143"}, {"text": "issue 4275", "href": "https://github.com/scrapy/scrapy/issues/4275"}, {"text": "issue 2545", "href": "https://github.com/scrapy/scrapy/issues/2545"}, {"text": "issue 4114", "href": "https://github.com/scrapy/scrapy/issues/4114"}, {"text": "Bandit", "href": "https://bandit.readthedocs.io/"}, {"text": "issue 4162", "href": "https://github.com/scrapy/scrapy/issues/4162"}, {"text": "issue 4181", "href": "https://github.com/scrapy/scrapy/issues/4181"}, {"text": "Flake8", "href": "https://flake8.pycqa.org/en/latest/"}, {"text": "issue 3944", "href": "https://github.com/scrapy/scrapy/issues/3944"}, {"text": "issue 3945", "href": "https://github.com/scrapy/scrapy/issues/3945"}, {"text": "issue 4137", "href": "https://github.com/scrapy/scrapy/issues/4137"}, {"text": "issue 4157", "href": "https://github.com/scrapy/scrapy/issues/4157"}, {"text": "issue 4167", "href": "https://github.com/scrapy/scrapy/issues/4167"}, {"text": "issue 4174", "href": "https://github.com/scrapy/scrapy/issues/4174"}, {"text": "issue 4186", "href": "https://github.com/scrapy/scrapy/issues/4186"}, {"text": "issue 4195", "href": "https://github.com/scrapy/scrapy/issues/4195"}, {"text": "issue 4238", "href": "https://github.com/scrapy/scrapy/issues/4238"}, {"text": "issue 4246", "href": "https://github.com/scrapy/scrapy/issues/4246"}, {"text": "issue 4355", "href": "https://github.com/scrapy/scrapy/issues/4355"}, {"text": "issue 4360", "href": "https://github.com/scrapy/scrapy/issues/4360"}, {"text": "issue 4365", "href": "https://github.com/scrapy/scrapy/issues/4365"}, {"text": "issue 4097", "href": "https://github.com/scrapy/scrapy/issues/4097"}, {"text": "issue 4218", "href": "https://github.com/scrapy/scrapy/issues/4218"}, {"text": "issue 4236", "href": "https://github.com/scrapy/scrapy/issues/4236"}, {"text": "issue 4163", "href": "https://github.com/scrapy/scrapy/issues/4163"}, {"text": "issue 4164", "href": "https://github.com/scrapy/scrapy/issues/4164"}, {"text": "issue 4014", "href": "https://github.com/scrapy/scrapy/issues/4014"}, {"text": "issue 4095", "href": "https://github.com/scrapy/scrapy/issues/4095"}, {"text": "issue 4244", "href": "https://github.com/scrapy/scrapy/issues/4244"}, {"text": "issue 4268", "href": "https://github.com/scrapy/scrapy/issues/4268"}, {"text": "issue 4372", "href": "https://github.com/scrapy/scrapy/issues/4372"}, {"text": "tox", "href": "https://tox.wiki/en/latest/index.html"}, {"text": "Bandit", "href": "https://bandit.readthedocs.io/"}, {"text": "Flake8", "href": "https://flake8.pycqa.org/en/latest/"}, {"text": "issue 4179", "href": "https://github.com/scrapy/scrapy/issues/4179"}, {"text": "issue 3937", "href": "https://github.com/scrapy/scrapy/issues/3937"}, {"text": "issue 4208", "href": "https://github.com/scrapy/scrapy/issues/4208"}, {"text": "issue 4209", "href": "https://github.com/scrapy/scrapy/issues/4209"}, {"text": "issue 4210", "href": "https://github.com/scrapy/scrapy/issues/4210"}, {"text": "issue 4212", "href": "https://github.com/scrapy/scrapy/issues/4212"}, {"text": "issue 4369", "href": "https://github.com/scrapy/scrapy/issues/4369"}, {"text": "issue 4376", "href": "https://github.com/scrapy/scrapy/issues/4376"}, {"text": "issue 4378", "href": "https://github.com/scrapy/scrapy/issues/4378"}, {"text": "scrapy.core.scheduler", "href": "topics/scheduler.html#module-scrapy.core.scheduler"}, {"text": "issue 3884", "href": "https://github.com/scrapy/scrapy/issues/3884"}, {"text": "HttpProxyMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "HttpProxyMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "HttpProxyMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "HttpProxyMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "cjvr-mfj7-j4j8 security advisory", "href": "https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "public suffix", "href": "https://publicsuffix.org/"}, {"text": "mfjm-vh54-3f96\nsecurity advisory", "href": "https://github.com/scrapy/scrapy/security/advisories/GHSA-mfjm-vh54-3f96"}, {"text": "HttpAuthMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware"}, {"text": "w3lib.http.basic_auth_header()", "href": "https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header"}, {"text": "scrapy-splash", "href": "https://github.com/scrapy-plugins/scrapy-splash"}, {"text": "Request.from_curl", "href": "topics/request-response.html#scrapy.http.Request.from_curl"}, {"text": "ROBOTSTXT_PARSER", "href": "topics/settings.html#std-setting-ROBOTSTXT_PARSER"}, {"text": "ROBOTSTXT_USER_AGENT", "href": "topics/settings.html#std-setting-ROBOTSTXT_USER_AGENT"}, {"text": "DOWNLOADER_CLIENT_TLS_CIPHERS", "href": "topics/settings.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS"}, {"text": "DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING", "href": "topics/settings.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING"}, {"text": "cssselect", "href": "https://cssselect.readthedocs.io/en/latest/index.html"}, {"text": "cryptography", "href": "https://cryptography.io/en/latest/"}, {"text": "lxml", "href": "https://lxml.de/"}, {"text": "pyOpenSSL", "href": "https://www.pyopenssl.org/en/stable/"}, {"text": "queuelib", "href": "https://github.com/scrapy/queuelib"}, {"text": "service_identity", "href": "https://service-identity.readthedocs.io/en/stable/"}, {"text": "six", "href": "https://six.readthedocs.io/"}, {"text": "Twisted", "href": "https://twistedmatrix.com/trac/"}, {"text": "zope.interface", "href": "https://zopeinterface.readthedocs.io/en/latest/"}, {"text": "issue 3892", "href": "https://github.com/scrapy/scrapy/issues/3892"}, {"text": "JsonRequest", "href": "topics/request-response.html#scrapy.http.JsonRequest"}, {"text": "issue 3929", "href": "https://github.com/scrapy/scrapy/issues/3929"}, {"text": "issue 3982", "href": "https://github.com/scrapy/scrapy/issues/3982"}, {"text": "DOWNLOADER_CLIENTCONTEXTFACTORY", "href": "topics/settings.html#std-setting-DOWNLOADER_CLIENTCONTEXTFACTORY"}, {"text": "issue 2111", "href": "https://github.com/scrapy/scrapy/issues/2111"}, {"text": "issue 3392", "href": "https://github.com/scrapy/scrapy/issues/3392"}, {"text": "issue 3442", "href": "https://github.com/scrapy/scrapy/issues/3442"}, {"text": "issue 3450", "href": "https://github.com/scrapy/scrapy/issues/3450"}, {"text": "ItemLoader", "href": "topics/loaders.html#scrapy.loader.ItemLoader"}, {"text": "issue 3804", "href": "https://github.com/scrapy/scrapy/issues/3804"}, {"text": "issue 3819", "href": "https://github.com/scrapy/scrapy/issues/3819"}, {"text": "issue 3897", "href": "https://github.com/scrapy/scrapy/issues/3897"}, {"text": "issue 3976", "href": "https://github.com/scrapy/scrapy/issues/3976"}, {"text": "issue 3998", "href": "https://github.com/scrapy/scrapy/issues/3998"}, {"text": "issue 4036", "href": "https://github.com/scrapy/scrapy/issues/4036"}, {"text": "Request.from_curl", "href": "topics/request-response.html#scrapy.http.Request.from_curl"}, {"text": "creating a request from a cURL command", "href": "topics/developer-tools.html#requests-from-curl"}, {"text": "issue 2985", "href": "https://github.com/scrapy/scrapy/issues/2985"}, {"text": "issue 3862", "href": "https://github.com/scrapy/scrapy/issues/3862"}, {"text": "ROBOTSTXT_PARSER", "href": "topics/settings.html#std-setting-ROBOTSTXT_PARSER"}, {"text": "robots.txt", "href": "https://www.robotstxt.org/"}, {"text": "RobotFileParser", "href": "topics/downloader-middleware.html#python-robotfileparser"}, {"text": "Protego", "href": "topics/downloader-middleware.html#protego-parser"}, {"text": "Reppy", "href": "topics/downloader-middleware.html#reppy-parser"}, {"text": "Robotexclusionrulesparser", "href": "topics/downloader-middleware.html#rerp-parser"}, {"text": "implement support for additional parsers", "href": "topics/downloader-middleware.html#support-for-new-robots-parser"}, {"text": "issue 754", "href": "https://github.com/scrapy/scrapy/issues/754"}, {"text": "issue 2669", "href": "https://github.com/scrapy/scrapy/issues/2669"}, {"text": "issue 3796", "href": "https://github.com/scrapy/scrapy/issues/3796"}, {"text": "issue 3935", "href": "https://github.com/scrapy/scrapy/issues/3935"}, {"text": "issue 3969", "href": "https://github.com/scrapy/scrapy/issues/3969"}, {"text": "issue 4006", "href": "https://github.com/scrapy/scrapy/issues/4006"}, {"text": "ROBOTSTXT_USER_AGENT", "href": "topics/settings.html#std-setting-ROBOTSTXT_USER_AGENT"}, {"text": "robots.txt", "href": "https://www.robotstxt.org/"}, {"text": "issue 3931", "href": "https://github.com/scrapy/scrapy/issues/3931"}, {"text": "issue 3966", "href": "https://github.com/scrapy/scrapy/issues/3966"}, {"text": "Rule", "href": "topics/spiders.html#scrapy.spiders.Rule"}, {"text": "LinkExtractor", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"}, {"text": "issue 781", "href": "https://github.com/scrapy/scrapy/issues/781"}, {"text": "issue 4016", "href": "https://github.com/scrapy/scrapy/issues/4016"}, {"text": "DOWNLOADER_CLIENT_TLS_CIPHERS", "href": "topics/settings.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS"}, {"text": "issue 3392", "href": "https://github.com/scrapy/scrapy/issues/3392"}, {"text": "issue 3442", "href": "https://github.com/scrapy/scrapy/issues/3442"}, {"text": "DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING", "href": "topics/settings.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING"}, {"text": "issue 2111", "href": "https://github.com/scrapy/scrapy/issues/2111"}, {"text": "issue 3450", "href": "https://github.com/scrapy/scrapy/issues/3450"}, {"text": "Request.cb_kwargs", "href": "topics/request-response.html#scrapy.http.Request.cb_kwargs"}, {"text": "@cb_kwargs", "href": "topics/contracts.html#scrapy.contracts.default.CallbackKeywordArgumentsContract"}, {"text": "spider contract", "href": "topics/contracts.html#topics-contracts"}, {"text": "issue 3985", "href": "https://github.com/scrapy/scrapy/issues/3985"}, {"text": "issue 3988", "href": "https://github.com/scrapy/scrapy/issues/3988"}, {"text": "@scrapes", "href": "topics/contracts.html#scrapy.contracts.default.ScrapesContract"}, {"text": "issue 766", "href": "https://github.com/scrapy/scrapy/issues/766"}, {"text": "issue 3939", "href": "https://github.com/scrapy/scrapy/issues/3939"}, {"text": "Custom log formats", "href": "topics/logging.html#custom-log-formats"}, {"text": "LOG_FORMATTER", "href": "topics/settings.html#std-setting-LOG_FORMATTER"}, {"text": "issue 3984", "href": "https://github.com/scrapy/scrapy/issues/3984"}, {"text": "issue 3987", "href": "https://github.com/scrapy/scrapy/issues/3987"}, {"text": "Zsh", "href": "https://www.zsh.org/"}, {"text": "issue 4069", "href": "https://github.com/scrapy/scrapy/issues/4069"}, {"text": "ItemLoader.load_item()", "href": "topics/loaders.html#scrapy.loader.ItemLoader.load_item"}, {"text": "ItemLoader.get_output_value()", "href": "topics/loaders.html#scrapy.loader.ItemLoader.get_output_value"}, {"text": "ItemLoader.load_item()", "href": "topics/loaders.html#scrapy.loader.ItemLoader.load_item"}, {"text": "issue 3804", "href": "https://github.com/scrapy/scrapy/issues/3804"}, {"text": "issue 3819", "href": "https://github.com/scrapy/scrapy/issues/3819"}, {"text": "issue 3897", "href": "https://github.com/scrapy/scrapy/issues/3897"}, {"text": "issue 3976", "href": "https://github.com/scrapy/scrapy/issues/3976"}, {"text": "issue 3998", "href": "https://github.com/scrapy/scrapy/issues/3998"}, {"text": "issue 4036", "href": "https://github.com/scrapy/scrapy/issues/4036"}, {"text": "DummyStatsCollector", "href": "topics/stats.html#scrapy.statscollectors.DummyStatsCollector"}, {"text": "TypeError", "href": "https://docs.python.org/3/library/exceptions.html#TypeError"}, {"text": "issue 4007", "href": "https://github.com/scrapy/scrapy/issues/4007"}, {"text": "issue 4052", "href": "https://github.com/scrapy/scrapy/issues/4052"}, {"text": "FilesPipeline.file_path", "href": "topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.file_path"}, {"text": "ImagesPipeline.file_path", "href": "topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline.file_path"}, {"text": "registered with IANA", "href": "https://www.iana.org/assignments/media-types/media-types.xhtml"}, {"text": "issue 1287", "href": "https://github.com/scrapy/scrapy/issues/1287"}, {"text": "issue 3953", "href": "https://github.com/scrapy/scrapy/issues/3953"}, {"text": "issue 3954", "href": "https://github.com/scrapy/scrapy/issues/3954"}, {"text": "botocore", "href": "https://github.com/boto/botocore"}, {"text": "issue 3904", "href": "https://github.com/scrapy/scrapy/issues/3904"}, {"text": "issue 3905", "href": "https://github.com/scrapy/scrapy/issues/3905"}, {"text": "issue 3941", "href": "https://github.com/scrapy/scrapy/issues/3941"}, {"text": "issue 3920", "href": "https://github.com/scrapy/scrapy/issues/3920"}, {"text": "custom log\nformat", "href": "topics/logging.html#custom-log-formats"}, {"text": "issue 3616", "href": "https://github.com/scrapy/scrapy/issues/3616"}, {"text": "issue 3660", "href": "https://github.com/scrapy/scrapy/issues/3660"}, {"text": "MarshalItemExporter", "href": "topics/exporters.html#scrapy.exporters.MarshalItemExporter"}, {"text": "PythonItemExporter", "href": "topics/exporters.html#scrapy.exporters.PythonItemExporter"}, {"text": "issue 3973", "href": "https://github.com/scrapy/scrapy/issues/3973"}, {"text": "ItemMeta", "href": "topics/items.html#scrapy.item.ItemMeta"}, {"text": "issue 3999", "href": "https://github.com/scrapy/scrapy/issues/3999"}, {"text": "issue 2998", "href": "https://github.com/scrapy/scrapy/issues/2998"}, {"text": "issue 3398", "href": "https://github.com/scrapy/scrapy/issues/3398"}, {"text": "issue 3597", "href": "https://github.com/scrapy/scrapy/issues/3597"}, {"text": "issue 3894", "href": "https://github.com/scrapy/scrapy/issues/3894"}, {"text": "issue 3934", "href": "https://github.com/scrapy/scrapy/issues/3934"}, {"text": "issue 3978", "href": "https://github.com/scrapy/scrapy/issues/3978"}, {"text": "issue 3993", "href": "https://github.com/scrapy/scrapy/issues/3993"}, {"text": "issue 4022", "href": "https://github.com/scrapy/scrapy/issues/4022"}, {"text": "issue 4028", "href": "https://github.com/scrapy/scrapy/issues/4028"}, {"text": "issue 4033", "href": "https://github.com/scrapy/scrapy/issues/4033"}, {"text": "issue 4046", "href": "https://github.com/scrapy/scrapy/issues/4046"}, {"text": "issue 4050", "href": "https://github.com/scrapy/scrapy/issues/4050"}, {"text": "issue 4055", "href": "https://github.com/scrapy/scrapy/issues/4055"}, {"text": "issue 4056", "href": "https://github.com/scrapy/scrapy/issues/4056"}, {"text": "issue 4061", "href": "https://github.com/scrapy/scrapy/issues/4061"}, {"text": "issue 4072", "href": "https://github.com/scrapy/scrapy/issues/4072"}, {"text": "issue 4071", "href": "https://github.com/scrapy/scrapy/issues/4071"}, {"text": "issue 4079", "href": "https://github.com/scrapy/scrapy/issues/4079"}, {"text": "issue 4081", "href": "https://github.com/scrapy/scrapy/issues/4081"}, {"text": "issue 4089", "href": "https://github.com/scrapy/scrapy/issues/4089"}, {"text": "issue 4093", "href": "https://github.com/scrapy/scrapy/issues/4093"}, {"text": "issue 4015", "href": "https://github.com/scrapy/scrapy/issues/4015"}, {"text": "LevelDB", "href": "https://github.com/google/leveldb"}, {"text": "HttpCacheMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware"}, {"text": "issue 4085", "href": "https://github.com/scrapy/scrapy/issues/4085"}, {"text": "issue 4092", "href": "https://github.com/scrapy/scrapy/issues/4092"}, {"text": "issue 3910", "href": "https://github.com/scrapy/scrapy/issues/3910"}, {"text": "issue 3999", "href": "https://github.com/scrapy/scrapy/issues/3999"}, {"text": "botocore", "href": "https://github.com/boto/botocore"}, {"text": "Pillow", "href": "https://python-pillow.org/"}, {"text": "issue 3892", "href": "https://github.com/scrapy/scrapy/issues/3892"}, {"text": "issue 3126", "href": "https://github.com/scrapy/scrapy/issues/3126"}, {"text": "issue 3471", "href": "https://github.com/scrapy/scrapy/issues/3471"}, {"text": "issue 3749", "href": "https://github.com/scrapy/scrapy/issues/3749"}, {"text": "issue 3754", "href": "https://github.com/scrapy/scrapy/issues/3754"}, {"text": "issue 3923", "href": "https://github.com/scrapy/scrapy/issues/3923"}, {"text": "issue 3391", "href": "https://github.com/scrapy/scrapy/issues/3391"}, {"text": "issue 3907", "href": "https://github.com/scrapy/scrapy/issues/3907"}, {"text": "issue 3946", "href": "https://github.com/scrapy/scrapy/issues/3946"}, {"text": "issue 3950", "href": "https://github.com/scrapy/scrapy/issues/3950"}, {"text": "issue 4023", "href": "https://github.com/scrapy/scrapy/issues/4023"}, {"text": "issue 4031", "href": "https://github.com/scrapy/scrapy/issues/4031"}, {"text": "issue 3804", "href": "https://github.com/scrapy/scrapy/issues/3804"}, {"text": "issue 3819", "href": "https://github.com/scrapy/scrapy/issues/3819"}, {"text": "issue 3897", "href": "https://github.com/scrapy/scrapy/issues/3897"}, {"text": "issue 3976", "href": "https://github.com/scrapy/scrapy/issues/3976"}, {"text": "ItemLoader.load_item()", "href": "topics/loaders.html#scrapy.loader.ItemLoader.load_item"}, {"text": "ItemLoader.get_output_value()", "href": "topics/loaders.html#scrapy.loader.ItemLoader.get_output_value"}, {"text": "ItemLoader.load_item()", "href": "topics/loaders.html#scrapy.loader.ItemLoader.load_item"}, {"text": "issue 3912", "href": "https://github.com/scrapy/scrapy/issues/3912"}, {"text": "issue 3918", "href": "https://github.com/scrapy/scrapy/issues/3918"}, {"text": "issue 3889", "href": "https://github.com/scrapy/scrapy/issues/3889"}, {"text": "issue 3893", "href": "https://github.com/scrapy/scrapy/issues/3893"}, {"text": "issue 3896", "href": "https://github.com/scrapy/scrapy/issues/3896"}, {"text": "RETRY_HTTP_CODES", "href": "topics/downloader-middleware.html#std-setting-RETRY_HTTP_CODES"}, {"text": "RETRY_HTTP_CODES", "href": "topics/downloader-middleware.html#std-setting-RETRY_HTTP_CODES"}, {"text": "Crawler", "href": "topics/api.html#scrapy.crawler.Crawler"}, {"text": "CrawlerRunner.crawl", "href": "topics/api.html#scrapy.crawler.CrawlerRunner.crawl"}, {"text": "CrawlerRunner.create_crawler", "href": "topics/api.html#scrapy.crawler.CrawlerRunner.create_crawler"}, {"text": "Spider", "href": "topics/spiders.html#scrapy.spiders.Spider"}, {"text": "Spider", "href": "topics/spiders.html#scrapy.spiders.Spider"}, {"text": "Spider", "href": "topics/spiders.html#scrapy.spiders.Spider"}, {"text": "Spider", "href": "topics/spiders.html#scrapy.spiders.Spider"}, {"text": "SCHEDULER_PRIORITY_QUEUE", "href": "topics/settings.html#std-setting-SCHEDULER_PRIORITY_QUEUE"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Scheduler", "href": "topics/scheduler.html#scrapy.core.scheduler.Scheduler"}, {"text": "SCHEDULER", "href": "topics/settings.html#std-setting-SCHEDULER"}, {"text": "enabled", "href": "topics/broad-crawls.html#broad-crawls-scheduler-priority-queue"}, {"text": "CONCURRENT_REQUESTS_PER_IP", "href": "topics/settings.html#std-setting-CONCURRENT_REQUESTS_PER_IP"}, {"text": "issue 3520", "href": "https://github.com/scrapy/scrapy/issues/3520"}, {"text": "Request.cb_kwargs", "href": "topics/request-response.html#scrapy.http.Request.cb_kwargs"}, {"text": "issue 1138", "href": "https://github.com/scrapy/scrapy/issues/1138"}, {"text": "issue 3563", "href": "https://github.com/scrapy/scrapy/issues/3563"}, {"text": "JSONRequest", "href": "topics/request-response.html#scrapy.http.JsonRequest"}, {"text": "issue 3504", "href": "https://github.com/scrapy/scrapy/issues/3504"}, {"text": "issue 3505", "href": "https://github.com/scrapy/scrapy/issues/3505"}, {"text": "Rule", "href": "topics/spiders.html#scrapy.spiders.Rule"}, {"text": "Response", "href": "topics/request-response.html#scrapy.http.Response"}, {"text": "issue 3682", "href": "https://github.com/scrapy/scrapy/issues/3682"}, {"text": "LinkExtractor", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"}, {"text": "issue 3622", "href": "https://github.com/scrapy/scrapy/issues/3622"}, {"text": "issue 3635", "href": "https://github.com/scrapy/scrapy/issues/3635"}, {"text": "FEED_STORAGE_S3_ACL", "href": "topics/feed-exports.html#std-setting-FEED_STORAGE_S3_ACL"}, {"text": "issue 3607", "href": "https://github.com/scrapy/scrapy/issues/3607"}, {"text": "FEED_STORAGE_FTP_ACTIVE", "href": "topics/feed-exports.html#std-setting-FEED_STORAGE_FTP_ACTIVE"}, {"text": "issue 3829", "href": "https://github.com/scrapy/scrapy/issues/3829"}, {"text": "METAREFRESH_IGNORE_TAGS", "href": "topics/downloader-middleware.html#std-setting-METAREFRESH_IGNORE_TAGS"}, {"text": "issue 1422", "href": "https://github.com/scrapy/scrapy/issues/1422"}, {"text": "issue 3768", "href": "https://github.com/scrapy/scrapy/issues/3768"}, {"text": "redirect_reasons", "href": "topics/downloader-middleware.html#std-reqmeta-redirect_reasons"}, {"text": "issue 3581", "href": "https://github.com/scrapy/scrapy/issues/3581"}, {"text": "issue 3687", "href": "https://github.com/scrapy/scrapy/issues/3687"}, {"text": "check", "href": "topics/commands.html#std-command-check"}, {"text": "detecting contract\ncheck runs from code", "href": "topics/contracts.html#detecting-contract-check-runs"}, {"text": "issue 3704", "href": "https://github.com/scrapy/scrapy/issues/3704"}, {"text": "issue 3739", "href": "https://github.com/scrapy/scrapy/issues/3739"}, {"text": "deep-copy items", "href": "topics/items.html#copying-items"}, {"text": "issue 1493", "href": "https://github.com/scrapy/scrapy/issues/1493"}, {"text": "issue 3671", "href": "https://github.com/scrapy/scrapy/issues/3671"}, {"text": "CoreStats", "href": "topics/extensions.html#scrapy.extensions.corestats.CoreStats"}, {"text": "issue 3638", "href": "https://github.com/scrapy/scrapy/issues/3638"}, {"text": "ItemLoader", "href": "topics/loaders.html#scrapy.loader.ItemLoader"}, {"text": "input and output\nprocessors", "href": "topics/loaders.html#topics-loaders-processors"}, {"text": "issue 3836", "href": "https://github.com/scrapy/scrapy/issues/3836"}, {"text": "issue 3840", "href": "https://github.com/scrapy/scrapy/issues/3840"}, {"text": "Crawler", "href": "topics/api.html#scrapy.crawler.Crawler"}, {"text": "CrawlerRunner.crawl", "href": "topics/api.html#scrapy.crawler.CrawlerRunner.crawl"}, {"text": "CrawlerRunner.create_crawler", "href": "topics/api.html#scrapy.crawler.CrawlerRunner.create_crawler"}, {"text": "Spider", "href": "topics/spiders.html#scrapy.spiders.Spider"}, {"text": "issue 2283", "href": "https://github.com/scrapy/scrapy/issues/2283"}, {"text": "issue 3610", "href": "https://github.com/scrapy/scrapy/issues/3610"}, {"text": "issue 3872", "href": "https://github.com/scrapy/scrapy/issues/3872"}, {"text": "process_spider_exception()", "href": "topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"}, {"text": "issue 220", "href": "https://github.com/scrapy/scrapy/issues/220"}, {"text": "issue 2061", "href": "https://github.com/scrapy/scrapy/issues/2061"}, {"text": "KeyboardInterrupt", "href": "https://docs.python.org/3/library/exceptions.html#KeyboardInterrupt"}, {"text": "issue 3726", "href": "https://github.com/scrapy/scrapy/issues/3726"}, {"text": "ItemLoader.load_item()", "href": "topics/loaders.html#scrapy.loader.ItemLoader.load_item"}, {"text": "ItemLoader.get_output_value()", "href": "topics/loaders.html#scrapy.loader.ItemLoader.get_output_value"}, {"text": "ItemLoader.load_item()", "href": "topics/loaders.html#scrapy.loader.ItemLoader.load_item"}, {"text": "issue 3804", "href": "https://github.com/scrapy/scrapy/issues/3804"}, {"text": "issue 3819", "href": "https://github.com/scrapy/scrapy/issues/3819"}, {"text": "ImagesPipeline", "href": "topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline"}, {"text": "AWS_ENDPOINT_URL", "href": "topics/settings.html#std-setting-AWS_ENDPOINT_URL"}, {"text": "AWS_REGION_NAME", "href": "topics/settings.html#std-setting-AWS_REGION_NAME"}, {"text": "AWS_USE_SSL", "href": "topics/settings.html#std-setting-AWS_USE_SSL"}, {"text": "AWS_VERIFY", "href": "topics/settings.html#std-setting-AWS_VERIFY"}, {"text": "issue 3625", "href": "https://github.com/scrapy/scrapy/issues/3625"}, {"text": "issue 3813", "href": "https://github.com/scrapy/scrapy/issues/3813"}, {"text": "issue 3790", "href": "https://github.com/scrapy/scrapy/issues/3790"}, {"text": "issue 3777", "href": "https://github.com/scrapy/scrapy/issues/3777"}, {"text": "issue 3794", "href": "https://github.com/scrapy/scrapy/issues/3794"}, {"text": "Selecting dynamically-loaded content", "href": "topics/dynamic-content.html#topics-dynamic-content"}, {"text": "issue 3703", "href": "https://github.com/scrapy/scrapy/issues/3703"}, {"text": "Broad Crawls", "href": "topics/broad-crawls.html#topics-broad-crawls"}, {"text": "issue 1264", "href": "https://github.com/scrapy/scrapy/issues/1264"}, {"text": "issue 3866", "href": "https://github.com/scrapy/scrapy/issues/3866"}, {"text": "Rule", "href": "topics/spiders.html#scrapy.spiders.Rule"}, {"text": "CrawlSpider", "href": "topics/spiders.html#scrapy.spiders.CrawlSpider"}, {"text": "issue 3711", "href": "https://github.com/scrapy/scrapy/issues/3711"}, {"text": "issue 3712", "href": "https://github.com/scrapy/scrapy/issues/3712"}, {"text": "Writing your own storage backend", "href": "topics/downloader-middleware.html#httpcache-storage-custom"}, {"text": "HttpCacheMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware"}, {"text": "issue 3683", "href": "https://github.com/scrapy/scrapy/issues/3683"}, {"text": "issue 3692", "href": "https://github.com/scrapy/scrapy/issues/3692"}, {"text": "FAQ", "href": "faq.html#faq"}, {"text": "How to split an item into multiple items in an item pipeline?", "href": "faq.html#faq-split-item"}, {"text": "issue 2240", "href": "https://github.com/scrapy/scrapy/issues/2240"}, {"text": "issue 3672", "href": "https://github.com/scrapy/scrapy/issues/3672"}, {"text": "FAQ entry about crawl order", "href": "faq.html#faq-bfo-dfo"}, {"text": "issue 1739", "href": "https://github.com/scrapy/scrapy/issues/1739"}, {"text": "issue 3621", "href": "https://github.com/scrapy/scrapy/issues/3621"}, {"text": "LOGSTATS_INTERVAL", "href": "topics/settings.html#std-setting-LOGSTATS_INTERVAL"}, {"text": "issue 3730", "href": "https://github.com/scrapy/scrapy/issues/3730"}, {"text": "FilesPipeline.file_path", "href": "topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.file_path"}, {"text": "ImagesPipeline.file_path", "href": "topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline.file_path"}, {"text": "issue 2253", "href": "https://github.com/scrapy/scrapy/issues/2253"}, {"text": "issue 3609", "href": "https://github.com/scrapy/scrapy/issues/3609"}, {"text": "Crawler.stop()", "href": "topics/api.html#scrapy.crawler.Crawler.stop"}, {"text": "issue 3842", "href": "https://github.com/scrapy/scrapy/issues/3842"}, {"text": "issue 1347", "href": "https://github.com/scrapy/scrapy/issues/1347"}, {"text": "issue 1789", "href": "https://github.com/scrapy/scrapy/issues/1789"}, {"text": "issue 2289", "href": "https://github.com/scrapy/scrapy/issues/2289"}, {"text": "issue 3069", "href": "https://github.com/scrapy/scrapy/issues/3069"}, {"text": "issue 3615", "href": "https://github.com/scrapy/scrapy/issues/3615"}, {"text": "issue 3626", "href": "https://github.com/scrapy/scrapy/issues/3626"}, {"text": "issue 3668", "href": "https://github.com/scrapy/scrapy/issues/3668"}, {"text": "issue 3670", "href": "https://github.com/scrapy/scrapy/issues/3670"}, {"text": "issue 3673", "href": "https://github.com/scrapy/scrapy/issues/3673"}, {"text": "issue 3728", "href": "https://github.com/scrapy/scrapy/issues/3728"}, {"text": "issue 3762", "href": "https://github.com/scrapy/scrapy/issues/3762"}, {"text": "issue 3861", "href": "https://github.com/scrapy/scrapy/issues/3861"}, {"text": "issue 3882", "href": "https://github.com/scrapy/scrapy/issues/3882"}, {"text": "issue 3648", "href": "https://github.com/scrapy/scrapy/issues/3648"}, {"text": "issue 3649", "href": "https://github.com/scrapy/scrapy/issues/3649"}, {"text": "issue 3662", "href": "https://github.com/scrapy/scrapy/issues/3662"}, {"text": "issue 3674", "href": "https://github.com/scrapy/scrapy/issues/3674"}, {"text": "issue 3676", "href": "https://github.com/scrapy/scrapy/issues/3676"}, {"text": "issue 3694", "href": "https://github.com/scrapy/scrapy/issues/3694"}, {"text": "issue 3724", "href": "https://github.com/scrapy/scrapy/issues/3724"}, {"text": "issue 3764", "href": "https://github.com/scrapy/scrapy/issues/3764"}, {"text": "issue 3767", "href": "https://github.com/scrapy/scrapy/issues/3767"}, {"text": "issue 3791", "href": "https://github.com/scrapy/scrapy/issues/3791"}, {"text": "issue 3797", "href": "https://github.com/scrapy/scrapy/issues/3797"}, {"text": "issue 3806", "href": "https://github.com/scrapy/scrapy/issues/3806"}, {"text": "issue 3812", "href": "https://github.com/scrapy/scrapy/issues/3812"}, {"text": "issue 3578", "href": "https://github.com/scrapy/scrapy/issues/3578"}, {"text": "Crawler.settings", "href": "topics/api.html#scrapy.crawler.Crawler.settings"}, {"text": "ItemLoader", "href": "topics/loaders.html#scrapy.loader.ItemLoader"}, {"text": "Logging", "href": "topics/logging.html#topics-logging"}, {"text": "Selector", "href": "topics/selectors.html#scrapy.selector.Selector"}, {"text": "Selector", "href": "topics/selectors.html#scrapy.selector.Selector"}, {"text": "Selector", "href": "topics/selectors.html#scrapy.selector.Selector"}, {"text": "Selector", "href": "topics/selectors.html#scrapy.selector.Selector"}, {"text": "parsel.csstranslator.GenericTranslator", "href": "https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.GenericTranslator"}, {"text": "parsel.csstranslator.HTMLTranslator", "href": "https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.HTMLTranslator"}, {"text": "parsel.csstranslator.XPathExpr", "href": "https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.XPathExpr"}, {"text": "Selector", "href": "topics/selectors.html#scrapy.selector.Selector"}, {"text": "SelectorList", "href": "topics/selectors.html#scrapy.selector.SelectorList"}, {"text": "Spider", "href": "topics/spiders.html#scrapy.spiders.Spider"}, {"text": "Spider", "href": "topics/spiders.html#scrapy.spiders.Spider"}, {"text": "download_delay", "href": "topics/settings.html#spider-download-delay-attribute"}, {"text": "SpiderLoader", "href": "topics/api.html#scrapy.spiderloader.SpiderLoader"}, {"text": "scrapy.extensions.telnet", "href": "topics/extensions.html#module-scrapy.extensions.telnet"}, {"text": "issue 3578", "href": "https://github.com/scrapy/scrapy/issues/3578"}, {"text": "SPIDER_LOADER_CLASS", "href": "topics/settings.html#std-setting-SPIDER_LOADER_CLASS"}, {"text": "SCHEDULER_PRIORITY_QUEUE", "href": "topics/settings.html#std-setting-SCHEDULER_PRIORITY_QUEUE"}, {"text": "Rule", "href": "topics/spiders.html#scrapy.spiders.Rule"}, {"text": "w3lib.http", "href": "https://w3lib.readthedocs.io/en/latest/w3lib.html#module-w3lib.http"}, {"text": "w3lib.html", "href": "https://w3lib.readthedocs.io/en/latest/w3lib.html#module-w3lib.html"}, {"text": "urllib3", "href": "https://urllib3.readthedocs.io/en/latest/index.html"}, {"text": "ChainMap", "href": "https://docs.python.org/3/library/collections.html#collections.ChainMap"}, {"text": "issue 3878", "href": "https://github.com/scrapy/scrapy/issues/3878"}, {"text": "tox", "href": "https://pypi.org/project/tox/"}, {"text": "this and other ways to run\ntests", "href": "contributing.html#running-tests"}, {"text": "issue 3707", "href": "https://github.com/scrapy/scrapy/issues/3707"}, {"text": "issue 3806", "href": "https://github.com/scrapy/scrapy/issues/3806"}, {"text": "issue 3810", "href": "https://github.com/scrapy/scrapy/issues/3810"}, {"text": "issue 3860", "href": "https://github.com/scrapy/scrapy/issues/3860"}, {"text": "documentation policies", "href": "contributing.html#documentation-policies"}, {"text": "docstrings", "href": "https://docs.python.org/3/glossary.html#term-docstring"}, {"text": "issue 3701", "href": "https://github.com/scrapy/scrapy/issues/3701"}, {"text": "PEP 257", "href": "https://www.python.org/dev/peps/pep-0257/"}, {"text": "issue 3748", "href": "https://github.com/scrapy/scrapy/issues/3748"}, {"text": "issue 3629", "href": "https://github.com/scrapy/scrapy/issues/3629"}, {"text": "issue 3643", "href": "https://github.com/scrapy/scrapy/issues/3643"}, {"text": "issue 3684", "href": "https://github.com/scrapy/scrapy/issues/3684"}, {"text": "issue 3698", "href": "https://github.com/scrapy/scrapy/issues/3698"}, {"text": "issue 3734", "href": "https://github.com/scrapy/scrapy/issues/3734"}, {"text": "issue 3735", "href": "https://github.com/scrapy/scrapy/issues/3735"}, {"text": "issue 3736", "href": "https://github.com/scrapy/scrapy/issues/3736"}, {"text": "issue 3737", "href": "https://github.com/scrapy/scrapy/issues/3737"}, {"text": "issue 3809", "href": "https://github.com/scrapy/scrapy/issues/3809"}, {"text": "issue 3821", "href": "https://github.com/scrapy/scrapy/issues/3821"}, {"text": "issue 3825", "href": "https://github.com/scrapy/scrapy/issues/3825"}, {"text": "issue 3827", "href": "https://github.com/scrapy/scrapy/issues/3827"}, {"text": "issue 3833", "href": "https://github.com/scrapy/scrapy/issues/3833"}, {"text": "issue 3857", "href": "https://github.com/scrapy/scrapy/issues/3857"}, {"text": "issue 3877", "href": "https://github.com/scrapy/scrapy/issues/3877"}, {"text": "item_error", "href": "topics/signals.html#std-signal-item_error"}, {"text": "request_reached_downloader", "href": "topics/signals.html#std-signal-request_reached_downloader"}, {"text": "parsel", "href": "https://github.com/scrapy/parsel"}, {"text": "extract() and extract_first()", "href": "topics/selectors.html#old-extraction-api"}, {"text": "Selecting element attributes", "href": "topics/selectors.html#selecting-attributes"}, {"text": "parsel changelog", "href": "https://parsel.readthedocs.io/en/latest/history.html"}, {"text": "Telnet Console", "href": "topics/telnetconsole.html#topics-telnetconsole"}, {"text": "issue 1605", "href": "https://github.com/scrapy/scrapy/issues/1605"}, {"text": "issue 3348", "href": "https://github.com/scrapy/scrapy/issues/3348"}, {"text": "issue 2956", "href": "https://github.com/scrapy/scrapy/issues/2956"}, {"text": "item_error", "href": "topics/signals.html#std-signal-item_error"}, {"text": "issue 3256", "href": "https://github.com/scrapy/scrapy/issues/3256"}, {"text": "request_reached_downloader", "href": "topics/signals.html#std-signal-request_reached_downloader"}, {"text": "issue 3393", "href": "https://github.com/scrapy/scrapy/issues/3393"}, {"text": "sitemap_filter()", "href": "topics/spiders.html#scrapy.spiders.SitemapSpider.sitemap_filter"}, {"text": "issue 3512", "href": "https://github.com/scrapy/scrapy/issues/3512"}, {"text": "issue 3394", "href": "https://github.com/scrapy/scrapy/issues/3394"}, {"text": "AWS_ENDPOINT_URL", "href": "topics/settings.html#std-setting-AWS_ENDPOINT_URL"}, {"text": "AWS_USE_SSL", "href": "topics/settings.html#std-setting-AWS_USE_SSL"}, {"text": "AWS_VERIFY", "href": "topics/settings.html#std-setting-AWS_VERIFY"}, {"text": "AWS_REGION_NAME", "href": "topics/settings.html#std-setting-AWS_REGION_NAME"}, {"text": "issue 2609", "href": "https://github.com/scrapy/scrapy/issues/2609"}, {"text": "issue 3548", "href": "https://github.com/scrapy/scrapy/issues/3548"}, {"text": "FILES_STORE_GCS_ACL", "href": "topics/media-pipeline.html#std-setting-FILES_STORE_GCS_ACL"}, {"text": "IMAGES_STORE_GCS_ACL", "href": "topics/media-pipeline.html#std-setting-IMAGES_STORE_GCS_ACL"}, {"text": "issue 3199", "href": "https://github.com/scrapy/scrapy/issues/3199"}, {"text": "issue 3377", "href": "https://github.com/scrapy/scrapy/issues/3377"}, {"text": "issue 3381", "href": "https://github.com/scrapy/scrapy/issues/3381"}, {"text": "issue 3383", "href": "https://github.com/scrapy/scrapy/issues/3383"}, {"text": "issue 3371", "href": "https://github.com/scrapy/scrapy/issues/3371"}, {"text": "issue 3100", "href": "https://github.com/scrapy/scrapy/issues/3100"}, {"text": "issue 3115", "href": "https://github.com/scrapy/scrapy/issues/3115"}, {"text": "issue 3113", "href": "https://github.com/scrapy/scrapy/issues/3113"}, {"text": "issue 3131", "href": "https://github.com/scrapy/scrapy/issues/3131"}, {"text": "issue 3226", "href": "https://github.com/scrapy/scrapy/issues/3226"}, {"text": "issue 3152", "href": "https://github.com/scrapy/scrapy/issues/3152"}, {"text": "issue 3165", "href": "https://github.com/scrapy/scrapy/issues/3165"}, {"text": "issue 3358", "href": "https://github.com/scrapy/scrapy/issues/3358"}, {"text": "issue 3496", "href": "https://github.com/scrapy/scrapy/issues/3496"}, {"text": "issue 3588", "href": "https://github.com/scrapy/scrapy/issues/3588"}, {"text": "issue 3039", "href": "https://github.com/scrapy/scrapy/issues/3039"}, {"text": "issue 3082", "href": "https://github.com/scrapy/scrapy/issues/3082"}, {"text": "issue 3342", "href": "https://github.com/scrapy/scrapy/issues/3342"}, {"text": "issue 3153", "href": "https://github.com/scrapy/scrapy/issues/3153"}, {"text": "issue 3247", "href": "https://github.com/scrapy/scrapy/issues/3247"}, {"text": "Selectors", "href": "topics/selectors.html#topics-selectors"}, {"text": "Selecting element attributes", "href": "topics/selectors.html#selecting-attributes"}, {"text": "Extensions to CSS Selectors", "href": "topics/selectors.html#topics-selectors-css-extensions"}, {"text": "issue 3390", "href": "https://github.com/scrapy/scrapy/issues/3390"}, {"text": "Using your browser’s Developer Tools for scraping", "href": "topics/developer-tools.html#topics-developer-tools"}, {"text": "issue 3400", "href": "https://github.com/scrapy/scrapy/issues/3400"}, {"text": "issue 3518", "href": "https://github.com/scrapy/scrapy/issues/3518"}, {"text": "issue 3517", "href": "https://github.com/scrapy/scrapy/issues/3517"}, {"text": "issue 3367", "href": "https://github.com/scrapy/scrapy/issues/3367"}, {"text": "issue 3468", "href": "https://github.com/scrapy/scrapy/issues/3468"}, {"text": "RETRY_HTTP_CODES", "href": "topics/downloader-middleware.html#std-setting-RETRY_HTTP_CODES"}, {"text": "issue 3335", "href": "https://github.com/scrapy/scrapy/issues/3335"}, {"text": "issue 3245", "href": "https://github.com/scrapy/scrapy/issues/3245"}, {"text": "issue 3347", "href": "https://github.com/scrapy/scrapy/issues/3347"}, {"text": "issue 3350", "href": "https://github.com/scrapy/scrapy/issues/3350"}, {"text": "issue 3445", "href": "https://github.com/scrapy/scrapy/issues/3445"}, {"text": "issue 3544", "href": "https://github.com/scrapy/scrapy/issues/3544"}, {"text": "issue 3605", "href": "https://github.com/scrapy/scrapy/issues/3605"}, {"text": "issue 3318", "href": "https://github.com/scrapy/scrapy/issues/3318"}, {"text": "issue 3327", "href": "https://github.com/scrapy/scrapy/issues/3327"}, {"text": "issue 3327", "href": "https://github.com/scrapy/scrapy/issues/3327"}, {"text": "issue 3359", "href": "https://github.com/scrapy/scrapy/issues/3359"}, {"text": "issue 3315", "href": "https://github.com/scrapy/scrapy/issues/3315"}, {"text": "issue 3326", "href": "https://github.com/scrapy/scrapy/issues/3326"}, {"text": "issue 3150", "href": "https://github.com/scrapy/scrapy/issues/3150"}, {"text": "issue 3547", "href": "https://github.com/scrapy/scrapy/issues/3547"}, {"text": "issue 3526", "href": "https://github.com/scrapy/scrapy/issues/3526"}, {"text": "issue 3538", "href": "https://github.com/scrapy/scrapy/issues/3538"}, {"text": "issue 3308", "href": "https://github.com/scrapy/scrapy/issues/3308"}, {"text": "issue 3311", "href": "https://github.com/scrapy/scrapy/issues/3311"}, {"text": "issue 3309", "href": "https://github.com/scrapy/scrapy/issues/3309"}, {"text": "issue 3305", "href": "https://github.com/scrapy/scrapy/issues/3305"}, {"text": "issue 3210", "href": "https://github.com/scrapy/scrapy/issues/3210"}, {"text": "issue 3299", "href": "https://github.com/scrapy/scrapy/issues/3299"}, {"text": "issue 3231", "href": "https://github.com/scrapy/scrapy/issues/3231"}, {"text": "issue 3495", "href": "https://github.com/scrapy/scrapy/issues/3495"}, {"text": "issue 3405", "href": "https://github.com/scrapy/scrapy/issues/3405"}, {"text": "issue 3304", "href": "https://github.com/scrapy/scrapy/issues/3304"}, {"text": "issue 3519", "href": "https://github.com/scrapy/scrapy/issues/3519"}, {"text": "issue 3476", "href": "https://github.com/scrapy/scrapy/issues/3476"}, {"text": "http://localhost:6023", "href": "http://localhost:6023"}, {"text": "TELNETCONSOLE_PORT", "href": "topics/telnetconsole.html#std-setting-TELNETCONSOLE_PORT"}, {"text": "telnet console", "href": "topics/telnetconsole.html#topics-telnetconsole"}, {"text": "issue 3281", "href": "https://github.com/scrapy/scrapy/issues/3281"}, {"text": "issue 3166", "href": "https://github.com/scrapy/scrapy/issues/3166"}, {"text": "issue 3096", "href": "https://github.com/scrapy/scrapy/issues/3096"}, {"text": "issue 3092", "href": "https://github.com/scrapy/scrapy/issues/3092"}, {"text": "issue 3263", "href": "https://github.com/scrapy/scrapy/issues/3263"}, {"text": "issue 3058", "href": "https://github.com/scrapy/scrapy/issues/3058"}, {"text": "issue 3059", "href": "https://github.com/scrapy/scrapy/issues/3059"}, {"text": "issue 3089", "href": "https://github.com/scrapy/scrapy/issues/3089"}, {"text": "issue 3123", "href": "https://github.com/scrapy/scrapy/issues/3123"}, {"text": "issue 3127", "href": "https://github.com/scrapy/scrapy/issues/3127"}, {"text": "issue 3189", "href": "https://github.com/scrapy/scrapy/issues/3189"}, {"text": "issue 3224", "href": "https://github.com/scrapy/scrapy/issues/3224"}, {"text": "issue 3280", "href": "https://github.com/scrapy/scrapy/issues/3280"}, {"text": "issue 3279", "href": "https://github.com/scrapy/scrapy/issues/3279"}, {"text": "issue 3201", "href": "https://github.com/scrapy/scrapy/issues/3201"}, {"text": "issue 3260", "href": "https://github.com/scrapy/scrapy/issues/3260"}, {"text": "issue 3284", "href": "https://github.com/scrapy/scrapy/issues/3284"}, {"text": "issue 3298", "href": "https://github.com/scrapy/scrapy/issues/3298"}, {"text": "issue 3294", "href": "https://github.com/scrapy/scrapy/issues/3294"}, {"text": "issue 2983", "href": "https://github.com/scrapy/scrapy/issues/2983"}, {"text": "USER_AGENT", "href": "topics/settings.html#std-setting-USER_AGENT"}, {"text": "issue 1343", "href": "https://github.com/scrapy/scrapy/issues/1343"}, {"text": "issue 2851", "href": "https://github.com/scrapy/scrapy/issues/2851"}, {"text": "issue 2785", "href": "https://github.com/scrapy/scrapy/issues/2785"}, {"text": "issue 2654", "href": "https://github.com/scrapy/scrapy/issues/2654"}, {"text": "issue 2923", "href": "https://github.com/scrapy/scrapy/issues/2923"}, {"text": "issue 2883", "href": "https://github.com/scrapy/scrapy/issues/2883"}, {"text": "issue 2812", "href": "https://github.com/scrapy/scrapy/issues/2812"}, {"text": "issue 2844", "href": "https://github.com/scrapy/scrapy/issues/2844"}, {"text": "issue 2851", "href": "https://github.com/scrapy/scrapy/issues/2851"}, {"text": "issue 2857", "href": "https://github.com/scrapy/scrapy/issues/2857"}, {"text": "issue 2743", "href": "https://github.com/scrapy/scrapy/issues/2743"}, {"text": "issue 2755", "href": "https://github.com/scrapy/scrapy/issues/2755"}, {"text": "issue 2831", "href": "https://github.com/scrapy/scrapy/issues/2831"}, {"text": "issue 2921", "href": "https://github.com/scrapy/scrapy/issues/2921"}, {"text": "DOWNLOAD_WARNSIZE", "href": "topics/settings.html#std-setting-DOWNLOAD_WARNSIZE"}, {"text": "DOWNLOAD_MAXSIZE", "href": "topics/settings.html#std-setting-DOWNLOAD_MAXSIZE"}, {"text": "issue 2927", "href": "https://github.com/scrapy/scrapy/issues/2927"}, {"text": "issue 2250", "href": "https://github.com/scrapy/scrapy/issues/2250"}, {"text": "issue 1343", "href": "https://github.com/scrapy/scrapy/issues/1343"}, {"text": "issue 2983", "href": "https://github.com/scrapy/scrapy/issues/2983"}, {"text": "USER_AGENT", "href": "topics/settings.html#std-setting-USER_AGENT"}, {"text": "issue 2793", "href": "https://github.com/scrapy/scrapy/issues/2793"}, {"text": "issue 2935", "href": "https://github.com/scrapy/scrapy/issues/2935"}, {"text": "issue 2990", "href": "https://github.com/scrapy/scrapy/issues/2990"}, {"text": "issue 3050", "href": "https://github.com/scrapy/scrapy/issues/3050"}, {"text": "issue 2213", "href": "https://github.com/scrapy/scrapy/issues/2213"}, {"text": "issue 3048", "href": "https://github.com/scrapy/scrapy/issues/3048"}, {"text": "issue 2811", "href": "https://github.com/scrapy/scrapy/issues/2811"}, {"text": "issue 2848", "href": "https://github.com/scrapy/scrapy/issues/2848"}, {"text": "issue 2766", "href": "https://github.com/scrapy/scrapy/issues/2766"}, {"text": "issue 2849", "href": "https://github.com/scrapy/scrapy/issues/2849"}, {"text": "issue 2862", "href": "https://github.com/scrapy/scrapy/issues/2862"}, {"text": "issue 2876", "href": "https://github.com/scrapy/scrapy/issues/2876"}, {"text": "issue 2853", "href": "https://github.com/scrapy/scrapy/issues/2853"}, {"text": "issue 2756", "href": "https://github.com/scrapy/scrapy/issues/2756"}, {"text": "issue 2762", "href": "https://github.com/scrapy/scrapy/issues/2762"}, {"text": "https://", "href": "https://"}, {"text": "issue 2978", "href": "https://github.com/scrapy/scrapy/issues/2978"}, {"text": "issue 2982", "href": "https://github.com/scrapy/scrapy/issues/2982"}, {"text": "issue 2958", "href": "https://github.com/scrapy/scrapy/issues/2958"}, {"text": "issue 2759", "href": "https://github.com/scrapy/scrapy/issues/2759"}, {"text": "issue 2781", "href": "https://github.com/scrapy/scrapy/issues/2781"}, {"text": "issue 2828", "href": "https://github.com/scrapy/scrapy/issues/2828"}, {"text": "issue 2837", "href": "https://github.com/scrapy/scrapy/issues/2837"}, {"text": "issue 2884", "href": "https://github.com/scrapy/scrapy/issues/2884"}, {"text": "issue 2924", "href": "https://github.com/scrapy/scrapy/issues/2924"}, {"text": "issue 2826", "href": "https://github.com/scrapy/scrapy/issues/2826"}, {"text": "issue 2791", "href": "https://github.com/scrapy/scrapy/issues/2791"}, {"text": "issue 2764", "href": "https://github.com/scrapy/scrapy/issues/2764"}, {"text": "issue 2763", "href": "https://github.com/scrapy/scrapy/issues/2763"}, {"text": "issue 2866", "href": "https://github.com/scrapy/scrapy/issues/2866"}, {"text": "issue 2922", "href": "https://github.com/scrapy/scrapy/issues/2922"}, {"text": "issue 2374", "href": "https://github.com/scrapy/scrapy/issues/2374"}, {"text": "issue 2999", "href": "https://github.com/scrapy/scrapy/issues/2999"}, {"text": "issue 2964", "href": "https://github.com/scrapy/scrapy/issues/2964"}, {"text": "issue 2976", "href": "https://github.com/scrapy/scrapy/issues/2976"}, {"text": "issue 2989", "href": "https://github.com/scrapy/scrapy/issues/2989"}, {"text": "issue 3019", "href": "https://github.com/scrapy/scrapy/issues/3019"}, {"text": "FTP_USER", "href": "topics/settings.html#std-setting-FTP_USER"}, {"text": "FTP_PASSWORD", "href": "topics/settings.html#std-setting-FTP_PASSWORD"}, {"text": "response.follow", "href": "topics/request-response.html#scrapy.http.TextResponse.follow"}, {"text": "REFERRER_POLICY", "href": "topics/spider-middleware.html#std-setting-REFERRER_POLICY"}, {"text": "FEED_EXPORT_INDENT", "href": "topics/feed-exports.html#std-setting-FEED_EXPORT_INDENT"}, {"text": "scrapy.linkextractors.LinkExtractor", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"}, {"text": "issue 2537", "href": "https://github.com/scrapy/scrapy/issues/2537"}, {"text": "issue 1941", "href": "https://github.com/scrapy/scrapy/issues/1941"}, {"text": "issue 1982", "href": "https://github.com/scrapy/scrapy/issues/1982"}, {"text": "issue 2539", "href": "https://github.com/scrapy/scrapy/issues/2539"}, {"text": "issue 2187", "href": "https://github.com/scrapy/scrapy/issues/2187"}, {"text": "issue 1829", "href": "https://github.com/scrapy/scrapy/issues/1829"}, {"text": "issue 1728", "href": "https://github.com/scrapy/scrapy/issues/1728"}, {"text": "issue 1495", "href": "https://github.com/scrapy/scrapy/issues/1495"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "issue 2526", "href": "https://github.com/scrapy/scrapy/issues/2526"}, {"text": "brotli-compressed", "href": "https://www.ietf.org/rfc/rfc7932.txt"}, {"text": "brotlipy", "href": "https://github.com/python-hyper/brotlipy/"}, {"text": "issue 2535", "href": "https://github.com/scrapy/scrapy/issues/2535"}, {"text": "response.follow", "href": "intro/tutorial.html#response-follow-example"}, {"text": "issue 1940", "href": "https://github.com/scrapy/scrapy/issues/1940"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "issue 2047", "href": "https://github.com/scrapy/scrapy/issues/2047"}, {"text": "issue 2342", "href": "https://github.com/scrapy/scrapy/issues/2342"}, {"text": "RetryMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.retry.RetryMiddleware"}, {"text": "issue 2543", "href": "https://github.com/scrapy/scrapy/issues/2543"}, {"text": "HttpErrorMiddleware", "href": "topics/spider-middleware.html#scrapy.spidermiddlewares.httperror.HttpErrorMiddleware"}, {"text": "issue 2566", "href": "https://github.com/scrapy/scrapy/issues/2566"}, {"text": "Referrer policy", "href": "topics/spider-middleware.html#std-setting-REFERRER_POLICY"}, {"text": "RefererMiddleware", "href": "topics/spider-middleware.html#scrapy.spidermiddlewares.referer.RefererMiddleware"}, {"text": "issue 2306", "href": "https://github.com/scrapy/scrapy/issues/2306"}, {"text": "issue 2334", "href": "https://github.com/scrapy/scrapy/issues/2334"}, {"text": "issue 2156", "href": "https://github.com/scrapy/scrapy/issues/2156"}, {"text": "issue 2611", "href": "https://github.com/scrapy/scrapy/issues/2611"}, {"text": "issue 2604", "href": "https://github.com/scrapy/scrapy/issues/2604"}, {"text": "issue 2181", "href": "https://github.com/scrapy/scrapy/issues/2181"}, {"text": "issue 2646", "href": "https://github.com/scrapy/scrapy/issues/2646"}, {"text": "Media downloads", "href": "topics/media-pipeline.html#topics-media-pipeline"}, {"text": "FilesPipeline", "href": "topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline"}, {"text": "ImagesPipeline", "href": "topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline"}, {"text": "MEDIA_ALLOW_REDIRECTS", "href": "topics/media-pipeline.html#std-setting-MEDIA_ALLOW_REDIRECTS"}, {"text": "issue 2616", "href": "https://github.com/scrapy/scrapy/issues/2616"}, {"text": "issue 2004", "href": "https://github.com/scrapy/scrapy/issues/2004"}, {"text": "DOWNLOAD_FAIL_ON_DATALOSS", "href": "topics/settings.html#std-setting-DOWNLOAD_FAIL_ON_DATALOSS"}, {"text": "issue 2590", "href": "https://github.com/scrapy/scrapy/issues/2590"}, {"text": "issue 2586", "href": "https://github.com/scrapy/scrapy/issues/2586"}, {"text": "FEED_EXPORT_INDENT", "href": "topics/feed-exports.html#std-setting-FEED_EXPORT_INDENT"}, {"text": "issue 2456", "href": "https://github.com/scrapy/scrapy/issues/2456"}, {"text": "issue 1327", "href": "https://github.com/scrapy/scrapy/issues/1327"}, {"text": "issue 667", "href": "https://github.com/scrapy/scrapy/issues/667"}, {"text": "max_retry_times", "href": "topics/request-response.html#std-reqmeta-max_retry_times"}, {"text": "issue 2642", "href": "https://github.com/scrapy/scrapy/issues/2642"}, {"text": "issue 2740", "href": "https://github.com/scrapy/scrapy/issues/2740"}, {"text": "issue 2547", "href": "https://github.com/scrapy/scrapy/issues/2547"}, {"text": "issue 1614", "href": "https://github.com/scrapy/scrapy/issues/1614"}, {"text": "issue 2548", "href": "https://github.com/scrapy/scrapy/issues/2548"}, {"text": "issue 2495", "href": "https://github.com/scrapy/scrapy/issues/2495"}, {"text": "issue 2491", "href": "https://github.com/scrapy/scrapy/issues/2491"}, {"text": "issue 2599", "href": "https://github.com/scrapy/scrapy/issues/2599"}, {"text": "issue 2393", "href": "https://github.com/scrapy/scrapy/issues/2393"}, {"text": "issue 2145", "href": "https://github.com/scrapy/scrapy/issues/2145"}, {"text": "HttpCompressionMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware"}, {"text": "issue 2391", "href": "https://github.com/scrapy/scrapy/issues/2391"}, {"text": "issue 2581", "href": "https://github.com/scrapy/scrapy/issues/2581"}, {"text": "issue 1612", "href": "https://github.com/scrapy/scrapy/issues/1612"}, {"text": "issue 2661", "href": "https://github.com/scrapy/scrapy/issues/2661"}, {"text": "issue 2695", "href": "https://github.com/scrapy/scrapy/issues/2695"}, {"text": "issue 2677", "href": "https://github.com/scrapy/scrapy/issues/2677"}, {"text": "DOWNLOAD_MAXSIZE", "href": "topics/settings.html#std-setting-DOWNLOAD_MAXSIZE"}, {"text": "issue 1616", "href": "https://github.com/scrapy/scrapy/issues/1616"}, {"text": "issue 2675", "href": "https://github.com/scrapy/scrapy/issues/2675"}, {"text": "issue 2570", "href": "https://github.com/scrapy/scrapy/issues/2570"}, {"text": "issue 2569", "href": "https://github.com/scrapy/scrapy/issues/2569"}, {"text": "issue 2710", "href": "https://github.com/scrapy/scrapy/issues/2710"}, {"text": "issue 2562", "href": "https://github.com/scrapy/scrapy/issues/2562"}, {"text": "issue 2567", "href": "https://github.com/scrapy/scrapy/issues/2567"}, {"text": "issue 2557", "href": "https://github.com/scrapy/scrapy/issues/2557"}, {"text": "issue 2159", "href": "https://github.com/scrapy/scrapy/issues/2159"}, {"text": "issue 2750", "href": "https://github.com/scrapy/scrapy/issues/2750"}, {"text": "issue 2577", "href": "https://github.com/scrapy/scrapy/issues/2577"}, {"text": "issue 2560", "href": "https://github.com/scrapy/scrapy/issues/2560"}, {"text": "issue 2595", "href": "https://github.com/scrapy/scrapy/issues/2595"}, {"text": "issue 2617", "href": "https://github.com/scrapy/scrapy/issues/2617"}, {"text": "issue 2644", "href": "https://github.com/scrapy/scrapy/issues/2644"}, {"text": "issue 2720", "href": "https://github.com/scrapy/scrapy/issues/2720"}, {"text": "issue 2576", "href": "https://github.com/scrapy/scrapy/issues/2576"}, {"text": "issue 2564", "href": "https://github.com/scrapy/scrapy/issues/2564"}, {"text": "issue 2553", "href": "https://github.com/scrapy/scrapy/issues/2553"}, {"text": "issue 2572", "href": "https://github.com/scrapy/scrapy/issues/2572"}, {"text": "issue 2596", "href": "https://github.com/scrapy/scrapy/issues/2596"}, {"text": "ftp_user", "href": "topics/settings.html#std-reqmeta-ftp_user"}, {"text": "ftp_password", "href": "topics/settings.html#std-reqmeta-ftp_password"}, {"text": "issue 2587", "href": "https://github.com/scrapy/scrapy/issues/2587"}, {"text": "issue 2636", "href": "https://github.com/scrapy/scrapy/issues/2636"}, {"text": "issue 2477", "href": "https://github.com/scrapy/scrapy/issues/2477"}, {"text": "issue 2475", "href": "https://github.com/scrapy/scrapy/issues/2475"}, {"text": "issue 2690", "href": "https://github.com/scrapy/scrapy/issues/2690"}, {"text": "issue 2705", "href": "https://github.com/scrapy/scrapy/issues/2705"}, {"text": "SelectorList", "href": "topics/selectors.html#scrapy.selector.SelectorList"}, {"text": "issue 2683", "href": "https://github.com/scrapy/scrapy/issues/2683"}, {"text": "DUPEFILTER_CLASS", "href": "topics/settings.html#std-setting-DUPEFILTER_CLASS"}, {"text": "issue 2714", "href": "https://github.com/scrapy/scrapy/issues/2714"}, {"text": "issue 2668", "href": "https://github.com/scrapy/scrapy/issues/2668"}, {"text": "issue 2729", "href": "https://github.com/scrapy/scrapy/issues/2729"}, {"text": "issue 2670", "href": "https://github.com/scrapy/scrapy/issues/2670"}, {"text": "SPIDER_MODULES", "href": "topics/settings.html#std-setting-SPIDER_MODULES"}, {"text": "SPIDER_LOADER_WARN_ONLY", "href": "topics/settings.html#std-setting-SPIDER_LOADER_WARN_ONLY"}, {"text": "issue 2510", "href": "https://github.com/scrapy/scrapy/issues/2510"}, {"text": "issue 2551", "href": "https://github.com/scrapy/scrapy/issues/2551"}, {"text": "issue 2558", "href": "https://github.com/scrapy/scrapy/issues/2558"}, {"text": "issue 2519", "href": "https://github.com/scrapy/scrapy/issues/2519"}, {"text": "XPath variables", "href": "topics/selectors.html#topics-selectors-xpath-variables"}, {"text": "issue 2457", "href": "https://github.com/scrapy/scrapy/issues/2457"}, {"text": "issue 2485", "href": "https://github.com/scrapy/scrapy/issues/2485"}, {"text": "issue 2496", "href": "https://github.com/scrapy/scrapy/issues/2496"}, {"text": "view", "href": "topics/commands.html#std-command-view"}, {"text": "issue 2503", "href": "https://github.com/scrapy/scrapy/issues/2503"}, {"text": "issue 2460", "href": "https://github.com/scrapy/scrapy/issues/2460"}, {"text": "issue 2466", "href": "https://github.com/scrapy/scrapy/issues/2466"}, {"text": "issue 2496", "href": "https://github.com/scrapy/scrapy/issues/2496"}, {"text": "issue 2528", "href": "https://github.com/scrapy/scrapy/issues/2528"}, {"text": "issue 2511", "href": "https://github.com/scrapy/scrapy/issues/2511"}, {"text": "issue 2420", "href": "https://github.com/scrapy/scrapy/issues/2420"}, {"text": "issue 2469", "href": "https://github.com/scrapy/scrapy/issues/2469"}, {"text": "issue 2483", "href": "https://github.com/scrapy/scrapy/issues/2483"}, {"text": "issue 2497", "href": "https://github.com/scrapy/scrapy/issues/2497"}, {"text": "issue 2507", "href": "https://github.com/scrapy/scrapy/issues/2507"}, {"text": "issue 2525", "href": "https://github.com/scrapy/scrapy/issues/2525"}, {"text": "issue 2533", "href": "https://github.com/scrapy/scrapy/issues/2533"}, {"text": "issue 1704", "href": "https://github.com/scrapy/scrapy/issues/1704"}, {"text": "issue 2512", "href": "https://github.com/scrapy/scrapy/issues/2512"}, {"text": "issue 2534", "href": "https://github.com/scrapy/scrapy/issues/2534"}, {"text": "issue 2531", "href": "https://github.com/scrapy/scrapy/issues/2531"}, {"text": "issue 2542", "href": "https://github.com/scrapy/scrapy/issues/2542"}, {"text": "issue 2538", "href": "https://github.com/scrapy/scrapy/issues/2538"}, {"text": "issue 2544", "href": "https://github.com/scrapy/scrapy/issues/2544"}, {"text": "issue 2272", "href": "https://github.com/scrapy/scrapy/issues/2272"}, {"text": "issue 2290", "href": "https://github.com/scrapy/scrapy/issues/2290"}, {"text": "fetch", "href": "topics/commands.html#std-command-fetch"}, {"text": "shell", "href": "topics/commands.html#std-command-shell"}, {"text": "LOG_SHORT_NAMES", "href": "topics/settings.html#std-setting-LOG_SHORT_NAMES"}, {"text": "issue 2011", "href": "https://github.com/scrapy/scrapy/issues/2011"}, {"text": "issue 396", "href": "https://github.com/scrapy/scrapy/issues/396"}, {"text": "issue 2418", "href": "https://github.com/scrapy/scrapy/issues/2418"}, {"text": "issue 2390", "href": "https://github.com/scrapy/scrapy/issues/2390"}, {"text": "issue 2373", "href": "https://github.com/scrapy/scrapy/issues/2373"}, {"text": "issue 2033", "href": "https://github.com/scrapy/scrapy/issues/2033"}, {"text": "issue 2335", "href": "https://github.com/scrapy/scrapy/issues/2335"}, {"text": "issue 2346", "href": "https://github.com/scrapy/scrapy/issues/2346"}, {"text": "issue 2369", "href": "https://github.com/scrapy/scrapy/issues/2369"}, {"text": "issue 2369", "href": "https://github.com/scrapy/scrapy/issues/2369"}, {"text": "issue 2380", "href": "https://github.com/scrapy/scrapy/issues/2380"}, {"text": "issue 2354", "href": "https://github.com/scrapy/scrapy/issues/2354"}, {"text": "issue 2325", "href": "https://github.com/scrapy/scrapy/issues/2325"}, {"text": "issue 2414", "href": "https://github.com/scrapy/scrapy/issues/2414"}, {"text": "conda-forge", "href": "https://anaconda.org/conda-forge/scrapy"}, {"text": "issue 2387", "href": "https://github.com/scrapy/scrapy/issues/2387"}, {"text": "issue 2264", "href": "https://github.com/scrapy/scrapy/issues/2264"}, {"text": "issue 2335", "href": "https://github.com/scrapy/scrapy/issues/2335"}, {"text": "issue 2404", "href": "https://github.com/scrapy/scrapy/issues/2404"}, {"text": "issue 2386", "href": "https://github.com/scrapy/scrapy/issues/2386"}, {"text": "issue 2314", "href": "https://github.com/scrapy/scrapy/issues/2314"}, {"text": "issue 2321", "href": "https://github.com/scrapy/scrapy/issues/2321"}, {"text": "issue 2302", "href": "https://github.com/scrapy/scrapy/issues/2302"}, {"text": "issue 2330", "href": "https://github.com/scrapy/scrapy/issues/2330"}, {"text": "issue 2329", "href": "https://github.com/scrapy/scrapy/issues/2329"}, {"text": "issue 2327", "href": "https://github.com/scrapy/scrapy/issues/2327"}, {"text": "issue 2299", "href": "https://github.com/scrapy/scrapy/issues/2299"}, {"text": "FEED_EXPORT_ENCODING", "href": "topics/feed-exports.html#std-setting-FEED_EXPORT_ENCODING"}, {"text": "issue 2034", "href": "https://github.com/scrapy/scrapy/issues/2034"}, {"text": "issue 2005", "href": "https://github.com/scrapy/scrapy/issues/2005"}, {"text": "SCHEDULER_DEBUG", "href": "topics/settings.html#std-setting-SCHEDULER_DEBUG"}, {"text": "issue 1610", "href": "https://github.com/scrapy/scrapy/issues/1610"}, {"text": "issue 2058", "href": "https://github.com/scrapy/scrapy/issues/2058"}, {"text": "issue 1503", "href": "https://github.com/scrapy/scrapy/issues/1503"}, {"text": "shell", "href": "topics/commands.html#std-command-shell"}, {"text": "inspect_response", "href": "topics/shell.html#topics-shell-inspect-response"}, {"text": "issue 2248", "href": "https://github.com/scrapy/scrapy/issues/2248"}, {"text": "issue 2088", "href": "https://github.com/scrapy/scrapy/issues/2088"}, {"text": "issue 1581", "href": "https://github.com/scrapy/scrapy/issues/1581"}, {"text": "issue 2153", "href": "https://github.com/scrapy/scrapy/issues/2153"}, {"text": "issue 2169", "href": "https://github.com/scrapy/scrapy/issues/2169"}, {"text": "issue 1606", "href": "https://github.com/scrapy/scrapy/issues/1606"}, {"text": "scrapy parse", "href": "topics/commands.html#std-command-parse"}, {"text": "issue 2225", "href": "https://github.com/scrapy/scrapy/issues/2225"}, {"text": "issue 872", "href": "https://github.com/scrapy/scrapy/issues/872"}, {"text": "issue 2125", "href": "https://github.com/scrapy/scrapy/issues/2125"}, {"text": "w3lib.url", "href": "https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.url.canonicalize_url"}, {"text": "issue 2168", "href": "https://github.com/scrapy/scrapy/issues/2168"}, {"text": "issue 2128", "href": "https://github.com/scrapy/scrapy/issues/2128"}, {"text": "issue 1566", "href": "https://github.com/scrapy/scrapy/issues/1566"}, {"text": "issue 2160", "href": "https://github.com/scrapy/scrapy/issues/2160"}, {"text": "architecture diagram", "href": "topics/architecture.html#topics-architecture"}, {"text": "issue 2165", "href": "https://github.com/scrapy/scrapy/issues/2165"}, {"text": "issue 2197", "href": "https://github.com/scrapy/scrapy/issues/2197"}, {"text": "RANDOMIZE_DOWNLOAD_DELAY", "href": "topics/settings.html#std-setting-RANDOMIZE_DOWNLOAD_DELAY"}, {"text": "issue 2190", "href": "https://github.com/scrapy/scrapy/issues/2190"}, {"text": "issue 2257", "href": "https://github.com/scrapy/scrapy/issues/2257"}, {"text": "issue 2243", "href": "https://github.com/scrapy/scrapy/issues/2243"}, {"text": "issue 2198", "href": "https://github.com/scrapy/scrapy/issues/2198"}, {"text": "Overview", "href": "intro/overview.html#intro-overview"}, {"text": "tutorial", "href": "intro/tutorial.html#intro-tutorial"}, {"text": "http://toscrape.com", "href": "http://toscrape.com"}, {"text": "issue 2236", "href": "https://github.com/scrapy/scrapy/issues/2236"}, {"text": "issue 2249", "href": "https://github.com/scrapy/scrapy/issues/2249"}, {"text": "issue 2252", "href": "https://github.com/scrapy/scrapy/issues/2252"}, {"text": "IMAGES_STORE_S3_ACL", "href": "topics/media-pipeline.html#std-setting-IMAGES_STORE_S3_ACL"}, {"text": "IMAGES_EXPIRES", "href": "topics/media-pipeline.html#std-setting-IMAGES_EXPIRES"}, {"text": "issue 2069", "href": "https://github.com/scrapy/scrapy/issues/2069"}, {"text": "issue 2001", "href": "https://github.com/scrapy/scrapy/issues/2001"}, {"text": "issue 2000", "href": "https://github.com/scrapy/scrapy/issues/2000"}, {"text": "issue 2038", "href": "https://github.com/scrapy/scrapy/issues/2038"}, {"text": "issue 2010", "href": "https://github.com/scrapy/scrapy/issues/2010"}, {"text": "issue 2008", "href": "https://github.com/scrapy/scrapy/issues/2008"}, {"text": "issue 1899", "href": "https://github.com/scrapy/scrapy/issues/1899"}, {"text": "issue 2050", "href": "https://github.com/scrapy/scrapy/issues/2050"}, {"text": "issue 2049", "href": "https://github.com/scrapy/scrapy/issues/2049"}, {"text": "issue 2065", "href": "https://github.com/scrapy/scrapy/issues/2065"}, {"text": "issue 2063", "href": "https://github.com/scrapy/scrapy/issues/2063"}, {"text": "issue 2094", "href": "https://github.com/scrapy/scrapy/issues/2094"}, {"text": "issue 2092", "href": "https://github.com/scrapy/scrapy/issues/2092"}, {"text": "issue 1989", "href": "https://github.com/scrapy/scrapy/issues/1989"}, {"text": "issue 1985", "href": "https://github.com/scrapy/scrapy/issues/1985"}, {"text": "issue 2052", "href": "https://github.com/scrapy/scrapy/issues/2052"}, {"text": "issue 1974", "href": "https://github.com/scrapy/scrapy/issues/1974"}, {"text": "commit 9b3c72c", "href": "https://github.com/scrapy/scrapy/commit/9b3c72c"}, {"text": "issue 1994", "href": "https://github.com/scrapy/scrapy/issues/1994"}, {"text": "commit c2c8036", "href": "https://github.com/scrapy/scrapy/commit/c2c8036"}, {"text": "issue 1995", "href": "https://github.com/scrapy/scrapy/issues/1995"}, {"text": "issue 2015", "href": "https://github.com/scrapy/scrapy/issues/2015"}, {"text": "issue 2054", "href": "https://github.com/scrapy/scrapy/issues/2054"}, {"text": "issue 2120", "href": "https://github.com/scrapy/scrapy/issues/2120"}, {"text": "issue 2048", "href": "https://github.com/scrapy/scrapy/issues/2048"}, {"text": "issue 2060", "href": "https://github.com/scrapy/scrapy/issues/2060"}, {"text": "issue 2026", "href": "https://github.com/scrapy/scrapy/issues/2026"}, {"text": "issue 2095", "href": "https://github.com/scrapy/scrapy/issues/2095"}, {"text": "issue 1467", "href": "https://github.com/scrapy/scrapy/issues/1467"}, {"text": "issue 1382", "href": "https://github.com/scrapy/scrapy/issues/1382"}, {"text": "issue 1137", "href": "https://github.com/scrapy/scrapy/issues/1137"}, {"text": "AUTOTHROTTLE_TARGET_CONCURRENCY", "href": "topics/autothrottle.html#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY"}, {"text": "issue 1324", "href": "https://github.com/scrapy/scrapy/issues/1324"}, {"text": "issue 1730", "href": "https://github.com/scrapy/scrapy/issues/1730"}, {"text": "issue 1358", "href": "https://github.com/scrapy/scrapy/issues/1358"}, {"text": "issue 1473", "href": "https://github.com/scrapy/scrapy/issues/1473"}, {"text": "issue 1471", "href": "https://github.com/scrapy/scrapy/issues/1471"}, {"text": "HTTPCACHE_ALWAYS_STORE", "href": "topics/downloader-middleware.html#std-setting-HTTPCACHE_ALWAYS_STORE"}, {"text": "HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS", "href": "topics/downloader-middleware.html#std-setting-HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS"}, {"text": "issue 1151", "href": "https://github.com/scrapy/scrapy/issues/1151"}, {"text": "parsel", "href": "https://github.com/scrapy/parsel"}, {"text": "issue 1409", "href": "https://github.com/scrapy/scrapy/issues/1409"}, {"text": "DOWNLOADER_CLIENT_TLS_METHOD", "href": "topics/settings.html#std-setting-DOWNLOADER_CLIENT_TLS_METHOD"}, {"text": "issue 1289", "href": "https://github.com/scrapy/scrapy/issues/1289"}, {"text": "RETRY_HTTP_CODES", "href": "topics/downloader-middleware.html#std-setting-RETRY_HTTP_CODES"}, {"text": "issue 1710", "href": "https://github.com/scrapy/scrapy/issues/1710"}, {"text": "issue 1550", "href": "https://github.com/scrapy/scrapy/issues/1550"}, {"text": "http://index.html", "href": "http://index.html"}, {"text": "issue 1724", "href": "https://github.com/scrapy/scrapy/issues/1724"}, {"text": "issue 1735", "href": "https://github.com/scrapy/scrapy/issues/1735"}, {"text": "ROBOTSTXT_OBEY", "href": "topics/settings.html#std-setting-ROBOTSTXT_OBEY"}, {"text": "issue 1080", "href": "https://github.com/scrapy/scrapy/issues/1080"}, {"text": "PythonItemExporter", "href": "topics/exporters.html#scrapy.exporters.PythonItemExporter"}, {"text": "issue 1533", "href": "https://github.com/scrapy/scrapy/issues/1533"}, {"text": "FILES_STORE_S3_ACL", "href": "topics/media-pipeline.html#std-setting-FILES_STORE_S3_ACL"}, {"text": "issue 1947", "href": "https://github.com/scrapy/scrapy/issues/1947"}, {"text": "hard at work to make Scrapy run on Python 3", "href": "https://github.com/scrapy/scrapy/wiki/Python-3-Porting"}, {"text": "Code of Conduct", "href": "https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md"}, {"text": "issue 1681", "href": "https://github.com/scrapy/scrapy/issues/1681"}, {"text": "issue 934", "href": "https://github.com/scrapy/scrapy/issues/934"}, {"text": "issue 1100", "href": "https://github.com/scrapy/scrapy/issues/1100"}, {"text": "issue 1444", "href": "https://github.com/scrapy/scrapy/issues/1444"}, {"text": "issue 1498", "href": "https://github.com/scrapy/scrapy/issues/1498"}, {"text": "issue 1710", "href": "https://github.com/scrapy/scrapy/issues/1710"}, {"text": "issue 1550", "href": "https://github.com/scrapy/scrapy/issues/1550"}, {"text": "MEMUSAGE_CHECK_INTERVAL_SECONDS", "href": "topics/settings.html#std-setting-MEMUSAGE_CHECK_INTERVAL_SECONDS"}, {"text": "issue 1282", "href": "https://github.com/scrapy/scrapy/issues/1282"}, {"text": "issue 1390", "href": "https://github.com/scrapy/scrapy/issues/1390"}, {"text": "issue 1421", "href": "https://github.com/scrapy/scrapy/issues/1421"}, {"text": "issue 1794", "href": "https://github.com/scrapy/scrapy/issues/1794"}, {"text": "issue 1629", "href": "https://github.com/scrapy/scrapy/issues/1629"}, {"text": "issue 1334", "href": "https://github.com/scrapy/scrapy/issues/1334"}, {"text": "issue 1364", "href": "https://github.com/scrapy/scrapy/issues/1364"}, {"text": "issue 1447", "href": "https://github.com/scrapy/scrapy/issues/1447"}, {"text": "issue 1469", "href": "https://github.com/scrapy/scrapy/issues/1469"}, {"text": "issue 1472", "href": "https://github.com/scrapy/scrapy/issues/1472"}, {"text": "issue 1135", "href": "https://github.com/scrapy/scrapy/issues/1135"}, {"text": "issue 1149", "href": "https://github.com/scrapy/scrapy/issues/1149"}, {"text": "issue 1586", "href": "https://github.com/scrapy/scrapy/issues/1586"}, {"text": "issue 1662", "href": "https://github.com/scrapy/scrapy/issues/1662"}, {"text": "issue 1723", "href": "https://github.com/scrapy/scrapy/issues/1723"}, {"text": "issue 1725", "href": "https://github.com/scrapy/scrapy/issues/1725"}, {"text": "issue 1423", "href": "https://github.com/scrapy/scrapy/issues/1423"}, {"text": "issue 1528", "href": "https://github.com/scrapy/scrapy/issues/1528"}, {"text": "SCHEDULER_PRIORITY_QUEUE", "href": "topics/settings.html#std-setting-SCHEDULER_PRIORITY_QUEUE"}, {"text": "issue 1822", "href": "https://github.com/scrapy/scrapy/issues/1822"}, {"text": "issue 1835", "href": "https://github.com/scrapy/scrapy/issues/1835"}, {"text": "FEED_TEMPDIR", "href": "topics/settings.html#std-setting-FEED_TEMPDIR"}, {"text": "issue 1847", "href": "https://github.com/scrapy/scrapy/issues/1847"}, {"text": "issue 1891", "href": "https://github.com/scrapy/scrapy/issues/1891"}, {"text": "issue 1950", "href": "https://github.com/scrapy/scrapy/issues/1950"}, {"text": "issue 1761", "href": "https://github.com/scrapy/scrapy/issues/1761"}, {"text": "issue 1883", "href": "https://github.com/scrapy/scrapy/issues/1883"}, {"text": "issue 1291", "href": "https://github.com/scrapy/scrapy/issues/1291"}, {"text": "issue 1302", "href": "https://github.com/scrapy/scrapy/issues/1302"}, {"text": "issue 1335", "href": "https://github.com/scrapy/scrapy/issues/1335"}, {"text": "issue 1683", "href": "https://github.com/scrapy/scrapy/issues/1683"}, {"text": "issue 1660", "href": "https://github.com/scrapy/scrapy/issues/1660"}, {"text": "issue 1642", "href": "https://github.com/scrapy/scrapy/issues/1642"}, {"text": "issue 1721", "href": "https://github.com/scrapy/scrapy/issues/1721"}, {"text": "issue 1727", "href": "https://github.com/scrapy/scrapy/issues/1727"}, {"text": "issue 1879", "href": "https://github.com/scrapy/scrapy/issues/1879"}, {"text": "issue 1476", "href": "https://github.com/scrapy/scrapy/issues/1476"}, {"text": "issue 1481", "href": "https://github.com/scrapy/scrapy/issues/1481"}, {"text": "issue 1477", "href": "https://github.com/scrapy/scrapy/issues/1477"}, {"text": "issue 1315", "href": "https://github.com/scrapy/scrapy/issues/1315"}, {"text": "issue 1290", "href": "https://github.com/scrapy/scrapy/issues/1290"}, {"text": "issue 1750", "href": "https://github.com/scrapy/scrapy/issues/1750"}, {"text": "issue 1881", "href": "https://github.com/scrapy/scrapy/issues/1881"}, {"text": "issue 778", "href": "https://github.com/scrapy/scrapy/issues/778"}, {"text": "issue 1851", "href": "https://github.com/scrapy/scrapy/issues/1851"}, {"text": "issue 1359", "href": "https://github.com/scrapy/scrapy/issues/1359"}, {"text": "issue 1689", "href": "https://github.com/scrapy/scrapy/issues/1689"}, {"text": "issue 1720", "href": "https://github.com/scrapy/scrapy/issues/1720"}, {"text": "pydispatcher", "href": "https://pypi.org/project/PyDispatcher/"}, {"text": "issue 1524", "href": "https://github.com/scrapy/scrapy/issues/1524"}, {"text": "https://github.com/scrapy/scrapy/pull/1524#issuecomment-146985595", "href": "https://github.com/scrapy/scrapy/pull/1524#issuecomment-146985595"}, {"text": "issue 1289", "href": "https://github.com/scrapy/scrapy/issues/1289"}, {"text": "issue 1274", "href": "https://github.com/scrapy/scrapy/issues/1274"}, {"text": "issue 1333", "href": "https://github.com/scrapy/scrapy/issues/1333"}, {"text": "issue 1201", "href": "https://github.com/scrapy/scrapy/issues/1201"}, {"text": "issue 1564", "href": "https://github.com/scrapy/scrapy/issues/1564"}, {"text": "TEMPLATES_DIR", "href": "topics/settings.html#std-setting-TEMPLATES_DIR"}, {"text": "issue 1575", "href": "https://github.com/scrapy/scrapy/issues/1575"}, {"text": "issue 1595", "href": "https://github.com/scrapy/scrapy/issues/1595"}, {"text": "issue 1596", "href": "https://github.com/scrapy/scrapy/issues/1596"}, {"text": "issue 1597", "href": "https://github.com/scrapy/scrapy/issues/1597"}, {"text": "issue 1634", "href": "https://github.com/scrapy/scrapy/issues/1634"}, {"text": "issue 1738", "href": "https://github.com/scrapy/scrapy/issues/1738"}, {"text": "issue 1635", "href": "https://github.com/scrapy/scrapy/issues/1635"}, {"text": "PythonItemExporter", "href": "topics/exporters.html#scrapy.exporters.PythonItemExporter"}, {"text": "issue 1737", "href": "https://github.com/scrapy/scrapy/issues/1737"}, {"text": "issue 1294", "href": "https://github.com/scrapy/scrapy/issues/1294"}, {"text": "issue 1419", "href": "https://github.com/scrapy/scrapy/issues/1419"}, {"text": "issue 1263", "href": "https://github.com/scrapy/scrapy/issues/1263"}, {"text": "issue 1624", "href": "https://github.com/scrapy/scrapy/issues/1624"}, {"text": "issue 1654", "href": "https://github.com/scrapy/scrapy/issues/1654"}, {"text": "issue 1722", "href": "https://github.com/scrapy/scrapy/issues/1722"}, {"text": "issue 1726", "href": "https://github.com/scrapy/scrapy/issues/1726"}, {"text": "issue 1303", "href": "https://github.com/scrapy/scrapy/issues/1303"}, {"text": "issue 1212", "href": "https://github.com/scrapy/scrapy/issues/1212"}, {"text": "issue 1902", "href": "https://github.com/scrapy/scrapy/issues/1902"}, {"text": "issue 1912", "href": "https://github.com/scrapy/scrapy/issues/1912"}, {"text": "issue 1857", "href": "https://github.com/scrapy/scrapy/issues/1857"}, {"text": "issue 1875", "href": "https://github.com/scrapy/scrapy/issues/1875"}, {"text": "issue 1893", "href": "https://github.com/scrapy/scrapy/issues/1893"}, {"text": "issue 1869", "href": "https://github.com/scrapy/scrapy/issues/1869"}, {"text": "issue 907", "href": "https://github.com/scrapy/scrapy/issues/907"}, {"text": "commit 108195e", "href": "https://github.com/scrapy/scrapy/commit/108195e"}, {"text": "commit 1f3d90a", "href": "https://github.com/scrapy/scrapy/commit/1f3d90a"}, {"text": "commit 808a9ea", "href": "https://github.com/scrapy/scrapy/commit/808a9ea"}, {"text": "commit 803bd87", "href": "https://github.com/scrapy/scrapy/commit/803bd87"}, {"text": "commit aa94121", "href": "https://github.com/scrapy/scrapy/commit/aa94121"}, {"text": "commit 7dfa979", "href": "https://github.com/scrapy/scrapy/commit/7dfa979"}, {"text": "commit 6e42f0b", "href": "https://github.com/scrapy/scrapy/commit/6e42f0b"}, {"text": "commit 823a1cc", "href": "https://github.com/scrapy/scrapy/commit/823a1cc"}, {"text": "commit da3c155", "href": "https://github.com/scrapy/scrapy/commit/da3c155"}, {"text": "commit 4418fc3", "href": "https://github.com/scrapy/scrapy/commit/4418fc3"}, {"text": "commit a55078c", "href": "https://github.com/scrapy/scrapy/commit/a55078c"}, {"text": "commit 86fc330", "href": "https://github.com/scrapy/scrapy/commit/86fc330"}, {"text": "commit db4c9fe", "href": "https://github.com/scrapy/scrapy/commit/db4c9fe"}, {"text": "commit df2b944", "href": "https://github.com/scrapy/scrapy/commit/df2b944"}, {"text": "commit a83ab41", "href": "https://github.com/scrapy/scrapy/commit/a83ab41"}, {"text": "commit 73ac80d", "href": "https://github.com/scrapy/scrapy/commit/73ac80d"}, {"text": "commit 97d080e", "href": "https://github.com/scrapy/scrapy/commit/97d080e"}, {"text": "commit 97f2fb3", "href": "https://github.com/scrapy/scrapy/commit/97f2fb3"}, {"text": "file://", "href": "file://"}, {"text": "commit d9b4850", "href": "https://github.com/scrapy/scrapy/commit/d9b4850"}, {"text": "commit c0d0734", "href": "https://github.com/scrapy/scrapy/commit/c0d0734"}, {"text": "commit aa239ad", "href": "https://github.com/scrapy/scrapy/commit/aa239ad"}, {"text": "commit 10eb400", "href": "https://github.com/scrapy/scrapy/commit/10eb400"}, {"text": "commit 1c3600a", "href": "https://github.com/scrapy/scrapy/commit/1c3600a"}, {"text": "commit 7f4ddd5", "href": "https://github.com/scrapy/scrapy/commit/7f4ddd5"}, {"text": "commit b71f677", "href": "https://github.com/scrapy/scrapy/commit/b71f677"}, {"text": "commit 5456c0e", "href": "https://github.com/scrapy/scrapy/commit/5456c0e"}, {"text": "commit 0a1366e", "href": "https://github.com/scrapy/scrapy/commit/0a1366e"}, {"text": "commit ca8d60f", "href": "https://github.com/scrapy/scrapy/commit/ca8d60f"}, {"text": "commit 7067117", "href": "https://github.com/scrapy/scrapy/commit/7067117"}, {"text": "commit 32f115c", "href": "https://github.com/scrapy/scrapy/commit/32f115c"}, {"text": "commit 23fda69", "href": "https://github.com/scrapy/scrapy/commit/23fda69"}, {"text": "commit 98b63ee", "href": "https://github.com/scrapy/scrapy/commit/98b63ee"}, {"text": "commit 1925db1", "href": "https://github.com/scrapy/scrapy/commit/1925db1"}, {"text": "commit 5d10d6d", "href": "https://github.com/scrapy/scrapy/commit/5d10d6d"}, {"text": "commit 85c980e", "href": "https://github.com/scrapy/scrapy/commit/85c980e"}, {"text": "commit fbd010d", "href": "https://github.com/scrapy/scrapy/commit/fbd010d"}, {"text": "commit d8f4cba", "href": "https://github.com/scrapy/scrapy/commit/d8f4cba"}, {"text": "commit de73b1a", "href": "https://github.com/scrapy/scrapy/commit/de73b1a"}, {"text": "commit 1ddcc7b", "href": "https://github.com/scrapy/scrapy/commit/1ddcc7b"}, {"text": "commit 1b85bcf", "href": "https://github.com/scrapy/scrapy/commit/1b85bcf"}, {"text": "commit 55f7104", "href": "https://github.com/scrapy/scrapy/commit/55f7104"}, {"text": "commit b262411", "href": "https://github.com/scrapy/scrapy/commit/b262411"}, {"text": "commit a6535c2", "href": "https://github.com/scrapy/scrapy/commit/a6535c2"}, {"text": "commit 8876111", "href": "https://github.com/scrapy/scrapy/commit/8876111"}, {"text": "commit 5d4daf8", "href": "https://github.com/scrapy/scrapy/commit/5d4daf8"}, {"text": "commit f8d0682", "href": "https://github.com/scrapy/scrapy/commit/f8d0682"}, {"text": "commit 5f83a93", "href": "https://github.com/scrapy/scrapy/commit/5f83a93"}, {"text": "commit 3365c01", "href": "https://github.com/scrapy/scrapy/commit/3365c01"}, {"text": "commit 2d688cd", "href": "https://github.com/scrapy/scrapy/commit/2d688cd"}, {"text": "commit fbc1f25", "href": "https://github.com/scrapy/scrapy/commit/fbc1f25"}, {"text": "commit 7d6538c", "href": "https://github.com/scrapy/scrapy/commit/7d6538c"}, {"text": "commit 8752294", "href": "https://github.com/scrapy/scrapy/commit/8752294"}, {"text": "commit 13c45ac", "href": "https://github.com/scrapy/scrapy/commit/13c45ac"}, {"text": "commit cbc2501", "href": "https://github.com/scrapy/scrapy/commit/cbc2501"}, {"text": "commit 66af9cd", "href": "https://github.com/scrapy/scrapy/commit/66af9cd"}, {"text": "commit b04dd7d", "href": "https://github.com/scrapy/scrapy/commit/b04dd7d"}, {"text": "commit 6f85c7f", "href": "https://github.com/scrapy/scrapy/commit/6f85c7f"}, {"text": "commit 9c9d2e0", "href": "https://github.com/scrapy/scrapy/commit/9c9d2e0"}, {"text": "commit c63882b", "href": "https://github.com/scrapy/scrapy/commit/c63882b"}, {"text": "commit a9ae7b0", "href": "https://github.com/scrapy/scrapy/commit/a9ae7b0"}, {"text": "commit 7c8a4fe", "href": "https://github.com/scrapy/scrapy/commit/7c8a4fe"}, {"text": "commit cc00ad2", "href": "https://github.com/scrapy/scrapy/commit/cc00ad2"}, {"text": "commit eca227e", "href": "https://github.com/scrapy/scrapy/commit/eca227e"}, {"text": "commit b8567bc", "href": "https://github.com/scrapy/scrapy/commit/b8567bc"}, {"text": "commit 392233f", "href": "https://github.com/scrapy/scrapy/commit/392233f"}, {"text": "commit 5303c66", "href": "https://github.com/scrapy/scrapy/commit/5303c66"}, {"text": "commit c89fa29", "href": "https://github.com/scrapy/scrapy/commit/c89fa29"}, {"text": "overview", "href": "intro/overview.html#intro-overview"}, {"text": "tutorial", "href": "intro/tutorial.html#intro-tutorial"}, {"text": "Settings", "href": "topics/settings.html#topics-settings"}, {"text": "Logging", "href": "topics/logging.html#topics-logging"}, {"text": "Core API", "href": "topics/api.html#topics-api"}, {"text": "Common Practices", "href": "topics/practices.html#topics-practices"}, {"text": "scrapyd-client", "href": "https://github.com/scrapy/scrapyd-client"}, {"text": "Deploying Spiders", "href": "topics/deploy.html#topics-deploy"}, {"text": "scrapy-djangoitem", "href": "https://github.com/scrapy-plugins/scrapy-djangoitem"}, {"text": "scrapy-jsonrpc", "href": "https://github.com/scrapy-plugins/scrapy-jsonrpc"}, {"text": "issue 1060", "href": "https://github.com/scrapy/scrapy/issues/1060"}, {"text": "issue 1235", "href": "https://github.com/scrapy/scrapy/issues/1235"}, {"text": "issue 1236", "href": "https://github.com/scrapy/scrapy/issues/1236"}, {"text": "issue 1240", "href": "https://github.com/scrapy/scrapy/issues/1240"}, {"text": "issue 1259", "href": "https://github.com/scrapy/scrapy/issues/1259"}, {"text": "issue 1278", "href": "https://github.com/scrapy/scrapy/issues/1278"}, {"text": "issue 1286", "href": "https://github.com/scrapy/scrapy/issues/1286"}, {"text": "issue 1159", "href": "https://github.com/scrapy/scrapy/issues/1159"}, {"text": "issue 1224", "href": "https://github.com/scrapy/scrapy/issues/1224"}, {"text": "issue 1132", "href": "https://github.com/scrapy/scrapy/issues/1132"}, {"text": "issue 963", "href": "https://github.com/scrapy/scrapy/issues/963"}, {"text": "issue 1123", "href": "https://github.com/scrapy/scrapy/issues/1123"}, {"text": "issue 1081", "href": "https://github.com/scrapy/scrapy/issues/1081"}, {"text": "issue 1086", "href": "https://github.com/scrapy/scrapy/issues/1086"}, {"text": "issue 1098", "href": "https://github.com/scrapy/scrapy/issues/1098"}, {"text": "issue 1101", "href": "https://github.com/scrapy/scrapy/issues/1101"}, {"text": "issue 624", "href": "https://github.com/scrapy/scrapy/issues/624"}, {"text": "issue 1145", "href": "https://github.com/scrapy/scrapy/issues/1145"}, {"text": "issue 1016", "href": "https://github.com/scrapy/scrapy/issues/1016"}, {"text": "issue 1020", "href": "https://github.com/scrapy/scrapy/issues/1020"}, {"text": "issue 983", "href": "https://github.com/scrapy/scrapy/issues/983"}, {"text": "issue 821", "href": "https://github.com/scrapy/scrapy/issues/821"}, {"text": "issue 961", "href": "https://github.com/scrapy/scrapy/issues/961"}, {"text": "issue 946", "href": "https://github.com/scrapy/scrapy/issues/946"}, {"text": "issue 882", "href": "https://github.com/scrapy/scrapy/issues/882"}, {"text": "issue 795", "href": "https://github.com/scrapy/scrapy/issues/795"}, {"text": "issue 896", "href": "https://github.com/scrapy/scrapy/issues/896"}, {"text": "issue 854", "href": "https://github.com/scrapy/scrapy/issues/854"}, {"text": "issue 817", "href": "https://github.com/scrapy/scrapy/issues/817"}, {"text": "issue 816", "href": "https://github.com/scrapy/scrapy/issues/816"}, {"text": "issue 1128", "href": "https://github.com/scrapy/scrapy/issues/1128"}, {"text": "issue 1147", "href": "https://github.com/scrapy/scrapy/issues/1147"}, {"text": "issue 1148", "href": "https://github.com/scrapy/scrapy/issues/1148"}, {"text": "issue 1156", "href": "https://github.com/scrapy/scrapy/issues/1156"}, {"text": "issue 1185", "href": "https://github.com/scrapy/scrapy/issues/1185"}, {"text": "issue 1187", "href": "https://github.com/scrapy/scrapy/issues/1187"}, {"text": "issue 1258", "href": "https://github.com/scrapy/scrapy/issues/1258"}, {"text": "issue 1268", "href": "https://github.com/scrapy/scrapy/issues/1268"}, {"text": "issue 1276", "href": "https://github.com/scrapy/scrapy/issues/1276"}, {"text": "issue 1285", "href": "https://github.com/scrapy/scrapy/issues/1285"}, {"text": "issue 1284", "href": "https://github.com/scrapy/scrapy/issues/1284"}, {"text": "issue 1074", "href": "https://github.com/scrapy/scrapy/issues/1074"}, {"text": "issue 1075", "href": "https://github.com/scrapy/scrapy/issues/1075"}, {"text": "issue 1297", "href": "https://github.com/scrapy/scrapy/issues/1297"}, {"text": "issue 1205", "href": "https://github.com/scrapy/scrapy/issues/1205"}, {"text": "issue 1155", "href": "https://github.com/scrapy/scrapy/issues/1155"}, {"text": "issue 925", "href": "https://github.com/scrapy/scrapy/issues/925"}, {"text": "issue 895", "href": "https://github.com/scrapy/scrapy/issues/895"}, {"text": "issue 911", "href": "https://github.com/scrapy/scrapy/issues/911"}, {"text": "issue 777", "href": "https://github.com/scrapy/scrapy/issues/777"}, {"text": "issue 1242", "href": "https://github.com/scrapy/scrapy/issues/1242"}, {"text": "issue 1218", "href": "https://github.com/scrapy/scrapy/issues/1218"}, {"text": "issue 1233", "href": "https://github.com/scrapy/scrapy/issues/1233"}, {"text": "issue 1181", "href": "https://github.com/scrapy/scrapy/issues/1181"}, {"text": "issue 1210", "href": "https://github.com/scrapy/scrapy/issues/1210"}, {"text": "issue 1166", "href": "https://github.com/scrapy/scrapy/issues/1166"}, {"text": "issue 1177", "href": "https://github.com/scrapy/scrapy/issues/1177"}, {"text": "issue 1102", "href": "https://github.com/scrapy/scrapy/issues/1102"}, {"text": "issue 1134", "href": "https://github.com/scrapy/scrapy/issues/1134"}, {"text": "issue 914", "href": "https://github.com/scrapy/scrapy/issues/914"}, {"text": "issue 859", "href": "https://github.com/scrapy/scrapy/issues/859"}, {"text": "issue 827", "href": "https://github.com/scrapy/scrapy/issues/827"}, {"text": "issue 841", "href": "https://github.com/scrapy/scrapy/issues/841"}, {"text": "issue 1267", "href": "https://github.com/scrapy/scrapy/issues/1267"}, {"text": "issue 1190", "href": "https://github.com/scrapy/scrapy/issues/1190"}, {"text": "issue 1188", "href": "https://github.com/scrapy/scrapy/issues/1188"}, {"text": "issue 1180", "href": "https://github.com/scrapy/scrapy/issues/1180"}, {"text": "issue 1150", "href": "https://github.com/scrapy/scrapy/issues/1150"}, {"text": "issue 1164", "href": "https://github.com/scrapy/scrapy/issues/1164"}, {"text": "issue 1124", "href": "https://github.com/scrapy/scrapy/issues/1124"}, {"text": "issue 1073", "href": "https://github.com/scrapy/scrapy/issues/1073"}, {"text": "issue 1106", "href": "https://github.com/scrapy/scrapy/issues/1106"}, {"text": "issue 647", "href": "https://github.com/scrapy/scrapy/issues/647"}, {"text": "issue 1022", "href": "https://github.com/scrapy/scrapy/issues/1022"}, {"text": "issue 1071", "href": "https://github.com/scrapy/scrapy/issues/1071"}, {"text": "issue 898", "href": "https://github.com/scrapy/scrapy/issues/898"}, {"text": "issue 893", "href": "https://github.com/scrapy/scrapy/issues/893"}, {"text": "issue 894", "href": "https://github.com/scrapy/scrapy/issues/894"}, {"text": "issue 904", "href": "https://github.com/scrapy/scrapy/issues/904"}, {"text": "issue 1292", "href": "https://github.com/scrapy/scrapy/issues/1292"}, {"text": "issue 1220", "href": "https://github.com/scrapy/scrapy/issues/1220"}, {"text": "issue 1219", "href": "https://github.com/scrapy/scrapy/issues/1219"}, {"text": "issue 1196", "href": "https://github.com/scrapy/scrapy/issues/1196"}, {"text": "issue 1172", "href": "https://github.com/scrapy/scrapy/issues/1172"}, {"text": "issue 1171", "href": "https://github.com/scrapy/scrapy/issues/1171"}, {"text": "issue 1169", "href": "https://github.com/scrapy/scrapy/issues/1169"}, {"text": "issue 1160", "href": "https://github.com/scrapy/scrapy/issues/1160"}, {"text": "issue 1154", "href": "https://github.com/scrapy/scrapy/issues/1154"}, {"text": "issue 1127", "href": "https://github.com/scrapy/scrapy/issues/1127"}, {"text": "issue 1112", "href": "https://github.com/scrapy/scrapy/issues/1112"}, {"text": "issue 1105", "href": "https://github.com/scrapy/scrapy/issues/1105"}, {"text": "issue 1041", "href": "https://github.com/scrapy/scrapy/issues/1041"}, {"text": "issue 1082", "href": "https://github.com/scrapy/scrapy/issues/1082"}, {"text": "issue 1033", "href": "https://github.com/scrapy/scrapy/issues/1033"}, {"text": "issue 944", "href": "https://github.com/scrapy/scrapy/issues/944"}, {"text": "issue 866", "href": "https://github.com/scrapy/scrapy/issues/866"}, {"text": "issue 864", "href": "https://github.com/scrapy/scrapy/issues/864"}, {"text": "issue 796", "href": "https://github.com/scrapy/scrapy/issues/796"}, {"text": "issue 1260", "href": "https://github.com/scrapy/scrapy/issues/1260"}, {"text": "issue 1271", "href": "https://github.com/scrapy/scrapy/issues/1271"}, {"text": "issue 1293", "href": "https://github.com/scrapy/scrapy/issues/1293"}, {"text": "issue 1298", "href": "https://github.com/scrapy/scrapy/issues/1298"}, {"text": "issue 353", "href": "https://github.com/scrapy/scrapy/issues/353"}, {"text": "issue 1228", "href": "https://github.com/scrapy/scrapy/issues/1228"}, {"text": "issue 722", "href": "https://github.com/scrapy/scrapy/issues/722"}, {"text": "issue 1131", "href": "https://github.com/scrapy/scrapy/issues/1131"}, {"text": "issue 1197", "href": "https://github.com/scrapy/scrapy/issues/1197"}, {"text": "issue 954", "href": "https://github.com/scrapy/scrapy/issues/954"}, {"text": "issue 902", "href": "https://github.com/scrapy/scrapy/issues/902"}, {"text": "issue 878", "href": "https://github.com/scrapy/scrapy/issues/878"}, {"text": "issue 879", "href": "https://github.com/scrapy/scrapy/issues/879"}, {"text": "issue 846", "href": "https://github.com/scrapy/scrapy/issues/846"}, {"text": "issue 1161", "href": "https://github.com/scrapy/scrapy/issues/1161"}, {"text": "issue 1162", "href": "https://github.com/scrapy/scrapy/issues/1162"}, {"text": "issue 1121", "href": "https://github.com/scrapy/scrapy/issues/1121"}, {"text": "issue 1070", "href": "https://github.com/scrapy/scrapy/issues/1070"}, {"text": "issue 1066", "href": "https://github.com/scrapy/scrapy/issues/1066"}, {"text": "issue 909", "href": "https://github.com/scrapy/scrapy/issues/909"}, {"text": "issue 830", "href": "https://github.com/scrapy/scrapy/issues/830"}, {"text": "issue 810", "href": "https://github.com/scrapy/scrapy/issues/810"}, {"text": "issue 803", "href": "https://github.com/scrapy/scrapy/issues/803"}, {"text": "issue 801", "href": "https://github.com/scrapy/scrapy/issues/801"}, {"text": "issue 800", "href": "https://github.com/scrapy/scrapy/issues/800"}, {"text": "issue 799", "href": "https://github.com/scrapy/scrapy/issues/799"}, {"text": "issue 798", "href": "https://github.com/scrapy/scrapy/issues/798"}, {"text": "issue 797", "href": "https://github.com/scrapy/scrapy/issues/797"}, {"text": "issue 776", "href": "https://github.com/scrapy/scrapy/issues/776"}, {"text": "issue 1243", "href": "https://github.com/scrapy/scrapy/issues/1243"}, {"text": "issue 1206", "href": "https://github.com/scrapy/scrapy/issues/1206"}, {"text": "issue 1234", "href": "https://github.com/scrapy/scrapy/issues/1234"}, {"text": "issue 1165", "href": "https://github.com/scrapy/scrapy/issues/1165"}, {"text": "issue 1168", "href": "https://github.com/scrapy/scrapy/issues/1168"}, {"text": "issue 1152", "href": "https://github.com/scrapy/scrapy/issues/1152"}, {"text": "issue 1089", "href": "https://github.com/scrapy/scrapy/issues/1089"}, {"text": "issue 1044", "href": "https://github.com/scrapy/scrapy/issues/1044"}, {"text": "issue 835", "href": "https://github.com/scrapy/scrapy/issues/835"}, {"text": "issue 779", "href": "https://github.com/scrapy/scrapy/issues/779"}, {"text": "issue 1079", "href": "https://github.com/scrapy/scrapy/issues/1079"}, {"text": "issue 1078", "href": "https://github.com/scrapy/scrapy/issues/1078"}, {"text": "issue 992", "href": "https://github.com/scrapy/scrapy/issues/992"}, {"text": "issue 871", "href": "https://github.com/scrapy/scrapy/issues/871"}, {"text": "issue 805", "href": "https://github.com/scrapy/scrapy/issues/805"}, {"text": "issue 775", "href": "https://github.com/scrapy/scrapy/issues/775"}, {"text": "commit 07cb3e5", "href": "https://github.com/scrapy/scrapy/commit/07cb3e5"}, {"text": "commit 2c8e573", "href": "https://github.com/scrapy/scrapy/commit/2c8e573"}, {"text": "commit d694019", "href": "https://github.com/scrapy/scrapy/commit/d694019"}, {"text": "commit f92fa83", "href": "https://github.com/scrapy/scrapy/commit/f92fa83"}, {"text": "commit c2c6d15", "href": "https://github.com/scrapy/scrapy/commit/c2c6d15"}, {"text": "commit 540b9bc", "href": "https://github.com/scrapy/scrapy/commit/540b9bc"}, {"text": "commit b4c454b", "href": "https://github.com/scrapy/scrapy/commit/b4c454b"}, {"text": "commit e3c1260", "href": "https://github.com/scrapy/scrapy/commit/e3c1260"}, {"text": "commit 9e13f42", "href": "https://github.com/scrapy/scrapy/commit/9e13f42"}, {"text": "commit cdb9a0b", "href": "https://github.com/scrapy/scrapy/commit/cdb9a0b"}, {"text": "commit bb3a848", "href": "https://github.com/scrapy/scrapy/commit/bb3a848"}, {"text": "commit edb07a4", "href": "https://github.com/scrapy/scrapy/commit/edb07a4"}, {"text": "commit 7ee6f7a", "href": "https://github.com/scrapy/scrapy/commit/7ee6f7a"}, {"text": "commit 874fcdd", "href": "https://github.com/scrapy/scrapy/commit/874fcdd"}, {"text": "commit c6b21f0", "href": "https://github.com/scrapy/scrapy/commit/c6b21f0"}, {"text": "commit c3a6628", "href": "https://github.com/scrapy/scrapy/commit/c3a6628"}, {"text": "commit d0bf957", "href": "https://github.com/scrapy/scrapy/commit/d0bf957"}, {"text": "commit eeb589a", "href": "https://github.com/scrapy/scrapy/commit/eeb589a"}, {"text": "commit 5fdab02", "href": "https://github.com/scrapy/scrapy/commit/5fdab02"}, {"text": "commit b0ae199", "href": "https://github.com/scrapy/scrapy/commit/b0ae199"}, {"text": "commit 5cb0cfb", "href": "https://github.com/scrapy/scrapy/commit/5cb0cfb"}, {"text": "commit 781286b", "href": "https://github.com/scrapy/scrapy/commit/781286b"}, {"text": "commit b415d04", "href": "https://github.com/scrapy/scrapy/commit/b415d04"}, {"text": "commit 627b9ba", "href": "https://github.com/scrapy/scrapy/commit/627b9ba"}, {"text": "commit de909ad", "href": "https://github.com/scrapy/scrapy/commit/de909ad"}, {"text": "commit 3f3263d", "href": "https://github.com/scrapy/scrapy/commit/3f3263d"}, {"text": "commit 49b40f0", "href": "https://github.com/scrapy/scrapy/commit/49b40f0"}, {"text": "commit 5eddc68", "href": "https://github.com/scrapy/scrapy/commit/5eddc68"}, {"text": "commit d6cb999", "href": "https://github.com/scrapy/scrapy/commit/d6cb999"}, {"text": "commit 8e080c1", "href": "https://github.com/scrapy/scrapy/commit/8e080c1"}, {"text": "commit 1d0c096", "href": "https://github.com/scrapy/scrapy/commit/1d0c096"}, {"text": "commit 4c701d7", "href": "https://github.com/scrapy/scrapy/commit/4c701d7"}, {"text": "commit d109c13", "href": "https://github.com/scrapy/scrapy/commit/d109c13"}, {"text": "commit 39d2ce5", "href": "https://github.com/scrapy/scrapy/commit/39d2ce5"}, {"text": "commit 180d3ad", "href": "https://github.com/scrapy/scrapy/commit/180d3ad"}, {"text": "commit a51ee8b", "href": "https://github.com/scrapy/scrapy/commit/a51ee8b"}, {"text": "commit ee3b371", "href": "https://github.com/scrapy/scrapy/commit/ee3b371"}, {"text": "commit c3861cf", "href": "https://github.com/scrapy/scrapy/commit/c3861cf"}, {"text": "commit 362e322", "href": "https://github.com/scrapy/scrapy/commit/362e322"}, {"text": "commit 94a5c65", "href": "https://github.com/scrapy/scrapy/commit/94a5c65"}, {"text": "commit a274a7f", "href": "https://github.com/scrapy/scrapy/commit/a274a7f"}, {"text": "commit ae1e2cc", "href": "https://github.com/scrapy/scrapy/commit/ae1e2cc"}, {"text": "commit e49c96a", "href": "https://github.com/scrapy/scrapy/commit/e49c96a"}, {"text": "commit 1ca489d", "href": "https://github.com/scrapy/scrapy/commit/1ca489d"}, {"text": "commit 65c8f05", "href": "https://github.com/scrapy/scrapy/commit/65c8f05"}, {"text": "commit 037f6ab", "href": "https://github.com/scrapy/scrapy/commit/037f6ab"}, {"text": "commit 2d103e0", "href": "https://github.com/scrapy/scrapy/commit/2d103e0"}, {"text": "https://github.com/scrapy/w3lib/pull/23", "href": "https://github.com/scrapy/w3lib/pull/23"}, {"text": "commit f8d366a", "href": "https://github.com/scrapy/scrapy/commit/f8d366a"}, {"text": "commit 81344ea", "href": "https://github.com/scrapy/scrapy/commit/81344ea"}, {"text": "commit f7c4ea8", "href": "https://github.com/scrapy/scrapy/commit/f7c4ea8"}, {"text": "commit db59ed9", "href": "https://github.com/scrapy/scrapy/commit/db59ed9"}, {"text": "commit f090260", "href": "https://github.com/scrapy/scrapy/commit/f090260"}, {"text": "commit d8793af", "href": "https://github.com/scrapy/scrapy/commit/d8793af"}, {"text": "commit ed1f376", "href": "https://github.com/scrapy/scrapy/commit/ed1f376"}, {"text": "commit 91a1106", "href": "https://github.com/scrapy/scrapy/commit/91a1106"}, {"text": "commit 743e1e2", "href": "https://github.com/scrapy/scrapy/commit/743e1e2"}, {"text": "commit e22daaf", "href": "https://github.com/scrapy/scrapy/commit/e22daaf"}, {"text": "commit 5ec430b", "href": "https://github.com/scrapy/scrapy/commit/5ec430b"}, {"text": "commit e5e8133", "href": "https://github.com/scrapy/scrapy/commit/e5e8133"}, {"text": "commit 3cd6146", "href": "https://github.com/scrapy/scrapy/commit/3cd6146"}, {"text": "commit fa5d76b", "href": "https://github.com/scrapy/scrapy/commit/fa5d76b"}, {"text": "commit c6a9e20", "href": "https://github.com/scrapy/scrapy/commit/c6a9e20"}, {"text": "commit 8e3f20a", "href": "https://github.com/scrapy/scrapy/commit/8e3f20a"}, {"text": "issue 494", "href": "https://github.com/scrapy/scrapy/issues/494"}, {"text": "issue 684", "href": "https://github.com/scrapy/scrapy/issues/684"}, {"text": "issue 554", "href": "https://github.com/scrapy/scrapy/issues/554"}, {"text": "issue 690", "href": "https://github.com/scrapy/scrapy/issues/690"}, {"text": "issue 559", "href": "https://github.com/scrapy/scrapy/issues/559"}, {"text": "issue 761", "href": "https://github.com/scrapy/scrapy/issues/761"}, {"text": "issue 763", "href": "https://github.com/scrapy/scrapy/issues/763"}, {"text": "issue 737", "href": "https://github.com/scrapy/scrapy/issues/737"}, {"text": "issue 688", "href": "https://github.com/scrapy/scrapy/issues/688"}, {"text": "issue 762", "href": "https://github.com/scrapy/scrapy/issues/762"}, {"text": "issue 699", "href": "https://github.com/scrapy/scrapy/issues/699"}, {"text": "issue 509", "href": "https://github.com/scrapy/scrapy/issues/509"}, {"text": "issue 549", "href": "https://github.com/scrapy/scrapy/issues/549"}, {"text": "issue 535", "href": "https://github.com/scrapy/scrapy/issues/535"}, {"text": "issue 541", "href": "https://github.com/scrapy/scrapy/issues/541"}, {"text": "issue 500", "href": "https://github.com/scrapy/scrapy/issues/500"}, {"text": "issue 571", "href": "https://github.com/scrapy/scrapy/issues/571"}, {"text": "issue 557", "href": "https://github.com/scrapy/scrapy/issues/557"}, {"text": "issue 570", "href": "https://github.com/scrapy/scrapy/issues/570"}, {"text": "issue 566", "href": "https://github.com/scrapy/scrapy/issues/566"}, {"text": "issue 555", "href": "https://github.com/scrapy/scrapy/issues/555"}, {"text": "issue 553", "href": "https://github.com/scrapy/scrapy/issues/553"}, {"text": "issue 602", "href": "https://github.com/scrapy/scrapy/issues/602"}, {"text": "issue 622", "href": "https://github.com/scrapy/scrapy/issues/622"}, {"text": "issue 565", "href": "https://github.com/scrapy/scrapy/issues/565"}, {"text": "issue 629", "href": "https://github.com/scrapy/scrapy/issues/629"}, {"text": "issue 630", "href": "https://github.com/scrapy/scrapy/issues/630"}, {"text": "issue 638", "href": "https://github.com/scrapy/scrapy/issues/638"}, {"text": "issue 632", "href": "https://github.com/scrapy/scrapy/issues/632"}, {"text": "issue 636", "href": "https://github.com/scrapy/scrapy/issues/636"}, {"text": "issue 640", "href": "https://github.com/scrapy/scrapy/issues/640"}, {"text": "issue 635", "href": "https://github.com/scrapy/scrapy/issues/635"}, {"text": "issue 634", "href": "https://github.com/scrapy/scrapy/issues/634"}, {"text": "issue 639", "href": "https://github.com/scrapy/scrapy/issues/639"}, {"text": "issue 637", "href": "https://github.com/scrapy/scrapy/issues/637"}, {"text": "issue 631", "href": "https://github.com/scrapy/scrapy/issues/631"}, {"text": "issue 633", "href": "https://github.com/scrapy/scrapy/issues/633"}, {"text": "issue 641", "href": "https://github.com/scrapy/scrapy/issues/641"}, {"text": "issue 642", "href": "https://github.com/scrapy/scrapy/issues/642"}, {"text": "issue 646", "href": "https://github.com/scrapy/scrapy/issues/646"}, {"text": "issue 645", "href": "https://github.com/scrapy/scrapy/issues/645"}, {"text": "issue 650", "href": "https://github.com/scrapy/scrapy/issues/650"}, {"text": "issue 654", "href": "https://github.com/scrapy/scrapy/issues/654"}, {"text": "issue 612", "href": "https://github.com/scrapy/scrapy/issues/612"}, {"text": "issue 656", "href": "https://github.com/scrapy/scrapy/issues/656"}, {"text": "issue 193", "href": "https://github.com/scrapy/scrapy/issues/193"}, {"text": "issue 660", "href": "https://github.com/scrapy/scrapy/issues/660"}, {"text": "issue 674", "href": "https://github.com/scrapy/scrapy/issues/674"}, {"text": "issue 679", "href": "https://github.com/scrapy/scrapy/issues/679"}, {"text": "issue 687", "href": "https://github.com/scrapy/scrapy/issues/687"}, {"text": "issue 681", "href": "https://github.com/scrapy/scrapy/issues/681"}, {"text": "issue 692", "href": "https://github.com/scrapy/scrapy/issues/692"}, {"text": "issue 546", "href": "https://github.com/scrapy/scrapy/issues/546"}, {"text": "issue 659", "href": "https://github.com/scrapy/scrapy/issues/659"}, {"text": "issue 760", "href": "https://github.com/scrapy/scrapy/issues/760"}, {"text": "issue 693", "href": "https://github.com/scrapy/scrapy/issues/693"}, {"text": "issue 698", "href": "https://github.com/scrapy/scrapy/issues/698"}, {"text": "issue 597", "href": "https://github.com/scrapy/scrapy/issues/597"}, {"text": "issue 705", "href": "https://github.com/scrapy/scrapy/issues/705"}, {"text": "issue 727", "href": "https://github.com/scrapy/scrapy/issues/727"}, {"text": "issue 738", "href": "https://github.com/scrapy/scrapy/issues/738"}, {"text": "issue 724", "href": "https://github.com/scrapy/scrapy/issues/724"}, {"text": "issue 733", "href": "https://github.com/scrapy/scrapy/issues/733"}, {"text": "issue 752", "href": "https://github.com/scrapy/scrapy/issues/752"}, {"text": "issue 719", "href": "https://github.com/scrapy/scrapy/issues/719"}, {"text": "issue 746", "href": "https://github.com/scrapy/scrapy/issues/746"}, {"text": "issue 697", "href": "https://github.com/scrapy/scrapy/issues/697"}, {"text": "issue 626", "href": "https://github.com/scrapy/scrapy/issues/626"}, {"text": "issue 500", "href": "https://github.com/scrapy/scrapy/issues/500"}, {"text": "issue 742", "href": "https://github.com/scrapy/scrapy/issues/742"}, {"text": "issue 575", "href": "https://github.com/scrapy/scrapy/issues/575"}, {"text": "issue 587", "href": "https://github.com/scrapy/scrapy/issues/587"}, {"text": "issue 590", "href": "https://github.com/scrapy/scrapy/issues/590"}, {"text": "issue 596", "href": "https://github.com/scrapy/scrapy/issues/596"}, {"text": "issue 610", "href": "https://github.com/scrapy/scrapy/issues/610"}, {"text": "issue 617", "href": "https://github.com/scrapy/scrapy/issues/617"}, {"text": "issue 618", "href": "https://github.com/scrapy/scrapy/issues/618"}, {"text": "issue 627", "href": "https://github.com/scrapy/scrapy/issues/627"}, {"text": "issue 613", "href": "https://github.com/scrapy/scrapy/issues/613"}, {"text": "issue 643", "href": "https://github.com/scrapy/scrapy/issues/643"}, {"text": "issue 654", "href": "https://github.com/scrapy/scrapy/issues/654"}, {"text": "issue 675", "href": "https://github.com/scrapy/scrapy/issues/675"}, {"text": "issue 663", "href": "https://github.com/scrapy/scrapy/issues/663"}, {"text": "issue 711", "href": "https://github.com/scrapy/scrapy/issues/711"}, {"text": "issue 714", "href": "https://github.com/scrapy/scrapy/issues/714"}, {"text": "issue 561", "href": "https://github.com/scrapy/scrapy/issues/561"}, {"text": "issue 556", "href": "https://github.com/scrapy/scrapy/issues/556"}, {"text": "issue 485", "href": "https://github.com/scrapy/scrapy/issues/485"}, {"text": "issue 574", "href": "https://github.com/scrapy/scrapy/issues/574"}, {"text": "issue 581", "href": "https://github.com/scrapy/scrapy/issues/581"}, {"text": "issue 584", "href": "https://github.com/scrapy/scrapy/issues/584"}, {"text": "issue 582", "href": "https://github.com/scrapy/scrapy/issues/582"}, {"text": "issue 593", "href": "https://github.com/scrapy/scrapy/issues/593"}, {"text": "issue 594", "href": "https://github.com/scrapy/scrapy/issues/594"}, {"text": "issue 603", "href": "https://github.com/scrapy/scrapy/issues/603"}, {"text": "issue 628", "href": "https://github.com/scrapy/scrapy/issues/628"}, {"text": "issue 661", "href": "https://github.com/scrapy/scrapy/issues/661"}, {"text": "issue 676", "href": "https://github.com/scrapy/scrapy/issues/676"}, {"text": "issue 707", "href": "https://github.com/scrapy/scrapy/issues/707"}, {"text": "issue 745", "href": "https://github.com/scrapy/scrapy/issues/745"}, {"text": "issue 585", "href": "https://github.com/scrapy/scrapy/issues/585"}, {"text": "commit 13c099a", "href": "https://github.com/scrapy/scrapy/commit/13c099a"}, {"text": "commit 8ae11bf", "href": "https://github.com/scrapy/scrapy/commit/8ae11bf"}, {"text": "commit 1346037", "href": "https://github.com/scrapy/scrapy/commit/1346037"}, {"text": "commit 2ec2279", "href": "https://github.com/scrapy/scrapy/commit/2ec2279"}, {"text": "commit cc3eda3", "href": "https://github.com/scrapy/scrapy/commit/cc3eda3"}, {"text": "commit 8cb44f9", "href": "https://github.com/scrapy/scrapy/commit/8cb44f9"}, {"text": "commit 46d98d6", "href": "https://github.com/scrapy/scrapy/commit/46d98d6"}, {"text": "commit 13846de", "href": "https://github.com/scrapy/scrapy/commit/13846de"}, {"text": "commit 368a946", "href": "https://github.com/scrapy/scrapy/commit/368a946"}, {"text": "commit b566388", "href": "https://github.com/scrapy/scrapy/commit/b566388"}, {"text": "commit c1cb418", "href": "https://github.com/scrapy/scrapy/commit/c1cb418"}, {"text": "commit 7e4d627", "href": "https://github.com/scrapy/scrapy/commit/7e4d627"}, {"text": "commit 76c7e20", "href": "https://github.com/scrapy/scrapy/commit/76c7e20"}, {"text": "commit 5f87b17", "href": "https://github.com/scrapy/scrapy/commit/5f87b17"}, {"text": "commit d0ee545", "href": "https://github.com/scrapy/scrapy/commit/d0ee545"}, {"text": "commit 8da65de", "href": "https://github.com/scrapy/scrapy/commit/8da65de"}, {"text": "commit 875b9ab", "href": "https://github.com/scrapy/scrapy/commit/875b9ab"}, {"text": "commit f89efaf", "href": "https://github.com/scrapy/scrapy/commit/f89efaf"}, {"text": "commit 5349cec", "href": "https://github.com/scrapy/scrapy/commit/5349cec"}, {"text": "commit 387f414", "href": "https://github.com/scrapy/scrapy/commit/387f414"}, {"text": "commit 0632546", "href": "https://github.com/scrapy/scrapy/commit/0632546"}, {"text": "commit cde9a8c", "href": "https://github.com/scrapy/scrapy/commit/cde9a8c"}, {"text": "commit fb5c9c5", "href": "https://github.com/scrapy/scrapy/commit/fb5c9c5"}, {"text": "commit 70fb105", "href": "https://github.com/scrapy/scrapy/commit/70fb105"}, {"text": "commit 6f70b6a", "href": "https://github.com/scrapy/scrapy/commit/6f70b6a"}, {"text": "commit 725900d", "href": "https://github.com/scrapy/scrapy/commit/725900d"}, {"text": "commit af0219a", "href": "https://github.com/scrapy/scrapy/commit/af0219a"}, {"text": "commit b7f58f4", "href": "https://github.com/scrapy/scrapy/commit/b7f58f4"}, {"text": "issue 541", "href": "https://github.com/scrapy/scrapy/issues/541"}, {"text": "issue 392", "href": "https://github.com/scrapy/scrapy/issues/392"}, {"text": "issue 397", "href": "https://github.com/scrapy/scrapy/issues/397"}, {"text": "issue 343", "href": "https://github.com/scrapy/scrapy/issues/343"}, {"text": "issue 510", "href": "https://github.com/scrapy/scrapy/issues/510"}, {"text": "issue 519", "href": "https://github.com/scrapy/scrapy/issues/519"}, {"text": "issue 472", "href": "https://github.com/scrapy/scrapy/issues/472"}, {"text": "issue 461", "href": "https://github.com/scrapy/scrapy/issues/461"}, {"text": "issue 533", "href": "https://github.com/scrapy/scrapy/issues/533"}, {"text": "issue 525", "href": "https://github.com/scrapy/scrapy/issues/525"}, {"text": "issue 520", "href": "https://github.com/scrapy/scrapy/issues/520"}, {"text": "issue 506", "href": "https://github.com/scrapy/scrapy/issues/506"}, {"text": "issue 503", "href": "https://github.com/scrapy/scrapy/issues/503"}, {"text": "issue 498", "href": "https://github.com/scrapy/scrapy/issues/498"}, {"text": "issue 490", "href": "https://github.com/scrapy/scrapy/issues/490"}, {"text": "issue 478", "href": "https://github.com/scrapy/scrapy/issues/478"}, {"text": "issue 475", "href": "https://github.com/scrapy/scrapy/issues/475"}, {"text": "issue 469", "href": "https://github.com/scrapy/scrapy/issues/469"}, {"text": "issue 466", "href": "https://github.com/scrapy/scrapy/issues/466"}, {"text": "issue 497", "href": "https://github.com/scrapy/scrapy/issues/497"}, {"text": "issue 527", "href": "https://github.com/scrapy/scrapy/issues/527"}, {"text": "issue 524", "href": "https://github.com/scrapy/scrapy/issues/524"}, {"text": "issue 521", "href": "https://github.com/scrapy/scrapy/issues/521"}, {"text": "issue 517", "href": "https://github.com/scrapy/scrapy/issues/517"}, {"text": "issue 512", "href": "https://github.com/scrapy/scrapy/issues/512"}, {"text": "issue 505", "href": "https://github.com/scrapy/scrapy/issues/505"}, {"text": "issue 502", "href": "https://github.com/scrapy/scrapy/issues/502"}, {"text": "issue 489", "href": "https://github.com/scrapy/scrapy/issues/489"}, {"text": "issue 465", "href": "https://github.com/scrapy/scrapy/issues/465"}, {"text": "issue 460", "href": "https://github.com/scrapy/scrapy/issues/460"}, {"text": "issue 425", "href": "https://github.com/scrapy/scrapy/issues/425"}, {"text": "issue 536", "href": "https://github.com/scrapy/scrapy/issues/536"}, {"text": "issue 484", "href": "https://github.com/scrapy/scrapy/issues/484"}, {"text": "issue 464", "href": "https://github.com/scrapy/scrapy/issues/464"}, {"text": "issue 462", "href": "https://github.com/scrapy/scrapy/issues/462"}, {"text": "issue 523", "href": "https://github.com/scrapy/scrapy/issues/523"}, {"text": "issue 537", "href": "https://github.com/scrapy/scrapy/issues/537"}, {"text": "issue 531", "href": "https://github.com/scrapy/scrapy/issues/531"}, {"text": "issue 530", "href": "https://github.com/scrapy/scrapy/issues/530"}, {"text": "issue 529", "href": "https://github.com/scrapy/scrapy/issues/529"}, {"text": "issue 507", "href": "https://github.com/scrapy/scrapy/issues/507"}, {"text": "issue 513", "href": "https://github.com/scrapy/scrapy/issues/513"}, {"text": "issue 479", "href": "https://github.com/scrapy/scrapy/issues/479"}, {"text": "commit 6d1457d", "href": "https://github.com/scrapy/scrapy/commit/6d1457d"}, {"text": "commit b4fc359", "href": "https://github.com/scrapy/scrapy/commit/b4fc359"}, {"text": "commit 5ba1ad5", "href": "https://github.com/scrapy/scrapy/commit/5ba1ad5"}, {"text": "commit 419a780", "href": "https://github.com/scrapy/scrapy/commit/419a780"}, {"text": "issue 395", "href": "https://github.com/scrapy/scrapy/issues/395"}, {"text": "issue 426", "href": "https://github.com/scrapy/scrapy/issues/426"}, {"text": "ITEM_PIPELINES", "href": "topics/settings.html#std-setting-ITEM_PIPELINES"}, {"text": "issue 360", "href": "https://github.com/scrapy/scrapy/issues/360"}, {"text": "issue 416", "href": "https://github.com/scrapy/scrapy/issues/416"}, {"text": "issue 435", "href": "https://github.com/scrapy/scrapy/issues/435"}, {"text": "issue 436", "href": "https://github.com/scrapy/scrapy/issues/436"}, {"text": "issue 431", "href": "https://github.com/scrapy/scrapy/issues/431"}, {"text": "issue 452", "href": "https://github.com/scrapy/scrapy/issues/452"}, {"text": "issue 366", "href": "https://github.com/scrapy/scrapy/issues/366"}, {"text": "commit b43b5f575", "href": "https://github.com/scrapy/scrapy/commit/b43b5f575"}, {"text": "issue 327", "href": "https://github.com/scrapy/scrapy/issues/327"}, {"text": "issue 370", "href": "https://github.com/scrapy/scrapy/issues/370"}, {"text": "issue 409", "href": "https://github.com/scrapy/scrapy/issues/409"}, {"text": "issue 317", "href": "https://github.com/scrapy/scrapy/issues/317"}, {"text": "commit 86230c0", "href": "https://github.com/scrapy/scrapy/commit/86230c0"}, {"text": "issue 410", "href": "https://github.com/scrapy/scrapy/issues/410"}, {"text": "issue 422", "href": "https://github.com/scrapy/scrapy/issues/422"}, {"text": "issue 421", "href": "https://github.com/scrapy/scrapy/issues/421"}, {"text": "issue 420", "href": "https://github.com/scrapy/scrapy/issues/420"}, {"text": "issue 419", "href": "https://github.com/scrapy/scrapy/issues/419"}, {"text": "issue 423", "href": "https://github.com/scrapy/scrapy/issues/423"}, {"text": "issue 418", "href": "https://github.com/scrapy/scrapy/issues/418"}, {"text": "commit ecfa7431", "href": "https://github.com/scrapy/scrapy/commit/ecfa7431"}, {"text": "issue 430", "href": "https://github.com/scrapy/scrapy/issues/430"}, {"text": "issue 432", "href": "https://github.com/scrapy/scrapy/issues/432"}, {"text": "issue 445", "href": "https://github.com/scrapy/scrapy/issues/445"}, {"text": "issue 372", "href": "https://github.com/scrapy/scrapy/issues/372"}, {"text": "issue 450", "href": "https://github.com/scrapy/scrapy/issues/450"}, {"text": "commit b326b87", "href": "https://github.com/scrapy/scrapy/commit/b326b87"}, {"text": "commit 684cfc0", "href": "https://github.com/scrapy/scrapy/commit/684cfc0"}, {"text": "commit b6bed44c", "href": "https://github.com/scrapy/scrapy/commit/b6bed44c"}, {"text": "issue 406", "href": "https://github.com/scrapy/scrapy/issues/406"}, {"text": "issue 418", "href": "https://github.com/scrapy/scrapy/issues/418"}, {"text": "issue 407", "href": "https://github.com/scrapy/scrapy/issues/407"}, {"text": "issue 429", "href": "https://github.com/scrapy/scrapy/issues/429"}, {"text": "issue 387", "href": "https://github.com/scrapy/scrapy/issues/387"}, {"text": "issue 391", "href": "https://github.com/scrapy/scrapy/issues/391"}, {"text": "issue 399", "href": "https://github.com/scrapy/scrapy/issues/399"}, {"text": "issue 400", "href": "https://github.com/scrapy/scrapy/issues/400"}, {"text": "issue 401", "href": "https://github.com/scrapy/scrapy/issues/401"}, {"text": "issue 402", "href": "https://github.com/scrapy/scrapy/issues/402"}, {"text": "issue 404", "href": "https://github.com/scrapy/scrapy/issues/404"}, {"text": "commit 37c24e01d7", "href": "https://github.com/scrapy/scrapy/commit/37c24e01d7"}, {"text": "issue 226", "href": "https://github.com/scrapy/scrapy/issues/226"}, {"text": "issue 448", "href": "https://github.com/scrapy/scrapy/issues/448"}, {"text": "cssselect", "href": "https://cssselect.readthedocs.io/en/latest/index.html"}, {"text": "lxml", "href": "https://lxml.de/"}, {"text": "issue 390", "href": "https://github.com/scrapy/scrapy/issues/390"}, {"text": "commit 3d32c4f", "href": "https://github.com/scrapy/scrapy/commit/3d32c4f"}, {"text": "commit b1d8919", "href": "https://github.com/scrapy/scrapy/commit/b1d8919"}, {"text": "commit 89faf52", "href": "https://github.com/scrapy/scrapy/commit/89faf52"}, {"text": "commit 12693a5", "href": "https://github.com/scrapy/scrapy/commit/12693a5"}, {"text": "commit e429f63", "href": "https://github.com/scrapy/scrapy/commit/e429f63"}, {"text": "commit 912202e", "href": "https://github.com/scrapy/scrapy/commit/912202e"}, {"text": "commit cfc2d46", "href": "https://github.com/scrapy/scrapy/commit/cfc2d46"}, {"text": "commit 06149e0", "href": "https://github.com/scrapy/scrapy/commit/06149e0"}, {"text": "issue 339", "href": "https://github.com/scrapy/scrapy/issues/339"}, {"text": "commit d20304e", "href": "https://github.com/scrapy/scrapy/commit/d20304e"}, {"text": "commit 1994f38", "href": "https://github.com/scrapy/scrapy/commit/1994f38"}, {"text": "commit abf756f", "href": "https://github.com/scrapy/scrapy/commit/abf756f"}, {"text": "commit b15470d", "href": "https://github.com/scrapy/scrapy/commit/b15470d"}, {"text": "commit c4bf324", "href": "https://github.com/scrapy/scrapy/commit/c4bf324"}, {"text": "commit 6cbe684", "href": "https://github.com/scrapy/scrapy/commit/6cbe684"}, {"text": "commit 1a20bba", "href": "https://github.com/scrapy/scrapy/commit/1a20bba"}, {"text": "commit 3b01bb8", "href": "https://github.com/scrapy/scrapy/commit/3b01bb8"}, {"text": "commit fa766d7", "href": "https://github.com/scrapy/scrapy/commit/fa766d7"}, {"text": "commit 3283809", "href": "https://github.com/scrapy/scrapy/commit/3283809"}, {"text": "commit 1411923", "href": "https://github.com/scrapy/scrapy/commit/1411923"}, {"text": "commit bb35ed0", "href": "https://github.com/scrapy/scrapy/commit/bb35ed0"}, {"text": "commit de3e451", "href": "https://github.com/scrapy/scrapy/commit/de3e451"}, {"text": "commit c45e5f1", "href": "https://github.com/scrapy/scrapy/commit/c45e5f1"}, {"text": "commit 0b60031", "href": "https://github.com/scrapy/scrapy/commit/0b60031"}, {"text": "commit 3fe2a32", "href": "https://github.com/scrapy/scrapy/commit/3fe2a32"}, {"text": "issue 347", "href": "https://github.com/scrapy/scrapy/issues/347"}, {"text": "issue 352", "href": "https://github.com/scrapy/scrapy/issues/352"}, {"text": "issue 359", "href": "https://github.com/scrapy/scrapy/issues/359"}, {"text": "issue 12", "href": "https://github.com/scrapy/scrapy/issues/12"}, {"text": "issue 19", "href": "https://github.com/scrapy/scrapy/issues/19"}, {"text": "commit 4dc76e", "href": "https://github.com/scrapy/scrapy/commit/4dc76e"}, {"text": "issue 24", "href": "https://github.com/scrapy/scrapy/issues/24"}, {"text": "issue 59", "href": "https://github.com/scrapy/scrapy/issues/59"}, {"text": "issue 66", "href": "https://github.com/scrapy/scrapy/issues/66"}, {"text": "issue 77", "href": "https://github.com/scrapy/scrapy/issues/77"}, {"text": "issue 105", "href": "https://github.com/scrapy/scrapy/issues/105"}, {"text": "issue 78", "href": "https://github.com/scrapy/scrapy/issues/78"}, {"text": "issue 109", "href": "https://github.com/scrapy/scrapy/issues/109"}, {"text": "issue 318", "href": "https://github.com/scrapy/scrapy/issues/318"}, {"text": "issue 185", "href": "https://github.com/scrapy/scrapy/issues/185"}, {"text": "issue 199", "href": "https://github.com/scrapy/scrapy/issues/199"}, {"text": "issue 205", "href": "https://github.com/scrapy/scrapy/issues/205"}, {"text": "issue 206", "href": "https://github.com/scrapy/scrapy/issues/206"}, {"text": "issue 212", "href": "https://github.com/scrapy/scrapy/issues/212"}, {"text": "issue 214", "href": "https://github.com/scrapy/scrapy/issues/214"}, {"text": "issue 217", "href": "https://github.com/scrapy/scrapy/issues/217"}, {"text": "issue 218", "href": "https://github.com/scrapy/scrapy/issues/218"}, {"text": "issue 221", "href": "https://github.com/scrapy/scrapy/issues/221"}, {"text": "issue 260", "href": "https://github.com/scrapy/scrapy/issues/260"}, {"text": "issue 261", "href": "https://github.com/scrapy/scrapy/issues/261"}, {"text": "issue 269", "href": "https://github.com/scrapy/scrapy/issues/269"}, {"text": "issue 271", "href": "https://github.com/scrapy/scrapy/issues/271"}, {"text": "issue 290", "href": "https://github.com/scrapy/scrapy/issues/290"}, {"text": "issue 297", "href": "https://github.com/scrapy/scrapy/issues/297"}, {"text": "issue 329", "href": "https://github.com/scrapy/scrapy/issues/329"}, {"text": "Benchmarking", "href": "topics/benchmarking.html#benchmarking"}, {"text": "queuelib", "href": "https://github.com/scrapy/queuelib"}, {"text": "issue 260", "href": "https://github.com/scrapy/scrapy/issues/260"}, {"text": "XPathSelector.remove_namespaces", "href": "topics/selectors.html#scrapy.selector.Selector.remove_namespaces"}, {"text": "Selectors", "href": "topics/selectors.html#topics-selectors"}, {"text": "commit 8c4fcee", "href": "https://github.com/scrapy/scrapy/commit/8c4fcee"}, {"text": "commit 40667cb", "href": "https://github.com/scrapy/scrapy/commit/40667cb"}, {"text": "commit bd58bfa", "href": "https://github.com/scrapy/scrapy/commit/bd58bfa"}, {"text": "commit e3d6945", "href": "https://github.com/scrapy/scrapy/commit/e3d6945"}, {"text": "commit a274276", "href": "https://github.com/scrapy/scrapy/commit/a274276"}, {"text": "commit 6d2b3aa", "href": "https://github.com/scrapy/scrapy/commit/6d2b3aa"}, {"text": "commit c90de33", "href": "https://github.com/scrapy/scrapy/commit/c90de33"}, {"text": "commit c16150c", "href": "https://github.com/scrapy/scrapy/commit/c16150c"}, {"text": "commit 56b45fc", "href": "https://github.com/scrapy/scrapy/commit/56b45fc"}, {"text": "commit 243be84", "href": "https://github.com/scrapy/scrapy/commit/243be84"}, {"text": "commit 1fbb715", "href": "https://github.com/scrapy/scrapy/commit/1fbb715"}, {"text": "commit c72e682", "href": "https://github.com/scrapy/scrapy/commit/c72e682"}, {"text": "commit 28eac7a", "href": "https://github.com/scrapy/scrapy/commit/28eac7a"}, {"text": "commit 487b9b5", "href": "https://github.com/scrapy/scrapy/commit/487b9b5"}, {"text": "commit 8232569", "href": "https://github.com/scrapy/scrapy/commit/8232569"}, {"text": "commit 8dcf8aa", "href": "https://github.com/scrapy/scrapy/commit/8dcf8aa"}, {"text": "commit 7b5310d", "href": "https://github.com/scrapy/scrapy/commit/7b5310d"}, {"text": "commit 80f9bb6", "href": "https://github.com/scrapy/scrapy/commit/80f9bb6"}, {"text": "commit 2aa491b", "href": "https://github.com/scrapy/scrapy/commit/2aa491b"}, {"text": "commit bdf61c4", "href": "https://github.com/scrapy/scrapy/commit/bdf61c4"}, {"text": "commit 7184094", "href": "https://github.com/scrapy/scrapy/commit/7184094"}, {"text": "commit a4a9199", "href": "https://github.com/scrapy/scrapy/commit/a4a9199"}, {"text": "commit ec41673", "href": "https://github.com/scrapy/scrapy/commit/ec41673"}, {"text": "commit 86635e4", "href": "https://github.com/scrapy/scrapy/commit/86635e4"}, {"text": "commit c9b690d", "href": "https://github.com/scrapy/scrapy/commit/c9b690d"}, {"text": "commit dd55067", "href": "https://github.com/scrapy/scrapy/commit/dd55067"}, {"text": "commit 58998f4", "href": "https://github.com/scrapy/scrapy/commit/58998f4"}, {"text": "commit 8c780fd", "href": "https://github.com/scrapy/scrapy/commit/8c780fd"}, {"text": "commit 3403089", "href": "https://github.com/scrapy/scrapy/commit/3403089"}, {"text": "commit c4da0b5", "href": "https://github.com/scrapy/scrapy/commit/c4da0b5"}, {"text": "commit d52c188", "href": "https://github.com/scrapy/scrapy/commit/d52c188"}, {"text": "commit fa4f7f9", "href": "https://github.com/scrapy/scrapy/commit/fa4f7f9"}, {"text": "commit e292246", "href": "https://github.com/scrapy/scrapy/commit/e292246"}, {"text": "Spiders Contracts", "href": "topics/contracts.html#topics-contracts"}, {"text": "runspider", "href": "topics/commands.html#std-command-runspider"}, {"text": "AutoThrottle extension", "href": "topics/autothrottle.html"}, {"text": "AUTOTHROTTLE_ENABLED", "href": "topics/autothrottle.html#std-setting-AUTOTHROTTLE_ENABLED"}, {"text": "process_start_requests()", "href": "topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_start_requests"}, {"text": "Core API", "href": "topics/api.html#topics-api"}, {"text": "lxml", "href": "https://lxml.de/"}, {"text": "ClientForm", "href": "http://wwwsearch.sourceforge.net/old/ClientForm/"}, {"text": "commit 10ed28b", "href": "https://github.com/scrapy/scrapy/commit/10ed28b"}, {"text": "commit fe2ce93", "href": "https://github.com/scrapy/scrapy/commit/fe2ce93"}, {"text": "cookiejar", "href": "topics/downloader-middleware.html#std-reqmeta-cookiejar"}, {"text": "w3lib.encoding", "href": "https://github.com/scrapy/w3lib/blob/master/w3lib/encoding.py"}, {"text": "https://blog.scrapinghub.com/2012/02/27/scrapy-0-15-dropping-support-for-python-2-5/", "href": "https://blog.scrapinghub.com/2012/02/27/scrapy-0-15-dropping-support-for-python-2-5/"}, {"text": "REFERER_ENABLED", "href": "topics/spider-middleware.html#std-setting-REFERER_ENABLED"}, {"text": "w3lib", "href": "https://github.com/scrapy/w3lib"}, {"text": "DjangoItem", "href": "topics/djangoitem.html#topics-djangoitem"}, {"text": "issue 164", "href": "https://github.com/scrapy/scrapy/issues/164"}, {"text": "commit dcef7b0", "href": "https://github.com/scrapy/scrapy/commit/dcef7b0"}, {"text": "DOWNLOAD_HANDLERS", "href": "topics/settings.html#std-setting-DOWNLOAD_HANDLERS"}, {"text": "resource", "href": "https://docs.python.org/2/library/resource.html"}, {"text": "trackrefs", "href": "topics/leaks.html#topics-leaks-trackrefs"}, {"text": "commit b7e46df", "href": "https://github.com/scrapy/scrapy/commit/b7e46df"}, {"text": "https://groups.google.com/forum/#!topic/scrapy-users/qgVBmFybNAQ/discussion", "href": "https://groups.google.com/forum/#!topic/scrapy-users/qgVBmFybNAQ/discussion"}, {"text": "commit 340fbdb", "href": "https://github.com/scrapy/scrapy/commit/340fbdb"}, {"text": "commit 0cb68af", "href": "https://github.com/scrapy/scrapy/commit/0cb68af"}, {"text": "commit 4d17048", "href": "https://github.com/scrapy/scrapy/commit/4d17048"}, {"text": "commit b7b2e7f", "href": "https://github.com/scrapy/scrapy/commit/b7b2e7f"}, {"text": "commit fd85f9c", "href": "https://github.com/scrapy/scrapy/commit/fd85f9c"}, {"text": "commit c897793", "href": "https://github.com/scrapy/scrapy/commit/c897793"}, {"text": "commit 2548dcc", "href": "https://github.com/scrapy/scrapy/commit/2548dcc"}, {"text": "commit 668e352", "href": "https://github.com/scrapy/scrapy/commit/668e352"}, {"text": "commit 8e9f607", "href": "https://github.com/scrapy/scrapy/commit/8e9f607"}, {"text": "commit b830e95", "href": "https://github.com/scrapy/scrapy/commit/b830e95"}, {"text": "commit bf3c9ee", "href": "https://github.com/scrapy/scrapy/commit/bf3c9ee"}, {"text": "commit ba14f38", "href": "https://github.com/scrapy/scrapy/commit/ba14f38"}, {"text": "commit 0665175", "href": "https://github.com/scrapy/scrapy/commit/0665175"}, {"text": "commit 6a5bef2", "href": "https://github.com/scrapy/scrapy/commit/6a5bef2"}, {"text": "commit 9817df1", "href": "https://github.com/scrapy/scrapy/commit/9817df1"}, {"text": "commit 673a120", "href": "https://github.com/scrapy/scrapy/commit/673a120"}, {"text": "commit 11133e9", "href": "https://github.com/scrapy/scrapy/commit/11133e9"}, {"text": "commit 1423140", "href": "https://github.com/scrapy/scrapy/commit/1423140"}, {"text": "commit 0de3fb4", "href": "https://github.com/scrapy/scrapy/commit/0de3fb4"}, {"text": "commit 454a21d", "href": "https://github.com/scrapy/scrapy/commit/454a21d"}, {"text": "commit 2fbd662", "href": "https://github.com/scrapy/scrapy/commit/2fbd662"}, {"text": "commit 0a070f5", "href": "https://github.com/scrapy/scrapy/commit/0a070f5"}, {"text": "commit 2b4e4c3", "href": "https://github.com/scrapy/scrapy/commit/2b4e4c3"}, {"text": "commit caffe0e", "href": "https://github.com/scrapy/scrapy/commit/caffe0e"}, {"text": "commit caffe0e", "href": "https://github.com/scrapy/scrapy/commit/caffe0e"}, {"text": "commit 6cb9e1c", "href": "https://github.com/scrapy/scrapy/commit/6cb9e1c"}, {"text": "commit 4b86bd6", "href": "https://github.com/scrapy/scrapy/commit/4b86bd6"}, {"text": "commit 1aeccdd", "href": "https://github.com/scrapy/scrapy/commit/1aeccdd"}, {"text": "commit 8bf19e6", "href": "https://github.com/scrapy/scrapy/commit/8bf19e6"}, {"text": "commit 14a8e6e", "href": "https://github.com/scrapy/scrapy/commit/14a8e6e"}, {"text": "commit 5223575", "href": "https://github.com/scrapy/scrapy/commit/5223575"}, {"text": "commit 63d583d", "href": "https://github.com/scrapy/scrapy/commit/63d583d"}, {"text": "commit bcb3198", "href": "https://github.com/scrapy/scrapy/commit/bcb3198"}, {"text": "commit 98f3f87", "href": "https://github.com/scrapy/scrapy/commit/98f3f87"}, {"text": "commit 175a4b5", "href": "https://github.com/scrapy/scrapy/commit/175a4b5"}, {"text": "AJAX crawlable urls", "href": "https://developers.google.com/search/docs/ajax-crawling/docs/getting-started?csw=1"}, {"text": "r2737", "href": "http://hg.scrapy.org/scrapy/changeset/2737"}, {"text": "r2779", "href": "http://hg.scrapy.org/scrapy/changeset/2779"}, {"text": "r2783", "href": "http://hg.scrapy.org/scrapy/changeset/2783"}, {"text": "chunked transfer encoding", "href": "https://en.wikipedia.org/wiki/Chunked_transfer_encoding"}, {"text": "r2769", "href": "http://hg.scrapy.org/scrapy/changeset/2769"}, {"text": "r2763", "href": "http://hg.scrapy.org/scrapy/changeset/2763"}, {"text": "marshal", "href": "https://docs.python.org/2/library/marshal.html"}, {"text": "r2744", "href": "http://hg.scrapy.org/scrapy/changeset/2744"}, {"text": "r2738", "href": "http://hg.scrapy.org/scrapy/changeset/2738"}, {"text": "r2732", "href": "http://hg.scrapy.org/scrapy/changeset/2732"}, {"text": "CONCURRENT_REQUESTS", "href": "topics/settings.html#std-setting-CONCURRENT_REQUESTS"}, {"text": "CONCURRENT_REQUESTS_PER_DOMAIN", "href": "topics/settings.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"}, {"text": "CONCURRENT_REQUESTS_PER_IP", "href": "topics/settings.html#std-setting-CONCURRENT_REQUESTS_PER_IP"}, {"text": "r2728", "href": "http://hg.scrapy.org/scrapy/changeset/2728"}, {"text": "https://github.com/scrapinghub/scaws", "href": "https://github.com/scrapinghub/scaws"}, {"text": "r2706", "href": "http://hg.scrapy.org/scrapy/changeset/2706"}, {"text": "r2714", "href": "http://hg.scrapy.org/scrapy/changeset/2714"}, {"text": "r2708", "href": "http://hg.scrapy.org/scrapy/changeset/2708"}, {"text": "r2781", "href": "http://hg.scrapy.org/scrapy/changeset/2781"}, {"text": "r2704", "href": "http://hg.scrapy.org/scrapy/changeset/2704"}, {"text": "REDIRECT_ENABLED", "href": "topics/downloader-middleware.html#std-setting-REDIRECT_ENABLED"}, {"text": "r2697", "href": "http://hg.scrapy.org/scrapy/changeset/2697"}, {"text": "RETRY_ENABLED", "href": "topics/downloader-middleware.html#std-setting-RETRY_ENABLED"}, {"text": "r2694", "href": "http://hg.scrapy.org/scrapy/changeset/2694"}, {"text": "r2691", "href": "http://hg.scrapy.org/scrapy/changeset/2691"}, {"text": "r2690", "href": "http://hg.scrapy.org/scrapy/changeset/2690"}, {"text": "r2688", "href": "http://hg.scrapy.org/scrapy/changeset/2688"}, {"text": "r2658", "href": "http://hg.scrapy.org/scrapy/changeset/2658"}, {"text": "r2657", "href": "http://hg.scrapy.org/scrapy/changeset/2657"}, {"text": "r2643", "href": "http://hg.scrapy.org/scrapy/changeset/2643"}, {"text": "r2639", "href": "http://hg.scrapy.org/scrapy/changeset/2639"}, {"text": "r2636", "href": "http://hg.scrapy.org/scrapy/changeset/2636"}, {"text": "r2653", "href": "http://hg.scrapy.org/scrapy/changeset/2653"}, {"text": "r2631", "href": "http://hg.scrapy.org/scrapy/changeset/2631"}, {"text": "spider_error", "href": "topics/signals.html#std-signal-spider_error"}, {"text": "r2628", "href": "http://hg.scrapy.org/scrapy/changeset/2628"}, {"text": "COOKIES_ENABLED", "href": "topics/downloader-middleware.html#std-setting-COOKIES_ENABLED"}, {"text": "r2625", "href": "http://hg.scrapy.org/scrapy/changeset/2625"}, {"text": "STATS_DUMP", "href": "topics/settings.html#std-setting-STATS_DUMP"}, {"text": "r2599", "href": "http://hg.scrapy.org/scrapy/changeset/2599"}, {"text": "r2576", "href": "http://hg.scrapy.org/scrapy/changeset/2576"}, {"text": "r2571", "href": "http://hg.scrapy.org/scrapy/changeset/2571"}, {"text": "r2578", "href": "http://hg.scrapy.org/scrapy/changeset/2578"}, {"text": "r2552", "href": "http://hg.scrapy.org/scrapy/changeset/2552"}, {"text": "r2579", "href": "http://hg.scrapy.org/scrapy/changeset/2579"}, {"text": "r2630", "href": "http://hg.scrapy.org/scrapy/changeset/2630"}, {"text": "w3lib", "href": "https://github.com/scrapy/w3lib"}, {"text": "r2584", "href": "http://hg.scrapy.org/scrapy/changeset/2584"}, {"text": "scrapely", "href": "https://github.com/scrapy/scrapely"}, {"text": "r2586", "href": "http://hg.scrapy.org/scrapy/changeset/2586"}, {"text": "r2577", "href": "http://hg.scrapy.org/scrapy/changeset/2577"}, {"text": "https://github.com/scrapy/dirbot", "href": "https://github.com/scrapy/dirbot"}, {"text": "r2616", "href": "http://hg.scrapy.org/scrapy/changeset/2616"}, {"text": "r2632", "href": "http://hg.scrapy.org/scrapy/changeset/2632"}, {"text": "r2640", "href": "http://hg.scrapy.org/scrapy/changeset/2640"}, {"text": "r2704", "href": "http://hg.scrapy.org/scrapy/changeset/2704"}, {"text": "r2704", "href": "http://hg.scrapy.org/scrapy/changeset/2704"}, {"text": "r2780", "href": "http://hg.scrapy.org/scrapy/changeset/2780"}, {"text": "r2789", "href": "http://hg.scrapy.org/scrapy/changeset/2789"}, {"text": "r2717", "href": "http://hg.scrapy.org/scrapy/changeset/2717"}, {"text": "r2718", "href": "http://hg.scrapy.org/scrapy/changeset/2718"}, {"text": "CLOSESPIDER_ITEMCOUNT", "href": "topics/extensions.html#std-setting-CLOSESPIDER_ITEMCOUNT"}, {"text": "r2655", "href": "http://hg.scrapy.org/scrapy/changeset/2655"}, {"text": "item_passed", "href": "topics/signals.html#std-signal-item_scraped"}, {"text": "CLOSESPIDER_PAGECOUNT", "href": "topics/extensions.html#std-setting-CLOSESPIDER_PAGECOUNT"}, {"text": "CLOSESPIDER_ERRORCOUNT", "href": "topics/extensions.html#std-setting-CLOSESPIDER_ERRORCOUNT"}, {"text": "http://localhost:6800", "href": "http://localhost:6800"}, {"text": "r2065", "href": "http://hg.scrapy.org/scrapy/changeset/2065"}, {"text": "r2039", "href": "http://hg.scrapy.org/scrapy/changeset/2039"}, {"text": "r2053", "href": "http://hg.scrapy.org/scrapy/changeset/2053"}, {"text": "r1988", "href": "http://hg.scrapy.org/scrapy/changeset/1988"}, {"text": "r2054", "href": "http://hg.scrapy.org/scrapy/changeset/2054"}, {"text": "r2055", "href": "http://hg.scrapy.org/scrapy/changeset/2055"}, {"text": "r2056", "href": "http://hg.scrapy.org/scrapy/changeset/2056"}, {"text": "r2057", "href": "http://hg.scrapy.org/scrapy/changeset/2057"}, {"text": "r2011", "href": "http://hg.scrapy.org/scrapy/changeset/2011"}, {"text": "r1961", "href": "http://hg.scrapy.org/scrapy/changeset/1961"}, {"text": "r1969", "href": "http://hg.scrapy.org/scrapy/changeset/1969"}, {"text": "r1956", "href": "http://hg.scrapy.org/scrapy/changeset/1956"}, {"text": "r1923", "href": "http://hg.scrapy.org/scrapy/changeset/1923"}, {"text": "r1955", "href": "http://hg.scrapy.org/scrapy/changeset/1955"}, {"text": "r1960", "href": "http://hg.scrapy.org/scrapy/changeset/1960"}, {"text": "r2022", "href": "http://hg.scrapy.org/scrapy/changeset/2022"}, {"text": "r2023", "href": "http://hg.scrapy.org/scrapy/changeset/2023"}, {"text": "r2024", "href": "http://hg.scrapy.org/scrapy/changeset/2024"}, {"text": "r2025", "href": "http://hg.scrapy.org/scrapy/changeset/2025"}, {"text": "r2026", "href": "http://hg.scrapy.org/scrapy/changeset/2026"}, {"text": "r2027", "href": "http://hg.scrapy.org/scrapy/changeset/2027"}, {"text": "r2028", "href": "http://hg.scrapy.org/scrapy/changeset/2028"}, {"text": "r2029", "href": "http://hg.scrapy.org/scrapy/changeset/2029"}, {"text": "r2030", "href": "http://hg.scrapy.org/scrapy/changeset/2030"}, {"text": "r2047", "href": "http://hg.scrapy.org/scrapy/changeset/2047"}, {"text": "r2050", "href": "http://hg.scrapy.org/scrapy/changeset/2050"}, {"text": "r1975", "href": "http://hg.scrapy.org/scrapy/changeset/1975"}, {"text": "r1961", "href": "http://hg.scrapy.org/scrapy/changeset/1961"}, {"text": "r2006", "href": "http://hg.scrapy.org/scrapy/changeset/2006"}, {"text": "r2035", "href": "http://hg.scrapy.org/scrapy/changeset/2035"}, {"text": "r2036", "href": "http://hg.scrapy.org/scrapy/changeset/2036"}, {"text": "r2037", "href": "http://hg.scrapy.org/scrapy/changeset/2037"}, {"text": "r2034", "href": "http://hg.scrapy.org/scrapy/changeset/2034"}, {"text": "r2039", "href": "http://hg.scrapy.org/scrapy/changeset/2039"}, {"text": "r2033", "href": "http://hg.scrapy.org/scrapy/changeset/2033"}, {"text": "r2047", "href": "http://hg.scrapy.org/scrapy/changeset/2047"}, {"text": "r1939", "href": "http://hg.scrapy.org/scrapy/changeset/1939"}, {"text": "r1809", "href": "http://hg.scrapy.org/scrapy/changeset/1809"}, {"text": "r1813", "href": "http://hg.scrapy.org/scrapy/changeset/1813"}, {"text": "r1816", "href": "http://hg.scrapy.org/scrapy/changeset/1816"}, {"text": "r1802", "href": "http://hg.scrapy.org/scrapy/changeset/1802"}, {"text": "r1803", "href": "http://hg.scrapy.org/scrapy/changeset/1803"}, {"text": "r1781", "href": "http://hg.scrapy.org/scrapy/changeset/1781"}, {"text": "r1785", "href": "http://hg.scrapy.org/scrapy/changeset/1785"}, {"text": "r1841", "href": "http://hg.scrapy.org/scrapy/changeset/1841"}, {"text": "r1804", "href": "http://hg.scrapy.org/scrapy/changeset/1804"}, {"text": "r1838", "href": "http://hg.scrapy.org/scrapy/changeset/1838"}, {"text": "r1836", "href": "http://hg.scrapy.org/scrapy/changeset/1836"}, {"text": "r1822", "href": "http://hg.scrapy.org/scrapy/changeset/1822"}, {"text": "r1822", "href": "http://hg.scrapy.org/scrapy/changeset/1822"}, {"text": "r1827", "href": "http://hg.scrapy.org/scrapy/changeset/1827"}, {"text": "r1849", "href": "http://hg.scrapy.org/scrapy/changeset/1849"}, {"text": "r1833", "href": "http://hg.scrapy.org/scrapy/changeset/1833"}, {"text": "r1840", "href": "http://hg.scrapy.org/scrapy/changeset/1840"}, {"text": "r1830", "href": "http://hg.scrapy.org/scrapy/changeset/1830"}, {"text": "r1844", "href": "http://hg.scrapy.org/scrapy/changeset/1844"}, {"text": "r1830", "href": "http://hg.scrapy.org/scrapy/changeset/1830"}, {"text": "r1843", "href": "http://hg.scrapy.org/scrapy/changeset/1843"}, {"text": "r1859", "href": "http://hg.scrapy.org/scrapy/changeset/1859"}, {"text": "r1861", "href": "http://hg.scrapy.org/scrapy/changeset/1861"}, {"text": "r1865", "href": "http://hg.scrapy.org/scrapy/changeset/1865"}], "timestamp": "2023-10-12T00:10:30.600302", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/api.html", "content": "                              Core API This section documents the Scrapy core API, and it’s intended for developers of extensions and middlewares.  Crawler API The main entry point to Scrapy API is the Crawler object, passed to extensions through the from_crawler class method. This object provides access to all Scrapy core components, and it’s the only way for extensions to access them and hook their functionality into Scrapy. The Extension Manager is responsible for loading and keeping track of installed extensions and it’s configured through the EXTENSIONS setting which contains a dictionary of all available extensions and their order similar to how you configure the downloader middlewares.   class scrapy.crawler.Crawler(spidercls, settings)[source] The Crawler object must be instantiated with a scrapy.Spider subclass and a scrapy.settings.Settings object.   request_fingerprinter The request fingerprint builder of this crawler. This is used from extensions and middlewares to build short, unique identifiers for requests. See Request fingerprints.     settings The settings manager of this crawler. This is used by extensions & middlewares to access the Scrapy settings of this crawler. For an introduction on Scrapy settings see Settings. For the API see Settings class.     signals The signals manager of this crawler. This is used by extensions & middlewares to hook themselves into Scrapy functionality. For an introduction on signals see Signals. For the API see SignalManager class.     stats The stats collector of this crawler. This is used from extensions & middlewares to record stats of their behaviour, or access stats collected by other extensions. For an introduction on stats collection see Stats Collection. For the API see StatsCollector class.     extensions The extension manager that keeps track of enabled extensions. Most extensions won’t need to access this attribute. For an introduction on extensions and a list of available extensions on Scrapy see Extensions.     engine The execution engine, which coordinates the core crawling logic between the scheduler, downloader and spiders. Some extension may want to access the Scrapy engine, to inspect  or modify the downloader and scheduler behaviour, although this is an advanced use and this API is not yet stable.     spider Spider currently being crawled. This is an instance of the spider class provided while constructing the crawler, and it is created after the arguments given in the crawl() method.     crawl(*args, **kwargs)[source] Starts the crawler by instantiating its spider class with the given args and kwargs arguments, while setting the execution engine in motion. Should be called only once. Returns a deferred that is fired when the crawl is finished.     stop() → Generator[Deferred, Any, None][source] Starts a graceful stop of the crawler and returns a deferred that is fired when the crawler is stopped.       class scrapy.crawler.CrawlerRunner(settings: Optional[Union[Dict[str, Any], Settings]] = None)[source] This is a convenient helper class that keeps track of, manages and runs crawlers inside an already setup reactor. The CrawlerRunner object must be instantiated with a Settings object. This class shouldn’t be needed (since Scrapy is responsible of using it accordingly) unless writing scripts that manually handle the crawling process. See Run Scrapy from a script for an example.   crawl(crawler_or_spidercls: Union[Type[Spider], str, Crawler], *args: Any, **kwargs: Any) → Deferred[source] Run a crawler with the provided arguments. It will call the given Crawler’s crawl() method, while keeping track of it so it can be stopped later. If crawler_or_spidercls isn’t a Crawler instance, this method will try to create one using this parameter as the spider class given to it. Returns a deferred that is fired when the crawling is finished.  Parameters  crawler_or_spidercls (Crawler instance, Spider subclass or string) – already created crawler, or a spider class or spider’s name inside the project to create it args – arguments to initialize the spider kwargs – keyword arguments to initialize the spider        property crawlers Set of crawlers started by crawl() and managed by this class.     create_crawler(crawler_or_spidercls: Union[Type[Spider], str, Crawler]) → Crawler[source] Return a Crawler object.  If crawler_or_spidercls is a Crawler, it is returned as-is. If crawler_or_spidercls is a Spider subclass, a new Crawler is constructed for it. If crawler_or_spidercls is a string, this function finds a spider with this name in a Scrapy project (using spider loader), then creates a Crawler instance for it.      join()[source] Returns a deferred that is fired when all managed crawlers have completed their executions.     stop() → Deferred[source] Stops simultaneously all the crawling jobs taking place. Returns a deferred that is fired when they all have ended.       class scrapy.crawler.CrawlerProcess(settings: Optional[Union[Dict[str, Any], Settings]] = None, install_root_handler: bool = True)[source] Bases: CrawlerRunner A class to run multiple scrapy crawlers in a process simultaneously. This class extends CrawlerRunner by adding support for starting a reactor and handling shutdown signals, like the keyboard interrupt command Ctrl-C. It also configures top-level logging. This utility should be a better fit than CrawlerRunner if you aren’t running another reactor within your application. The CrawlerProcess object must be instantiated with a Settings object.  Parameters install_root_handler – whether to install root logging handler (default: True)   This class shouldn’t be needed (since Scrapy is responsible of using it accordingly) unless writing scripts that manually handle the crawling process. See Run Scrapy from a script for an example.   crawl(crawler_or_spidercls: Union[Type[Spider], str, Crawler], *args: Any, **kwargs: Any) → Deferred Run a crawler with the provided arguments. It will call the given Crawler’s crawl() method, while keeping track of it so it can be stopped later. If crawler_or_spidercls isn’t a Crawler instance, this method will try to create one using this parameter as the spider class given to it. Returns a deferred that is fired when the crawling is finished.  Parameters  crawler_or_spidercls (Crawler instance, Spider subclass or string) – already created crawler, or a spider class or spider’s name inside the project to create it args – arguments to initialize the spider kwargs – keyword arguments to initialize the spider        property crawlers Set of crawlers started by crawl() and managed by this class.     create_crawler(crawler_or_spidercls: Union[Type[Spider], str, Crawler]) → Crawler Return a Crawler object.  If crawler_or_spidercls is a Crawler, it is returned as-is. If crawler_or_spidercls is a Spider subclass, a new Crawler is constructed for it. If crawler_or_spidercls is a string, this function finds a spider with this name in a Scrapy project (using spider loader), then creates a Crawler instance for it.      join() Returns a deferred that is fired when all managed crawlers have completed their executions.     start(stop_after_crawl: bool = True, install_signal_handlers: bool = True) → None[source] This method starts a reactor, adjusts its pool size to REACTOR_THREADPOOL_MAXSIZE, and installs a DNS cache based on DNSCACHE_ENABLED and DNSCACHE_SIZE. If stop_after_crawl is True, the reactor will be stopped after all crawlers have finished, using join().  Parameters  stop_after_crawl (bool) – stop or not the reactor when all crawlers have finished install_signal_handlers (bool) – whether to install the shutdown handlers (default: True)        stop() → Deferred Stops simultaneously all the crawling jobs taking place. Returns a deferred that is fired when they all have ended.       Settings API   scrapy.settings.SETTINGS_PRIORITIES Dictionary that sets the key name and priority level of the default settings priorities used in Scrapy. Each item defines a settings entry point, giving it a code name for identification and an integer priority. Greater priorities take more precedence over lesser ones when setting and retrieving values in the Settings class. SETTINGS_PRIORITIES = {     \"default\": 0,     \"command\": 10,     \"addon\": 15,     \"project\": 20,     \"spider\": 30,     \"cmdline\": 40, }   For a detailed explanation on each settings sources, see: Settings.     scrapy.settings.get_settings_priority(priority: Union[int, str]) → int[source] Small helper function that looks up a given string priority in the SETTINGS_PRIORITIES dictionary and returns its numerical value, or directly returns a given numerical priority.     class scrapy.settings.Settings(values: _SettingsInputT = None, priority: Union[int, str] = 'project')[source] Bases: BaseSettings This object stores Scrapy settings for the configuration of internal components, and can be used for any further customization. It is a direct subclass and supports all methods of BaseSettings. Additionally, after instantiation of this class, the new object will have the global default settings described on Built-in settings reference already populated.     class scrapy.settings.BaseSettings(values: _SettingsInputT = None, priority: Union[int, str] = 'project')[source] Instances of this class behave like dictionaries, but store priorities along with their (key, value) pairs, and can be frozen (i.e. marked immutable). Key-value entries can be passed on initialization with the values argument, and they would take the priority level (unless values is already an instance of BaseSettings, in which case the existing priority levels will be kept).  If the priority argument is a string, the priority name will be looked up in SETTINGS_PRIORITIES. Otherwise, a specific integer should be provided. Once the object is created, new settings can be loaded or updated with the set() method, and can be accessed with the square bracket notation of dictionaries, or with the get() method of the instance and its value conversion variants. When requesting a stored key, the value with the highest priority will be retrieved.   copy() → Self[source] Make a deep copy of current settings. This method returns a new instance of the Settings class, populated with the same values and their priorities. Modifications to the new object won’t be reflected on the original settings.     copy_to_dict() → Dict[Optional[Union[bool, float, int, str]], Any][source] Make a copy of current settings and convert to a dict. This method returns a new dict populated with the same values and their priorities as the current settings. Modifications to the returned dict won’t be reflected on the original settings. This method can be useful for example for printing settings in Scrapy shell.     freeze() → None[source] Disable further changes to the current settings. After calling this method, the present state of the settings will become immutable. Trying to change values through the set() method and its variants won’t be possible and will be alerted.     frozencopy() → Self[source] Return an immutable copy of the current settings. Alias for a freeze() call in the object returned by copy().     get(name: Optional[Union[bool, float, int, str]], default: Any = None) → Any[source] Get a setting value without affecting its original type.  Parameters  name (str) – the setting name default (object) – the value to return if no setting is found        getbool(name: Optional[Union[bool, float, int, str]], default: bool = False) → bool[source] Get a setting value as a boolean. 1, '1', True` and 'True' return True, while 0, '0', False, 'False' and None return False. For example, settings populated through environment variables set to '0' will return False when using this method.  Parameters  name (str) – the setting name default (object) – the value to return if no setting is found        getdict(name: Optional[Union[bool, float, int, str]], default: Optional[Dict[Any, Any]] = None) → Dict[Any, Any][source] Get a setting value as a dictionary. If the setting original type is a dictionary, a copy of it will be returned. If it is a string it will be evaluated as a JSON dictionary. In the case that it is a BaseSettings instance itself, it will be converted to a dictionary, containing all its current settings values as they would be returned by get(), and losing all information about priority and mutability.  Parameters  name (str) – the setting name default (object) – the value to return if no setting is found        getdictorlist(name: Optional[Union[bool, float, int, str]], default: Optional[Union[Dict[Any, Any], List[Any], Tuple[Any]]] = None) → Union[Dict[Any, Any], List[Any]][source] Get a setting value as either a dict or a list. If the setting is already a dict or a list, a copy of it will be returned. If it is a string it will be evaluated as JSON, or as a comma-separated list of strings as a fallback. For example, settings populated from the command line will return:  {'key1': 'value1', 'key2': 'value2'} if set to '{\"key1\": \"value1\", \"key2\": \"value2\"}' ['one', 'two'] if set to '[\"one\", \"two\"]' or 'one,two'   Parameters  name (string) – the setting name default (any) – the value to return if no setting is found        getfloat(name: Optional[Union[bool, float, int, str]], default: float = 0.0) → float[source] Get a setting value as a float.  Parameters  name (str) – the setting name default (object) – the value to return if no setting is found        getint(name: Optional[Union[bool, float, int, str]], default: int = 0) → int[source] Get a setting value as an int.  Parameters  name (str) – the setting name default (object) – the value to return if no setting is found        getlist(name: Optional[Union[bool, float, int, str]], default: Optional[List[Any]] = None) → List[Any][source] Get a setting value as a list. If the setting original type is a list, a copy of it will be returned. If it’s a string it will be split by “,”. For example, settings populated through environment variables set to 'one,two' will return a list [‘one’, ‘two’] when using this method.  Parameters  name (str) – the setting name default (object) – the value to return if no setting is found        getpriority(name: Optional[Union[bool, float, int, str]]) → Optional[int][source] Return the current numerical priority value of a setting, or None if the given name does not exist.  Parameters name (str) – the setting name       getwithbase(name: Optional[Union[bool, float, int, str]]) → BaseSettings[source] Get a composition of a dictionary-like setting and its _BASE counterpart.  Parameters name (str) – name of the dictionary-like setting       maxpriority() → int[source] Return the numerical value of the highest priority present throughout all settings, or the numerical value for default from SETTINGS_PRIORITIES if there are no settings stored.     pop(k[, d]) → v, remove specified key and return the corresponding value.[source] If key is not found, d is returned if given, otherwise KeyError is raised.     set(name: Optional[Union[bool, float, int, str]], value: Any, priority: Union[int, str] = 'project') → None[source] Store a key/value attribute with a given priority. Settings should be populated before configuring the Crawler object (through the configure() method), otherwise they won’t have any effect.  Parameters  name (str) – the setting name value (object) – the value to associate with the setting priority (str or int) – the priority of the setting. Should be a key of SETTINGS_PRIORITIES or an integer        setdefault(k[, d]) → D.get(k,d), also set D[k]=d if k not in D[source]     setmodule(module: Union[module, str], priority: Union[int, str] = 'project') → None[source] Store settings from a module with a given priority. This is a helper function that calls set() for every globally declared uppercase variable of module with the provided priority.  Parameters  module (types.ModuleType or str) – the module or the path of the module priority (str or int) – the priority of the settings. Should be a key of SETTINGS_PRIORITIES or an integer        update(values: _SettingsInputT, priority: Union[int, str] = 'project') → None[source] Store key/value pairs with a given priority. This is a helper function that calls set() for every item of values with the provided priority. If values is a string, it is assumed to be JSON-encoded and parsed into a dict with json.loads() first. If it is a BaseSettings instance, the per-key priorities will be used and the priority parameter ignored. This allows inserting/updating settings with different priorities with a single command.  Parameters  values (dict or string or BaseSettings) – the settings names and values priority (str or int) – the priority of the settings. Should be a key of SETTINGS_PRIORITIES or an integer          SpiderLoader API   class scrapy.spiderloader.SpiderLoader[source] This class is in charge of retrieving and handling the spider classes defined across the project. Custom spider loaders can be employed by specifying their path in the SPIDER_LOADER_CLASS project setting. They must fully implement the scrapy.interfaces.ISpiderLoader interface to guarantee an errorless execution.   from_settings(settings)[source] This class method is used by Scrapy to create an instance of the class. It’s called with the current project settings, and it loads the spiders found recursively in the modules of the SPIDER_MODULES setting.  Parameters settings (Settings instance) – project settings       load(spider_name)[source] Get the Spider class with the given name. It’ll look into the previously loaded spiders for a spider class with name spider_name and will raise a KeyError if not found.  Parameters spider_name (str) – spider class name       list()[source] Get the names of the available spiders in the project.     find_by_request(request)[source] List the spiders’ names that can handle the given request. Will try to match the request’s url against the domains of the spiders.  Parameters request (Request instance) – queried request         Signals API   class scrapy.signalmanager.SignalManager(sender: Any = _Anonymous)[source]   connect(receiver: Any, signal: Any, **kwargs: Any) → None[source] Connect a receiver function to a signal. The signal can be any object, although Scrapy comes with some predefined signals that are documented in the Signals section.  Parameters  receiver (collections.abc.Callable) – the function to be connected signal (object) – the signal to connect to        disconnect(receiver: Any, signal: Any, **kwargs: Any) → None[source] Disconnect a receiver function from a signal. This has the opposite effect of the connect() method, and the arguments are the same.     disconnect_all(signal: Any, **kwargs: Any) → None[source] Disconnect all receivers from the given signal.  Parameters signal (object) – the signal to disconnect from       send_catch_log(signal: Any, **kwargs: Any) → List[Tuple[Any, Any]][source] Send a signal, catch exceptions and log them. The keyword arguments are passed to the signal handlers (connected through the connect() method).     send_catch_log_deferred(signal: Any, **kwargs: Any) → Deferred[source] Like send_catch_log() but supports returning Deferred objects from signal handlers. Returns a Deferred that gets fired once all signal handlers deferreds were fired. Send a signal, catch exceptions and log them. The keyword arguments are passed to the signal handlers (connected through the connect() method).       Stats Collector API There are several Stats Collectors available under the scrapy.statscollectors module and they all implement the Stats Collector API defined by the StatsCollector class (which they all inherit from).   class scrapy.statscollectors.StatsCollector[source]   get_value(key, default=None)[source] Return the value for the given stats key or default if it doesn’t exist.     get_stats()[source] Get all stats from the currently running spider as a dict.     set_value(key, value)[source] Set the given value for the given stats key.     set_stats(stats)[source] Override the current stats with the dict passed in stats argument.     inc_value(key, count=1, start=0)[source] Increment the value of the given stats key, by the given count, assuming the start value given (when it’s not set).     max_value(key, value)[source] Set the given value for the given key only if current value for the same key is lower than value. If there is no current value for the given key, the value is always set.     min_value(key, value)[source] Set the given value for the given key only if current value for the same key is greater than value. If there is no current value for the given key, the value is always set.     clear_stats()[source] Clear all stats.   The following methods are not part of the stats collection api but instead used when implementing custom stats collectors:   open_spider(spider)[source] Open the given spider for stats collection.     close_spider(spider)[source] Close the given spider. After this is called, no more specific stats can be accessed or collected.                               ", "code_blocks": ["SETTINGS_PRIORITIES = {\n    \"default\": 0,\n    \"command\": 10,\n    \"addon\": 15,\n    \"project\": 20,\n    \"spider\": 30,\n    \"cmdline\": 40,\n}\n</pre>"], "links": [{"text": "EXTENSIONS", "href": "settings.html#std-setting-EXTENSIONS"}, {"text": "configure the downloader middlewares", "href": "downloader-middleware.html#topics-downloader-middleware-setting"}, {"text": "[source]", "href": "../_modules/scrapy/crawler.html#Crawler"}, {"text": "scrapy.Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Request fingerprints", "href": "request-response.html#request-fingerprints"}, {"text": "Settings", "href": "settings.html#topics-settings"}, {"text": "Signals", "href": "signals.html#topics-signals"}, {"text": "Stats Collection", "href": "stats.html#topics-stats"}, {"text": "Extensions", "href": "extensions.html#topics-extensions"}, {"text": "[source]", "href": "../_modules/scrapy/crawler.html#Crawler.crawl"}, {"text": "Generator", "href": "https://docs.python.org/3/library/typing.html#typing.Generator"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "None", "href": "https://docs.python.org/3/library/constants.html#None"}, {"text": "[source]", "href": "../_modules/scrapy/crawler.html#Crawler.stop"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "Dict", "href": "https://docs.python.org/3/library/typing.html#typing.Dict"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "[source]", "href": "../_modules/scrapy/crawler.html#CrawlerRunner"}, {"text": "reactor", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html"}, {"text": "Run Scrapy from a script", "href": "practices.html#run-from-script"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "Type", "href": "https://docs.python.org/3/library/typing.html#typing.Type"}, {"text": "Spider", "href": "spiders.html#scrapy.spiders.Spider"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "[source]", "href": "../_modules/scrapy/crawler.html#CrawlerRunner.crawl"}, {"text": "Spider", "href": "spiders.html#scrapy.spiders.Spider"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "Type", "href": "https://docs.python.org/3/library/typing.html#typing.Type"}, {"text": "Spider", "href": "spiders.html#scrapy.spiders.Spider"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "[source]", "href": "../_modules/scrapy/crawler.html#CrawlerRunner.create_crawler"}, {"text": "[source]", "href": "../_modules/scrapy/crawler.html#CrawlerRunner.join"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "[source]", "href": "../_modules/scrapy/crawler.html#CrawlerRunner.stop"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "Dict", "href": "https://docs.python.org/3/library/typing.html#typing.Dict"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "[source]", "href": "../_modules/scrapy/crawler.html#CrawlerProcess"}, {"text": "reactor", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html"}, {"text": "reactor", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html"}, {"text": "Run Scrapy from a script", "href": "practices.html#run-from-script"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "Type", "href": "https://docs.python.org/3/library/typing.html#typing.Type"}, {"text": "Spider", "href": "spiders.html#scrapy.spiders.Spider"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "Spider", "href": "spiders.html#scrapy.spiders.Spider"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "Type", "href": "https://docs.python.org/3/library/typing.html#typing.Type"}, {"text": "Spider", "href": "spiders.html#scrapy.spiders.Spider"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "None", "href": "https://docs.python.org/3/library/constants.html#None"}, {"text": "[source]", "href": "../_modules/scrapy/crawler.html#CrawlerProcess.start"}, {"text": "reactor", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html"}, {"text": "REACTOR_THREADPOOL_MAXSIZE", "href": "settings.html#std-setting-REACTOR_THREADPOOL_MAXSIZE"}, {"text": "DNSCACHE_ENABLED", "href": "settings.html#std-setting-DNSCACHE_ENABLED"}, {"text": "DNSCACHE_SIZE", "href": "settings.html#std-setting-DNSCACHE_SIZE"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "Settings", "href": "settings.html#topics-settings"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#get_settings_priority"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#Settings"}, {"text": "Built-in settings reference", "href": "settings.html#topics-settings-ref"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.copy"}, {"text": "Dict", "href": "https://docs.python.org/3/library/typing.html#typing.Dict"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.copy_to_dict"}, {"text": "None", "href": "https://docs.python.org/3/library/constants.html#None"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.freeze"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.frozencopy"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.get"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "object", "href": "https://docs.python.org/3/library/functions.html#object"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.getbool"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "object", "href": "https://docs.python.org/3/library/functions.html#object"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Dict", "href": "https://docs.python.org/3/library/typing.html#typing.Dict"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Dict", "href": "https://docs.python.org/3/library/typing.html#typing.Dict"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.getdict"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "object", "href": "https://docs.python.org/3/library/functions.html#object"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "Dict", "href": "https://docs.python.org/3/library/typing.html#typing.Dict"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "List", "href": "https://docs.python.org/3/library/typing.html#typing.List"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Tuple", "href": "https://docs.python.org/3/library/typing.html#typing.Tuple"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "Dict", "href": "https://docs.python.org/3/library/typing.html#typing.Dict"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "List", "href": "https://docs.python.org/3/library/typing.html#typing.List"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.getdictorlist"}, {"text": "dict", "href": "https://docs.python.org/3/library/stdtypes.html#dict"}, {"text": "list", "href": "https://docs.python.org/3/library/stdtypes.html#list"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.getfloat"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "object", "href": "https://docs.python.org/3/library/functions.html#object"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.getint"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "object", "href": "https://docs.python.org/3/library/functions.html#object"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "List", "href": "https://docs.python.org/3/library/typing.html#typing.List"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "List", "href": "https://docs.python.org/3/library/typing.html#typing.List"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.getlist"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "object", "href": "https://docs.python.org/3/library/functions.html#object"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.getpriority"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.getwithbase"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.maxpriority"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.pop"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "None", "href": "https://docs.python.org/3/library/constants.html#None"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.set"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "object", "href": "https://docs.python.org/3/library/functions.html#object"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.setdefault"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "None", "href": "https://docs.python.org/3/library/constants.html#None"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.setmodule"}, {"text": "types.ModuleType", "href": "https://docs.python.org/3/library/types.html#types.ModuleType"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "None", "href": "https://docs.python.org/3/library/constants.html#None"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.update"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "[source]", "href": "../_modules/scrapy/spiderloader.html#SpiderLoader"}, {"text": "SPIDER_LOADER_CLASS", "href": "settings.html#std-setting-SPIDER_LOADER_CLASS"}, {"text": "[source]", "href": "../_modules/scrapy/spiderloader.html#SpiderLoader.from_settings"}, {"text": "SPIDER_MODULES", "href": "settings.html#std-setting-SPIDER_MODULES"}, {"text": "[source]", "href": "../_modules/scrapy/spiderloader.html#SpiderLoader.load"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "[source]", "href": "../_modules/scrapy/spiderloader.html#SpiderLoader.list"}, {"text": "[source]", "href": "../_modules/scrapy/spiderloader.html#SpiderLoader.find_by_request"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "[source]", "href": "../_modules/scrapy/signalmanager.html#SignalManager"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "None", "href": "https://docs.python.org/3/library/constants.html#None"}, {"text": "[source]", "href": "../_modules/scrapy/signalmanager.html#SignalManager.connect"}, {"text": "Signals", "href": "signals.html#topics-signals"}, {"text": "collections.abc.Callable", "href": "https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable"}, {"text": "object", "href": "https://docs.python.org/3/library/functions.html#object"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "None", "href": "https://docs.python.org/3/library/constants.html#None"}, {"text": "[source]", "href": "../_modules/scrapy/signalmanager.html#SignalManager.disconnect"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "None", "href": "https://docs.python.org/3/library/constants.html#None"}, {"text": "[source]", "href": "../_modules/scrapy/signalmanager.html#SignalManager.disconnect_all"}, {"text": "object", "href": "https://docs.python.org/3/library/functions.html#object"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "List", "href": "https://docs.python.org/3/library/typing.html#typing.List"}, {"text": "Tuple", "href": "https://docs.python.org/3/library/typing.html#typing.Tuple"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "[source]", "href": "../_modules/scrapy/signalmanager.html#SignalManager.send_catch_log"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "[source]", "href": "../_modules/scrapy/signalmanager.html#SignalManager.send_catch_log_deferred"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "[source]", "href": "../_modules/scrapy/statscollectors.html#StatsCollector"}, {"text": "[source]", "href": "../_modules/scrapy/statscollectors.html#StatsCollector.get_value"}, {"text": "[source]", "href": "../_modules/scrapy/statscollectors.html#StatsCollector.get_stats"}, {"text": "[source]", "href": "../_modules/scrapy/statscollectors.html#StatsCollector.set_value"}, {"text": "[source]", "href": "../_modules/scrapy/statscollectors.html#StatsCollector.set_stats"}, {"text": "[source]", "href": "../_modules/scrapy/statscollectors.html#StatsCollector.inc_value"}, {"text": "[source]", "href": "../_modules/scrapy/statscollectors.html#StatsCollector.max_value"}, {"text": "[source]", "href": "../_modules/scrapy/statscollectors.html#StatsCollector.min_value"}, {"text": "[source]", "href": "../_modules/scrapy/statscollectors.html#StatsCollector.clear_stats"}, {"text": "[source]", "href": "../_modules/scrapy/statscollectors.html#StatsCollector.open_spider"}, {"text": "[source]", "href": "../_modules/scrapy/statscollectors.html#StatsCollector.close_spider"}], "timestamp": "2023-10-12T00:10:34.265775", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/components.html", "content": "                              Components A Scrapy component is any class whose objects are created using scrapy.utils.misc.create_instance(). That includes the classes that you may assign to the following settings:  DNS_RESOLVER DOWNLOAD_HANDLERS DOWNLOADER_CLIENTCONTEXTFACTORY DOWNLOADER_MIDDLEWARES DUPEFILTER_CLASS EXTENSIONS FEED_EXPORTERS FEED_STORAGES ITEM_PIPELINES SCHEDULER SCHEDULER_DISK_QUEUE SCHEDULER_MEMORY_QUEUE SCHEDULER_PRIORITY_QUEUE SPIDER_MIDDLEWARES  Third-party Scrapy components may also let you define additional Scrapy components, usually configurable through settings, to modify their behavior.  Enforcing component requirements Sometimes, your components may only be intended to work under certain conditions. For example, they may require a minimum version of Scrapy to work as intended, or they may require certain settings to have specific values. In addition to describing those conditions in the documentation of your component, it is a good practice to raise an exception from the __init__ method of your component if those conditions are not met at run time. In the case of downloader middlewares, extensions, item pipelines, and spider middlewares, you should raise scrapy.exceptions.NotConfigured, passing a description of the issue as a parameter to the exception so that it is printed in the logs, for the user to see. For other components, feel free to raise whatever other exception feels right to you; for example, RuntimeError would make sense for a Scrapy version mismatch, while ValueError may be better if the issue is the value of a setting. If your requirement is a minimum Scrapy version, you may use scrapy.__version__ to enforce your requirement. For example: from packaging.version import parse as parse_version  import scrapy   class MyComponent:     def __init__(self):         if parse_version(scrapy.__version__) < parse_version(\"2.7\"):             raise RuntimeError(                 f\"{MyComponent.__qualname__} requires Scrapy 2.7 or \"                 f\"later, which allow defining the process_spider_output \"                 f\"method of spider middlewares as an asynchronous \"                 f\"generator.\"             )                             ", "code_blocks": ["from packaging.version import parse as parse_version\n\nimport scrapy\n\n\nclass MyComponent:\n    def __init__(self):\n        if parse_version(scrapy.__version__) < parse_version(\"2.7\"):\n            raise RuntimeError(\n                f\"{MyComponent.__qualname__} requires Scrapy 2.7 or \"\n                f\"later, which allow defining the process_spider_output \"\n                f\"method of spider middlewares as an asynchronous \"\n                f\"generator.\"\n            )\n</pre>"], "links": [{"text": "DNS_RESOLVER", "href": "settings.html#std-setting-DNS_RESOLVER"}, {"text": "DOWNLOAD_HANDLERS", "href": "settings.html#std-setting-DOWNLOAD_HANDLERS"}, {"text": "DOWNLOADER_CLIENTCONTEXTFACTORY", "href": "settings.html#std-setting-DOWNLOADER_CLIENTCONTEXTFACTORY"}, {"text": "DOWNLOADER_MIDDLEWARES", "href": "settings.html#std-setting-DOWNLOADER_MIDDLEWARES"}, {"text": "DUPEFILTER_CLASS", "href": "settings.html#std-setting-DUPEFILTER_CLASS"}, {"text": "EXTENSIONS", "href": "settings.html#std-setting-EXTENSIONS"}, {"text": "FEED_EXPORTERS", "href": "feed-exports.html#std-setting-FEED_EXPORTERS"}, {"text": "FEED_STORAGES", "href": "feed-exports.html#std-setting-FEED_STORAGES"}, {"text": "ITEM_PIPELINES", "href": "settings.html#std-setting-ITEM_PIPELINES"}, {"text": "SCHEDULER", "href": "settings.html#std-setting-SCHEDULER"}, {"text": "SCHEDULER_DISK_QUEUE", "href": "settings.html#std-setting-SCHEDULER_DISK_QUEUE"}, {"text": "SCHEDULER_MEMORY_QUEUE", "href": "settings.html#std-setting-SCHEDULER_MEMORY_QUEUE"}, {"text": "SCHEDULER_PRIORITY_QUEUE", "href": "settings.html#std-setting-SCHEDULER_PRIORITY_QUEUE"}, {"text": "SPIDER_MIDDLEWARES", "href": "settings.html#std-setting-SPIDER_MIDDLEWARES"}, {"text": "settings", "href": "settings.html#topics-settings"}, {"text": "downloader middlewares", "href": "downloader-middleware.html#topics-downloader-middleware"}, {"text": "extensions", "href": "extensions.html#topics-extensions"}, {"text": "item pipelines", "href": "item-pipeline.html#topics-item-pipeline"}, {"text": "spider middlewares", "href": "spider-middleware.html#topics-spider-middleware"}, {"text": "scrapy.exceptions.NotConfigured", "href": "exceptions.html#scrapy.exceptions.NotConfigured"}, {"text": "RuntimeError", "href": "https://docs.python.org/3/library/exceptions.html#RuntimeError"}, {"text": "ValueError", "href": "https://docs.python.org/3/library/exceptions.html#ValueError"}], "timestamp": "2023-10-12T00:10:37.162207", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/exporters.html", "content": "                              Item Exporters Once you have scraped your items, you often want to persist or export those items, to use the data in some other application. That is, after all, the whole purpose of the scraping process. For this purpose Scrapy provides a collection of Item Exporters for different output formats, such as XML, CSV or JSON.  Using Item Exporters If you are in a hurry, and just want to use an Item Exporter to output scraped data see the Feed exports. Otherwise, if you want to know how Item Exporters work or need more custom functionality (not covered by the default exports), continue reading below. In order to use an Item Exporter, you  must instantiate it with its required args. Each Item Exporter requires different arguments, so check each exporter documentation to be sure, in Built-in Item Exporters reference. After you have instantiated your exporter, you have to: 1. call the method start_exporting() in order to signal the beginning of the exporting process 2. call the export_item() method for each item you want to export 3. and finally call the finish_exporting() to signal the end of the exporting process Here you can see an Item Pipeline which uses multiple Item Exporters to group scraped items to different files according to the value of one of their fields: from itemadapter import ItemAdapter from scrapy.exporters import XmlItemExporter   class PerYearXmlExportPipeline:     \"\"\"Distribute items across multiple XML files according to their 'year' field\"\"\"      def open_spider(self, spider):         self.year_to_exporter = {}      def close_spider(self, spider):         for exporter, xml_file in self.year_to_exporter.values():             exporter.finish_exporting()             xml_file.close()      def _exporter_for_item(self, item):         adapter = ItemAdapter(item)         year = adapter[\"year\"]         if year not in self.year_to_exporter:             xml_file = open(f\"{year}.xml\", \"wb\")             exporter = XmlItemExporter(xml_file)             exporter.start_exporting()             self.year_to_exporter[year] = (exporter, xml_file)         return self.year_to_exporter[year][0]      def process_item(self, item, spider):         exporter = self._exporter_for_item(item)         exporter.export_item(item)         return item     Serialization of item fields By default, the field values are passed unmodified to the underlying serialization library, and the decision of how to serialize them is delegated to each particular serialization library. However, you can customize how each field value is serialized before it is passed to the serialization library. There are two ways to customize how a field will be serialized, which are described next.  1. Declaring a serializer in the field If you use Item you can declare a serializer in the field metadata. The serializer must be a callable which receives a value and returns its serialized form. Example: import scrapy   def serialize_price(value):     return f\"$ {str(value)}\"   class Product(scrapy.Item):     name = scrapy.Field()     price = scrapy.Field(serializer=serialize_price)     2. Overriding the serialize_field() method You can also override the serialize_field() method to customize how your field value will be exported. Make sure you call the base class serialize_field() method after your custom code. Example: from scrapy.exporters import XmlItemExporter   class ProductXmlExporter(XmlItemExporter):     def serialize_field(self, field, name, value):         if name == \"price\":             return f\"$ {str(value)}\"         return super().serialize_field(field, name, value)      Built-in Item Exporters reference Here is a list of the Item Exporters bundled with Scrapy. Some of them contain output examples, which assume you’re exporting these two items: Item(name=\"Color TV\", price=\"1200\") Item(name=\"DVD player\", price=\"200\")    BaseItemExporter   class scrapy.exporters.BaseItemExporter(fields_to_export=None, export_empty_fields=False, encoding='utf-8', indent=0, dont_fail=False)[source] This is the (abstract) base class for all Item Exporters. It provides support for common features used by all (concrete) Item Exporters, such as defining what fields to export, whether to export empty fields, or which encoding to use. These features can be configured through the __init__ method arguments which populate their respective instance attributes: fields_to_export, export_empty_fields, encoding, indent.  New in version 2.0: The dont_fail parameter.    export_item(item)[source] Exports the given item. This method must be implemented in subclasses.     serialize_field(field, name, value)[source] Return the serialized value for the given field. You can override this method (in your custom Item Exporters) if you want to control how a particular field or value will be serialized/exported. By default, this method looks for a serializer declared in the item field and returns the result of applying that serializer to the value. If no serializer is found, it returns the value unchanged.  Parameters  field (Field object or a dict instance) – the field being serialized. If the source item object does not define field metadata, field is an empty dict. name (str) – the name of the field being serialized value – the value being serialized        start_exporting()[source] Signal the beginning of the exporting process. Some exporters may use this to generate some required header (for example, the XmlItemExporter). You must call this method before exporting any items.     finish_exporting()[source] Signal the end of the exporting process. Some exporters may use this to generate some required footer (for example, the XmlItemExporter). You must always call this method after you have no more items to export.     fields_to_export Fields to export, their order 1 and their output names. Possible values are:  None (all fields 2, default) A list of fields: ['field1', 'field2']    A dict where keys are fields and values are output names: {'field1': 'Field 1', 'field2': 'Field 2'}      1 Not all exporters respect the specified field order.  2 When using item objects that do not expose all their possible fields, exporters that do not support exporting a different subset of fields per item will only export the fields found in the first item exported.       export_empty_fields Whether to include empty/unpopulated item fields in the exported data. Defaults to False. Some exporters (like CsvItemExporter) ignore this attribute and always export all empty fields. This option is ignored for dict items.     encoding The output character encoding.     indent Amount of spaces used to indent the output on each level. Defaults to 0.  indent=None selects the most compact representation, all items in the same line with no indentation indent<=0 each item on its own line, no indentation indent>0 each item on its own line, indented with the provided numeric value        PythonItemExporter   class scrapy.exporters.PythonItemExporter(*, dont_fail=False, **kwargs)[source] This is a base class for item exporters that extends BaseItemExporter with support for nested items. It serializes items to built-in Python types, so that any serialization library (e.g. json or msgpack) can be used on top of it.     XmlItemExporter   class scrapy.exporters.XmlItemExporter(file, item_element='item', root_element='items', **kwargs)[source] Exports items in XML format to the specified file object.  Parameters  file – the file-like object to use for exporting the data. Its write method should accept bytes (a disk file opened in binary mode, a io.BytesIO object, etc) root_element (str) – The name of root element in the exported XML. item_element (str) – The name of each item element in the exported XML.    The additional keyword arguments of this __init__ method are passed to the BaseItemExporter __init__ method. A typical output of this exporter would be: <?xml version=\"1.0\" encoding=\"utf-8\"?> <items>   <item>     <name>Color TV</name>     <price>1200</price>  </item>   <item>     <name>DVD player</name>     <price>200</price>  </item> </items>   Unless overridden in the serialize_field() method, multi-valued fields are exported by serializing each value inside a <value> element. This is for convenience, as multi-valued fields are very common. For example, the item: Item(name=['John', 'Doe'], age='23')   Would be serialized as: <?xml version=\"1.0\" encoding=\"utf-8\"?> <items>   <item>     <name>       <value>John</value>       <value>Doe</value>     </name>     <age>23</age>   </item> </items>       CsvItemExporter   class scrapy.exporters.CsvItemExporter(file, include_headers_line=True, join_multivalued=',', errors=None, **kwargs)[source] Exports items in CSV format to the given file-like object. If the fields_to_export attribute is set, it will be used to define the CSV columns, their order and their column names. The export_empty_fields attribute has no effect on this exporter.  Parameters  file – the file-like object to use for exporting the data. Its write method should accept bytes (a disk file opened in binary mode, a io.BytesIO object, etc) include_headers_line (str) – If enabled, makes the exporter output a header line with the field names taken from BaseItemExporter.fields_to_export or the first exported item fields. join_multivalued – The char (or chars) that will be used for joining multi-valued fields, if found. errors (str) – The optional string that specifies how encoding and decoding errors are to be handled. For more information see io.TextIOWrapper.    The additional keyword arguments of this __init__ method are passed to the BaseItemExporter __init__ method, and the leftover arguments to the csv.writer() function, so you can use any csv.writer() function argument to customize this exporter. A typical output of this exporter would be: product,price Color TV,1200 DVD player,200       PickleItemExporter   class scrapy.exporters.PickleItemExporter(file, protocol=0, **kwargs)[source] Exports items in pickle format to the given file-like object.  Parameters  file – the file-like object to use for exporting the data. Its write method should accept bytes (a disk file opened in binary mode, a io.BytesIO object, etc) protocol (int) – The pickle protocol to use.    For more information, see pickle. The additional keyword arguments of this __init__ method are passed to the BaseItemExporter __init__ method. Pickle isn’t a human readable format, so no output examples are provided.     PprintItemExporter   class scrapy.exporters.PprintItemExporter(file, **kwargs)[source] Exports items in pretty print format to the specified file object.  Parameters file – the file-like object to use for exporting the data. Its write method should accept bytes (a disk file opened in binary mode, a io.BytesIO object, etc)   The additional keyword arguments of this __init__ method are passed to the BaseItemExporter __init__ method. A typical output of this exporter would be: {'name': 'Color TV', 'price': '1200'} {'name': 'DVD player', 'price': '200'}   Longer lines (when present) are pretty-formatted.     JsonItemExporter   class scrapy.exporters.JsonItemExporter(file, **kwargs)[source] Exports items in JSON format to the specified file-like object, writing all objects as a list of objects. The additional __init__ method arguments are passed to the BaseItemExporter __init__ method, and the leftover arguments to the JSONEncoder __init__ method, so you can use any JSONEncoder __init__ method argument to customize this exporter.  Parameters file – the file-like object to use for exporting the data. Its write method should accept bytes (a disk file opened in binary mode, a io.BytesIO object, etc)   A typical output of this exporter would be: [{\"name\": \"Color TV\", \"price\": \"1200\"}, {\"name\": \"DVD player\", \"price\": \"200\"}]    Warning JSON is very simple and flexible serialization format, but it doesn’t scale well for large amounts of data since incremental (aka. stream-mode) parsing is not well supported (if at all) among JSON parsers (on any language), and most of them just parse the entire object in memory. If you want the power and simplicity of JSON with a more stream-friendly format, consider using JsonLinesItemExporter instead, or splitting the output in multiple chunks.      JsonLinesItemExporter   class scrapy.exporters.JsonLinesItemExporter(file, **kwargs)[source] Exports items in JSON format to the specified file-like object, writing one JSON-encoded item per line. The additional __init__ method arguments are passed to the BaseItemExporter __init__ method, and the leftover arguments to the JSONEncoder __init__ method, so you can use any JSONEncoder __init__ method argument to customize this exporter.  Parameters file – the file-like object to use for exporting the data. Its write method should accept bytes (a disk file opened in binary mode, a io.BytesIO object, etc)   A typical output of this exporter would be: {\"name\": \"Color TV\", \"price\": \"1200\"} {\"name\": \"DVD player\", \"price\": \"200\"}   Unlike the one produced by JsonItemExporter, the format produced by this exporter is well suited for serializing large amounts of data.     MarshalItemExporter   class scrapy.exporters.MarshalItemExporter(file, **kwargs)[source] Exports items in a Python-specific binary format (see marshal).  Parameters file – The file-like object to use for exporting the data. Its write method should accept bytes (a disk file opened in binary mode, a BytesIO object, etc)                                ", "code_blocks": ["from itemadapter import ItemAdapter\nfrom scrapy.exporters import XmlItemExporter\n\n\nclass PerYearXmlExportPipeline:\n    \"\"\"Distribute items across multiple XML files according to their 'year' field\"\"\"\n\n    def open_spider(self, spider):\n        self.year_to_exporter = {}\n\n    def close_spider(self, spider):\n        for exporter, xml_file in self.year_to_exporter.values():\n            exporter.finish_exporting()\n            xml_file.close()\n\n    def _exporter_for_item(self, item):\n        adapter = ItemAdapter(item)\n        year = adapter[\"year\"]\n        if year not in self.year_to_exporter:\n            xml_file = open(f\"{year}.xml\", \"wb\")\n            exporter = XmlItemExporter(xml_file)\n            exporter.start_exporting()\n            self.year_to_exporter[year] = (exporter, xml_file)\n        return self.year_to_exporter[year][0]\n\n    def process_item(self, item, spider):\n        exporter = self._exporter_for_item(item)\n        exporter.export_item(item)\n        return item\n</pre>", "import scrapy\n\n\ndef serialize_price(value):\n    return f\"$ {str(value)}\"\n\n\nclass Product(scrapy.Item):\n    name = scrapy.Field()\n    price = scrapy.Field(serializer=serialize_price)\n</pre>", "from scrapy.exporters import XmlItemExporter\n\n\nclass ProductXmlExporter(XmlItemExporter):\n    def serialize_field(self, field, name, value):\n        if name == \"price\":\n            return f\"$ {str(value)}\"\n        return super().serialize_field(field, name, value)\n</pre>", "Item(name=\"Color TV\", price=\"1200\")\nItem(name=\"DVD player\", price=\"200\")\n</pre>", "['field1', 'field2']\n</pre>", "{'field1': 'Field 1', 'field2': 'Field 2'}\n</pre>", "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<items>\n  <item>\n    <name>Color TV</name>\n    1200</price>\n </item>\n  <item>\n    <name>DVD player</name>\n    200</price>\n </item>\n</items>\n</pre>", "Item(name=['John', 'Doe'], age='23')\n</pre>", "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<items>\n  <item>\n    <name>\n      <value>John</value>\n      <value>Doe</value>\n    </name>\n    23</age>\n  </item>\n</items>\n</pre>", "product,price\nColor TV,1200\nDVD player,200\n</pre>", "{'name': 'Color TV', 'price': '1200'}\n{'name': 'DVD player', 'price': '200'}\n</pre>", "[{\"name\": \"Color TV\", \"price\": \"1200\"},\n{\"name\": \"DVD player\", \"price\": \"200\"}]\n</pre>", "{\"name\": \"Color TV\", \"price\": \"1200\"}\n{\"name\": \"DVD player\", \"price\": \"200\"}\n</pre>"], "links": [{"text": "Feed exports", "href": "feed-exports.html#topics-feed-exports"}, {"text": "Item Pipeline", "href": "item-pipeline.html"}, {"text": "field metadata", "href": "items.html#topics-items-fields"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#BaseItemExporter"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#BaseItemExporter.export_item"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#BaseItemExporter.serialize_field"}, {"text": "dict", "href": "https://docs.python.org/3/library/stdtypes.html#dict"}, {"text": "item object", "href": "items.html#item-types"}, {"text": "dict", "href": "https://docs.python.org/3/library/stdtypes.html#dict"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#BaseItemExporter.start_exporting"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#BaseItemExporter.finish_exporting"}, {"text": "item objects", "href": "items.html#item-types"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#PythonItemExporter"}, {"text": "json", "href": "https://docs.python.org/3/library/json.html#module-json"}, {"text": "msgpack", "href": "https://pypi.org/project/msgpack/"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#XmlItemExporter"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#CsvItemExporter"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "io.TextIOWrapper", "href": "https://docs.python.org/3/library/io.html#io.TextIOWrapper"}, {"text": "csv.writer()", "href": "https://docs.python.org/3/library/csv.html#csv.writer"}, {"text": "csv.writer()", "href": "https://docs.python.org/3/library/csv.html#csv.writer"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#PickleItemExporter"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "pickle", "href": "https://docs.python.org/3/library/pickle.html#module-pickle"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#PprintItemExporter"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#JsonItemExporter"}, {"text": "JSONEncoder", "href": "https://docs.python.org/3/library/json.html#json.JSONEncoder"}, {"text": "JSONEncoder", "href": "https://docs.python.org/3/library/json.html#json.JSONEncoder"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#JsonLinesItemExporter"}, {"text": "JSONEncoder", "href": "https://docs.python.org/3/library/json.html#json.JSONEncoder"}, {"text": "JSONEncoder", "href": "https://docs.python.org/3/library/json.html#json.JSONEncoder"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#MarshalItemExporter"}, {"text": "marshal", "href": "https://docs.python.org/3/library/marshal.html#module-marshal"}, {"text": "bytes", "href": "https://docs.python.org/3/library/stdtypes.html#bytes"}, {"text": "BytesIO", "href": "https://docs.python.org/3/library/io.html#io.BytesIO"}], "timestamp": "2023-10-12T00:10:39.309973", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/scheduler.html", "content": "                              Scheduler The scheduler component receives requests from the engine and stores them into persistent and/or non-persistent data structures. It also gets those requests and feeds them back to the engine when it asks for a next request to be downloaded.  Overriding the default scheduler You can use your own custom scheduler class by supplying its full Python path in the SCHEDULER setting.   Minimal scheduler interface   class scrapy.core.scheduler.BaseScheduler[source] The scheduler component is responsible for storing requests received from the engine, and feeding them back upon request (also to the engine). The original sources of said requests are:  Spider: start_requests method, requests created for URLs in the start_urls attribute, request callbacks Spider middleware: process_spider_output and process_spider_exception methods Downloader middleware: process_request, process_response and process_exception methods  The order in which the scheduler returns its stored requests (via the next_request method) plays a great part in determining the order in which those requests are downloaded. The methods defined in this class constitute the minimal interface that the Scrapy engine will interact with.   close(reason: str) → Optional[Deferred][source] Called when the spider is closed by the engine. It receives the reason why the crawl finished as argument and it’s useful to execute cleaning code.  Parameters reason (str) – a string which describes the reason why the spider was closed       abstract enqueue_request(request: Request) → bool[source] Process a request received by the engine. Return True if the request is stored correctly, False otherwise. If False, the engine will fire a request_dropped signal, and will not make further attempts to schedule the request at a later time. For reference, the default Scrapy scheduler returns False when the request is rejected by the dupefilter.     classmethod from_crawler(crawler: Crawler) → Self[source] Factory method which receives the current Crawler object as argument.     abstract has_pending_requests() → bool[source] True if the scheduler has enqueued requests, False otherwise     abstract next_request() → Optional[Request][source] Return the next Request to be processed, or None to indicate that there are no requests to be considered ready at the moment. Returning None implies that no request from the scheduler will be sent to the downloader in the current reactor cycle. The engine will continue calling next_request until has_pending_requests is False.     open(spider: Spider) → Optional[Deferred][source] Called when the spider is opened by the engine. It receives the spider instance as argument and it’s useful to execute initialization code.  Parameters spider (Spider) – the spider object for the current crawl         Default Scrapy scheduler   class scrapy.core.scheduler.Scheduler(dupefilter: BaseDupeFilter, jobdir: Optional[str] = None, dqclass=None, mqclass=None, logunser: bool = False, stats: Optional[StatsCollector] = None, pqclass=None, crawler: Optional[Crawler] = None)[source] Default Scrapy scheduler. This implementation also handles duplication filtering via the dupefilter. This scheduler stores requests into several priority queues (defined by the SCHEDULER_PRIORITY_QUEUE setting). In turn, said priority queues are backed by either memory or disk based queues (respectively defined by the SCHEDULER_MEMORY_QUEUE and SCHEDULER_DISK_QUEUE settings). Request prioritization is almost entirely delegated to the priority queue. The only prioritization performed by this scheduler is using the disk-based queue if present (i.e. if the JOBDIR setting is defined) and falling back to the memory-based queue if a serialization error occurs. If the disk queue is not present, the memory one is used directly.  Parameters  dupefilter (scrapy.dupefilters.BaseDupeFilter instance or similar: any class that implements the BaseDupeFilter interface) – An object responsible for checking and filtering duplicate requests. The value for the DUPEFILTER_CLASS setting is used by default. jobdir (str or None) – The path of a directory to be used for persisting the crawl’s state. The value for the JOBDIR setting is used by default. See Jobs: pausing and resuming crawls. dqclass (class) – A class to be used as persistent request queue. The value for the SCHEDULER_DISK_QUEUE setting is used by default. mqclass (class) – A class to be used as non-persistent request queue. The value for the SCHEDULER_MEMORY_QUEUE setting is used by default. logunser (bool) – A boolean that indicates whether or not unserializable requests should be logged. The value for the SCHEDULER_DEBUG setting is used by default. stats (scrapy.statscollectors.StatsCollector instance or similar: any class that implements the StatsCollector interface) – A stats collector object to record stats about the request scheduling process. The value for the STATS_CLASS setting is used by default. pqclass (class) – A class to be used as priority queue for requests. The value for the SCHEDULER_PRIORITY_QUEUE setting is used by default. crawler (scrapy.crawler.Crawler) – The crawler object corresponding to the current crawl.      __len__() → int[source] Return the total amount of enqueued requests     close(reason: str) → Optional[Deferred][source]  dump pending requests to disk if there is a disk queue return the result of the dupefilter’s close method      enqueue_request(request: Request) → bool[source] Unless the received request is filtered out by the Dupefilter, attempt to push it into the disk queue, falling back to pushing it into the memory queue. Increment the appropriate stats, such as: scheduler/enqueued, scheduler/enqueued/disk, scheduler/enqueued/memory. Return True if the request was stored successfully, False otherwise.     classmethod from_crawler(crawler: Crawler) → SchedulerTV[source] Factory method, initializes the scheduler with arguments taken from the crawl settings     has_pending_requests() → bool[source] True if the scheduler has enqueued requests, False otherwise     next_request() → Optional[Request][source] Return a Request object from the memory queue, falling back to the disk queue if the memory queue is empty. Return None if there are no more enqueued requests. Increment the appropriate stats, such as: scheduler/dequeued, scheduler/dequeued/disk, scheduler/dequeued/memory.     open(spider: Spider) → Optional[Deferred][source]  initialize the memory queue initialize the disk queue if the jobdir attribute is a valid directory return the result of the dupefilter’s open method                                ", "code_blocks": [], "links": [{"text": "engine", "href": "architecture.html#component-engine"}, {"text": "SCHEDULER", "href": "settings.html#std-setting-SCHEDULER"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#BaseScheduler"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#BaseScheduler.close"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Request", "href": "request-response.html#scrapy.http.Request"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#BaseScheduler.enqueue_request"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#BaseScheduler.from_crawler"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#BaseScheduler.has_pending_requests"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Request", "href": "request-response.html#scrapy.http.Request"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#BaseScheduler.next_request"}, {"text": "Request", "href": "request-response.html#scrapy.http.Request"}, {"text": "Spider", "href": "spiders.html#scrapy.spiders.Spider"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#BaseScheduler.open"}, {"text": "Spider", "href": "spiders.html#scrapy.spiders.Spider"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "StatsCollector", "href": "api.html#scrapy.statscollectors.StatsCollector"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#Scheduler"}, {"text": "dupefilter", "href": "settings.html#std-setting-DUPEFILTER_CLASS"}, {"text": "SCHEDULER_PRIORITY_QUEUE", "href": "settings.html#std-setting-SCHEDULER_PRIORITY_QUEUE"}, {"text": "SCHEDULER_MEMORY_QUEUE", "href": "settings.html#std-setting-SCHEDULER_MEMORY_QUEUE"}, {"text": "SCHEDULER_DISK_QUEUE", "href": "settings.html#std-setting-SCHEDULER_DISK_QUEUE"}, {"text": "JOBDIR", "href": "settings.html#std-setting-JOBDIR"}, {"text": "DUPEFILTER_CLASS", "href": "settings.html#std-setting-DUPEFILTER_CLASS"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "JOBDIR", "href": "settings.html#std-setting-JOBDIR"}, {"text": "Jobs: pausing and resuming crawls", "href": "jobs.html#topics-jobs"}, {"text": "SCHEDULER_DISK_QUEUE", "href": "settings.html#std-setting-SCHEDULER_DISK_QUEUE"}, {"text": "SCHEDULER_MEMORY_QUEUE", "href": "settings.html#std-setting-SCHEDULER_MEMORY_QUEUE"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "SCHEDULER_DEBUG", "href": "settings.html#std-setting-SCHEDULER_DEBUG"}, {"text": "scrapy.statscollectors.StatsCollector", "href": "api.html#scrapy.statscollectors.StatsCollector"}, {"text": "STATS_CLASS", "href": "settings.html#std-setting-STATS_CLASS"}, {"text": "SCHEDULER_PRIORITY_QUEUE", "href": "settings.html#std-setting-SCHEDULER_PRIORITY_QUEUE"}, {"text": "scrapy.crawler.Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#Scheduler.__len__"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#Scheduler.close"}, {"text": "Request", "href": "request-response.html#scrapy.http.Request"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#Scheduler.enqueue_request"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#Scheduler.from_crawler"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#Scheduler.has_pending_requests"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Request", "href": "request-response.html#scrapy.http.Request"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#Scheduler.next_request"}, {"text": "Request", "href": "request-response.html#scrapy.http.Request"}, {"text": "Spider", "href": "spiders.html#scrapy.spiders.Spider"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#Scheduler.open"}], "timestamp": "2023-10-12T00:10:43.575880", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/signals.html", "content": "                              Signals Scrapy uses signals extensively to notify when certain events occur. You can catch some of those signals in your Scrapy project (using an extension, for example) to perform additional tasks or extend Scrapy to add functionality not provided out of the box. Even though signals provide several arguments, the handlers that catch them don’t need to accept all of them - the signal dispatching mechanism will only deliver the arguments that the handler receives. You can connect to signals (or send your own) through the Signals API. Here is a simple example showing how you can catch signals and perform some action: from scrapy import signals from scrapy import Spider   class DmozSpider(Spider):     name = \"dmoz\"     allowed_domains = [\"dmoz.org\"]     start_urls = [         \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",         \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\",     ]      @classmethod     def from_crawler(cls, crawler, *args, **kwargs):         spider = super(DmozSpider, cls).from_crawler(crawler, *args, **kwargs)         crawler.signals.connect(spider.spider_closed, signal=signals.spider_closed)         return spider      def spider_closed(self, spider):         spider.logger.info(\"Spider closed: %s\", spider.name)      def parse(self, response):         pass    Deferred signal handlers Some signals support returning Deferred or awaitable objects from their handlers, allowing you to run asynchronous code that does not block Scrapy. If a signal handler returns one of these objects, Scrapy waits for that asynchronous operation to finish. Let’s take an example using coroutines: import scrapy   class SignalSpider(scrapy.Spider):     name = \"signals\"     start_urls = [\"https://quotes.toscrape.com/page/1/\"]      @classmethod     def from_crawler(cls, crawler, *args, **kwargs):         spider = super(SignalSpider, cls).from_crawler(crawler, *args, **kwargs)         crawler.signals.connect(spider.item_scraped, signal=signals.item_scraped)         return spider      async def item_scraped(self, item):         # Send the scraped item to the server         response = await treq.post(             \"http://example.com/post\",             json.dumps(item).encode(\"ascii\"),             headers={b\"Content-Type\": [b\"application/json\"]},         )          return response      def parse(self, response):         for quote in response.css(\"div.quote\"):             yield {                 \"text\": quote.css(\"span.text::text\").get(),                 \"author\": quote.css(\"small.author::text\").get(),                 \"tags\": quote.css(\"div.tags a.tag::text\").getall(),             }   See the Built-in signals reference below to know which signals support Deferred and awaitable objects.   Built-in signals reference Here’s the list of Scrapy built-in signals and their meaning.  Engine signals  engine_started   scrapy.signals.engine_started() Sent when the Scrapy engine has started crawling. This signal supports returning deferreds from its handlers.    Note This signal may be fired after the spider_opened signal, depending on how the spider was started. So don’t rely on this signal getting fired before spider_opened.    engine_stopped   scrapy.signals.engine_stopped() Sent when the Scrapy engine is stopped (for example, when a crawling process has finished). This signal supports returning deferreds from its handlers.      Item signals  Note As at max CONCURRENT_ITEMS items are processed in parallel, many deferreds are fired together using DeferredList. Hence the next batch waits for the DeferredList to fire and then runs the respective item signal handler for the next batch of scraped items.   item_scraped   scrapy.signals.item_scraped(item, response, spider) Sent when an item has been scraped, after it has passed all the Item Pipeline stages (without being dropped). This signal supports returning deferreds from its handlers.  Parameters  item (item object) – the scraped item spider (Spider object) – the spider which scraped the item response (Response object) – the response from where the item was scraped        item_dropped   scrapy.signals.item_dropped(item, response, exception, spider) Sent after an item has been dropped from the Item Pipeline when some stage raised a DropItem exception. This signal supports returning deferreds from its handlers.  Parameters  item (item object) – the item dropped from the Item Pipeline spider (Spider object) – the spider which scraped the item response (Response object) – the response from where the item was dropped exception (DropItem exception) – the exception (which must be a DropItem subclass) which caused the item to be dropped        item_error   scrapy.signals.item_error(item, response, spider, failure) Sent when a Item Pipeline generates an error (i.e. raises an exception), except DropItem exception. This signal supports returning deferreds from its handlers.  Parameters  item (item object) – the item that caused the error in the Item Pipeline response (Response object) – the response being processed when the exception was raised spider (Spider object) – the spider which raised the exception failure (twisted.python.failure.Failure) – the exception raised         Spider signals  spider_closed   scrapy.signals.spider_closed(spider, reason) Sent after a spider has been closed. This can be used to release per-spider resources reserved on spider_opened. This signal supports returning deferreds from its handlers.  Parameters  spider (Spider object) – the spider which has been closed reason (str) – a string which describes the reason why the spider was closed. If it was closed because the spider has completed scraping, the reason is 'finished'. Otherwise, if the spider was manually closed by calling the close_spider engine method, then the reason is the one passed in the reason argument of that method (which defaults to 'cancelled'). If the engine was shutdown (for example, by hitting Ctrl-C to stop it) the reason will be 'shutdown'.        spider_opened   scrapy.signals.spider_opened(spider) Sent after a spider has been opened for crawling. This is typically used to reserve per-spider resources, but can be used for any task that needs to be performed when a spider is opened. This signal supports returning deferreds from its handlers.  Parameters spider (Spider object) – the spider which has been opened       spider_idle   scrapy.signals.spider_idle(spider) Sent when a spider has gone idle, which means the spider has no further:   requests waiting to be downloaded requests scheduled items being processed in the item pipeline   If the idle state persists after all handlers of this signal have finished, the engine starts closing the spider. After the spider has finished closing, the spider_closed signal is sent. You may raise a DontCloseSpider exception to prevent the spider from being closed. Alternatively, you may raise a CloseSpider exception to provide a custom spider closing reason. An idle handler is the perfect place to put some code that assesses the final spider results and update the final closing reason accordingly (e.g. setting it to ‘too_few_results’ instead of ‘finished’). This signal does not support returning deferreds from its handlers.  Parameters spider (Spider object) – the spider which has gone idle      Note Scheduling some requests in your spider_idle handler does not guarantee that it can prevent the spider from being closed, although it sometimes can. That’s because the spider may still remain idle if all the scheduled requests are rejected by the scheduler (e.g. filtered due to duplication).    spider_error   scrapy.signals.spider_error(failure, response, spider) Sent when a spider callback generates an error (i.e. raises an exception). This signal does not support returning deferreds from its handlers.  Parameters  failure (twisted.python.failure.Failure) – the exception raised response (Response object) – the response being processed when the exception was raised spider (Spider object) – the spider which raised the exception        feed_slot_closed   scrapy.signals.feed_slot_closed(slot) Sent when a feed exports slot is closed. This signal supports returning deferreds from its handlers.  Parameters slot (scrapy.extensions.feedexport.FeedSlot) – the slot closed       feed_exporter_closed   scrapy.signals.feed_exporter_closed() Sent when the feed exports extension is closed, during the handling of the spider_closed signal by the extension, after all feed exporting has been handled. This signal supports returning deferreds from its handlers.      Request signals  request_scheduled   scrapy.signals.request_scheduled(request, spider) Sent when the engine schedules a Request, to be downloaded later. This signal does not support returning deferreds from its handlers.  Parameters  request (Request object) – the request that reached the scheduler spider (Spider object) – the spider that yielded the request        request_dropped   scrapy.signals.request_dropped(request, spider) Sent when a Request, scheduled by the engine to be downloaded later, is rejected by the scheduler. This signal does not support returning deferreds from its handlers.  Parameters  request (Request object) – the request that reached the scheduler spider (Spider object) – the spider that yielded the request        request_reached_downloader   scrapy.signals.request_reached_downloader(request, spider) Sent when a Request reached downloader. This signal does not support returning deferreds from its handlers.  Parameters  request (Request object) – the request that reached downloader spider (Spider object) – the spider that yielded the request        request_left_downloader   scrapy.signals.request_left_downloader(request, spider)  New in version 2.0.  Sent when a Request leaves the downloader, even in case of failure. This signal does not support returning deferreds from its handlers.  Parameters  request (Request object) – the request that reached the downloader spider (Spider object) – the spider that yielded the request        bytes_received  New in version 2.2.    scrapy.signals.bytes_received(data, request, spider) Sent by the HTTP 1.1 and S3 download handlers when a group of bytes is received for a specific request. This signal might be fired multiple times for the same request, with partial data each time. For instance, a possible scenario for a 25 kb response would be two signals fired with 10 kb of data, and a final one with 5 kb of data. Handlers for this signal can stop the download of a response while it is in progress by raising the StopDownload exception. Please refer to the Stopping the download of a Response topic for additional information and examples. This signal does not support returning deferreds from its handlers.  Parameters  data (bytes object) – the data received by the download handler request (Request object) – the request that generated the download spider (Spider object) – the spider associated with the response        headers_received  New in version 2.5.    scrapy.signals.headers_received(headers, body_length, request, spider) Sent by the HTTP 1.1 and S3 download handlers when the response headers are available for a given request, before downloading any additional content. Handlers for this signal can stop the download of a response while it is in progress by raising the StopDownload exception. Please refer to the Stopping the download of a Response topic for additional information and examples. This signal does not support returning deferreds from its handlers.  Parameters  headers (scrapy.http.headers.Headers object) – the headers received by the download handler body_length (int) – expected size of the response body, in bytes request (Request object) – the request that generated the download spider (Spider object) – the spider associated with the response         Response signals  response_received   scrapy.signals.response_received(response, request, spider) Sent when the engine receives a new Response from the downloader. This signal does not support returning deferreds from its handlers.  Parameters  response (Response object) – the response received request (Request object) – the request that generated the response spider (Spider object) – the spider for which the response is intended       Note The request argument might not contain the original request that reached the downloader, if a Downloader Middleware modifies the Response object and sets a specific request attribute.    response_downloaded   scrapy.signals.response_downloaded(response, request, spider) Sent by the downloader right after a HTTPResponse is downloaded. This signal does not support returning deferreds from its handlers.  Parameters  response (Response object) – the response downloaded request (Request object) – the request that generated the response spider (Spider object) – the spider for which the response is intended                                  ", "code_blocks": ["from scrapy import signals\nfrom scrapy import Spider\n\n\nclass DmozSpider(Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\",\n    ]\n\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        spider = super(DmozSpider, cls).from_crawler(crawler, *args, **kwargs)\n        crawler.signals.connect(spider.spider_closed, signal=signals.spider_closed)\n        return spider\n\n    def spider_closed(self, spider):\n        spider.logger.info(\"Spider closed: %s\", spider.name)\n\n    def parse(self, response):\n        pass\n</pre>", "import scrapy\n\n\nclass SignalSpider(scrapy.Spider):\n    name = \"signals\"\n    start_urls = [\"https://quotes.toscrape.com/page/1/\"]\n\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        spider = super(SignalSpider, cls).from_crawler(crawler, *args, **kwargs)\n        crawler.signals.connect(spider.item_scraped, signal=signals.item_scraped)\n        return spider\n\n    async def item_scraped(self, item):\n        # Send the scraped item to the server\n        response = await treq.post(\n            \"http://example.com/post\",\n            json.dumps(item).encode(\"ascii\"),\n            headers={b\"Content-Type\": [b\"application/json\"]},\n        )\n\n        return response\n\n    def parse(self, response):\n        for quote in response.css(\"div.quote\"):\n            yield {\n                \"text\": quote.css(\"span.text::text\").get(),\n                \"author\": quote.css(\"small.author::text\").get(),\n                \"tags\": quote.css(\"div.tags a.tag::text\").getall(),\n            }\n</pre>"], "links": [{"text": "extension", "href": "extensions.html#topics-extensions"}, {"text": "Signals API", "href": "api.html#topics-api-signals"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "awaitable objects", "href": "https://docs.python.org/3/glossary.html#term-awaitable"}, {"text": "coroutines", "href": "coroutines.html#topics-coroutines"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "awaitable objects", "href": "https://docs.python.org/3/glossary.html#term-awaitable"}, {"text": "CONCURRENT_ITEMS", "href": "settings.html#std-setting-CONCURRENT_ITEMS"}, {"text": "DeferredList", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.DeferredList.html"}, {"text": "DeferredList", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.DeferredList.html"}, {"text": "Item Pipeline", "href": "item-pipeline.html#topics-item-pipeline"}, {"text": "item object", "href": "items.html#item-types"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Item Pipeline", "href": "item-pipeline.html#topics-item-pipeline"}, {"text": "DropItem", "href": "exceptions.html#scrapy.exceptions.DropItem"}, {"text": "item object", "href": "items.html#item-types"}, {"text": "Item Pipeline", "href": "item-pipeline.html#topics-item-pipeline"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "DropItem", "href": "exceptions.html#scrapy.exceptions.DropItem"}, {"text": "DropItem", "href": "exceptions.html#scrapy.exceptions.DropItem"}, {"text": "Item Pipeline", "href": "item-pipeline.html#topics-item-pipeline"}, {"text": "DropItem", "href": "exceptions.html#scrapy.exceptions.DropItem"}, {"text": "item object", "href": "items.html#item-types"}, {"text": "Item Pipeline", "href": "item-pipeline.html#topics-item-pipeline"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "twisted.python.failure.Failure", "href": "https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "DontCloseSpider", "href": "exceptions.html#scrapy.exceptions.DontCloseSpider"}, {"text": "CloseSpider", "href": "exceptions.html#scrapy.exceptions.CloseSpider"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "twisted.python.failure.Failure", "href": "https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "feed exports", "href": "feed-exports.html#topics-feed-exports"}, {"text": "feed exports", "href": "feed-exports.html#topics-feed-exports"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "StopDownload", "href": "exceptions.html#scrapy.exceptions.StopDownload"}, {"text": "Stopping the download of a Response", "href": "request-response.html#topics-stop-response-download"}, {"text": "bytes", "href": "https://docs.python.org/3/library/stdtypes.html#bytes"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "StopDownload", "href": "exceptions.html#scrapy.exceptions.StopDownload"}, {"text": "Stopping the download of a Response", "href": "request-response.html#topics-stop-response-download"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Downloader Middleware", "href": "downloader-middleware.html#topics-downloader-middleware"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}], "timestamp": "2023-10-12T00:10:46.633692", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/extensions.html", "content": "                              Extensions The extensions framework provides a mechanism for inserting your own custom functionality into Scrapy. Extensions are just regular classes.  Extension settings Extensions use the Scrapy settings to manage their settings, just like any other Scrapy code. It is customary for extensions to prefix their settings with their own name, to avoid collision with existing (and future) extensions. For example, a hypothetical extension to handle Google Sitemaps would use settings like GOOGLESITEMAP_ENABLED, GOOGLESITEMAP_DEPTH, and so on.   Loading & activating extensions Extensions are loaded and activated at startup by instantiating a single instance of the extension class per spider being run. All the extension initialization code must be performed in the class __init__ method. To make an extension available, add it to the EXTENSIONS setting in your Scrapy settings. In EXTENSIONS, each extension is represented by a string: the full Python path to the extension’s class name. For example: EXTENSIONS = {     \"scrapy.extensions.corestats.CoreStats\": 500,     \"scrapy.extensions.telnet.TelnetConsole\": 500, }   As you can see, the EXTENSIONS setting is a dict where the keys are the extension paths, and their values are the orders, which define the extension loading order. The EXTENSIONS setting is merged with the EXTENSIONS_BASE setting defined in Scrapy (and not meant to be overridden) and then sorted by order to get the final sorted list of enabled extensions. As extensions typically do not depend on each other, their loading order is irrelevant in most cases. This is why the EXTENSIONS_BASE setting defines all extensions with the same order (0). However, this feature can be exploited if you need to add an extension which depends on other extensions already loaded.   Available, enabled and disabled extensions Not all available extensions will be enabled. Some of them usually depend on a particular setting. For example, the HTTP Cache extension is available by default but disabled unless the HTTPCACHE_ENABLED setting is set.   Disabling an extension In order to disable an extension that comes enabled by default (i.e. those included in the EXTENSIONS_BASE setting) you must set its order to None. For example: EXTENSIONS = {     \"scrapy.extensions.corestats.CoreStats\": None, }     Writing your own extension Each extension is a Python class. The main entry point for a Scrapy extension (this also includes middlewares and pipelines) is the from_crawler class method which receives a Crawler instance. Through the Crawler object you can access settings, signals, stats, and also control the crawling behaviour. Typically, extensions connect to signals and perform tasks triggered by them. Finally, if the from_crawler method raises the NotConfigured exception, the extension will be disabled. Otherwise, the extension will be enabled.  Sample extension Here we will implement a simple extension to illustrate the concepts described in the previous section. This extension will log a message every time:  a spider is opened a spider is closed a specific number of items are scraped  The extension will be enabled through the MYEXT_ENABLED setting and the number of items will be specified through the MYEXT_ITEMCOUNT setting. Here is the code of such extension: import logging from scrapy import signals from scrapy.exceptions import NotConfigured  logger = logging.getLogger(__name__)   class SpiderOpenCloseLogging:     def __init__(self, item_count):         self.item_count = item_count         self.items_scraped = 0      @classmethod     def from_crawler(cls, crawler):         # first check if the extension should be enabled and raise         # NotConfigured otherwise         if not crawler.settings.getbool(\"MYEXT_ENABLED\"):             raise NotConfigured          # get the number of items from settings         item_count = crawler.settings.getint(\"MYEXT_ITEMCOUNT\", 1000)          # instantiate the extension object         ext = cls(item_count)          # connect the extension object to signals         crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)         crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)         crawler.signals.connect(ext.item_scraped, signal=signals.item_scraped)          # return the extension object         return ext      def spider_opened(self, spider):         logger.info(\"opened spider %s\", spider.name)      def spider_closed(self, spider):         logger.info(\"closed spider %s\", spider.name)      def item_scraped(self, item, spider):         self.items_scraped += 1         if self.items_scraped % self.item_count == 0:             logger.info(\"scraped %d items\", self.items_scraped)      Built-in extensions reference  General purpose extensions  Log Stats extension   class scrapy.extensions.logstats.LogStats[source]   Log basic stats like crawled pages and scraped items.   Core Stats extension   class scrapy.extensions.corestats.CoreStats[source]   Enable the collection of core statistics, provided the stats collection is enabled (see Stats Collection).   Telnet console extension   class scrapy.extensions.telnet.TelnetConsole[source]   Provides a telnet console for getting into a Python interpreter inside the currently running Scrapy process, which can be very useful for debugging. The telnet console must be enabled by the TELNETCONSOLE_ENABLED setting, and the server will listen in the port specified in TELNETCONSOLE_PORT.   Memory usage extension   class scrapy.extensions.memusage.MemoryUsage[source]    Note This extension does not work in Windows.  Monitors the memory used by the Scrapy process that runs the spider and:  sends a notification e-mail when it exceeds a certain value closes the spider when it exceeds a certain value  The notification e-mails can be triggered when a certain warning value is reached (MEMUSAGE_WARNING_MB) and when the maximum value is reached (MEMUSAGE_LIMIT_MB) which will also cause the spider to be closed and the Scrapy process to be terminated. This extension is enabled by the MEMUSAGE_ENABLED setting and can be configured with the following settings:  MEMUSAGE_LIMIT_MB MEMUSAGE_WARNING_MB MEMUSAGE_NOTIFY_MAIL MEMUSAGE_CHECK_INTERVAL_SECONDS    Memory debugger extension   class scrapy.extensions.memdebug.MemoryDebugger[source]   An extension for debugging memory usage. It collects information about:  objects uncollected by the Python garbage collector objects left alive that shouldn’t. For more info, see Debugging memory leaks with trackref  To enable this extension, turn on the MEMDEBUG_ENABLED setting. The info will be stored in the stats.   Close spider extension   class scrapy.extensions.closespider.CloseSpider[source]   Closes a spider automatically when some conditions are met, using a specific closing reason for each condition. The conditions for closing a spider can be configured through the following settings:  CLOSESPIDER_TIMEOUT CLOSESPIDER_TIMEOUT_NO_ITEM CLOSESPIDER_ITEMCOUNT CLOSESPIDER_PAGECOUNT CLOSESPIDER_ERRORCOUNT   Note When a certain closing condition is met, requests which are currently in the downloader queue (up to CONCURRENT_REQUESTS requests) are still processed.   CLOSESPIDER_TIMEOUT Default: 0 An integer which specifies a number of seconds. If the spider remains open for more than that number of second, it will be automatically closed with the reason closespider_timeout. If zero (or non set), spiders won’t be closed by timeout.   CLOSESPIDER_TIMEOUT_NO_ITEM Default: 0 An integer which specifies a number of seconds. If the spider has not produced any items in the last number of seconds, it will be closed with the reason closespider_timeout_no_item. If zero (or non set), spiders won’t be closed regardless if it hasn’t produced any items.   CLOSESPIDER_ITEMCOUNT Default: 0 An integer which specifies a number of items. If the spider scrapes more than that amount and those items are passed by the item pipeline, the spider will be closed with the reason closespider_itemcount. If zero (or non set), spiders won’t be closed by number of passed items.   CLOSESPIDER_PAGECOUNT Default: 0 An integer which specifies the maximum number of responses to crawl. If the spider crawls more than that, the spider will be closed with the reason closespider_pagecount. If zero (or non set), spiders won’t be closed by number of crawled responses.   CLOSESPIDER_ERRORCOUNT Default: 0 An integer which specifies the maximum number of errors to receive before closing the spider. If the spider generates more than that number of errors, it will be closed with the reason closespider_errorcount. If zero (or non set), spiders won’t be closed by number of errors.    StatsMailer extension   class scrapy.extensions.statsmailer.StatsMailer[source]   This simple extension can be used to send a notification e-mail every time a domain has finished scraping, including the Scrapy stats collected. The email will be sent to all recipients specified in the STATSMAILER_RCPTS setting. Emails can be sent using the MailSender class. To see a full list of parameters, including examples on how to instantiate MailSender and use mail settings, see Sending e-mail.   Periodic log extension   class scrapy.extensions.periodic_log.PeriodicLog[source]   This extension periodically logs rich stat data as a JSON object: 2023-08-04 02:30:57 [scrapy.extensions.logstats] INFO: Crawled 976 pages (at 162 pages/min), scraped 925 items (at 161 items/min) 2023-08-04 02:30:57 [scrapy.extensions.periodic_log] INFO: {     \"delta\": {         \"downloader/request_bytes\": 55582,         \"downloader/request_count\": 162,         \"downloader/request_method_count/GET\": 162,         \"downloader/response_bytes\": 618133,         \"downloader/response_count\": 162,         \"downloader/response_status_count/200\": 162,         \"item_scraped_count\": 161     },     \"stats\": {         \"downloader/request_bytes\": 338243,         \"downloader/request_count\": 992,         \"downloader/request_method_count/GET\": 992,         \"downloader/response_bytes\": 3836736,         \"downloader/response_count\": 976,         \"downloader/response_status_count/200\": 976,         \"item_scraped_count\": 925,         \"log_count/INFO\": 21,         \"log_count/WARNING\": 1,         \"scheduler/dequeued\": 992,         \"scheduler/dequeued/memory\": 992,         \"scheduler/enqueued\": 1050,         \"scheduler/enqueued/memory\": 1050     },     \"time\": {         \"elapsed\": 360.008903,         \"log_interval\": 60.0,         \"log_interval_real\": 60.006694,         \"start_time\": \"2023-08-03 23:24:57\",         \"utcnow\": \"2023-08-03 23:30:57\"     } }   This extension logs the following configurable sections:  \"delta\" shows how some numeric stats have changed since the last stats log message. The PERIODIC_LOG_DELTA setting determines the target stats. They must have int or float values.  \"stats\" shows the current value of some stats. The PERIODIC_LOG_STATS setting determines the target stats.  \"time\" shows detailed timing data. The PERIODIC_LOG_TIMING_ENABLED setting determines whether or not to show this section.   This extension logs data at the start, then on a fixed time interval configurable through the LOGSTATS_INTERVAL setting, and finally right before the crawl ends. Example extension configuration: custom_settings = {     \"LOG_LEVEL\": \"INFO\",     \"PERIODIC_LOG_STATS\": {         \"include\": [\"downloader/\", \"scheduler/\", \"log_count/\", \"item_scraped_count/\"],     },     \"PERIODIC_LOG_DELTA\": {\"include\": [\"downloader/\"]},     \"PERIODIC_LOG_TIMING_ENABLED\": True,     \"EXTENSIONS\": {         \"scrapy.extensions.periodic_log.PeriodicLog\": 0,     }, }    PERIODIC_LOG_DELTA Default: None  \"PERIODIC_LOG_DELTA\": True - show deltas for all int and float stat values. \"PERIODIC_LOG_DELTA\": {\"include\": [\"downloader/\", \"scheduler/\"]} - show deltas for stats with names containing any configured substring. \"PERIODIC_LOG_DELTA\": {\"exclude\": [\"downloader/\"]} - show deltas for all stats with names not containing any configured substring.    PERIODIC_LOG_STATS Default: None  \"PERIODIC_LOG_STATS\": True - show the current value of all stats. \"PERIODIC_LOG_STATS\": {\"include\": [\"downloader/\", \"scheduler/\"]} - show current values for stats with names containing any configured substring. \"PERIODIC_LOG_STATS\": {\"exclude\": [\"downloader/\"]} - show current values for all stats with names not containing any configured substring.    PERIODIC_LOG_TIMING_ENABLED Default: False True enables logging of timing data (i.e. the \"time\" section).     Debugging extensions  Stack trace dump extension   class scrapy.extensions.periodic_log.StackTraceDump   Dumps information about the running process when a SIGQUIT or SIGUSR2 signal is received. The information dumped is the following:  engine status (using scrapy.utils.engine.get_engine_status()) live references (see Debugging memory leaks with trackref) stack trace of all threads  After the stack trace and engine status is dumped, the Scrapy process continues running normally. This extension only works on POSIX-compliant platforms (i.e. not Windows), because the SIGQUIT and SIGUSR2 signals are not available on Windows. There are at least two ways to send Scrapy the SIGQUIT signal:  By pressing Ctrl-while a Scrapy process is running (Linux only?) By running this command (assuming <pid> is the process id of the Scrapy process): kill -QUIT <pid>       Debugger extension   class scrapy.extensions.periodic_log.Debugger   Invokes a Python debugger inside a running Scrapy process when a SIGUSR2 signal is received. After the debugger is exited, the Scrapy process continues running normally. For more info see Debugging in Python. This extension only works on POSIX-compliant platforms (i.e. not Windows).                             ", "code_blocks": ["EXTENSIONS = {\n    \"scrapy.extensions.corestats.CoreStats\": 500,\n    \"scrapy.extensions.telnet.TelnetConsole\": 500,\n}\n</pre>", "EXTENSIONS = {\n    \"scrapy.extensions.corestats.CoreStats\": None,\n}\n</pre>", "import logging\nfrom scrapy import signals\nfrom scrapy.exceptions import NotConfigured\n\nlogger = logging.getLogger(__name__)\n\n\nclass SpiderOpenCloseLogging:\n    def __init__(self, item_count):\n        self.item_count = item_count\n        self.items_scraped = 0\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # first check if the extension should be enabled and raise\n        # NotConfigured otherwise\n        if not crawler.settings.getbool(\"MYEXT_ENABLED\"):\n            raise NotConfigured\n\n        # get the number of items from settings\n        item_count = crawler.settings.getint(\"MYEXT_ITEMCOUNT\", 1000)\n\n        # instantiate the extension object\n        ext = cls(item_count)\n\n        # connect the extension object to signals\n        crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(ext.item_scraped, signal=signals.item_scraped)\n\n        # return the extension object\n        return ext\n\n    def spider_opened(self, spider):\n        logger.info(\"opened spider %s\", spider.name)\n\n    def spider_closed(self, spider):\n        logger.info(\"closed spider %s\", spider.name)\n\n    def item_scraped(self, item, spider):\n        self.items_scraped += 1\n        if self.items_scraped % self.item_count == 0:\n            logger.info(\"scraped %d items\", self.items_scraped)\n</pre>", "2023-08-04 02:30:57 [scrapy.extensions.logstats] INFO: Crawled 976 pages (at 162 pages/min), scraped 925 items (at 161 items/min)\n2023-08-04 02:30:57 [scrapy.extensions.periodic_log] INFO: {\n    \"delta\": {\n        \"downloader/request_bytes\": 55582,\n        \"downloader/request_count\": 162,\n        \"downloader/request_method_count/GET\": 162,\n        \"downloader/response_bytes\": 618133,\n        \"downloader/response_count\": 162,\n        \"downloader/response_status_count/200\": 162,\n        \"item_scraped_count\": 161\n    },\n    \"stats\": {\n        \"downloader/request_bytes\": 338243,\n        \"downloader/request_count\": 992,\n        \"downloader/request_method_count/GET\": 992,\n        \"downloader/response_bytes\": 3836736,\n        \"downloader/response_count\": 976,\n        \"downloader/response_status_count/200\": 976,\n        \"item_scraped_count\": 925,\n        \"log_count/INFO\": 21,\n        \"log_count/WARNING\": 1,\n        \"scheduler/dequeued\": 992,\n        \"scheduler/dequeued/memory\": 992,\n        \"scheduler/enqueued\": 1050,\n        \"scheduler/enqueued/memory\": 1050\n    },\n    \"time\": {\n        \"elapsed\": 360.008903,\n        \"log_interval\": 60.0,\n        \"log_interval_real\": 60.006694,\n        \"start_time\": \"2023-08-03 23:24:57\",\n        \"utcnow\": \"2023-08-03 23:30:57\"\n    }\n}\n</pre>", "custom_settings = {\n    \"LOG_LEVEL\": \"INFO\",\n    \"PERIODIC_LOG_STATS\": {\n        \"include\": [\"downloader/\", \"scheduler/\", \"log_count/\", \"item_scraped_count/\"],\n    },\n    \"PERIODIC_LOG_DELTA\": {\"include\": [\"downloader/\"]},\n    \"PERIODIC_LOG_TIMING_ENABLED\": True,\n    \"EXTENSIONS\": {\n        \"scrapy.extensions.periodic_log.PeriodicLog\": 0,\n    },\n}\n</pre>", "kill -QUIT <pid>\n</pre>"], "links": [{"text": "Scrapy settings", "href": "settings.html#topics-settings"}, {"text": "Google Sitemaps", "href": "https://en.wikipedia.org/wiki/Sitemaps"}, {"text": "EXTENSIONS", "href": "settings.html#std-setting-EXTENSIONS"}, {"text": "EXTENSIONS", "href": "settings.html#std-setting-EXTENSIONS"}, {"text": "EXTENSIONS", "href": "settings.html#std-setting-EXTENSIONS"}, {"text": "EXTENSIONS", "href": "settings.html#std-setting-EXTENSIONS"}, {"text": "EXTENSIONS_BASE", "href": "settings.html#std-setting-EXTENSIONS_BASE"}, {"text": "EXTENSIONS_BASE", "href": "settings.html#std-setting-EXTENSIONS_BASE"}, {"text": "HTTPCACHE_ENABLED", "href": "downloader-middleware.html#std-setting-HTTPCACHE_ENABLED"}, {"text": "EXTENSIONS_BASE", "href": "settings.html#std-setting-EXTENSIONS_BASE"}, {"text": "signals", "href": "signals.html#topics-signals"}, {"text": "NotConfigured", "href": "exceptions.html#scrapy.exceptions.NotConfigured"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/logstats.html#LogStats"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/corestats.html#CoreStats"}, {"text": "Stats Collection", "href": "stats.html#topics-stats"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/telnet.html#TelnetConsole"}, {"text": "TELNETCONSOLE_ENABLED", "href": "settings.html#std-setting-TELNETCONSOLE_ENABLED"}, {"text": "TELNETCONSOLE_PORT", "href": "telnetconsole.html#std-setting-TELNETCONSOLE_PORT"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/memusage.html#MemoryUsage"}, {"text": "MEMUSAGE_WARNING_MB", "href": "settings.html#std-setting-MEMUSAGE_WARNING_MB"}, {"text": "MEMUSAGE_LIMIT_MB", "href": "settings.html#std-setting-MEMUSAGE_LIMIT_MB"}, {"text": "MEMUSAGE_ENABLED", "href": "settings.html#std-setting-MEMUSAGE_ENABLED"}, {"text": "MEMUSAGE_LIMIT_MB", "href": "settings.html#std-setting-MEMUSAGE_LIMIT_MB"}, {"text": "MEMUSAGE_WARNING_MB", "href": "settings.html#std-setting-MEMUSAGE_WARNING_MB"}, {"text": "MEMUSAGE_NOTIFY_MAIL", "href": "settings.html#std-setting-MEMUSAGE_NOTIFY_MAIL"}, {"text": "MEMUSAGE_CHECK_INTERVAL_SECONDS", "href": "settings.html#std-setting-MEMUSAGE_CHECK_INTERVAL_SECONDS"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/memdebug.html#MemoryDebugger"}, {"text": "Debugging memory leaks with trackref", "href": "leaks.html#topics-leaks-trackrefs"}, {"text": "MEMDEBUG_ENABLED", "href": "settings.html#std-setting-MEMDEBUG_ENABLED"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/closespider.html#CloseSpider"}, {"text": "CONCURRENT_REQUESTS", "href": "settings.html#std-setting-CONCURRENT_REQUESTS"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/statsmailer.html#StatsMailer"}, {"text": "STATSMAILER_RCPTS", "href": "settings.html#std-setting-STATSMAILER_RCPTS"}, {"text": "MailSender", "href": "email.html#scrapy.mail.MailSender"}, {"text": "MailSender", "href": "email.html#scrapy.mail.MailSender"}, {"text": "Sending e-mail", "href": "email.html#topics-email"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/periodic_log.html#PeriodicLog"}, {"text": "LOGSTATS_INTERVAL", "href": "settings.html#std-setting-LOGSTATS_INTERVAL"}, {"text": "SIGQUIT", "href": "https://en.wikipedia.org/wiki/SIGQUIT"}, {"text": "SIGUSR2", "href": "https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2"}, {"text": "Debugging memory leaks with trackref", "href": "leaks.html#topics-leaks-trackrefs"}, {"text": "SIGQUIT", "href": "https://en.wikipedia.org/wiki/SIGQUIT"}, {"text": "SIGUSR2", "href": "https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2"}, {"text": "SIGQUIT", "href": "https://en.wikipedia.org/wiki/SIGQUIT"}, {"text": "Python debugger", "href": "https://docs.python.org/3/library/pdb.html"}, {"text": "SIGUSR2", "href": "https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2"}, {"text": "Debugging in Python", "href": "https://pythonconquerstheuniverse.wordpress.com/2009/09/10/debugging-in-python/"}], "timestamp": "2023-10-12T00:10:50.582298", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/spider-middleware.html", "content": "                              Spider Middleware The spider middleware is a framework of hooks into Scrapy’s spider processing mechanism where you can plug custom functionality to process the responses that are sent to Spiders for processing and to process the requests and items that are generated from spiders.  Activating a spider middleware To activate a spider middleware component, add it to the SPIDER_MIDDLEWARES setting, which is a dict whose keys are the middleware class path and their values are the middleware orders. Here’s an example: SPIDER_MIDDLEWARES = {     \"myproject.middlewares.CustomSpiderMiddleware\": 543, }   The SPIDER_MIDDLEWARES setting is merged with the SPIDER_MIDDLEWARES_BASE setting defined in Scrapy (and not meant to be overridden) and then sorted by order to get the final sorted list of enabled middlewares: the first middleware is the one closer to the engine and the last is the one closer to the spider. In other words, the process_spider_input() method of each middleware will be invoked in increasing middleware order (100, 200, 300, …), and the process_spider_output() method of each middleware will be invoked in decreasing order. To decide which order to assign to your middleware see the SPIDER_MIDDLEWARES_BASE setting and pick a value according to where you want to insert the middleware. The order does matter because each middleware performs a different action and your middleware could depend on some previous (or subsequent) middleware being applied. If you want to disable a builtin middleware (the ones defined in SPIDER_MIDDLEWARES_BASE, and enabled by default) you must define it in your project SPIDER_MIDDLEWARES setting and assign None as its value.  For example, if you want to disable the off-site middleware: SPIDER_MIDDLEWARES = {     \"myproject.middlewares.CustomSpiderMiddleware\": 543,     \"scrapy.spidermiddlewares.offsite.OffsiteMiddleware\": None, }   Finally, keep in mind that some middlewares may need to be enabled through a particular setting. See each middleware documentation for more info.   Writing your own spider middleware Each spider middleware is a Python class that defines one or more of the methods defined below. The main entry point is the from_crawler class method, which receives a Crawler instance. The Crawler object gives you access, for example, to the settings.   class scrapy.spidermiddlewares.SpiderMiddleware   process_spider_input(response, spider) This method is called for each response that goes through the spider middleware and into the spider, for processing. process_spider_input() should return None or raise an exception. If it returns None, Scrapy will continue processing this response, executing all other middlewares until, finally, the response is handed to the spider for processing. If it raises an exception, Scrapy won’t bother calling any other spider middleware process_spider_input() and will call the request errback if there is one, otherwise it will start the process_spider_exception() chain. The output of the errback is chained back in the other direction for process_spider_output() to process it, or process_spider_exception() if it raised an exception.  Parameters  response (Response object) – the response being processed spider (Spider object) – the spider for which this response is intended        process_spider_output(response, result, spider) This method is called with the results returned from the Spider, after it has processed the response. process_spider_output() must return an iterable of Request objects and item objects.  Changed in version 2.7: This method may be defined as an asynchronous generator, in which case result is an asynchronous iterable.  Consider defining this method as an asynchronous generator, which will be a requirement in a future version of Scrapy. However, if you plan on sharing your spider middleware with other people, consider either enforcing Scrapy 2.7 as a minimum requirement of your spider middleware, or making your spider middleware universal so that it works with Scrapy versions earlier than Scrapy 2.7.  Parameters  response (Response object) – the response which generated this output from the spider result (an iterable of Request objects and item objects) – the result returned by the spider spider (Spider object) – the spider whose result is being processed        process_spider_output_async(response, result, spider)  New in version 2.7.  If defined, this method must be an asynchronous generator, which will be called instead of process_spider_output() if result is an asynchronous iterable.     process_spider_exception(response, exception, spider) This method is called when a spider or process_spider_output() method (from a previous spider middleware) raises an exception. process_spider_exception() should return either None or an iterable of Request or item objects. If it returns None, Scrapy will continue processing this exception, executing any other process_spider_exception() in the following middleware components, until no middleware components are left and the exception reaches the engine (where it’s logged and discarded). If it returns an iterable the process_spider_output() pipeline kicks in, starting from the next spider middleware, and no other process_spider_exception() will be called.  Parameters  response (Response object) – the response being processed when the exception was raised exception (Exception object) – the exception raised spider (Spider object) – the spider which raised the exception        process_start_requests(start_requests, spider) This method is called with the start requests of the spider, and works similarly to the process_spider_output() method, except that it doesn’t have a response associated and must return only requests (not items). It receives an iterable (in the start_requests parameter) and must return another iterable of Request objects.  Note When implementing this method in your spider middleware, you should always return an iterable (that follows the input one) and not consume all start_requests iterator because it can be very large (or even unbounded) and cause a memory overflow. The Scrapy engine is designed to pull start requests while it has capacity to process them, so the start requests iterator can be effectively endless where there is some other condition for stopping the spider (like a time limit or item/page count).   Parameters  start_requests (an iterable of Request) – the start requests spider (Spider object) – the spider to whom the start requests belong        from_crawler(cls, crawler) If present, this classmethod is called to create a middleware instance from a Crawler. It must return a new instance of the middleware. Crawler object provides access to all Scrapy core components like settings and signals; it is a way for middleware to access them and hook its functionality into Scrapy.  Parameters crawler (Crawler object) – crawler that uses this middleware         Built-in spider middleware reference This page describes all spider middleware components that come with Scrapy. For information on how to use them and how to write your own spider middleware, see the spider middleware usage guide. For a list of the components enabled by default (and their orders) see the SPIDER_MIDDLEWARES_BASE setting.  DepthMiddleware   class scrapy.spidermiddlewares.depth.DepthMiddleware[source] DepthMiddleware is used for tracking the depth of each Request inside the site being scraped. It works by setting request.meta['depth'] = 0 whenever there is no value previously set (usually just the first Request) and incrementing it by 1 otherwise. It can be used to limit the maximum depth to scrape, control Request priority based on their depth, and things like that. The DepthMiddleware can be configured through the following settings (see the settings documentation for more info):   DEPTH_LIMIT - The maximum depth that will be allowed to crawl for any site. If zero, no limit will be imposed. DEPTH_STATS_VERBOSE - Whether to collect the number of requests for each depth. DEPTH_PRIORITY - Whether to prioritize the requests based on their depth.       HttpErrorMiddleware   class scrapy.spidermiddlewares.httperror.HttpErrorMiddleware[source] Filter out unsuccessful (erroneous) HTTP responses so that spiders don’t have to deal with them, which (most of the time) imposes an overhead, consumes more resources, and makes the spider logic more complex.   According to the HTTP standard, successful responses are those whose status codes are in the 200-300 range. If you still want to process response codes outside that range, you can specify which response codes the spider is able to handle using the handle_httpstatus_list spider attribute or HTTPERROR_ALLOWED_CODES setting. For example, if you want your spider to handle 404 responses you can do this: from scrapy.spiders import CrawlSpider   class MySpider(CrawlSpider):     handle_httpstatus_list = [404]   The handle_httpstatus_list key of Request.meta can also be used to specify which response codes to allow on a per-request basis. You can also set the meta key handle_httpstatus_all to True if you want to allow any response code for a request, and False to disable the effects of the handle_httpstatus_all key. Keep in mind, however, that it’s usually a bad idea to handle non-200 responses, unless you really know what you’re doing. For more information see: HTTP Status Code Definitions.  HttpErrorMiddleware settings  HTTPERROR_ALLOWED_CODES Default: [] Pass all responses with non-200 status codes contained in this list.   HTTPERROR_ALLOW_ALL Default: False Pass all responses, regardless of its status code.     OffsiteMiddleware   class scrapy.spidermiddlewares.offsite.OffsiteMiddleware[source] Filters out Requests for URLs outside the domains covered by the spider. This middleware filters out every request whose host names aren’t in the spider’s allowed_domains attribute. All subdomains of any domain in the list are also allowed. E.g. the rule www.example.org will also allow bob.www.example.org but not www2.example.com nor example.com. When your spider returns a request for a domain not belonging to those covered by the spider, this middleware will log a debug message similar to this one: DEBUG: Filtered offsite request to 'www.othersite.com': <GET http://www.othersite.com/some/page.html>   To avoid filling the log with too much noise, it will only print one of these messages for each new domain filtered. So, for example, if another request for www.othersite.com is filtered, no log message will be printed. But if a request for someothersite.com is filtered, a message will be printed (but only for the first request filtered). If the spider doesn’t define an allowed_domains attribute, or the attribute is empty, the offsite middleware will allow all requests. If the request has the dont_filter attribute set, the offsite middleware will allow the request even if its domain is not listed in allowed domains.     RefererMiddleware   class scrapy.spidermiddlewares.referer.RefererMiddleware[source] Populates Request Referer header, based on the URL of the Response which generated it.    RefererMiddleware settings  REFERER_ENABLED Default: True Whether to enable referer middleware.   REFERRER_POLICY Default: 'scrapy.spidermiddlewares.referer.DefaultReferrerPolicy' Referrer Policy to apply when populating Request “Referer” header.  Note You can also set the Referrer Policy per request, using the special \"referrer_policy\" Request.meta key, with the same acceptable values as for the REFERRER_POLICY setting.   Acceptable values for REFERRER_POLICY  either a path to a scrapy.spidermiddlewares.referer.ReferrerPolicy subclass — a custom policy or one of the built-in ones (see classes below), or one of the standard W3C-defined string values, or the special \"scrapy-default\".        String value Class name (as a string)    \"scrapy-default\" (default) scrapy.spidermiddlewares.referer.DefaultReferrerPolicy  “no-referrer” scrapy.spidermiddlewares.referer.NoReferrerPolicy  “no-referrer-when-downgrade” scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy  “same-origin” scrapy.spidermiddlewares.referer.SameOriginPolicy  “origin” scrapy.spidermiddlewares.referer.OriginPolicy  “strict-origin” scrapy.spidermiddlewares.referer.StrictOriginPolicy  “origin-when-cross-origin” scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy  “strict-origin-when-cross-origin” scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy  “unsafe-url” scrapy.spidermiddlewares.referer.UnsafeUrlPolicy      class scrapy.spidermiddlewares.referer.DefaultReferrerPolicy[source] A variant of “no-referrer-when-downgrade”, with the addition that “Referer” is not sent if the parent request was using file:// or s3:// scheme.    Warning Scrapy’s default referrer policy — just like “no-referrer-when-downgrade”, the W3C-recommended value for browsers — will send a non-empty “Referer” header from any http(s):// to any https:// URL, even if the domain is different. “same-origin” may be a better choice if you want to remove referrer information for cross-domain requests.    class scrapy.spidermiddlewares.referer.NoReferrerPolicy[source] https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer The simplest policy is “no-referrer”, which specifies that no referrer information is to be sent along with requests made from a particular request client to any origin. The header will be omitted entirely.     class scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy[source] https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade The “no-referrer-when-downgrade” policy sends a full URL along with requests from a TLS-protected environment settings object to a potentially trustworthy URL, and requests from clients which are not TLS-protected to any origin. Requests from TLS-protected clients to non-potentially trustworthy URLs, on the other hand, will contain no referrer information. A Referer HTTP header will not be sent. This is a user agent’s default behavior, if no policy is otherwise specified.    Note “no-referrer-when-downgrade” policy is the W3C-recommended default, and is used by major web browsers. However, it is NOT Scrapy’s default referrer policy (see DefaultReferrerPolicy).    class scrapy.spidermiddlewares.referer.SameOriginPolicy[source] https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin The “same-origin” policy specifies that a full URL, stripped for use as a referrer, is sent as referrer information when making same-origin requests from a particular request client. Cross-origin requests, on the other hand, will contain no referrer information. A Referer HTTP header will not be sent.     class scrapy.spidermiddlewares.referer.OriginPolicy[source] https://www.w3.org/TR/referrer-policy/#referrer-policy-origin The “origin” policy specifies that only the ASCII serialization of the origin of the request client is sent as referrer information when making both same-origin requests and cross-origin requests from a particular request client.     class scrapy.spidermiddlewares.referer.StrictOriginPolicy[source] https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin The “strict-origin” policy sends the ASCII serialization of the origin of the request client when making requests: - from a TLS-protected environment settings object to a potentially trustworthy URL, and - from non-TLS-protected environment settings objects to any origin. Requests from TLS-protected request clients to non- potentially trustworthy URLs, on the other hand, will contain no referrer information. A Referer HTTP header will not be sent.     class scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy[source] https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin The “origin-when-cross-origin” policy specifies that a full URL, stripped for use as a referrer, is sent as referrer information when making same-origin requests from a particular request client, and only the ASCII serialization of the origin of the request client is sent as referrer information when making cross-origin requests from a particular request client.     class scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy[source] https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin The “strict-origin-when-cross-origin” policy specifies that a full URL, stripped for use as a referrer, is sent as referrer information when making same-origin requests from a particular request client, and only the ASCII serialization of the origin of the request client when making cross-origin requests:  from a TLS-protected environment settings object to a potentially trustworthy URL, and from non-TLS-protected environment settings objects to any origin.  Requests from TLS-protected clients to non- potentially trustworthy URLs, on the other hand, will contain no referrer information. A Referer HTTP header will not be sent.     class scrapy.spidermiddlewares.referer.UnsafeUrlPolicy[source] https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url The “unsafe-url” policy specifies that a full URL, stripped for use as a referrer, is sent along with both cross-origin requests and same-origin requests made from a particular request client. Note: The policy’s name doesn’t lie; it is unsafe. This policy will leak origins and paths from TLS-protected resources to insecure origins. Carefully consider the impact of setting such a policy for potentially sensitive documents.    Warning “unsafe-url” policy is NOT recommended.       UrlLengthMiddleware   class scrapy.spidermiddlewares.urllength.UrlLengthMiddleware[source] Filters out requests with URLs longer than URLLENGTH_LIMIT The UrlLengthMiddleware can be configured through the following settings (see the settings documentation for more info):   URLLENGTH_LIMIT - The maximum URL length to allow for crawled URLs.                                ", "code_blocks": ["SPIDER_MIDDLEWARES = {\n    \"myproject.middlewares.CustomSpiderMiddleware\": 543,\n}\n</pre>", "SPIDER_MIDDLEWARES = {\n    \"myproject.middlewares.CustomSpiderMiddleware\": 543,\n    \"scrapy.spidermiddlewares.offsite.OffsiteMiddleware\": None,\n}\n</pre>", "from scrapy.spiders import CrawlSpider\n\n\nclass MySpider(CrawlSpider):\n    handle_httpstatus_list = [404]\n</pre>", "DEBUG: Filtered offsite request to 'www.othersite.com': <GET http://www.othersite.com/some/page.html>\n</pre>"], "links": [{"text": "Spiders", "href": "spiders.html#topics-spiders"}, {"text": "SPIDER_MIDDLEWARES", "href": "settings.html#std-setting-SPIDER_MIDDLEWARES"}, {"text": "SPIDER_MIDDLEWARES", "href": "settings.html#std-setting-SPIDER_MIDDLEWARES"}, {"text": "SPIDER_MIDDLEWARES_BASE", "href": "settings.html#std-setting-SPIDER_MIDDLEWARES_BASE"}, {"text": "SPIDER_MIDDLEWARES_BASE", "href": "settings.html#std-setting-SPIDER_MIDDLEWARES_BASE"}, {"text": "SPIDER_MIDDLEWARES_BASE", "href": "settings.html#std-setting-SPIDER_MIDDLEWARES_BASE"}, {"text": "SPIDER_MIDDLEWARES", "href": "settings.html#std-setting-SPIDER_MIDDLEWARES"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "settings", "href": "settings.html#topics-settings"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "item objects", "href": "items.html#topics-items"}, {"text": "asynchronous generator", "href": "https://docs.python.org/3/glossary.html#term-asynchronous-generator"}, {"text": "asynchronous iterable", "href": "https://docs.python.org/3/glossary.html#term-asynchronous-iterable"}, {"text": "asynchronous generator", "href": "https://docs.python.org/3/glossary.html#term-asynchronous-generator"}, {"text": "enforcing Scrapy 2.7", "href": "components.html#enforce-component-requirements"}, {"text": "making\nyour spider middleware universal", "href": "coroutines.html#universal-spider-middleware"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "item objects", "href": "items.html#topics-items"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "asynchronous generator", "href": "https://docs.python.org/3/glossary.html#term-asynchronous-generator"}, {"text": "asynchronous iterable", "href": "https://docs.python.org/3/glossary.html#term-asynchronous-iterable"}, {"text": "item", "href": "items.html#topics-items"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Exception", "href": "https://docs.python.org/3/library/exceptions.html#Exception"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "SPIDER_MIDDLEWARES_BASE", "href": "settings.html#std-setting-SPIDER_MIDDLEWARES_BASE"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/depth.html#DepthMiddleware"}, {"text": "DEPTH_LIMIT", "href": "settings.html#std-setting-DEPTH_LIMIT"}, {"text": "DEPTH_STATS_VERBOSE", "href": "settings.html#std-setting-DEPTH_STATS_VERBOSE"}, {"text": "DEPTH_PRIORITY", "href": "settings.html#std-setting-DEPTH_PRIORITY"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/httperror.html#HttpErrorMiddleware"}, {"text": "HTTP standard", "href": "https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html"}, {"text": "HTTP Status Code Definitions", "href": "https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/offsite.html#OffsiteMiddleware"}, {"text": "allowed_domains", "href": "spiders.html#scrapy.Spider.allowed_domains"}, {"text": "allowed_domains", "href": "spiders.html#scrapy.Spider.allowed_domains"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/referer.html#RefererMiddleware"}, {"text": "Referrer Policy", "href": "https://www.w3.org/TR/referrer-policy"}, {"text": "Request.meta", "href": "request-response.html#topics-request-meta"}, {"text": "“no-referrer”", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer"}, {"text": "“no-referrer-when-downgrade”", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade"}, {"text": "“same-origin”", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin"}, {"text": "“origin”", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-origin"}, {"text": "“strict-origin”", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin"}, {"text": "“origin-when-cross-origin”", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin"}, {"text": "“strict-origin-when-cross-origin”", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin"}, {"text": "“unsafe-url”", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/referer.html#DefaultReferrerPolicy"}, {"text": "“no-referrer-when-downgrade”", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade"}, {"text": "“same-origin”", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/referer.html#NoReferrerPolicy"}, {"text": "https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/referer.html#NoReferrerWhenDowngradePolicy"}, {"text": "https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/referer.html#SameOriginPolicy"}, {"text": "https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/referer.html#OriginPolicy"}, {"text": "https://www.w3.org/TR/referrer-policy/#referrer-policy-origin", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-origin"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/referer.html#StrictOriginPolicy"}, {"text": "https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/referer.html#OriginWhenCrossOriginPolicy"}, {"text": "https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/referer.html#StrictOriginWhenCrossOriginPolicy"}, {"text": "https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/referer.html#UnsafeUrlPolicy"}, {"text": "https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/urllength.html#UrlLengthMiddleware"}, {"text": "URLLENGTH_LIMIT", "href": "settings.html#std-setting-URLLENGTH_LIMIT"}], "timestamp": "2023-10-12T00:10:55.053561", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/downloader-middleware.html", "content": "                              Downloader Middleware The downloader middleware is a framework of hooks into Scrapy’s request/response processing.  It’s a light, low-level system for globally altering Scrapy’s requests and responses.  Activating a downloader middleware To activate a downloader middleware component, add it to the DOWNLOADER_MIDDLEWARES setting, which is a dict whose keys are the middleware class paths and their values are the middleware orders. Here’s an example: DOWNLOADER_MIDDLEWARES = {     \"myproject.middlewares.CustomDownloaderMiddleware\": 543, }   The DOWNLOADER_MIDDLEWARES setting is merged with the DOWNLOADER_MIDDLEWARES_BASE setting defined in Scrapy (and not meant to be overridden) and then sorted by order to get the final sorted list of enabled middlewares: the first middleware is the one closer to the engine and the last is the one closer to the downloader. In other words, the process_request() method of each middleware will be invoked in increasing middleware order (100, 200, 300, …) and the process_response() method of each middleware will be invoked in decreasing order. To decide which order to assign to your middleware see the DOWNLOADER_MIDDLEWARES_BASE setting and pick a value according to where you want to insert the middleware. The order does matter because each middleware performs a different action and your middleware could depend on some previous (or subsequent) middleware being applied. If you want to disable a built-in middleware (the ones defined in DOWNLOADER_MIDDLEWARES_BASE and enabled by default) you must define it in your project’s DOWNLOADER_MIDDLEWARES setting and assign None as its value.  For example, if you want to disable the user-agent middleware: DOWNLOADER_MIDDLEWARES = {     \"myproject.middlewares.CustomDownloaderMiddleware\": 543,     \"scrapy.downloadermiddlewares.useragent.UserAgentMiddleware\": None, }   Finally, keep in mind that some middlewares may need to be enabled through a particular setting. See each middleware documentation for more info.   Writing your own downloader middleware Each downloader middleware is a Python class that defines one or more of the methods defined below. The main entry point is the from_crawler class method, which receives a Crawler instance. The Crawler object gives you access, for example, to the settings.   class scrapy.downloadermiddlewares.DownloaderMiddleware  Note Any of the downloader middleware methods may also return a deferred.    process_request(request, spider) This method is called for each request that goes through the download middleware. process_request() should either: return None, return a Response object, return a Request object, or raise IgnoreRequest. If it returns None, Scrapy will continue processing this request, executing all other middlewares until, finally, the appropriate downloader handler is called the request performed (and its response downloaded). If it returns a Response object, Scrapy won’t bother calling any other process_request() or process_exception() methods, or the appropriate download function; it’ll return that response. The process_response() methods of installed middleware is always called on every response. If it returns a Request object, Scrapy will stop calling process_request() methods and reschedule the returned request. Once the newly returned request is performed, the appropriate middleware chain will be called on the downloaded response. If it raises an IgnoreRequest exception, the process_exception() methods of installed downloader middleware will be called. If none of them handle the exception, the errback function of the request (Request.errback) is called. If no code handles the raised exception, it is ignored and not logged (unlike other exceptions).  Parameters  request (Request object) – the request being processed spider (Spider object) – the spider for which this request is intended        process_response(request, response, spider) process_response() should either: return a Response object, return a Request object or raise a IgnoreRequest exception. If it returns a Response (it could be the same given response, or a brand-new one), that response will continue to be processed with the process_response() of the next middleware in the chain. If it returns a Request object, the middleware chain is halted and the returned request is rescheduled to be downloaded in the future. This is the same behavior as if a request is returned from process_request(). If it raises an IgnoreRequest exception, the errback function of the request (Request.errback) is called. If no code handles the raised exception, it is ignored and not logged (unlike other exceptions).  Parameters  request (is a Request object) – the request that originated the response response (Response object) – the response being processed spider (Spider object) – the spider for which this response is intended        process_exception(request, exception, spider) Scrapy calls process_exception() when a download handler or a process_request() (from a downloader middleware) raises an exception (including an IgnoreRequest exception) process_exception() should return: either None, a Response object, or a Request object. If it returns None, Scrapy will continue processing this exception, executing any other process_exception() methods of installed middleware, until no middleware is left and the default exception handling kicks in. If it returns a Response object, the process_response() method chain of installed middleware is started, and Scrapy won’t bother calling any other process_exception() methods of middleware. If it returns a Request object, the returned request is rescheduled to be downloaded in the future. This stops the execution of process_exception() methods of the middleware the same as returning a response would.  Parameters  request (is a Request object) – the request that generated the exception exception (an Exception object) – the raised exception spider (Spider object) – the spider for which this request is intended        from_crawler(cls, crawler) If present, this classmethod is called to create a middleware instance from a Crawler. It must return a new instance of the middleware. Crawler object provides access to all Scrapy core components like settings and signals; it is a way for middleware to access them and hook its functionality into Scrapy.  Parameters crawler (Crawler object) – crawler that uses this middleware         Built-in downloader middleware reference This page describes all downloader middleware components that come with Scrapy. For information on how to use them and how to write your own downloader middleware, see the downloader middleware usage guide. For a list of the components enabled by default (and their orders) see the DOWNLOADER_MIDDLEWARES_BASE setting.  CookiesMiddleware   class scrapy.downloadermiddlewares.cookies.CookiesMiddleware[source] This middleware enables working with sites that require cookies, such as those that use sessions. It keeps track of cookies sent by web servers, and sends them back on subsequent requests (from that spider), just like web browsers do.  Caution When non-UTF8 encoded byte sequences are passed to a Request, the CookiesMiddleware will log a warning. Refer to Advanced customization to customize the logging behaviour.   Caution Cookies set via the Cookie header are not considered by the CookiesMiddleware. If you need to set cookies for a request, use the Request.cookies parameter. This is a known current limitation that is being worked on.    The following settings can be used to configure the cookie middleware:  COOKIES_ENABLED COOKIES_DEBUG   Multiple cookie sessions per spider There is support for keeping multiple cookie sessions per spider by using the cookiejar Request meta key. By default it uses a single cookie jar (session), but you can pass an identifier to use different ones. For example: for i, url in enumerate(urls):     yield scrapy.Request(url, meta={\"cookiejar\": i}, callback=self.parse_page)   Keep in mind that the cookiejar meta key is not “sticky”. You need to keep passing it along on subsequent requests. For example: def parse_page(self, response):     # do some processing     return scrapy.Request(         \"http://www.example.com/otherpage\",         meta={\"cookiejar\": response.meta[\"cookiejar\"]},         callback=self.parse_other_page,     )     COOKIES_ENABLED Default: True Whether to enable the cookies middleware. If disabled, no cookies will be sent to web servers. Notice that despite the value of COOKIES_ENABLED setting if Request.meta['dont_merge_cookies'] evaluates to True the request cookies will not be sent to the web server and received cookies in Response will not be merged with the existing cookies. For more detailed information see the cookies parameter in Request.   COOKIES_DEBUG Default: False If enabled, Scrapy will log all cookies sent in requests (i.e. Cookie header) and all cookies received in responses (i.e. Set-Cookie header). Here’s an example of a log with COOKIES_DEBUG enabled: 2011-04-06 14:35:10-0300 [scrapy.core.engine] INFO: Spider opened 2011-04-06 14:35:10-0300 [scrapy.downloadermiddlewares.cookies] DEBUG: Sending cookies to: <GET http://www.diningcity.com/netherlands/index.html>         Cookie: clientlanguage_nl=en_EN 2011-04-06 14:35:14-0300 [scrapy.downloadermiddlewares.cookies] DEBUG: Received cookies from: <200 http://www.diningcity.com/netherlands/index.html>         Set-Cookie: JSESSIONID=B~FA4DC0C496C8762AE4F1A620EAB34F38; Path=/         Set-Cookie: ip_isocode=US         Set-Cookie: clientlanguage_nl=en_EN; Expires=Thu, 07-Apr-2011 21:21:34 GMT; Path=/ 2011-04-06 14:49:50-0300 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.diningcity.com/netherlands/index.html> (referer: None) [...]      DefaultHeadersMiddleware   class scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware[source] This middleware sets all default requests headers specified in the DEFAULT_REQUEST_HEADERS setting.     DownloadTimeoutMiddleware   class scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware[source] This middleware sets the download timeout for requests specified in the DOWNLOAD_TIMEOUT setting or download_timeout spider attribute.    Note You can also set download timeout per-request using download_timeout Request.meta key; this is supported even when DownloadTimeoutMiddleware is disabled.    HttpAuthMiddleware   class scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware[source] This middleware authenticates all requests generated from certain spiders using Basic access authentication (aka. HTTP auth). To enable HTTP authentication for a spider, set the http_user and http_pass spider attributes to the authentication data and the http_auth_domain spider attribute to the domain which requires this authentication (its subdomains will be also handled in the same way). You can set http_auth_domain to None to enable the authentication for all requests but you risk leaking your authentication credentials to unrelated domains.  Warning In previous Scrapy versions HttpAuthMiddleware sent the authentication data with all requests, which is a security problem if the spider makes requests to several different domains. Currently if the http_auth_domain attribute is not set, the middleware will use the domain of the first request, which will work for some spiders but not for others. In the future the middleware will produce an error instead.  Example: from scrapy.spiders import CrawlSpider   class SomeIntranetSiteSpider(CrawlSpider):     http_user = \"someuser\"     http_pass = \"somepass\"     http_auth_domain = \"intranet.example.com\"     name = \"intranet.example.com\"      # .. rest of the spider code omitted ...       HttpCacheMiddleware   class scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware[source] This middleware provides low-level cache to all HTTP requests and responses. It has to be combined with a cache storage backend as well as a cache policy. Scrapy ships with the following HTTP cache storage backends:   Filesystem storage backend (default) DBM storage backend   You can change the HTTP cache storage backend with the HTTPCACHE_STORAGE setting. Or you can also implement your own storage backend. Scrapy ships with two HTTP cache policies:   RFC2616 policy Dummy policy (default)   You can change the HTTP cache policy with the HTTPCACHE_POLICY setting. Or you can also implement your own policy. You can also avoid caching a response on every policy using dont_cache meta key equals True.    Dummy policy (default)   class scrapy.extensions.httpcache.DummyPolicy[source] This policy has no awareness of any HTTP Cache-Control directives. Every request and its corresponding response are cached.  When the same request is seen again, the response is returned without transferring anything from the Internet. The Dummy policy is useful for testing spiders faster (without having to wait for downloads every time) and for trying your spider offline, when an Internet connection is not available. The goal is to be able to “replay” a spider run exactly as it ran before.     RFC2616 policy   class scrapy.extensions.httpcache.RFC2616Policy[source] This policy provides a RFC2616 compliant HTTP cache, i.e. with HTTP Cache-Control awareness, aimed at production and used in continuous runs to avoid downloading unmodified data (to save bandwidth and speed up crawls). What is implemented:  Do not attempt to store responses/requests with no-store cache-control directive set Do not serve responses from cache if no-cache cache-control directive is set even for fresh responses Compute freshness lifetime from max-age cache-control directive Compute freshness lifetime from Expires response header Compute freshness lifetime from Last-Modified response header (heuristic used by Firefox) Compute current age from Age response header Compute current age from Date header Revalidate stale responses based on Last-Modified response header Revalidate stale responses based on ETag response header Set Date header for any received response missing it Support max-stale cache-control directive in requests  This allows spiders to be configured with the full RFC2616 cache policy, but avoid revalidation on a request-by-request basis, while remaining conformant with the HTTP spec. Example: Add Cache-Control: max-stale=600 to Request headers to accept responses that have exceeded their expiration time by no more than 600 seconds. See also: RFC2616, 14.9.3 What is missing:  Pragma: no-cache support https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1 Vary header support https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6 Invalidation after updates or deletes https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10 … probably others ..      Filesystem storage backend (default)   class scrapy.extensions.httpcache.FilesystemCacheStorage[source] File system storage backend is available for the HTTP cache middleware. Each request/response pair is stored in a different directory containing the following files:  request_body - the plain request body request_headers - the request headers (in raw HTTP format) response_body - the plain response body response_headers - the request headers (in raw HTTP format) meta - some metadata of this cache resource in Python repr() format (grep-friendly format) pickled_meta - the same metadata in meta but pickled for more efficient deserialization  The directory name is made from the request fingerprint (see scrapy.utils.request.fingerprint), and one level of subdirectories is used to avoid creating too many files into the same directory (which is inefficient in many file systems). An example directory could be: /path/to/cache/dir/example.com/72/72811f648e718090f041317756c03adb0ada46c7       DBM storage backend   class scrapy.extensions.httpcache.DbmCacheStorage[source] A DBM storage backend is also available for the HTTP cache middleware. By default, it uses the dbm, but you can change it with the HTTPCACHE_DBM_MODULE setting.     Writing your own storage backend You can implement a cache storage backend by creating a Python class that defines the methods described below.   class scrapy.extensions.httpcache.CacheStorage   open_spider(spider) This method gets called after a spider has been opened for crawling. It handles the open_spider signal.  Parameters spider (Spider object) – the spider which has been opened       close_spider(spider) This method gets called after a spider has been closed. It handles the close_spider signal.  Parameters spider (Spider object) – the spider which has been closed       retrieve_response(spider, request) Return response if present in cache, or None otherwise.  Parameters  spider (Spider object) – the spider which generated the request request (Request object) – the request to find cached response for        store_response(spider, request, response) Store the given response in the cache.  Parameters  spider (Spider object) – the spider for which the response is intended request (Request object) – the corresponding request the spider generated response (Response object) – the response to store in the cache        In order to use your storage backend, set:  HTTPCACHE_STORAGE to the Python import path of your custom storage class.    HTTPCache middleware settings The HttpCacheMiddleware can be configured through the following settings:  HTTPCACHE_ENABLED Default: False Whether the HTTP cache will be enabled.   HTTPCACHE_EXPIRATION_SECS Default: 0 Expiration time for cached requests, in seconds. Cached requests older than this time will be re-downloaded. If zero, cached requests will never expire.   HTTPCACHE_DIR Default: 'httpcache' The directory to use for storing the (low-level) HTTP cache. If empty, the HTTP cache will be disabled. If a relative path is given, is taken relative to the project data dir. For more info see: Default structure of Scrapy projects.   HTTPCACHE_IGNORE_HTTP_CODES Default: [] Don’t cache response with these HTTP codes.   HTTPCACHE_IGNORE_MISSING Default: False If enabled, requests not found in the cache will be ignored instead of downloaded.   HTTPCACHE_IGNORE_SCHEMES Default: ['file'] Don’t cache responses with these URI schemes.   HTTPCACHE_STORAGE Default: 'scrapy.extensions.httpcache.FilesystemCacheStorage' The class which implements the cache storage backend.   HTTPCACHE_DBM_MODULE Default: 'dbm' The database module to use in the DBM storage backend. This setting is specific to the DBM backend.   HTTPCACHE_POLICY Default: 'scrapy.extensions.httpcache.DummyPolicy' The class which implements the cache policy.   HTTPCACHE_GZIP Default: False If enabled, will compress all cached data with gzip. This setting is specific to the Filesystem backend.   HTTPCACHE_ALWAYS_STORE Default: False If enabled, will cache pages unconditionally. A spider may wish to have all responses available in the cache, for future use with Cache-Control: max-stale, for instance. The DummyPolicy caches all responses but never revalidates them, and sometimes a more nuanced policy is desirable. This setting still respects Cache-Control: no-store directives in responses. If you don’t want that, filter no-store out of the Cache-Control headers in responses you feed to the cache middleware.   HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS Default: [] List of Cache-Control directives in responses to be ignored. Sites often set “no-store”, “no-cache”, “must-revalidate”, etc., but get upset at the traffic a spider can generate if it actually respects those directives. This allows to selectively ignore Cache-Control directives that are known to be unimportant for the sites being crawled. We assume that the spider will not issue Cache-Control directives in requests unless it actually needs them, so directives in requests are not filtered.     HttpCompressionMiddleware   class scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware[source] This middleware allows compressed (gzip, deflate) traffic to be sent/received from web sites. This middleware also supports decoding brotli-compressed as well as zstd-compressed responses, provided that brotli or zstandard is installed, respectively.    HttpCompressionMiddleware Settings  COMPRESSION_ENABLED Default: True Whether the Compression middleware will be enabled.     HttpProxyMiddleware   class scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware[source] This middleware sets the HTTP proxy to use for requests, by setting the proxy meta value for Request objects. Like the Python standard library module urllib.request, it obeys the following environment variables:  http_proxy https_proxy no_proxy  You can also set the meta key proxy per-request, to a value like http://some_proxy_server:port or http://username:password@some_proxy_server:port. Keep in mind this value will take precedence over http_proxy/https_proxy environment variables, and it will also ignore no_proxy environment variable.     RedirectMiddleware   class scrapy.downloadermiddlewares.redirect.RedirectMiddleware[source] This middleware handles redirection of requests based on response status.   The urls which the request goes through (while being redirected) can be found in the redirect_urls Request.meta key. The reason behind each redirect in redirect_urls can be found in the redirect_reasons Request.meta key. For example: [301, 302, 307, 'meta refresh']. The format of a reason depends on the middleware that handled the corresponding redirect. For example, RedirectMiddleware indicates the triggering response status code as an integer, while MetaRefreshMiddleware always uses the 'meta refresh' string as reason. The RedirectMiddleware can be configured through the following settings (see the settings documentation for more info):  REDIRECT_ENABLED REDIRECT_MAX_TIMES  If Request.meta has dont_redirect key set to True, the request will be ignored by this middleware. If you want to handle some redirect status codes in your spider, you can specify these in the handle_httpstatus_list spider attribute. For example, if you want the redirect middleware to ignore 301 and 302 responses (and pass them through to your spider) you can do this: class MySpider(CrawlSpider):     handle_httpstatus_list = [301, 302]   The handle_httpstatus_list key of Request.meta can also be used to specify which response codes to allow on a per-request basis. You can also set the meta key handle_httpstatus_all to True if you want to allow any response code for a request.  RedirectMiddleware settings  REDIRECT_ENABLED Default: True Whether the Redirect middleware will be enabled.   REDIRECT_MAX_TIMES Default: 20 The maximum number of redirections that will be followed for a single request. After this maximum, the request’s response is returned as is.     MetaRefreshMiddleware   class scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware[source] This middleware handles redirection of requests based on meta-refresh html tag.   The MetaRefreshMiddleware can be configured through the following settings (see the settings documentation for more info):  METAREFRESH_ENABLED METAREFRESH_IGNORE_TAGS METAREFRESH_MAXDELAY  This middleware obey REDIRECT_MAX_TIMES setting, dont_redirect, redirect_urls and redirect_reasons request meta keys as described for RedirectMiddleware  MetaRefreshMiddleware settings  METAREFRESH_ENABLED Default: True Whether the Meta Refresh middleware will be enabled.   METAREFRESH_IGNORE_TAGS Default: [] Meta tags within these tags are ignored.  Changed in version 2.0: The default value of METAREFRESH_IGNORE_TAGS changed from ['script', 'noscript'] to [].    METAREFRESH_MAXDELAY Default: 100 The maximum meta-refresh delay (in seconds) to follow the redirection. Some sites use meta-refresh for redirecting to a session expired page, so we restrict automatic redirection to the maximum delay.     RetryMiddleware   class scrapy.downloadermiddlewares.retry.RetryMiddleware[source] A middleware to retry failed requests that are potentially caused by temporary problems such as a connection timeout or HTTP 500 error.   Failed pages are collected on the scraping process and rescheduled at the end, once the spider has finished crawling all regular (non failed) pages. The RetryMiddleware can be configured through the following settings (see the settings documentation for more info):  RETRY_ENABLED RETRY_TIMES RETRY_HTTP_CODES RETRY_EXCEPTIONS  If Request.meta has dont_retry key set to True, the request will be ignored by this middleware. To retry requests from a spider callback, you can use the get_retry_request() function:   scrapy.downloadermiddlewares.retry.get_retry_request(request: ~scrapy.http.request.Request, *, spider: ~scrapy.spiders.Spider, reason: ~typing.Union[str, Exception, ~typing.Type[Exception]] = 'unspecified', max_retry_times: ~typing.Optional[int] = None, priority_adjust: ~typing.Optional[int] = None, logger: ~logging.Logger = <Logger scrapy.downloadermiddlewares.retry (WARNING)>, stats_base_key: str = 'retry')[source] Returns a new Request object to retry the specified request, or None if retries of the specified request have been exhausted. For example, in a Spider callback, you could use it as follows: def parse(self, response):     if not response.text:         new_request_or_none = get_retry_request(             response.request,             spider=self,             reason='empty',         )         return new_request_or_none   spider is the Spider instance which is asking for the retry request. It is used to access the settings and stats, and to provide extra logging context (see logging.debug()). reason is a string or an Exception object that indicates the reason why the request needs to be retried. It is used to name retry stats. max_retry_times is a number that determines the maximum number of times that request can be retried. If not specified or None, the number is read from the max_retry_times meta key of the request. If the max_retry_times meta key is not defined or None, the number is read from the RETRY_TIMES setting. priority_adjust is a number that determines how the priority of the new request changes in relation to request. If not specified, the number is read from the RETRY_PRIORITY_ADJUST setting. logger is the logging.Logger object to be used when logging messages stats_base_key is a string to be used as the base key for the retry-related job stats    RetryMiddleware Settings  RETRY_ENABLED Default: True Whether the Retry middleware will be enabled.   RETRY_TIMES Default: 2 Maximum number of times to retry, in addition to the first download. Maximum number of retries can also be specified per-request using max_retry_times attribute of Request.meta. When initialized, the max_retry_times meta key takes higher precedence over the RETRY_TIMES setting.   RETRY_HTTP_CODES Default: [500, 502, 503, 504, 522, 524, 408, 429] Which HTTP response codes to retry. Other errors (DNS lookup issues, connections lost, etc) are always retried. In some cases you may want to add 400 to RETRY_HTTP_CODES because it is a common code used to indicate server overload. It is not included by default because HTTP specs say so.   RETRY_EXCEPTIONS Default: [     'twisted.internet.defer.TimeoutError',     'twisted.internet.error.TimeoutError',     'twisted.internet.error.DNSLookupError',     'twisted.internet.error.ConnectionRefusedError',     'twisted.internet.error.ConnectionDone',     'twisted.internet.error.ConnectError',     'twisted.internet.error.ConnectionLost',     'twisted.internet.error.TCPTimedOutError',     'twisted.web.client.ResponseFailed',     IOError,     'scrapy.core.downloader.handlers.http11.TunnelError', ]   List of exceptions to retry. Each list entry may be an exception type or its import path as a string. An exception will not be caught when the exception type is not in RETRY_EXCEPTIONS or when the maximum number of retries for a request has been exceeded (see RETRY_TIMES). To learn about uncaught exception propagation, see process_exception().   RETRY_PRIORITY_ADJUST Default: -1 Adjust retry request priority relative to original request:  a positive priority adjust means higher priority. a negative priority adjust (default) means lower priority.      RobotsTxtMiddleware   class scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware[source] This middleware filters out requests forbidden by the robots.txt exclusion standard. To make sure Scrapy respects robots.txt make sure the middleware is enabled and the ROBOTSTXT_OBEY setting is enabled. The ROBOTSTXT_USER_AGENT setting can be used to specify the user agent string to use for matching in the robots.txt file. If it is None, the User-Agent header you are sending with the request or the USER_AGENT setting (in that order) will be used for determining the user agent to use in the robots.txt file. This middleware has to be combined with a robots.txt parser. Scrapy ships with support for the following robots.txt parsers:  Protego (default) RobotFileParser Reppy Robotexclusionrulesparser  You can change the robots.txt parser with the ROBOTSTXT_PARSER setting. Or you can also implement support for a new parser.   If Request.meta has dont_obey_robotstxt key set to True the request will be ignored by this middleware even if ROBOTSTXT_OBEY is enabled. Parsers vary in several aspects:  Language of implementation Supported specification Support for wildcard matching Usage of length based rule: in particular for Allow and Disallow directives, where the most specific rule based on the length of the path trumps the less specific (shorter) rule  Performance comparison of different parsers is available at the following link.  Protego parser Based on Protego:  implemented in Python is compliant with Google’s Robots.txt Specification supports wildcard matching uses the length based rule  Scrapy uses this parser by default.   RobotFileParser Based on RobotFileParser:  is Python’s built-in robots.txt parser is compliant with Martijn Koster’s 1996 draft specification lacks support for wildcard matching doesn’t use the length based rule  It is faster than Protego and backward-compatible with versions of Scrapy before 1.8.0. In order to use this parser, set:  ROBOTSTXT_PARSER to scrapy.robotstxt.PythonRobotParser    Reppy parser Based on Reppy:  is a Python wrapper around Robots Exclusion Protocol Parser for C++ is compliant with Martijn Koster’s 1996 draft specification supports wildcard matching uses the length based rule  Native implementation, provides better speed than Protego. In order to use this parser:  Install Reppy by running pip install reppy   Warning Upstream issue #122 prevents reppy usage in Python 3.9+.    Set ROBOTSTXT_PARSER setting to scrapy.robotstxt.ReppyRobotParser    Robotexclusionrulesparser Based on Robotexclusionrulesparser:  implemented in Python is compliant with Martijn Koster’s 1996 draft specification supports wildcard matching doesn’t use the length based rule  In order to use this parser:  Install Robotexclusionrulesparser by running pip install robotexclusionrulesparser Set ROBOTSTXT_PARSER setting to scrapy.robotstxt.RerpRobotParser    Implementing support for a new parser You can implement support for a new robots.txt parser by subclassing the abstract base class RobotParser and implementing the methods described below.   class scrapy.robotstxt.RobotParser[source]   abstract allowed(url, user_agent)[source] Return True if  user_agent is allowed to crawl url, otherwise return False.  Parameters  url (str) – Absolute URL user_agent (str) – User agent        abstract classmethod from_crawler(crawler, robotstxt_body)[source] Parse the content of a robots.txt file as bytes. This must be a class method. It must return a new instance of the parser backend.  Parameters  crawler (Crawler instance) – crawler which made the request robotstxt_body (bytes) – content of a robots.txt file.           DownloaderStats   class scrapy.downloadermiddlewares.stats.DownloaderStats[source] Middleware that stores stats of all requests, responses and exceptions that pass through it. To use this middleware you must enable the DOWNLOADER_STATS setting.     UserAgentMiddleware   class scrapy.downloadermiddlewares.useragent.UserAgentMiddleware[source] Middleware that allows spiders to override the default user agent. In order for a spider to override the default user agent, its user_agent attribute must be set.     AjaxCrawlMiddleware   class scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware[source] Middleware that finds ‘AJAX crawlable’ page variants based on meta-fragment html tag. See https://developers.google.com/search/docs/ajax-crawling/docs/getting-started for more info.  Note Scrapy finds ‘AJAX crawlable’ pages for URLs like 'http://example.com/!#foo=bar' even without this middleware. AjaxCrawlMiddleware is necessary when URL doesn’t contain '!#'. This is often a case for ‘index’ or ‘main’ website pages.     AjaxCrawlMiddleware Settings  AJAXCRAWL_ENABLED Default: False Whether the AjaxCrawlMiddleware will be enabled. You may want to enable it for broad crawls.    HttpProxyMiddleware settings  HTTPPROXY_ENABLED Default: True Whether or not to enable the HttpProxyMiddleware.   HTTPPROXY_AUTH_ENCODING Default: \"latin-1\" The default encoding for proxy authentication on HttpProxyMiddleware.                              ", "code_blocks": ["DOWNLOADER_MIDDLEWARES = {\n    \"myproject.middlewares.CustomDownloaderMiddleware\": 543,\n}\n</pre>", "DOWNLOADER_MIDDLEWARES = {\n    \"myproject.middlewares.CustomDownloaderMiddleware\": 543,\n    \"scrapy.downloadermiddlewares.useragent.UserAgentMiddleware\": None,\n}\n</pre>", "for i, url in enumerate(urls):\n    yield scrapy.Request(url, meta={\"cookiejar\": i}, callback=self.parse_page)\n</pre>", "def parse_page(self, response):\n    # do some processing\n    return scrapy.Request(\n        \"http://www.example.com/otherpage\",\n        meta={\"cookiejar\": response.meta[\"cookiejar\"]},\n        callback=self.parse_other_page,\n    )\n</pre>", "2011-04-06 14:35:10-0300 [scrapy.core.engine] INFO: Spider opened\n2011-04-06 14:35:10-0300 [scrapy.downloadermiddlewares.cookies] DEBUG: Sending cookies to: <GET http://www.diningcity.com/netherlands/index.html>\n        Cookie: clientlanguage_nl=en_EN\n2011-04-06 14:35:14-0300 [scrapy.downloadermiddlewares.cookies] DEBUG: Received cookies from: <200 http://www.diningcity.com/netherlands/index.html>\n        Set-Cookie: JSESSIONID=B~FA4DC0C496C8762AE4F1A620EAB34F38; Path=/\n        Set-Cookie: ip_isocode=US\n        Set-Cookie: clientlanguage_nl=en_EN; Expires=Thu, 07-Apr-2011 21:21:34 GMT; Path=/\n2011-04-06 14:49:50-0300 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.diningcity.com/netherlands/index.html> (referer: None)\n[...]\n</pre>", "from scrapy.spiders import CrawlSpider\n\n\nclass SomeIntranetSiteSpider(CrawlSpider):\n    http_user = \"someuser\"\n    http_pass = \"somepass\"\n    http_auth_domain = \"intranet.example.com\"\n    name = \"intranet.example.com\"\n\n    # .. rest of the spider code omitted ...\n</pre>", "/path/to/cache/dir/example.com/72/72811f648e718090f041317756c03adb0ada46c7\n</pre>", "class MySpider(CrawlSpider):\n    handle_httpstatus_list = [301, 302]\n</pre>", "def parse(self, response):\n    if not response.text:\n        new_request_or_none = get_retry_request(\n            response.request,\n            spider=self,\n            reason='empty',\n        )\n        return new_request_or_none\n</pre>", "[\n    'twisted.internet.defer.TimeoutError',\n    'twisted.internet.error.TimeoutError',\n    'twisted.internet.error.DNSLookupError',\n    'twisted.internet.error.ConnectionRefusedError',\n    'twisted.internet.error.ConnectionDone',\n    'twisted.internet.error.ConnectError',\n    'twisted.internet.error.ConnectionLost',\n    'twisted.internet.error.TCPTimedOutError',\n    'twisted.web.client.ResponseFailed',\n    IOError,\n    'scrapy.core.downloader.handlers.http11.TunnelError',\n]\n</pre>"], "links": [{"text": "DOWNLOADER_MIDDLEWARES", "href": "settings.html#std-setting-DOWNLOADER_MIDDLEWARES"}, {"text": "DOWNLOADER_MIDDLEWARES", "href": "settings.html#std-setting-DOWNLOADER_MIDDLEWARES"}, {"text": "DOWNLOADER_MIDDLEWARES_BASE", "href": "settings.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE"}, {"text": "DOWNLOADER_MIDDLEWARES_BASE", "href": "settings.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE"}, {"text": "DOWNLOADER_MIDDLEWARES_BASE", "href": "settings.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE"}, {"text": "DOWNLOADER_MIDDLEWARES", "href": "settings.html#std-setting-DOWNLOADER_MIDDLEWARES"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "settings", "href": "settings.html#topics-settings"}, {"text": "Request", "href": "request-response.html#scrapy.http.Request"}, {"text": "IgnoreRequest", "href": "exceptions.html#scrapy.exceptions.IgnoreRequest"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "IgnoreRequest", "href": "exceptions.html#scrapy.exceptions.IgnoreRequest"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "IgnoreRequest", "href": "exceptions.html#scrapy.exceptions.IgnoreRequest"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "IgnoreRequest", "href": "exceptions.html#scrapy.exceptions.IgnoreRequest"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "IgnoreRequest", "href": "exceptions.html#scrapy.exceptions.IgnoreRequest"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "DOWNLOADER_MIDDLEWARES_BASE", "href": "settings.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/cookies.html#CookiesMiddleware"}, {"text": "Advanced customization", "href": "logging.html#topics-logging-advanced-customization"}, {"text": "meta['dont_merge_cookies']", "href": "request-response.html#std-reqmeta-dont_merge_cookies"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/defaultheaders.html#DefaultHeadersMiddleware"}, {"text": "DEFAULT_REQUEST_HEADERS", "href": "settings.html#std-setting-DEFAULT_REQUEST_HEADERS"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/downloadtimeout.html#DownloadTimeoutMiddleware"}, {"text": "DOWNLOAD_TIMEOUT", "href": "settings.html#std-setting-DOWNLOAD_TIMEOUT"}, {"text": "download_timeout", "href": "request-response.html#std-reqmeta-download_timeout"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/httpauth.html#HttpAuthMiddleware"}, {"text": "Basic access authentication", "href": "https://en.wikipedia.org/wiki/Basic_access_authentication"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/httpcache.html#HttpCacheMiddleware"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/httpcache.html#DummyPolicy"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/httpcache.html#RFC2616Policy"}, {"text": "https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1", "href": "https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1"}, {"text": "https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6", "href": "https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6"}, {"text": "https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10", "href": "https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/httpcache.html#FilesystemCacheStorage"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/httpcache.html#DbmCacheStorage"}, {"text": "DBM", "href": "https://en.wikipedia.org/wiki/Dbm"}, {"text": "dbm", "href": "https://docs.python.org/3/library/dbm.html#module-dbm"}, {"text": "open_spider", "href": "signals.html#std-signal-spider_opened"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "close_spider", "href": "signals.html#std-signal-spider_closed"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Default structure of Scrapy projects", "href": "commands.html#topics-project-structure"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/httpcompression.html#HttpCompressionMiddleware"}, {"text": "brotli-compressed", "href": "https://www.ietf.org/rfc/rfc7932.txt"}, {"text": "zstd-compressed", "href": "https://www.ietf.org/rfc/rfc8478.txt"}, {"text": "brotli", "href": "https://pypi.org/project/Brotli/"}, {"text": "zstandard", "href": "https://pypi.org/project/zstandard/"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/httpproxy.html#HttpProxyMiddleware"}, {"text": "urllib.request", "href": "https://docs.python.org/3/library/urllib.request.html#module-urllib.request"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/redirect.html#RedirectMiddleware"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/redirect.html#MetaRefreshMiddleware"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/retry.html#RetryMiddleware"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/retry.html#get_retry_request"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "settings", "href": "settings.html#topics-settings"}, {"text": "stats", "href": "stats.html#topics-stats"}, {"text": "logging.debug()", "href": "https://docs.python.org/3/library/logging.html#logging.debug"}, {"text": "Exception", "href": "https://docs.python.org/3/library/exceptions.html#Exception"}, {"text": "max_retry_times", "href": "request-response.html#std-reqmeta-max_retry_times"}, {"text": "max_retry_times", "href": "request-response.html#std-reqmeta-max_retry_times"}, {"text": "max_retry_times", "href": "request-response.html#std-reqmeta-max_retry_times"}, {"text": "max_retry_times", "href": "request-response.html#std-reqmeta-max_retry_times"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/robotstxt.html#RobotsTxtMiddleware"}, {"text": "ROBOTSTXT_OBEY", "href": "settings.html#std-setting-ROBOTSTXT_OBEY"}, {"text": "ROBOTSTXT_USER_AGENT", "href": "settings.html#std-setting-ROBOTSTXT_USER_AGENT"}, {"text": "robots.txt", "href": "https://www.robotstxt.org/"}, {"text": "USER_AGENT", "href": "settings.html#std-setting-USER_AGENT"}, {"text": "robots.txt", "href": "https://www.robotstxt.org/"}, {"text": "robots.txt", "href": "https://www.robotstxt.org/"}, {"text": "robots.txt", "href": "https://www.robotstxt.org/"}, {"text": "robots.txt", "href": "https://www.robotstxt.org/"}, {"text": "ROBOTSTXT_PARSER", "href": "settings.html#std-setting-ROBOTSTXT_PARSER"}, {"text": "ROBOTSTXT_OBEY", "href": "settings.html#std-setting-ROBOTSTXT_OBEY"}, {"text": "length based rule", "href": "https://developers.google.com/search/reference/robots_txt#order-of-precedence-for-group-member-lines"}, {"text": "the following link", "href": "https://github.com/scrapy/scrapy/issues/3969"}, {"text": "Protego", "href": "https://github.com/scrapy/protego"}, {"text": "Google’s Robots.txt Specification", "href": "https://developers.google.com/search/reference/robots_txt"}, {"text": "RobotFileParser", "href": "https://docs.python.org/3/library/urllib.robotparser.html#urllib.robotparser.RobotFileParser"}, {"text": "robots.txt", "href": "https://www.robotstxt.org/"}, {"text": "Martijn Koster’s 1996 draft specification", "href": "https://www.robotstxt.org/norobots-rfc.txt"}, {"text": "ROBOTSTXT_PARSER", "href": "settings.html#std-setting-ROBOTSTXT_PARSER"}, {"text": "Reppy", "href": "https://github.com/seomoz/reppy/"}, {"text": "Robots Exclusion Protocol Parser for C++", "href": "https://github.com/seomoz/rep-cpp"}, {"text": "Martijn Koster’s 1996 draft specification", "href": "https://www.robotstxt.org/norobots-rfc.txt"}, {"text": "Reppy", "href": "https://github.com/seomoz/reppy/"}, {"text": "Upstream issue #122", "href": "https://github.com/seomoz/reppy/issues/122"}, {"text": "ROBOTSTXT_PARSER", "href": "settings.html#std-setting-ROBOTSTXT_PARSER"}, {"text": "Robotexclusionrulesparser", "href": "http://nikitathespider.com/python/rerp/"}, {"text": "Martijn Koster’s 1996 draft specification", "href": "https://www.robotstxt.org/norobots-rfc.txt"}, {"text": "Robotexclusionrulesparser", "href": "http://nikitathespider.com/python/rerp/"}, {"text": "ROBOTSTXT_PARSER", "href": "settings.html#std-setting-ROBOTSTXT_PARSER"}, {"text": "robots.txt", "href": "https://www.robotstxt.org/"}, {"text": "[source]", "href": "../_modules/scrapy/robotstxt.html#RobotParser"}, {"text": "[source]", "href": "../_modules/scrapy/robotstxt.html#RobotParser.allowed"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "[source]", "href": "../_modules/scrapy/robotstxt.html#RobotParser.from_crawler"}, {"text": "robots.txt", "href": "https://www.robotstxt.org/"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "bytes", "href": "https://docs.python.org/3/library/stdtypes.html#bytes"}, {"text": "robots.txt", "href": "https://www.robotstxt.org/"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/stats.html#DownloaderStats"}, {"text": "DOWNLOADER_STATS", "href": "settings.html#std-setting-DOWNLOADER_STATS"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/useragent.html#UserAgentMiddleware"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/ajaxcrawl.html#AjaxCrawlMiddleware"}, {"text": "https://developers.google.com/search/docs/ajax-crawling/docs/getting-started", "href": "https://developers.google.com/search/docs/ajax-crawling/docs/getting-started"}, {"text": "broad crawls", "href": "broad-crawls.html#topics-broad-crawls"}], "timestamp": "2023-10-12T00:10:59.296411", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/addons.html", "content": "                              Add-ons Scrapy’s add-on system is a framework which unifies managing and configuring components that extend Scrapy’s core functionality, such as middlewares, extensions, or pipelines. It provides users with a plug-and-play experience in Scrapy extension management, and grants extensive configuration control to developers.  Activating and configuring add-ons During Crawler initialization, the list of enabled add-ons is read from your ADDONS setting. The ADDONS setting is a dict in which every key is an add-on class or its import path and the value is its priority. This is an example where two add-ons are enabled in a project’s settings.py: ADDONS = {     'path.to.someaddon': 0,     SomeAddonClass: 1, }     Writing your own add-ons Add-ons are Python classes that include the following method:   update_settings(settings) This method is called during the initialization of the Crawler. Here, you should perform dependency checks (e.g. for external Python libraries) and update the Settings object as wished, e.g. enable components for this add-on or set required configuration of other extensions.  Parameters settings (Settings) – The settings object storing Scrapy/component configuration     They can also have the following method:   classmethod from_crawler(cls, crawler) If present, this class method is called to create an add-on instance from a Crawler. It must return a new instance of the add-on. The crawler object provides access to all Scrapy core components like settings and signals; it is a way for the add-on to access them and hook its functionality into Scrapy.  Parameters crawler (Crawler) – The crawler that uses this add-on     The settings set by the add-on should use the addon priority (see Populating the settings and scrapy.settings.BaseSettings.set()): class MyAddon:     def update_settings(self, settings):         settings.set(\"DNSCACHE_ENABLED\", True, \"addon\")   This allows users to override these settings in the project or spider configuration. This is not possible with settings that are mutable objects, such as the dict that is a value of ITEM_PIPELINES. In these cases you can provide an add-on-specific setting that governs whether the add-on will modify ITEM_PIPELINES: class MyAddon:     def update_settings(self, settings):         if settings.getbool(\"MYADDON_ENABLE_PIPELINE\"):             settings[\"ITEM_PIPELINES\"][\"path.to.mypipeline\"] = 200   If the update_settings method raises scrapy.exceptions.NotConfigured, the add-on will be skipped. This makes it easy to enable an add-on only when some conditions are met.  Fallbacks Some components provided by add-ons need to fall back to “default” implementations, e.g. a custom download handler needs to send the request that it doesn’t handle via the default download handler, or a stats collector that includes some additional processing but otherwise uses the default stats collector. And it’s possible that a project needs to use several custom components of the same type, e.g. two custom download handlers that support different kinds of custom requests and still need to use the default download handler for other requests. To make such use cases easier to configure, we recommend that such custom components should be written in the following way:  The custom component (e.g. MyDownloadHandler) shouldn’t inherit from the default Scrapy one (e.g. scrapy.core.downloader.handlers.http.HTTPDownloadHandler), but instead be able to load the class of the fallback component from a special setting (e.g. MY_FALLBACK_DOWNLOAD_HANDLER), create an instance of it and use it. The add-ons that include these components should read the current value of the default setting (e.g. DOWNLOAD_HANDLERS) in their update_settings() methods, save that value into the fallback setting (MY_FALLBACK_DOWNLOAD_HANDLER mentioned earlier) and set the default setting to the component provided by the add-on (e.g. MyDownloadHandler). If the fallback setting is already set by the user, they shouldn’t change it. This way, if there are several add-ons that want to modify the same setting, all of them will fallback to the component from the previous one and then to the Scrapy default. The order of that depends on the priority order in the ADDONS setting.     Add-on examples Set some basic configuration: class MyAddon:     def update_settings(self, settings):         settings[\"ITEM_PIPELINES\"][\"path.to.mypipeline\"] = 200         settings.set(\"DNSCACHE_ENABLED\", True, \"addon\")   Check dependencies: class MyAddon:     def update_settings(self, settings):         try:             import boto         except ImportError:             raise NotConfigured(\"MyAddon requires the boto library\")         ...   Access the crawler instance: class MyAddon:     def __init__(self, crawler) -> None:         super().__init__()         self.crawler = crawler      @classmethod     def from_crawler(cls, crawler):         return cls(crawler)      def update_settings(self, settings):         ...   Use a fallback component: from scrapy.core.downloader.handlers.http import HTTPDownloadHandler   FALLBACK_SETTING = \"MY_FALLBACK_DOWNLOAD_HANDLER\"   class MyHandler:     lazy = False      def __init__(self, settings, crawler):         dhcls = load_object(settings.get(FALLBACK_SETTING))         self._fallback_handler = create_instance(             dhcls,             settings=None,             crawler=crawler,         )      def download_request(self, request, spider):         if request.meta.get(\"my_params\"):             # handle the request             ...         else:             return self._fallback_handler.download_request(request, spider)   class MyAddon:     def update_settings(self, settings):         if not settings.get(FALLBACK_SETTING):             settings.set(                 FALLBACK_SETTING,                 settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"],                 \"addon\",             )         settings[\"DOWNLOAD_HANDLERS\"][\"https\"] = MyHandler                             ", "code_blocks": ["ADDONS = {\n    'path.to.someaddon': 0,\n    SomeAddonClass: 1,\n}\n</pre>", "class MyAddon:\n    def update_settings(self, settings):\n        settings.set(\"DNSCACHE_ENABLED\", True, \"addon\")\n</pre>", "class MyAddon:\n    def update_settings(self, settings):\n        if settings.getbool(\"MYADDON_ENABLE_PIPELINE\"):\n            settings[\"ITEM_PIPELINES\"][\"path.to.mypipeline\"] = 200\n</pre>", "class MyAddon:\n    def update_settings(self, settings):\n        settings[\"ITEM_PIPELINES\"][\"path.to.mypipeline\"] = 200\n        settings.set(\"DNSCACHE_ENABLED\", True, \"addon\")\n</pre>", "class MyAddon:\n    def update_settings(self, settings):\n        try:\n            import boto\n        except ImportError:\n            raise NotConfigured(\"MyAddon requires the boto library\")\n        ...\n</pre>", "class MyAddon:\n    def __init__(self, crawler) -> None:\n        super().__init__()\n        self.crawler = crawler\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(crawler)\n\n    def update_settings(self, settings):\n        ...\n</pre>", "from scrapy.core.downloader.handlers.http import HTTPDownloadHandler\n\n\nFALLBACK_SETTING = \"MY_FALLBACK_DOWNLOAD_HANDLER\"\n\n\nclass MyHandler:\n    lazy = False\n\n    def __init__(self, settings, crawler):\n        dhcls = load_object(settings.get(FALLBACK_SETTING))\n        self._fallback_handler = create_instance(\n            dhcls,\n            settings=None,\n            crawler=crawler,\n        )\n\n    def download_request(self, request, spider):\n        if request.meta.get(\"my_params\"):\n            # handle the request\n            ...\n        else:\n            return self._fallback_handler.download_request(request, spider)\n\n\nclass MyAddon:\n    def update_settings(self, settings):\n        if not settings.get(FALLBACK_SETTING):\n            settings.set(\n                FALLBACK_SETTING,\n                settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"],\n                \"addon\",\n            )\n        settings[\"DOWNLOAD_HANDLERS\"][\"https\"] = MyHandler\n</pre>"], "links": [{"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "Settings", "href": "api.html#scrapy.settings.Settings"}, {"text": "Settings", "href": "api.html#scrapy.settings.Settings"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "Populating the settings", "href": "settings.html#populating-settings"}, {"text": "scrapy.settings.BaseSettings.set()", "href": "api.html#scrapy.settings.BaseSettings.set"}, {"text": "ITEM_PIPELINES", "href": "settings.html#std-setting-ITEM_PIPELINES"}, {"text": "ITEM_PIPELINES", "href": "settings.html#std-setting-ITEM_PIPELINES"}, {"text": "scrapy.exceptions.NotConfigured", "href": "exceptions.html#scrapy.exceptions.NotConfigured"}], "timestamp": "2023-10-12T00:11:03.414572", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/architecture.html", "content": "                              Architecture overview This document describes the architecture of Scrapy and how its components interact.  Overview The following diagram shows an overview of the Scrapy architecture with its components and an outline of the data flow that takes place inside the system (shown by the red arrows). A brief description of the components is included below with links for more detailed information about them. The data flow is also described below.   Data flow  The data flow in Scrapy is controlled by the execution engine, and goes like this:  The Engine gets the initial Requests to crawl from the Spider. The Engine schedules the Requests in the Scheduler and asks for the next Requests to crawl. The Scheduler returns the next Requests to the Engine. The Engine sends the Requests to the Downloader, passing through the Downloader Middlewares (see process_request()). Once the page finishes downloading the Downloader generates a Response (with that page) and sends it to the Engine, passing through the Downloader Middlewares (see process_response()). The Engine receives the Response from the Downloader and sends it to the Spider for processing, passing through the Spider Middleware (see process_spider_input()). The Spider processes the Response and returns scraped items and new Requests (to follow) to the Engine, passing through the Spider Middleware (see process_spider_output()). The Engine sends processed items to Item Pipelines, then send processed Requests to the Scheduler and asks for possible next Requests to crawl. The process repeats (from step 3) until there are no more requests from the Scheduler.    Components  Scrapy Engine The engine is responsible for controlling the data flow between all components of the system, and triggering events when certain actions occur. See the Data Flow section above for more details.   Scheduler The scheduler receives requests from the engine and enqueues them for feeding them later (also to the engine) when the engine requests them.   Downloader The Downloader is responsible for fetching web pages and feeding them to the engine which, in turn, feeds them to the spiders.   Spiders Spiders are custom classes written by Scrapy users to parse responses and extract items from them or additional requests to follow. For more information see Spiders.   Item Pipeline The Item Pipeline is responsible for processing the items once they have been extracted (or scraped) by the spiders. Typical tasks include cleansing, validation and persistence (like storing the item in a database). For more information see Item Pipeline.   Downloader middlewares Downloader middlewares are specific hooks that sit between the Engine and the Downloader and process requests when they pass from the Engine to the Downloader, and responses that pass from Downloader to the Engine. Use a Downloader middleware if you need to do one of the following:  process a request just before it is sent to the Downloader (i.e. right before Scrapy sends the request to the website); change received response before passing it to a spider; send a new Request instead of passing received response to a spider; pass response to a spider without fetching a web page; silently drop some requests.  For more information see Downloader Middleware.   Spider middlewares Spider middlewares are specific hooks that sit between the Engine and the Spiders and are able to process spider input (responses) and output (items and requests). Use a Spider middleware if you need to  post-process output of spider callbacks - change/add/remove requests or items; post-process start_requests; handle spider exceptions; call errback instead of callback for some of the requests based on response content.  For more information see Spider Middleware.    Event-driven networking Scrapy is written with Twisted, a popular event-driven networking framework for Python. Thus, it’s implemented using a non-blocking (aka asynchronous) code for concurrency. For more information about asynchronous programming and Twisted see these links:  Introduction to Deferreds Twisted - hello, asynchronous programming Twisted Introduction - Krondo                            ", "code_blocks": [], "links": [{"text": "", "href": "../_images/scrapy_architecture_02.png"}, {"text": "process_request()", "href": "downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"}, {"text": "process_response()", "href": "downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"}, {"text": "process_spider_input()", "href": "spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input"}, {"text": "process_spider_output()", "href": "spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"}, {"text": "scheduler", "href": "scheduler.html#topics-scheduler"}, {"text": "items", "href": "items.html#topics-items"}, {"text": "Spiders", "href": "spiders.html#topics-spiders"}, {"text": "Item Pipeline", "href": "item-pipeline.html#topics-item-pipeline"}, {"text": "Downloader Middleware", "href": "downloader-middleware.html#topics-downloader-middleware"}, {"text": "Spider Middleware", "href": "spider-middleware.html#topics-spider-middleware"}, {"text": "Twisted", "href": "https://twistedmatrix.com/trac/"}, {"text": "Introduction to Deferreds", "href": "https://docs.twisted.org/en/stable/core/howto/defer-intro.html"}, {"text": "Twisted - hello, asynchronous programming", "href": "http://jessenoller.com/blog/2009/02/11/twisted-hello-asynchronous-programming/"}, {"text": "Twisted Introduction - Krondo", "href": "http://krondo.com/an-introduction-to-asynchronous-programming-and-twisted/"}], "timestamp": "2023-10-12T00:11:07.219473", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/asyncio.html", "content": "                              asyncio  New in version 2.0.  Scrapy has partial support for asyncio. After you install the asyncio reactor, you may use asyncio and asyncio-powered libraries in any coroutine.  Installing the asyncio reactor To enable asyncio support, set the TWISTED_REACTOR setting to 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'. If you are using CrawlerRunner, you also need to install the AsyncioSelectorReactor reactor manually. You can do that using install_reactor(): install_reactor('twisted.internet.asyncioreactor.AsyncioSelectorReactor')     Handling a pre-installed reactor twisted.internet.reactor and some other Twisted imports install the default Twisted reactor as a side effect. Once a Twisted reactor is installed, it is not possible to switch to a different reactor at run time. If you configure the asyncio Twisted reactor and, at run time, Scrapy complains that a different reactor is already installed, chances are you have some such imports in your code. You can usually fix the issue by moving those offending module-level Twisted imports to the method or function definitions where they are used. For example, if you have something like: from twisted.internet import reactor   def my_function():     reactor.callLater(...)   Switch to something like: def my_function():     from twisted.internet import reactor      reactor.callLater(...)   Alternatively, you can try to manually install the asyncio reactor, with install_reactor(), before those imports happen.   Awaiting on Deferreds When the asyncio reactor isn’t installed, you can await on Deferreds in the coroutines directly. When it is installed, this is not possible anymore, due to specifics of the Scrapy coroutine integration (the coroutines are wrapped into asyncio.Future objects, not into Deferred directly), and you need to wrap them into Futures. Scrapy provides two helpers for this:   scrapy.utils.defer.deferred_to_future(d: Deferred) → Future[source]  New in version 2.6.0.  Return an asyncio.Future object that wraps d. When using the asyncio reactor, you cannot await on Deferred objects from Scrapy callables defined as coroutines, you can only await on Future objects. Wrapping Deferred objects into Future objects allows you to wait on them: class MySpider(Spider):     ...     async def parse(self, response):         additional_request = scrapy.Request('https://example.org/price')         deferred = self.crawler.engine.download(additional_request)         additional_response = await deferred_to_future(deferred)       scrapy.utils.defer.maybe_deferred_to_future(d: Deferred) → Union[Deferred, Future][source]  New in version 2.6.0.  Return d as an object that can be awaited from a Scrapy callable defined as a coroutine. What you can await in Scrapy callables defined as coroutines depends on the value of TWISTED_REACTOR:  When not using the asyncio reactor, you can only await on Deferred objects. When using the asyncio reactor, you can only await on asyncio.Future objects.  If you want to write code that uses Deferred objects but works with any reactor, use this function on all Deferred objects: class MySpider(Spider):     ...     async def parse(self, response):         additional_request = scrapy.Request('https://example.org/price')         deferred = self.crawler.engine.download(additional_request)         additional_response = await maybe_deferred_to_future(deferred)      Tip If you need to use these functions in code that aims to be compatible with lower versions of Scrapy that do not provide these functions, down to Scrapy 2.0 (earlier versions do not support asyncio), you can copy the implementation of these functions into your own code.    Enforcing asyncio as a requirement If you are writing a component that requires asyncio to work, use scrapy.utils.reactor.is_asyncio_reactor_installed() to enforce it as a requirement. For example: from scrapy.utils.reactor import is_asyncio_reactor_installed   class MyComponent:     def __init__(self):         if not is_asyncio_reactor_installed():             raise ValueError(                 f\"{MyComponent.__qualname__} requires the asyncio Twisted \"                 f\"reactor. Make sure you have it configured in the \"                 f\"TWISTED_REACTOR setting. See the asyncio documentation \"                 f\"of Scrapy for more information.\"             )     Windows-specific notes The Windows implementation of asyncio can use two event loop implementations, ProactorEventLoop (default) and SelectorEventLoop. However, only SelectorEventLoop works with Twisted. Scrapy changes the event loop class to SelectorEventLoop automatically when you change the TWISTED_REACTOR setting or call install_reactor().  Note Other libraries you use may require ProactorEventLoop, e.g. because it supports subprocesses (this is the case with playwright), so you cannot use them together with Scrapy on Windows (but you should be able to use them on WSL or native Linux).    Using custom asyncio loops You can also use custom asyncio event loops with the asyncio reactor. Set the ASYNCIO_EVENT_LOOP setting to the import path of the desired event loop class to use it instead of the default asyncio event loop.                           ", "code_blocks": ["install_reactor('twisted.internet.asyncioreactor.AsyncioSelectorReactor')\n</pre>", "from twisted.internet import reactor\n\n\ndef my_function():\n    reactor.callLater(...)\n</pre>", "def my_function():\n    from twisted.internet import reactor\n\n    reactor.callLater(...)\n</pre>", "class MySpider(Spider):\n    ...\n    async def parse(self, response):\n        additional_request = scrapy.Request('https://example.org/price')\n        deferred = self.crawler.engine.download(additional_request)\n        additional_response = await deferred_to_future(deferred)\n</pre>", "class MySpider(Spider):\n    ...\n    async def parse(self, response):\n        additional_request = scrapy.Request('https://example.org/price')\n        deferred = self.crawler.engine.download(additional_request)\n        additional_response = await maybe_deferred_to_future(deferred)\n</pre>", "from scrapy.utils.reactor import is_asyncio_reactor_installed\n\n\nclass MyComponent:\n    def __init__(self):\n        if not is_asyncio_reactor_installed():\n            raise ValueError(\n                f\"{MyComponent.__qualname__} requires the asyncio Twisted \"\n                f\"reactor. Make sure you have it configured in the \"\n                f\"TWISTED_REACTOR setting. See the asyncio documentation \"\n                f\"of Scrapy for more information.\"\n            )\n</pre>"], "links": [{"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "coroutine", "href": "coroutines.html"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "TWISTED_REACTOR", "href": "settings.html#std-setting-TWISTED_REACTOR"}, {"text": "CrawlerRunner", "href": "api.html#scrapy.crawler.CrawlerRunner"}, {"text": "AsyncioSelectorReactor", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.asyncioreactor.AsyncioSelectorReactor.html"}, {"text": "install_reactor()", "href": "settings.html#scrapy.utils.reactor.install_reactor"}, {"text": "install_reactor()", "href": "settings.html#scrapy.utils.reactor.install_reactor"}, {"text": "asyncio.Future", "href": "https://docs.python.org/3/library/asyncio-future.html#asyncio.Future"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "[source]", "href": "../_modules/scrapy/utils/defer.html#deferred_to_future"}, {"text": "asyncio.Future", "href": "https://docs.python.org/3/library/asyncio-future.html#asyncio.Future"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "Scrapy\ncallables defined as coroutines", "href": "coroutines.html#coroutine-support"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "[source]", "href": "../_modules/scrapy/utils/defer.html#maybe_deferred_to_future"}, {"text": "Scrapy callable\ndefined as a coroutine", "href": "coroutines.html#coroutine-support"}, {"text": "TWISTED_REACTOR", "href": "settings.html#std-setting-TWISTED_REACTOR"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "asyncio.Future", "href": "https://docs.python.org/3/library/asyncio-future.html#asyncio.Future"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "component", "href": "components.html#topics-components"}, {"text": "enforce it as a requirement", "href": "components.html#enforce-component-requirements"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "ProactorEventLoop", "href": "https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.ProactorEventLoop"}, {"text": "SelectorEventLoop", "href": "https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.SelectorEventLoop"}, {"text": "SelectorEventLoop", "href": "https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.SelectorEventLoop"}, {"text": "SelectorEventLoop", "href": "https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.SelectorEventLoop"}, {"text": "TWISTED_REACTOR", "href": "settings.html#std-setting-TWISTED_REACTOR"}, {"text": "install_reactor()", "href": "settings.html#scrapy.utils.reactor.install_reactor"}, {"text": "ProactorEventLoop", "href": "https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.ProactorEventLoop"}, {"text": "playwright", "href": "https://github.com/microsoft/playwright-python"}, {"text": "ASYNCIO_EVENT_LOOP", "href": "settings.html#std-setting-ASYNCIO_EVENT_LOOP"}], "timestamp": "2023-10-12T00:11:10.609990", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/coroutines.html", "content": "                              Coroutines  New in version 2.0.  Scrapy has partial support for the coroutine syntax.  Supported callables The following callables may be defined as coroutines using async def, and hence use coroutine syntax (e.g. await, async for, async with):  Request callbacks. If you are using any custom or third-party spider middleware, see Mixing synchronous and asynchronous spider middlewares.  Changed in version 2.7: Output of async callbacks is now processed asynchronously instead of collecting all of it first.   The process_item() method of item pipelines. The process_request(), process_response(), and process_exception() methods of downloader middlewares. Signal handlers that support deferreds. The process_spider_output() method of spider middlewares. It must be defined as an asynchronous generator. The input result parameter is an asynchronous iterable. See also Mixing synchronous and asynchronous spider middlewares and Universal spider middlewares.  New in version 2.7.      General usage There are several use cases for coroutines in Scrapy. Code that would return Deferreds when written for previous Scrapy versions, such as downloader middlewares and signal handlers, can be rewritten to be shorter and cleaner: from itemadapter import ItemAdapter   class DbPipeline:     def _update_item(self, data, item):         adapter = ItemAdapter(item)         adapter[\"field\"] = data         return item      def process_item(self, item, spider):         adapter = ItemAdapter(item)         dfd = db.get_some_data(adapter[\"id\"])         dfd.addCallback(self._update_item, item)         return dfd   becomes: from itemadapter import ItemAdapter   class DbPipeline:     async def process_item(self, item, spider):         adapter = ItemAdapter(item)         adapter[\"field\"] = await db.get_some_data(adapter[\"id\"])         return item   Coroutines may be used to call asynchronous code. This includes other coroutines, functions that return Deferreds and functions that return awaitable objects such as Future. This means you can use many useful Python libraries providing such code: class MySpiderDeferred(Spider):     # ...     async def parse(self, response):         additional_response = await treq.get(\"https://additional.url\")         additional_data = await treq.content(additional_response)         # ... use response and additional_data to yield items and requests   class MySpiderAsyncio(Spider):     # ...     async def parse(self, response):         async with aiohttp.ClientSession() as session:             async with session.get(\"https://additional.url\") as additional_response:                 additional_data = await additional_response.text()         # ... use response and additional_data to yield items and requests    Note Many libraries that use coroutines, such as aio-libs, require the asyncio loop and to use them you need to enable asyncio support in Scrapy.   Note If you want to await on Deferreds while using the asyncio reactor, you need to wrap them.  Common use cases for asynchronous code include:  requesting data from websites, databases and other services (in callbacks, pipelines and middlewares); storing data in databases (in pipelines and middlewares); delaying the spider initialization until some external event (in the spider_opened handler); calling asynchronous Scrapy methods like ExecutionEngine.download() (see the screenshot pipeline example).    Inline requests The spider below shows how to send a request and await its response all from within a spider callback: from scrapy import Spider, Request from scrapy.utils.defer import maybe_deferred_to_future   class SingleRequestSpider(Spider):     name = \"single\"     start_urls = [\"https://example.org/product\"]      async def parse(self, response, **kwargs):         additional_request = Request(\"https://example.org/price\")         deferred = self.crawler.engine.download(additional_request)         additional_response = await maybe_deferred_to_future(deferred)         yield {             \"h1\": response.css(\"h1\").get(),             \"price\": additional_response.css(\"#price\").get(),         }   You can also send multiple requests in parallel: from scrapy import Spider, Request from scrapy.utils.defer import maybe_deferred_to_future from twisted.internet.defer import DeferredList   class MultipleRequestsSpider(Spider):     name = \"multiple\"     start_urls = [\"https://example.com/product\"]      async def parse(self, response, **kwargs):         additional_requests = [             Request(\"https://example.com/price\"),             Request(\"https://example.com/color\"),         ]         deferreds = []         for r in additional_requests:             deferred = self.crawler.engine.download(r)             deferreds.append(deferred)         responses = await maybe_deferred_to_future(DeferredList(deferreds))         yield {             \"h1\": response.css(\"h1::text\").get(),             \"price\": responses[0][1].css(\".price::text\").get(),             \"price2\": responses[1][1].css(\".color::text\").get(),         }     Mixing synchronous and asynchronous spider middlewares  New in version 2.7.  The output of a Request callback is passed as the result parameter to the process_spider_output() method of the first spider middleware from the list of active spider middlewares. Then the output of that process_spider_output method is passed to the process_spider_output method of the next spider middleware, and so on for every active spider middleware. Scrapy supports mixing coroutine methods and synchronous methods in this chain of calls. However, if any of the process_spider_output methods is defined as a synchronous method, and the previous Request callback or process_spider_output method is a coroutine, there are some drawbacks to the asynchronous-to-synchronous conversion that Scrapy does so that the synchronous process_spider_output method gets a synchronous iterable as its result parameter:  The whole output of the previous Request callback or process_spider_output method is awaited at this point. If an exception raises while awaiting the output of the previous Request callback or process_spider_output method, none of that output will be processed. This contrasts with the regular behavior, where all items yielded before an exception raises are processed.   Asynchronous-to-synchronous conversions are supported for backward compatibility, but they are deprecated and will stop working in a future version of Scrapy. To avoid asynchronous-to-synchronous conversions, when defining Request callbacks as coroutine methods or when using spider middlewares whose process_spider_output method is an asynchronous generator, all active spider middlewares must either have their process_spider_output method defined as an asynchronous generator or define a process_spider_output_async method.  Note When using third-party spider middlewares that only define a synchronous process_spider_output method, consider making them universal through subclassing.    Universal spider middlewares  New in version 2.7.  To allow writing a spider middleware that supports asynchronous execution of its process_spider_output method in Scrapy 2.7 and later (avoiding asynchronous-to-synchronous conversions) while maintaining support for older Scrapy versions, you may define process_spider_output as a synchronous method and define an asynchronous generator version of that method with an alternative name: process_spider_output_async. For example: class UniversalSpiderMiddleware:     def process_spider_output(self, response, result, spider):         for r in result:             # ... do something with r             yield r      async def process_spider_output_async(self, response, result, spider):         async for r in result:             # ... do something with r             yield r    Note This is an interim measure to allow, for a time, to write code that works in Scrapy 2.7 and later without requiring asynchronous-to-synchronous conversions, and works in earlier Scrapy versions as well. In some future version of Scrapy, however, this feature will be deprecated and, eventually, in a later version of Scrapy, this feature will be removed, and all spider middlewares will be expected to define their process_spider_output method as an asynchronous generator.                            ", "code_blocks": ["from itemadapter import ItemAdapter\n\n\nclass DbPipeline:\n    def _update_item(self, data, item):\n        adapter = ItemAdapter(item)\n        adapter[\"field\"] = data\n        return item\n\n    def process_item(self, item, spider):\n        adapter = ItemAdapter(item)\n        dfd = db.get_some_data(adapter[\"id\"])\n        dfd.addCallback(self._update_item, item)\n        return dfd\n</pre>", "from itemadapter import ItemAdapter\n\n\nclass DbPipeline:\n    async def process_item(self, item, spider):\n        adapter = ItemAdapter(item)\n        adapter[\"field\"] = await db.get_some_data(adapter[\"id\"])\n        return item\n</pre>", "class MySpiderDeferred(Spider):\n    # ...\n    async def parse(self, response):\n        additional_response = await treq.get(\"https://additional.url\")\n        additional_data = await treq.content(additional_response)\n        # ... use response and additional_data to yield items and requests\n\n\nclass MySpiderAsyncio(Spider):\n    # ...\n    async def parse(self, response):\n        async with aiohttp.ClientSession() as session:\n            async with session.get(\"https://additional.url\") as additional_response:\n                additional_data = await additional_response.text()\n        # ... use response and additional_data to yield items and requests\n</pre>", "from scrapy import Spider, Request\nfrom scrapy.utils.defer import maybe_deferred_to_future\n\n\nclass SingleRequestSpider(Spider):\n    name = \"single\"\n    start_urls = [\"https://example.org/product\"]\n\n    async def parse(self, response, **kwargs):\n        additional_request = Request(\"https://example.org/price\")\n        deferred = self.crawler.engine.download(additional_request)\n        additional_response = await maybe_deferred_to_future(deferred)\n        yield {\n            \"h1\": response.css(\"h1\").get(),\n            \"price\": additional_response.css(\"#price\").get(),\n        }\n</pre>", "from scrapy import Spider, Request\nfrom scrapy.utils.defer import maybe_deferred_to_future\nfrom twisted.internet.defer import DeferredList\n\n\nclass MultipleRequestsSpider(Spider):\n    name = \"multiple\"\n    start_urls = [\"https://example.com/product\"]\n\n    async def parse(self, response, **kwargs):\n        additional_requests = [\n            Request(\"https://example.com/price\"),\n            Request(\"https://example.com/color\"),\n        ]\n        deferreds = []\n        for r in additional_requests:\n            deferred = self.crawler.engine.download(r)\n            deferreds.append(deferred)\n        responses = await maybe_deferred_to_future(DeferredList(deferreds))\n        yield {\n            \"h1\": response.css(\"h1::text\").get(),\n            \"price\": responses[0][1].css(\".price::text\").get(),\n            \"price2\": responses[1][1].css(\".color::text\").get(),\n        }\n</pre>", "class UniversalSpiderMiddleware:\n    def process_spider_output(self, response, result, spider):\n        for r in result:\n            # ... do something with r\n            yield r\n\n    async def process_spider_output_async(self, response, result, spider):\n        async for r in result:\n            # ... do something with r\n            yield r\n</pre>"], "links": [{"text": "coroutine syntax", "href": "https://docs.python.org/3/reference/compound_stmts.html#async"}, {"text": "spider middleware", "href": "spider-middleware.html#topics-spider-middleware"}, {"text": "process_item()", "href": "item-pipeline.html#process_item"}, {"text": "item pipelines", "href": "item-pipeline.html#topics-item-pipeline"}, {"text": "process_request()", "href": "downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"}, {"text": "process_response()", "href": "downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"}, {"text": "process_exception()", "href": "downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"}, {"text": "downloader middlewares", "href": "downloader-middleware.html#topics-downloader-middleware-custom"}, {"text": "Signal handlers that support deferreds", "href": "signals.html#signal-deferred"}, {"text": "process_spider_output()", "href": "spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"}, {"text": "spider middlewares", "href": "spider-middleware.html#topics-spider-middleware"}, {"text": "asynchronous generator", "href": "https://docs.python.org/3/glossary.html#term-asynchronous-generator"}, {"text": "asynchronous iterable", "href": "https://docs.python.org/3/glossary.html#term-asynchronous-iterable"}, {"text": "awaitable objects", "href": "https://docs.python.org/3/glossary.html#term-awaitable"}, {"text": "Future", "href": "https://docs.python.org/3/library/asyncio-future.html#asyncio.Future"}, {"text": "aio-libs", "href": "https://github.com/aio-libs"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "enable asyncio support in Scrapy", "href": "asyncio.html"}, {"text": "wrap them", "href": "asyncio.html#asyncio-await-dfd"}, {"text": "spider_opened", "href": "signals.html#std-signal-spider_opened"}, {"text": "the screenshot pipeline example", "href": "item-pipeline.html#screenshotpipeline"}, {"text": "process_spider_output()", "href": "spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"}, {"text": "spider middleware", "href": "spider-middleware.html#topics-spider-middleware"}, {"text": "list of active spider middlewares", "href": "spider-middleware.html#topics-spider-middleware-setting"}, {"text": "coroutine methods", "href": "https://docs.python.org/3/reference/compound_stmts.html#async"}, {"text": "asynchronous generator", "href": "https://docs.python.org/3/glossary.html#term-asynchronous-generator"}, {"text": "subclassing", "href": "https://docs.python.org/3/tutorial/classes.html#tut-inheritance"}, {"text": "asynchronous generator", "href": "https://docs.python.org/3/glossary.html#term-asynchronous-generator"}], "timestamp": "2023-10-12T00:11:14.447957", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/jobs.html", "content": "                              Jobs: pausing and resuming crawls Sometimes, for big sites, it’s desirable to pause crawls and be able to resume them later. Scrapy supports this functionality out of the box by providing the following facilities:  a scheduler that persists scheduled requests on disk a duplicates filter that persists visited requests on disk an extension that keeps some spider state (key/value pairs) persistent between batches   Job directory To enable persistence support you just need to define a job directory through the JOBDIR setting. This directory will be for storing all required data to keep the state of a single job (i.e. a spider run).  It’s important to note that this directory must not be shared by different spiders, or even different jobs/runs of the same spider, as it’s meant to be used for storing the state of a single job.   How to use it To start a spider with persistence support enabled, run it like this: scrapy crawl somespider -s JOBDIR=crawls/somespider-1   Then, you can stop the spider safely at any time (by pressing Ctrl-C or sending a signal), and resume it later by issuing the same command: scrapy crawl somespider -s JOBDIR=crawls/somespider-1     Keeping persistent state between batches Sometimes you’ll want to keep some persistent spider state between pause/resume batches. You can use the spider.state attribute for that, which should be a dict. There’s a built-in extension that takes care of serializing, storing and loading that attribute from the job directory, when the spider starts and stops. Here’s an example of a callback that uses the spider state (other spider code is omitted for brevity): def parse_item(self, response):     # parse item here     self.state[\"items_count\"] = self.state.get(\"items_count\", 0) + 1     Persistence gotchas There are a few things to keep in mind if you want to be able to use the Scrapy persistence support:  Cookies expiration Cookies may expire. So, if you don’t resume your spider quickly the requests scheduled may no longer work. This won’t be an issue if your spider doesn’t rely on cookies.   Request serialization For persistence to work, Request objects must be serializable with pickle, except for the callback and errback values passed to their __init__ method, which must be methods of the running Spider class. If you wish to log the requests that couldn’t be serialized, you can set the SCHEDULER_DEBUG setting to True in the project’s settings page. It is False by default.                            ", "code_blocks": ["scrapy crawl somespider -s JOBDIR=crawls/somespider-1\n</pre>", "scrapy crawl somespider -s JOBDIR=crawls/somespider-1\n</pre>", "def parse_item(self, response):\n    # parse item here\n    self.state[\"items_count\"] = self.state.get(\"items_count\", 0) + 1\n</pre>"], "links": [{"text": "pickle", "href": "https://docs.python.org/3/library/pickle.html#module-pickle"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "SCHEDULER_DEBUG", "href": "settings.html#std-setting-SCHEDULER_DEBUG"}], "timestamp": "2023-10-12T00:11:16.804332", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/benchmarking.html", "content": "                              Benchmarking Scrapy comes with a simple benchmarking suite that spawns a local HTTP server and crawls it at the maximum possible speed. The goal of this benchmarking is to get an idea of how Scrapy performs in your hardware, in order to have a common baseline for comparisons. It uses a simple spider that does nothing and just follows links. To run it use: scrapy bench   You should see an output like this: 2016-12-16 21:18:48 [scrapy.utils.log] INFO: Scrapy 1.2.2 started (bot: quotesbot) 2016-12-16 21:18:48 [scrapy.utils.log] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 10, 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['quotesbot.spiders'], 'LOGSTATS_INTERVAL': 1, 'BOT_NAME': 'quotesbot', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'quotesbot.spiders'} 2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled extensions: ['scrapy.extensions.closespider.CloseSpider',  'scrapy.extensions.logstats.LogStats',  'scrapy.extensions.telnet.TelnetConsole',  'scrapy.extensions.corestats.CoreStats'] 2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled downloader middlewares: ['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',  'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',  'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',  'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',  'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',  'scrapy.downloadermiddlewares.retry.RetryMiddleware',  'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',  'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',  'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',  'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',  'scrapy.downloadermiddlewares.stats.DownloaderStats'] 2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled spider middlewares: ['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',  'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',  'scrapy.spidermiddlewares.referer.RefererMiddleware',  'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',  'scrapy.spidermiddlewares.depth.DepthMiddleware'] 2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled item pipelines: [] 2016-12-16 21:18:49 [scrapy.core.engine] INFO: Spider opened 2016-12-16 21:18:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min) 2016-12-16 21:18:50 [scrapy.extensions.logstats] INFO: Crawled 70 pages (at 4200 pages/min), scraped 0 items (at 0 items/min) 2016-12-16 21:18:51 [scrapy.extensions.logstats] INFO: Crawled 134 pages (at 3840 pages/min), scraped 0 items (at 0 items/min) 2016-12-16 21:18:52 [scrapy.extensions.logstats] INFO: Crawled 198 pages (at 3840 pages/min), scraped 0 items (at 0 items/min) 2016-12-16 21:18:53 [scrapy.extensions.logstats] INFO: Crawled 254 pages (at 3360 pages/min), scraped 0 items (at 0 items/min) 2016-12-16 21:18:54 [scrapy.extensions.logstats] INFO: Crawled 302 pages (at 2880 pages/min), scraped 0 items (at 0 items/min) 2016-12-16 21:18:55 [scrapy.extensions.logstats] INFO: Crawled 358 pages (at 3360 pages/min), scraped 0 items (at 0 items/min) 2016-12-16 21:18:56 [scrapy.extensions.logstats] INFO: Crawled 406 pages (at 2880 pages/min), scraped 0 items (at 0 items/min) 2016-12-16 21:18:57 [scrapy.extensions.logstats] INFO: Crawled 438 pages (at 1920 pages/min), scraped 0 items (at 0 items/min) 2016-12-16 21:18:58 [scrapy.extensions.logstats] INFO: Crawled 470 pages (at 1920 pages/min), scraped 0 items (at 0 items/min) 2016-12-16 21:18:59 [scrapy.core.engine] INFO: Closing spider (closespider_timeout) 2016-12-16 21:18:59 [scrapy.extensions.logstats] INFO: Crawled 518 pages (at 2880 pages/min), scraped 0 items (at 0 items/min) 2016-12-16 21:19:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats: {'downloader/request_bytes': 229995,  'downloader/request_count': 534,  'downloader/request_method_count/GET': 534,  'downloader/response_bytes': 1565504,  'downloader/response_count': 534,  'downloader/response_status_count/200': 534,  'finish_reason': 'closespider_timeout',  'finish_time': datetime.datetime(2016, 12, 16, 16, 19, 0, 647725),  'log_count/INFO': 17,  'request_depth_max': 19,  'response_received_count': 534,  'scheduler/dequeued': 533,  'scheduler/dequeued/memory': 533,  'scheduler/enqueued': 10661,  'scheduler/enqueued/memory': 10661,  'start_time': datetime.datetime(2016, 12, 16, 16, 18, 49, 799869)} 2016-12-16 21:19:00 [scrapy.core.engine] INFO: Spider closed (closespider_timeout)   That tells you that Scrapy is able to crawl about 3000 pages per minute in the hardware where you run it. Note that this is a very simple spider intended to follow links, any custom spider you write will probably do more stuff which results in slower crawl rates. How slower depends on how much your spider does and how well it’s written. Use scrapy-bench for more complex benchmarking.                          ", "code_blocks": ["scrapy bench\n</pre>", "2016-12-16 21:18:48 [scrapy.utils.log] INFO: Scrapy 1.2.2 started (bot: quotesbot)\n2016-12-16 21:18:48 [scrapy.utils.log] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 10, 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['quotesbot.spiders'], 'LOGSTATS_INTERVAL': 1, 'BOT_NAME': 'quotesbot', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'quotesbot.spiders'}\n2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.closespider.CloseSpider',\n 'scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats']\n2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2016-12-16 21:18:49 [scrapy.core.engine] INFO: Spider opened\n2016-12-16 21:18:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-12-16 21:18:50 [scrapy.extensions.logstats] INFO: Crawled 70 pages (at 4200 pages/min), scraped 0 items (at 0 items/min)\n2016-12-16 21:18:51 [scrapy.extensions.logstats] INFO: Crawled 134 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)\n2016-12-16 21:18:52 [scrapy.extensions.logstats] INFO: Crawled 198 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)\n2016-12-16 21:18:53 [scrapy.extensions.logstats] INFO: Crawled 254 pages (at 3360 pages/min), scraped 0 items (at 0 items/min)\n2016-12-16 21:18:54 [scrapy.extensions.logstats] INFO: Crawled 302 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)\n2016-12-16 21:18:55 [scrapy.extensions.logstats] INFO: Crawled 358 pages (at 3360 pages/min), scraped 0 items (at 0 items/min)\n2016-12-16 21:18:56 [scrapy.extensions.logstats] INFO: Crawled 406 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)\n2016-12-16 21:18:57 [scrapy.extensions.logstats] INFO: Crawled 438 pages (at 1920 pages/min), scraped 0 items (at 0 items/min)\n2016-12-16 21:18:58 [scrapy.extensions.logstats] INFO: Crawled 470 pages (at 1920 pages/min), scraped 0 items (at 0 items/min)\n2016-12-16 21:18:59 [scrapy.core.engine] INFO: Closing spider (closespider_timeout)\n2016-12-16 21:18:59 [scrapy.extensions.logstats] INFO: Crawled 518 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)\n2016-12-16 21:19:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 229995,\n 'downloader/request_count': 534,\n 'downloader/request_method_count/GET': 534,\n 'downloader/response_bytes': 1565504,\n 'downloader/response_count': 534,\n 'downloader/response_status_count/200': 534,\n 'finish_reason': 'closespider_timeout',\n 'finish_time': datetime.datetime(2016, 12, 16, 16, 19, 0, 647725),\n 'log_count/INFO': 17,\n 'request_depth_max': 19,\n 'response_received_count': 534,\n 'scheduler/dequeued': 533,\n 'scheduler/dequeued/memory': 533,\n 'scheduler/enqueued': 10661,\n 'scheduler/enqueued/memory': 10661,\n 'start_time': datetime.datetime(2016, 12, 16, 16, 18, 49, 799869)}\n2016-12-16 21:19:00 [scrapy.core.engine] INFO: Spider closed (closespider_timeout)\n</pre>"], "links": [{"text": "scrapy-bench", "href": "https://github.com/scrapy/scrapy-bench"}], "timestamp": "2023-10-12T00:11:20.697060", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/autothrottle.html", "content": "                              AutoThrottle extension This is an extension for automatically throttling crawling speed based on load of both the Scrapy server and the website you are crawling.  Design goals  be nicer to sites instead of using default download delay of zero automatically adjust Scrapy to the optimum crawling speed, so the user doesn’t have to tune the download delays to find the optimum one. The user only needs to specify the maximum concurrent requests it allows, and the extension does the rest.    How it works AutoThrottle extension adjusts download delays dynamically to make spider send AUTOTHROTTLE_TARGET_CONCURRENCY concurrent requests on average to each remote website. It uses download latency to compute the delays. The main idea is the following: if a server needs latency seconds to respond, a client should send a request each latency/N seconds to have N requests processed in parallel. Instead of adjusting the delays one can just set a small fixed download delay and impose hard limits on concurrency using CONCURRENT_REQUESTS_PER_DOMAIN or CONCURRENT_REQUESTS_PER_IP options. It will provide a similar effect, but there are some important differences:  because the download delay is small there will be occasional bursts of requests; often non-200 (error) responses can be returned faster than regular responses, so with a small download delay and a hard concurrency limit crawler will be sending requests to server faster when server starts to return errors. But this is an opposite of what crawler should do - in case of errors it makes more sense to slow down: these errors may be caused by the high request rate.  AutoThrottle doesn’t have these issues.   Throttling algorithm AutoThrottle algorithm adjusts download delays based on the following rules:  spiders always start with a download delay of AUTOTHROTTLE_START_DELAY; when a response is received, the target download delay is calculated as latency / N where latency is a latency of the response, and N is AUTOTHROTTLE_TARGET_CONCURRENCY. download delay for next requests is set to the average of previous download delay and the target download delay; latencies of non-200 responses are not allowed to decrease the delay; download delay can’t become less than DOWNLOAD_DELAY or greater than AUTOTHROTTLE_MAX_DELAY   Note The AutoThrottle extension honours the standard Scrapy settings for concurrency and delay. This means that it will respect CONCURRENT_REQUESTS_PER_DOMAIN and CONCURRENT_REQUESTS_PER_IP options and never set a download delay lower than DOWNLOAD_DELAY.  In Scrapy, the download latency is measured as the time elapsed between establishing the TCP connection and receiving the HTTP headers. Note that these latencies are very hard to measure accurately in a cooperative multitasking environment because Scrapy may be busy processing a spider callback, for example, and unable to attend downloads. However, these latencies should still give a reasonable estimate of how busy Scrapy (and ultimately, the server) is, and this extension builds on that premise.   Settings The settings used to control the AutoThrottle extension are:  AUTOTHROTTLE_ENABLED AUTOTHROTTLE_START_DELAY AUTOTHROTTLE_MAX_DELAY AUTOTHROTTLE_TARGET_CONCURRENCY AUTOTHROTTLE_DEBUG CONCURRENT_REQUESTS_PER_DOMAIN CONCURRENT_REQUESTS_PER_IP DOWNLOAD_DELAY  For more information see How it works.  AUTOTHROTTLE_ENABLED Default: False Enables the AutoThrottle extension.   AUTOTHROTTLE_START_DELAY Default: 5.0 The initial download delay (in seconds).   AUTOTHROTTLE_MAX_DELAY Default: 60.0 The maximum download delay (in seconds) to be set in case of high latencies.   AUTOTHROTTLE_TARGET_CONCURRENCY Default: 1.0 Average number of requests Scrapy should be sending in parallel to remote websites. By default, AutoThrottle adjusts the delay to send a single concurrent request to each of the remote websites. Set this option to a higher value (e.g. 2.0) to increase the throughput and the load on remote servers. A lower AUTOTHROTTLE_TARGET_CONCURRENCY value (e.g. 0.5) makes the crawler more conservative and polite. Note that CONCURRENT_REQUESTS_PER_DOMAIN and CONCURRENT_REQUESTS_PER_IP options are still respected when AutoThrottle extension is enabled. This means that if AUTOTHROTTLE_TARGET_CONCURRENCY is set to a value higher than CONCURRENT_REQUESTS_PER_DOMAIN or CONCURRENT_REQUESTS_PER_IP, the crawler won’t reach this number of concurrent requests. At every given time point Scrapy can be sending more or less concurrent requests than AUTOTHROTTLE_TARGET_CONCURRENCY; it is a suggested value the crawler tries to approach, not a hard limit.   AUTOTHROTTLE_DEBUG Default: False Enable AutoThrottle debug mode which will display stats on every response received, so you can see how the throttling parameters are being adjusted in real time.                            ", "code_blocks": [], "links": [{"text": "CONCURRENT_REQUESTS_PER_DOMAIN", "href": "settings.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"}, {"text": "CONCURRENT_REQUESTS_PER_IP", "href": "settings.html#std-setting-CONCURRENT_REQUESTS_PER_IP"}, {"text": "DOWNLOAD_DELAY", "href": "settings.html#std-setting-DOWNLOAD_DELAY"}, {"text": "CONCURRENT_REQUESTS_PER_DOMAIN", "href": "settings.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"}, {"text": "CONCURRENT_REQUESTS_PER_IP", "href": "settings.html#std-setting-CONCURRENT_REQUESTS_PER_IP"}, {"text": "DOWNLOAD_DELAY", "href": "settings.html#std-setting-DOWNLOAD_DELAY"}, {"text": "CONCURRENT_REQUESTS_PER_DOMAIN", "href": "settings.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"}, {"text": "CONCURRENT_REQUESTS_PER_IP", "href": "settings.html#std-setting-CONCURRENT_REQUESTS_PER_IP"}, {"text": "DOWNLOAD_DELAY", "href": "settings.html#std-setting-DOWNLOAD_DELAY"}, {"text": "CONCURRENT_REQUESTS_PER_DOMAIN", "href": "settings.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"}, {"text": "CONCURRENT_REQUESTS_PER_IP", "href": "settings.html#std-setting-CONCURRENT_REQUESTS_PER_IP"}, {"text": "CONCURRENT_REQUESTS_PER_DOMAIN", "href": "settings.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"}, {"text": "CONCURRENT_REQUESTS_PER_IP", "href": "settings.html#std-setting-CONCURRENT_REQUESTS_PER_IP"}], "timestamp": "2023-10-12T00:11:25.145194", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/master/contributing.html", "content": "                              Contributing to Scrapy  Important Double check that you are reading the most recent version of this document at https://docs.scrapy.org/en/master/contributing.html  There are many ways to contribute to Scrapy. Here are some of them:  Report bugs and request features in the issue tracker, trying to follow the guidelines detailed in Reporting bugs below. Submit patches for new functionalities and/or bug fixes. Please read Writing patches and Submitting patches below for details on how to write and submit a patch. Blog about Scrapy. Tell the world how you’re using Scrapy. This will help newcomers with more examples and will help the Scrapy project to increase its visibility. Join the Scrapy subreddit and share your ideas on how to improve Scrapy. We’re always open to suggestions. Answer Scrapy questions at Stack Overflow.   Reporting bugs  Note Please report security issues only to scrapy-security@googlegroups.com. This is a private list only open to trusted Scrapy developers, and its archives are not public.  Well-written bug reports are very helpful, so keep in mind the following guidelines when you’re going to report a new bug.  check the FAQ first to see if your issue is addressed in a well-known question if you have a general question about Scrapy usage, please ask it at Stack Overflow (use “scrapy” tag). check the open issues to see if the issue has already been reported. If it has, don’t dismiss the report, but check the ticket history and comments. If you have additional useful information, please leave a comment, or consider sending a pull request with a fix. search the scrapy-users list and Scrapy subreddit to see if it has been discussed there, or if you’re not sure if what you’re seeing is a bug. You can also ask in the #scrapy IRC channel. write complete, reproducible, specific bug reports. The smaller the test case, the better. Remember that other developers won’t have your project to reproduce the bug, so please include all relevant files required to reproduce it. See for example StackOverflow’s guide on creating a Minimal, Complete, and Verifiable example exhibiting the issue. the most awesome way to provide a complete reproducible example is to send a pull request which adds a failing test case to the Scrapy testing suite (see Submitting patches). This is helpful even if you don’t have an intention to fix the issue yourselves. include the output of scrapy version -v so developers working on your bug know exactly which version and platform it occurred on, which is often very helpful for reproducing it, or knowing if it was already fixed.    Writing patches Scrapy has a list of good first issues and help wanted issues that you can work on. These issues are a great way to get started with contributing to Scrapy. If you’re new to the codebase, you may want to focus on documentation or testing-related issues, as they are always useful and can help you get more familiar with the project. You can also check Scrapy’s test coverage to see which areas may benefit from more tests. The better a patch is written, the higher the chances that it’ll get accepted and the sooner it will be merged. Well-written patches should:  contain the minimum amount of code required for the specific change. Small patches are easier to review and merge. So, if you’re doing more than one change (or bug fix), please consider submitting one patch per change. Do not collapse multiple changes into a single patch. For big changes consider using a patch queue. pass all unit-tests. See Running tests below. include one (or more) test cases that check the bug fixed or the new functionality added. See Writing tests below. if you’re adding or changing a public (documented) API, please include the documentation changes in the same patch.  See Documentation policies below. if you’re adding a private API, please add a regular expression to the coverage_ignore_pyobjects variable of docs/conf.py to exclude the new private API from documentation coverage checks. To see if your private API is skipped properly, generate a documentation coverage report as follows: tox -e docs-coverage    if you are removing deprecated code, first make sure that at least 1 year (12 months) has passed since the release that introduced the deprecation. See Deprecation policy.    Submitting patches The best way to submit a patch is to issue a pull request on GitHub, optionally creating a new issue first. Remember to explain what was fixed or the new functionality (what it is, why it’s needed, etc). The more info you include, the easier will be for core developers to understand and accept your patch. You can also discuss the new functionality (or bug fix) before creating the patch, but it’s always good to have a patch ready to illustrate your arguments and show that you have put some additional thought into the subject. A good starting point is to send a pull request on GitHub. It can be simple enough to illustrate your idea, and leave documentation/tests for later, after the idea has been validated and proven useful. Alternatively, you can start a conversation in the Scrapy subreddit to discuss your idea first. Sometimes there is an existing pull request for the problem you’d like to solve, which is stalled for some reason. Often the pull request is in a right direction, but changes are requested by Scrapy maintainers, and the original pull request author hasn’t had time to address them. In this case consider picking up this pull request: open a new pull request with all commits from the original pull request, as well as additional changes to address the raised issues. Doing so helps a lot; it is not considered rude as long as the original author is acknowledged by keeping his/her commits. You can pull an existing pull request to a local branch by running git fetch upstream pull/$PR_NUMBER/head:$BRANCH_NAME_TO_CREATE (replace ‘upstream’ with a remote name for scrapy repository, $PR_NUMBER with an ID of the pull request, and $BRANCH_NAME_TO_CREATE with a name of the branch you want to create locally). See also: https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/checking-out-pull-requests-locally#modifying-an-inactive-pull-request-locally. When writing GitHub pull requests, try to keep titles short but descriptive. E.g. For bug #411: “Scrapy hangs if an exception raises in start_requests” prefer “Fix hanging when exception occurs in start_requests (#411)” instead of “Fix for #411”. Complete titles make it easy to skim through the issue tracker. Finally, try to keep aesthetic changes (PEP 8 compliance, unused imports removal, etc) in separate commits from functional changes. This will make pull requests easier to review and more likely to get merged.   Coding style Please follow these coding conventions when writing code for inclusion in Scrapy:  We use black for code formatting. There is a hook in the pre-commit config that will automatically format your code before every commit. You can also run black manually with tox -e black. Don’t put your name in the code you contribute; git provides enough metadata to identify author of the code. See https://help.github.com/en/github/using-git/setting-your-username-in-git for setup instructions.    Pre-commit We use pre-commit to automatically address simple code issues before every commit. After your create a local clone of your fork of the Scrapy repository:  Install pre-commit. On the root of your local clone of the Scrapy repository, run the following command: pre-commit install     Now pre-commit will check your changes every time you create a Git commit. Upon finding issues, pre-commit aborts your commit, and either fixes those issues automatically, or only reports them to you. If it fixes those issues automatically, creating your commit again should succeed. Otherwise, you may need to address the corresponding issues manually first.   Documentation policies For reference documentation of API members (classes, methods, etc.) use docstrings and make sure that the Sphinx documentation uses the autodoc extension to pull the docstrings. API reference documentation should follow docstring conventions (PEP 257) and be IDE-friendly: short, to the point, and it may provide short examples. Other types of documentation, such as tutorials or topics, should be covered in files within the docs/ directory. This includes documentation that is specific to an API member, but goes beyond API reference documentation. In any case, if something is covered in a docstring, use the autodoc extension to pull the docstring into the documentation instead of duplicating the docstring in files within the docs/ directory. Documentation updates that cover new or modified features must use Sphinx’s versionadded and versionchanged directives. Use VERSION as version, we will replace it with the actual version right before the corresponding release. When we release a new major or minor version of Scrapy, we remove these directives if they are older than 3 years. Documentation about deprecated features must be removed as those features are deprecated, so that new readers do not run into it. New deprecations and deprecation removals are documented in the release notes.   Tests Tests are implemented using the Twisted unit-testing framework. Running tests requires tox.  Running tests To run all tests: tox   To run a specific test (say tests/test_loader.py) use:  tox -- tests/test_loader.py  To run the tests on a specific tox environment, use -e <name> with an environment name from tox.ini. For example, to run the tests with Python 3.10 use: tox -e py310   You can also specify a comma-separated list of environments, and use tox’s parallel mode to run the tests on multiple environments in parallel: tox -e py39,py310 -p auto   To pass command-line options to pytest, add them after -- in your call to tox. Using -- overrides the default positional arguments defined in tox.ini, so you must include those default positional arguments (scrapy tests) after -- as well: tox -- scrapy tests -x  # stop after first failure   You can also use the pytest-xdist plugin. For example, to run all tests on the Python 3.10 tox environment using all your CPU cores: tox -e py310 -- scrapy tests -n auto   To see coverage report install coverage (pip install coverage) and run:  coverage report  see output of coverage --help for more options like html or xml report.   Writing tests All functionality (including new features and bug fixes) must include a test case to check that it works as expected, so please include tests for your patches if you want them to get accepted sooner. Scrapy uses unit-tests, which are located in the tests/ directory. Their module name typically resembles the full path of the module they’re testing. For example, the item loaders code is in: scrapy.loader   And their unit-tests are in: tests/test_loader.py                              ", "code_blocks": ["tox -e docs-coverage\n</pre>", "pre-commit install\n</pre>", "tox\n</pre>", "tox -e py310\n</pre>", "tox -e py39,py310 -p auto\n</pre>", "tox -- scrapy tests -x  # stop after first failure\n</pre>", "tox -e py310 -- scrapy tests -n auto\n</pre>", "scrapy.loader\n</pre>", "tests/test_loader.py\n</pre>"], "links": [{"text": "https://docs.scrapy.org/en/master/contributing.html", "href": "https://docs.scrapy.org/en/master/contributing.html"}, {"text": "issue tracker", "href": "https://github.com/scrapy/scrapy/issues"}, {"text": "Scrapy subreddit", "href": "https://reddit.com/r/scrapy"}, {"text": "Stack Overflow", "href": "https://stackoverflow.com/questions/tagged/scrapy"}, {"text": "FAQ", "href": "faq.html#faq"}, {"text": "Stack Overflow", "href": "https://stackoverflow.com/questions/tagged/scrapy"}, {"text": "open issues", "href": "https://github.com/scrapy/scrapy/issues"}, {"text": "scrapy-users", "href": "https://groups.google.com/forum/#!forum/scrapy-users"}, {"text": "Scrapy subreddit", "href": "https://reddit.com/r/scrapy"}, {"text": "Minimal, Complete, and Verifiable example", "href": "https://stackoverflow.com/help/mcve"}, {"text": "good first issues", "href": "https://github.com/scrapy/scrapy/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"}, {"text": "help wanted issues", "href": "https://github.com/scrapy/scrapy/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22"}, {"text": "test coverage", "href": "https://app.codecov.io/gh/scrapy/scrapy"}, {"text": "Deprecation policy", "href": "versioning.html#deprecation-policy"}, {"text": "pull request", "href": "https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request"}, {"text": "Scrapy subreddit", "href": "https://reddit.com/r/scrapy"}, {"text": "https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/checking-out-pull-requests-locally#modifying-an-inactive-pull-request-locally", "href": "https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/checking-out-pull-requests-locally#modifying-an-inactive-pull-request-locally"}, {"text": "PEP 8", "href": "https://peps.python.org/pep-0008/"}, {"text": "black", "href": "https://black.readthedocs.io/en/stable/"}, {"text": "https://help.github.com/en/github/using-git/setting-your-username-in-git", "href": "https://help.github.com/en/github/using-git/setting-your-username-in-git"}, {"text": "pre-commit", "href": "https://pre-commit.com/"}, {"text": "Install pre-commit", "href": "https://pre-commit.com/#installation"}, {"text": "autodoc", "href": "https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#module-sphinx.ext.autodoc"}, {"text": "PEP 257", "href": "https://www.python.org/dev/peps/pep-0257/"}, {"text": "autodoc", "href": "https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#module-sphinx.ext.autodoc"}, {"text": "versionadded", "href": "https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionadded"}, {"text": "versionchanged", "href": "https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionchanged"}, {"text": "release notes", "href": "news.html#news"}, {"text": "Twisted unit-testing framework", "href": "https://docs.twisted.org/en/stable/development/test-standard.html"}, {"text": "tox", "href": "https://tox.wiki/en/latest/index.html"}, {"text": "tox", "href": "https://tox.wiki/en/latest/index.html"}, {"text": "tox’s\nparallel mode", "href": "https://tox.wiki/en/latest/user_guide.html#parallel-mode"}, {"text": "pytest", "href": "https://docs.pytest.org/en/latest/index.html"}, {"text": "tox", "href": "https://tox.wiki/en/latest/index.html"}, {"text": "pytest-xdist", "href": "https://github.com/pytest-dev/pytest-xdist"}, {"text": "tox", "href": "https://tox.wiki/en/latest/index.html"}, {"text": "coverage", "href": "https://coverage.readthedocs.io/en/latest/index.html"}, {"text": "tests/", "href": "https://github.com/scrapy/scrapy/tree/master/tests"}], "timestamp": "2023-10-12T00:11:29.274251", "version": "2.11", "revision": "c6556798", "last_updated": "2023-10-06"},
{"url": "https://docs.scrapy.org/en/latest/topics/djangoitem.html", "content": "                              DjangoItem DjangoItem has been moved into a separate project. It is hosted at:  https://github.com/scrapy-plugins/scrapy-djangoitem                           ", "code_blocks": [], "links": [{"text": "https://github.com/scrapy-plugins/scrapy-djangoitem", "href": "https://github.com/scrapy-plugins/scrapy-djangoitem"}], "timestamp": "2023-10-12T00:11:33.563402", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/_modules/scrapy/statscollectors.html", "content": "                             Source code for scrapy.statscollectors \"\"\" Scrapy extension for collecting scraping stats \"\"\" import logging import pprint from typing import TYPE_CHECKING, Any, Dict, Optional  from scrapy import Spider  if TYPE_CHECKING:     from scrapy.crawler import Crawler  logger = logging.getLogger(__name__)   StatsT = Dict[str, Any]   [docs]class StatsCollector:     def __init__(self, crawler: \"Crawler\"):         self._dump: bool = crawler.settings.getbool(\"STATS_DUMP\")         self._stats: StatsT = {}  [docs]    def get_value(         self, key: str, default: Any = None, spider: Optional[Spider] = None     ) -> Any:         return self._stats.get(key, default)  [docs]    def get_stats(self, spider: Optional[Spider] = None) -> StatsT:         return self._stats  [docs]    def set_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:         self._stats[key] = value  [docs]    def set_stats(self, stats: StatsT, spider: Optional[Spider] = None) -> None:         self._stats = stats  [docs]    def inc_value(         self, key: str, count: int = 1, start: int = 0, spider: Optional[Spider] = None     ) -> None:         d = self._stats         d[key] = d.setdefault(key, start) + count  [docs]    def max_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:         self._stats[key] = max(self._stats.setdefault(key, value), value)  [docs]    def min_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:         self._stats[key] = min(self._stats.setdefault(key, value), value)  [docs]    def clear_stats(self, spider: Optional[Spider] = None) -> None:         self._stats.clear()  [docs]    def open_spider(self, spider: Spider) -> None:         pass  [docs]    def close_spider(self, spider: Spider, reason: str) -> None:         if self._dump:             logger.info(                 \"Dumping Scrapy stats:\\n\" + pprint.pformat(self._stats),                 extra={\"spider\": spider},             )         self._persist_stats(self._stats, spider)      def _persist_stats(self, stats: StatsT, spider: Spider) -> None:         pass   [docs]class MemoryStatsCollector(StatsCollector):     def __init__(self, crawler: \"Crawler\"):         super().__init__(crawler)         self.spider_stats: Dict[str, StatsT] = {}      def _persist_stats(self, stats: StatsT, spider: Spider) -> None:         self.spider_stats[spider.name] = stats   [docs]class DummyStatsCollector(StatsCollector):     def get_value(         self, key: str, default: Any = None, spider: Optional[Spider] = None     ) -> Any:         return default      def set_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:         pass      def set_stats(self, stats: StatsT, spider: Optional[Spider] = None) -> None:         pass      def inc_value(         self, key: str, count: int = 1, start: int = 0, spider: Optional[Spider] = None     ) -> None:         pass      def max_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:         pass      def min_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:         pass                         ", "code_blocks": ["\n\"\"\"\nScrapy extension for collecting scraping stats\n\"\"\"\nimport logging\nimport pprint\nfrom typing import TYPE_CHECKING, Any, Dict, Optional\n\nfrom scrapy import Spider\n\nif TYPE_CHECKING:\n    from scrapy.crawler import Crawler\n\nlogger = logging.getLogger(__name__)\n\n\nStatsT = Dict[str, Any]\n\n\n[docs]class StatsCollector:\n    def __init__(self, crawler: \"Crawler\"):\n        self._dump: bool = crawler.settings.getbool(\"STATS_DUMP\")\n        self._stats: StatsT = {}\n\n[docs]    def get_value(\n        self, key: str, default: Any = None, spider: Optional[Spider] = None\n    ) -> Any:\n        return self._stats.get(key, default)\n\n[docs]    def get_stats(self, spider: Optional[Spider] = None) -> StatsT:\n        return self._stats\n\n[docs]    def set_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        self._stats[key] = value\n\n[docs]    def set_stats(self, stats: StatsT, spider: Optional[Spider] = None) -> None:\n        self._stats = stats\n\n[docs]    def inc_value(\n        self, key: str, count: int = 1, start: int = 0, spider: Optional[Spider] = None\n    ) -> None:\n        d = self._stats\n        d[key] = d.setdefault(key, start) + count\n\n[docs]    def max_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        self._stats[key] = max(self._stats.setdefault(key, value), value)\n\n[docs]    def min_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        self._stats[key] = min(self._stats.setdefault(key, value), value)\n\n[docs]    def clear_stats(self, spider: Optional[Spider] = None) -> None:\n        self._stats.clear()\n\n[docs]    def open_spider(self, spider: Spider) -> None:\n        pass\n\n[docs]    def close_spider(self, spider: Spider, reason: str) -> None:\n        if self._dump:\n            logger.info(\n                \"Dumping Scrapy stats:\\n\" + pprint.pformat(self._stats),\n                extra={\"spider\": spider},\n            )\n        self._persist_stats(self._stats, spider)\n\n    def _persist_stats(self, stats: StatsT, spider: Spider) -> None:\n        pass\n\n\n[docs]class MemoryStatsCollector(StatsCollector):\n    def __init__(self, crawler: \"Crawler\"):\n        super().__init__(crawler)\n        self.spider_stats: Dict[str, StatsT] = {}\n\n    def _persist_stats(self, stats: StatsT, spider: Spider) -> None:\n        self.spider_stats[spider.name] = stats\n\n\n[docs]class DummyStatsCollector(StatsCollector):\n    def get_value(\n        self, key: str, default: Any = None, spider: Optional[Spider] = None\n    ) -> Any:\n        return default\n\n    def set_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        pass\n\n    def set_stats(self, stats: StatsT, spider: Optional[Spider] = None) -> None:\n        pass\n\n    def inc_value(\n        self, key: str, count: int = 1, start: int = 0, spider: Optional[Spider] = None\n    ) -> None:\n        pass\n\n    def max_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        pass\n\n    def min_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        pass\n</pre>"], "links": [{"text": "[docs]", "href": "../../topics/api.html#scrapy.statscollectors.StatsCollector"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.statscollectors.StatsCollector.get_value"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.statscollectors.StatsCollector.get_stats"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.statscollectors.StatsCollector.set_value"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.statscollectors.StatsCollector.set_stats"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.statscollectors.StatsCollector.inc_value"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.statscollectors.StatsCollector.max_value"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.statscollectors.StatsCollector.min_value"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.statscollectors.StatsCollector.clear_stats"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.statscollectors.StatsCollector.open_spider"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.statscollectors.StatsCollector.close_spider"}, {"text": "[docs]", "href": "../../topics/stats.html#scrapy.statscollectors.MemoryStatsCollector"}, {"text": "[docs]", "href": "../../topics/stats.html#scrapy.statscollectors.DummyStatsCollector"}], "timestamp": "2023-10-12T00:11:37.116419", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/_modules/scrapy/signalmanager.html", "content": "                             Source code for scrapy.signalmanager from typing import Any, List, Tuple  from pydispatch import dispatcher from twisted.internet.defer import Deferred  from scrapy.utils import signal as _signal   [docs]class SignalManager:     def __init__(self, sender: Any = dispatcher.Anonymous):         self.sender: Any = sender  [docs]    def connect(self, receiver: Any, signal: Any, **kwargs: Any) -> None:         \"\"\"         Connect a receiver function to a signal.          The signal can be any object, although Scrapy comes with some         predefined signals that are documented in the :ref:`topics-signals`         section.          :param receiver: the function to be connected         :type receiver: collections.abc.Callable          :param signal: the signal to connect to         :type signal: object         \"\"\"         kwargs.setdefault(\"sender\", self.sender)         dispatcher.connect(receiver, signal, **kwargs)  [docs]    def disconnect(self, receiver: Any, signal: Any, **kwargs: Any) -> None:         \"\"\"         Disconnect a receiver function from a signal. This has the         opposite effect of the :meth:`connect` method, and the arguments         are the same.         \"\"\"         kwargs.setdefault(\"sender\", self.sender)         dispatcher.disconnect(receiver, signal, **kwargs)  [docs]    def send_catch_log(self, signal: Any, **kwargs: Any) -> List[Tuple[Any, Any]]:         \"\"\"         Send a signal, catch exceptions and log them.          The keyword arguments are passed to the signal handlers (connected         through the :meth:`connect` method).         \"\"\"         kwargs.setdefault(\"sender\", self.sender)         return _signal.send_catch_log(signal, **kwargs)  [docs]    def send_catch_log_deferred(self, signal: Any, **kwargs: Any) -> Deferred:         \"\"\"         Like :meth:`send_catch_log` but supports returning         :class:`~twisted.internet.defer.Deferred` objects from signal handlers.          Returns a Deferred that gets fired once all signal handlers         deferreds were fired. Send a signal, catch exceptions and log them.          The keyword arguments are passed to the signal handlers (connected         through the :meth:`connect` method).         \"\"\"         kwargs.setdefault(\"sender\", self.sender)         return _signal.send_catch_log_deferred(signal, **kwargs)  [docs]    def disconnect_all(self, signal: Any, **kwargs: Any) -> None:         \"\"\"         Disconnect all receivers from the given signal.          :param signal: the signal to disconnect from         :type signal: object         \"\"\"         kwargs.setdefault(\"sender\", self.sender)         _signal.disconnect_all(signal, **kwargs)                         ", "code_blocks": ["\nfrom typing import Any, List, Tuple\n\nfrom pydispatch import dispatcher\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy.utils import signal as _signal\n\n\n[docs]class SignalManager:\n    def __init__(self, sender: Any = dispatcher.Anonymous):\n        self.sender: Any = sender\n\n[docs]    def connect(self, receiver: Any, signal: Any, **kwargs: Any) -> None:\n        \"\"\"\n        Connect a receiver function to a signal.\n\n        The signal can be any object, although Scrapy comes with some\n        predefined signals that are documented in the :ref:`topics-signals`\n        section.\n\n        :param receiver: the function to be connected\n        :type receiver: collections.abc.Callable\n\n        :param signal: the signal to connect to\n        :type signal: object\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        dispatcher.connect(receiver, signal, **kwargs)\n\n[docs]    def disconnect(self, receiver: Any, signal: Any, **kwargs: Any) -> None:\n        \"\"\"\n        Disconnect a receiver function from a signal. This has the\n        opposite effect of the :meth:`connect` method, and the arguments\n        are the same.\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        dispatcher.disconnect(receiver, signal, **kwargs)\n\n[docs]    def send_catch_log(self, signal: Any, **kwargs: Any) -> List[Tuple[Any, Any]]:\n        \"\"\"\n        Send a signal, catch exceptions and log them.\n\n        The keyword arguments are passed to the signal handlers (connected\n        through the :meth:`connect` method).\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        return _signal.send_catch_log(signal, **kwargs)\n\n[docs]    def send_catch_log_deferred(self, signal: Any, **kwargs: Any) -> Deferred:\n        \"\"\"\n        Like :meth:`send_catch_log` but supports returning\n        :class:`~twisted.internet.defer.Deferred` objects from signal handlers.\n\n        Returns a Deferred that gets fired once all signal handlers\n        deferreds were fired. Send a signal, catch exceptions and log them.\n\n        The keyword arguments are passed to the signal handlers (connected\n        through the :meth:`connect` method).\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        return _signal.send_catch_log_deferred(signal, **kwargs)\n\n[docs]    def disconnect_all(self, signal: Any, **kwargs: Any) -> None:\n        \"\"\"\n        Disconnect all receivers from the given signal.\n\n        :param signal: the signal to disconnect from\n        :type signal: object\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        _signal.disconnect_all(signal, **kwargs)\n</pre>"], "links": [{"text": "[docs]", "href": "../../topics/api.html#scrapy.signalmanager.SignalManager"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.signalmanager.SignalManager.connect"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.signalmanager.SignalManager.disconnect"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.signalmanager.SignalManager.send_catch_log"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.signalmanager.SignalManager.send_catch_log_deferred"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.signalmanager.SignalManager.disconnect_all"}], "timestamp": "2023-10-12T00:11:41.044268", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/_modules/scrapy/exporters.html", "content": "                             Source code for scrapy.exporters \"\"\" Item Exporters are used to export/serialize items into different formats. \"\"\"  import csv import io import marshal import pickle import pprint from collections.abc import Mapping from xml.sax.saxutils import XMLGenerator  from itemadapter import ItemAdapter, is_item  from scrapy.item import Item from scrapy.utils.python import is_listlike, to_bytes, to_unicode from scrapy.utils.serialize import ScrapyJSONEncoder  __all__ = [     \"BaseItemExporter\",     \"PprintItemExporter\",     \"PickleItemExporter\",     \"CsvItemExporter\",     \"XmlItemExporter\",     \"JsonLinesItemExporter\",     \"JsonItemExporter\",     \"MarshalItemExporter\", ]   [docs]class BaseItemExporter:     def __init__(self, *, dont_fail=False, **kwargs):         self._kwargs = kwargs         self._configure(kwargs, dont_fail=dont_fail)      def _configure(self, options, dont_fail=False):         \"\"\"Configure the exporter by popping options from the ``options`` dict.         If dont_fail is set, it won't raise an exception on unexpected options         (useful for using with keyword arguments in subclasses ``__init__`` methods)         \"\"\"         self.encoding = options.pop(\"encoding\", None)         self.fields_to_export = options.pop(\"fields_to_export\", None)         self.export_empty_fields = options.pop(\"export_empty_fields\", False)         self.indent = options.pop(\"indent\", None)         if not dont_fail and options:             raise TypeError(f\"Unexpected options: {', '.join(options.keys())}\")  [docs]    def export_item(self, item):         raise NotImplementedError  [docs]    def serialize_field(self, field, name, value):         serializer = field.get(\"serializer\", lambda x: x)         return serializer(value)  [docs]    def start_exporting(self):         pass  [docs]    def finish_exporting(self):         pass      def _get_serialized_fields(self, item, default_value=None, include_empty=None):         \"\"\"Return the fields to export as an iterable of tuples         (name, serialized_value)         \"\"\"         item = ItemAdapter(item)          if include_empty is None:             include_empty = self.export_empty_fields          if self.fields_to_export is None:             if include_empty:                 field_iter = item.field_names()             else:                 field_iter = item.keys()         elif isinstance(self.fields_to_export, Mapping):             if include_empty:                 field_iter = self.fields_to_export.items()             else:                 field_iter = (                     (x, y) for x, y in self.fields_to_export.items() if x in item                 )         else:             if include_empty:                 field_iter = self.fields_to_export             else:                 field_iter = (x for x in self.fields_to_export if x in item)          for field_name in field_iter:             if isinstance(field_name, str):                 item_field, output_field = field_name, field_name             else:                 item_field, output_field = field_name             if item_field in item:                 field_meta = item.get_field_meta(item_field)                 value = self.serialize_field(field_meta, output_field, item[item_field])             else:                 value = default_value              yield output_field, value   [docs]class JsonLinesItemExporter(BaseItemExporter):     def __init__(self, file, **kwargs):         super().__init__(dont_fail=True, **kwargs)         self.file = file         self._kwargs.setdefault(\"ensure_ascii\", not self.encoding)         self.encoder = ScrapyJSONEncoder(**self._kwargs)      def export_item(self, item):         itemdict = dict(self._get_serialized_fields(item))         data = self.encoder.encode(itemdict) + \"\\n\"         self.file.write(to_bytes(data, self.encoding))   [docs]class JsonItemExporter(BaseItemExporter):     def __init__(self, file, **kwargs):         super().__init__(dont_fail=True, **kwargs)         self.file = file         # there is a small difference between the behaviour or JsonItemExporter.indent         # and ScrapyJSONEncoder.indent. ScrapyJSONEncoder.indent=None is needed to prevent         # the addition of newlines everywhere         json_indent = (             self.indent if self.indent is not None and self.indent > 0 else None         )         self._kwargs.setdefault(\"indent\", json_indent)         self._kwargs.setdefault(\"ensure_ascii\", not self.encoding)         self.encoder = ScrapyJSONEncoder(**self._kwargs)         self.first_item = True      def _beautify_newline(self):         if self.indent is not None:             self.file.write(b\"\\n\")      def _add_comma_after_first(self):         if self.first_item:             self.first_item = False         else:             self.file.write(b\",\")             self._beautify_newline()      def start_exporting(self):         self.file.write(b\"[\")         self._beautify_newline()      def finish_exporting(self):         self._beautify_newline()         self.file.write(b\"]\")      def export_item(self, item):         itemdict = dict(self._get_serialized_fields(item))         data = to_bytes(self.encoder.encode(itemdict), self.encoding)         self._add_comma_after_first()         self.file.write(data)   [docs]class XmlItemExporter(BaseItemExporter):     def __init__(self, file, **kwargs):         self.item_element = kwargs.pop(\"item_element\", \"item\")         self.root_element = kwargs.pop(\"root_element\", \"items\")         super().__init__(**kwargs)         if not self.encoding:             self.encoding = \"utf-8\"         self.xg = XMLGenerator(file, encoding=self.encoding)      def _beautify_newline(self, new_item=False):         if self.indent is not None and (self.indent > 0 or new_item):             self.xg.characters(\"\\n\")      def _beautify_indent(self, depth=1):         if self.indent:             self.xg.characters(\" \" * self.indent * depth)      def start_exporting(self):         self.xg.startDocument()         self.xg.startElement(self.root_element, {})         self._beautify_newline(new_item=True)      def export_item(self, item):         self._beautify_indent(depth=1)         self.xg.startElement(self.item_element, {})         self._beautify_newline()         for name, value in self._get_serialized_fields(item, default_value=\"\"):             self._export_xml_field(name, value, depth=2)         self._beautify_indent(depth=1)         self.xg.endElement(self.item_element)         self._beautify_newline(new_item=True)      def finish_exporting(self):         self.xg.endElement(self.root_element)         self.xg.endDocument()      def _export_xml_field(self, name, serialized_value, depth):         self._beautify_indent(depth=depth)         self.xg.startElement(name, {})         if hasattr(serialized_value, \"items\"):             self._beautify_newline()             for subname, value in serialized_value.items():                 self._export_xml_field(subname, value, depth=depth + 1)             self._beautify_indent(depth=depth)         elif is_listlike(serialized_value):             self._beautify_newline()             for value in serialized_value:                 self._export_xml_field(\"value\", value, depth=depth + 1)             self._beautify_indent(depth=depth)         elif isinstance(serialized_value, str):             self.xg.characters(serialized_value)         else:             self.xg.characters(str(serialized_value))         self.xg.endElement(name)         self._beautify_newline()   [docs]class CsvItemExporter(BaseItemExporter):     def __init__(         self,         file,         include_headers_line=True,         join_multivalued=\",\",         errors=None,         **kwargs,     ):         super().__init__(dont_fail=True, **kwargs)         if not self.encoding:             self.encoding = \"utf-8\"         self.include_headers_line = include_headers_line         self.stream = io.TextIOWrapper(             file,             line_buffering=False,             write_through=True,             encoding=self.encoding,             newline=\"\",  # Windows needs this https://github.com/scrapy/scrapy/issues/3034             errors=errors,         )         self.csv_writer = csv.writer(self.stream, **self._kwargs)         self._headers_not_written = True         self._join_multivalued = join_multivalued      def serialize_field(self, field, name, value):         serializer = field.get(\"serializer\", self._join_if_needed)         return serializer(value)      def _join_if_needed(self, value):         if isinstance(value, (list, tuple)):             try:                 return self._join_multivalued.join(value)             except TypeError:  # list in value may not contain strings                 pass         return value      def export_item(self, item):         if self._headers_not_written:             self._headers_not_written = False             self._write_headers_and_set_fields_to_export(item)          fields = self._get_serialized_fields(item, default_value=\"\", include_empty=True)         values = list(self._build_row(x for _, x in fields))         self.csv_writer.writerow(values)      def finish_exporting(self):         self.stream.detach()  # Avoid closing the wrapped file.      def _build_row(self, values):         for s in values:             try:                 yield to_unicode(s, self.encoding)             except TypeError:                 yield s      def _write_headers_and_set_fields_to_export(self, item):         if self.include_headers_line:             if not self.fields_to_export:                 # use declared field names, or keys if the item is a dict                 self.fields_to_export = ItemAdapter(item).field_names()             if isinstance(self.fields_to_export, Mapping):                 fields = self.fields_to_export.values()             else:                 fields = self.fields_to_export             row = list(self._build_row(fields))             self.csv_writer.writerow(row)   [docs]class PickleItemExporter(BaseItemExporter):     def __init__(self, file, protocol=4, **kwargs):         super().__init__(**kwargs)         self.file = file         self.protocol = protocol      def export_item(self, item):         d = dict(self._get_serialized_fields(item))         pickle.dump(d, self.file, self.protocol)   [docs]class MarshalItemExporter(BaseItemExporter):     \"\"\"Exports items in a Python-specific binary format (see     :mod:`marshal`).      :param file: The file-like object to use for exporting the data. Its                  ``write`` method should accept :class:`bytes` (a disk file                  opened in binary mode, a :class:`~io.BytesIO` object, etc)     \"\"\"      def __init__(self, file, **kwargs):         super().__init__(**kwargs)         self.file = file      def export_item(self, item):         marshal.dump(dict(self._get_serialized_fields(item)), self.file)   [docs]class PprintItemExporter(BaseItemExporter):     def __init__(self, file, **kwargs):         super().__init__(**kwargs)         self.file = file      def export_item(self, item):         itemdict = dict(self._get_serialized_fields(item))         self.file.write(to_bytes(pprint.pformat(itemdict) + \"\\n\"))   [docs]class PythonItemExporter(BaseItemExporter):     \"\"\"This is a base class for item exporters that extends     :class:`BaseItemExporter` with support for nested items.      It serializes items to built-in Python types, so that any serialization     library (e.g. :mod:`json` or msgpack_) can be used on top of it.      .. _msgpack: https://pypi.org/project/msgpack/     \"\"\"      def _configure(self, options, dont_fail=False):         super()._configure(options, dont_fail)         if not self.encoding:             self.encoding = \"utf-8\"      def serialize_field(self, field, name, value):         serializer = field.get(\"serializer\", self._serialize_value)         return serializer(value)      def _serialize_value(self, value):         if isinstance(value, Item):             return self.export_item(value)         if is_item(value):             return dict(self._serialize_item(value))         if is_listlike(value):             return [self._serialize_value(v) for v in value]         if isinstance(value, (str, bytes)):             return to_unicode(value, encoding=self.encoding)         return value      def _serialize_item(self, item):         for key, value in ItemAdapter(item).items():             yield key, self._serialize_value(value)      def export_item(self, item):         result = dict(self._get_serialized_fields(item))         return result                         ", "code_blocks": ["\n\"\"\"\nItem Exporters are used to export/serialize items into different formats.\n\"\"\"\n\nimport csv\nimport io\nimport marshal\nimport pickle\nimport pprint\nfrom collections.abc import Mapping\nfrom xml.sax.saxutils import XMLGenerator\n\nfrom itemadapter import ItemAdapter, is_item\n\nfrom scrapy.item import Item\nfrom scrapy.utils.python import is_listlike, to_bytes, to_unicode\nfrom scrapy.utils.serialize import ScrapyJSONEncoder\n\n__all__ = [\n    \"BaseItemExporter\",\n    \"PprintItemExporter\",\n    \"PickleItemExporter\",\n    \"CsvItemExporter\",\n    \"XmlItemExporter\",\n    \"JsonLinesItemExporter\",\n    \"JsonItemExporter\",\n    \"MarshalItemExporter\",\n]\n\n\n[docs]class BaseItemExporter:\n    def __init__(self, *, dont_fail=False, **kwargs):\n        self._kwargs = kwargs\n        self._configure(kwargs, dont_fail=dont_fail)\n\n    def _configure(self, options, dont_fail=False):\n        \"\"\"Configure the exporter by popping options from the ``options`` dict.\n        If dont_fail is set, it won't raise an exception on unexpected options\n        (useful for using with keyword arguments in subclasses ``__init__`` methods)\n        \"\"\"\n        self.encoding = options.pop(\"encoding\", None)\n        self.fields_to_export = options.pop(\"fields_to_export\", None)\n        self.export_empty_fields = options.pop(\"export_empty_fields\", False)\n        self.indent = options.pop(\"indent\", None)\n        if not dont_fail and options:\n            raise TypeError(f\"Unexpected options: {', '.join(options.keys())}\")\n\n[docs]    def export_item(self, item):\n        raise NotImplementedError\n\n[docs]    def serialize_field(self, field, name, value):\n        serializer = field.get(\"serializer\", lambda x: x)\n        return serializer(value)\n\n[docs]    def start_exporting(self):\n        pass\n\n[docs]    def finish_exporting(self):\n        pass\n\n    def _get_serialized_fields(self, item, default_value=None, include_empty=None):\n        \"\"\"Return the fields to export as an iterable of tuples\n        (name, serialized_value)\n        \"\"\"\n        item = ItemAdapter(item)\n\n        if include_empty is None:\n            include_empty = self.export_empty_fields\n\n        if self.fields_to_export is None:\n            if include_empty:\n                field_iter = item.field_names()\n            else:\n                field_iter = item.keys()\n        elif isinstance(self.fields_to_export, Mapping):\n            if include_empty:\n                field_iter = self.fields_to_export.items()\n            else:\n                field_iter = (\n                    (x, y) for x, y in self.fields_to_export.items() if x in item\n                )\n        else:\n            if include_empty:\n                field_iter = self.fields_to_export\n            else:\n                field_iter = (x for x in self.fields_to_export if x in item)\n\n        for field_name in field_iter:\n            if isinstance(field_name, str):\n                item_field, output_field = field_name, field_name\n            else:\n                item_field, output_field = field_name\n            if item_field in item:\n                field_meta = item.get_field_meta(item_field)\n                value = self.serialize_field(field_meta, output_field, item[item_field])\n            else:\n                value = default_value\n\n            yield output_field, value\n\n\n[docs]class JsonLinesItemExporter(BaseItemExporter):\n    def __init__(self, file, **kwargs):\n        super().__init__(dont_fail=True, **kwargs)\n        self.file = file\n        self._kwargs.setdefault(\"ensure_ascii\", not self.encoding)\n        self.encoder = ScrapyJSONEncoder(**self._kwargs)\n\n    def export_item(self, item):\n        itemdict = dict(self._get_serialized_fields(item))\n        data = self.encoder.encode(itemdict) + \"\\n\"\n        self.file.write(to_bytes(data, self.encoding))\n\n\n[docs]class JsonItemExporter(BaseItemExporter):\n    def __init__(self, file, **kwargs):\n        super().__init__(dont_fail=True, **kwargs)\n        self.file = file\n        # there is a small difference between the behaviour or JsonItemExporter.indent\n        # and ScrapyJSONEncoder.indent. ScrapyJSONEncoder.indent=None is needed to prevent\n        # the addition of newlines everywhere\n        json_indent = (\n            self.indent if self.indent is not None and self.indent > 0 else None\n        )\n        self._kwargs.setdefault(\"indent\", json_indent)\n        self._kwargs.setdefault(\"ensure_ascii\", not self.encoding)\n        self.encoder = ScrapyJSONEncoder(**self._kwargs)\n        self.first_item = True\n\n    def _beautify_newline(self):\n        if self.indent is not None:\n            self.file.write(b\"\\n\")\n\n    def _add_comma_after_first(self):\n        if self.first_item:\n            self.first_item = False\n        else:\n            self.file.write(b\",\")\n            self._beautify_newline()\n\n    def start_exporting(self):\n        self.file.write(b\"[\")\n        self._beautify_newline()\n\n    def finish_exporting(self):\n        self._beautify_newline()\n        self.file.write(b\"]\")\n\n    def export_item(self, item):\n        itemdict = dict(self._get_serialized_fields(item))\n        data = to_bytes(self.encoder.encode(itemdict), self.encoding)\n        self._add_comma_after_first()\n        self.file.write(data)\n\n\n[docs]class XmlItemExporter(BaseItemExporter):\n    def __init__(self, file, **kwargs):\n        self.item_element = kwargs.pop(\"item_element\", \"item\")\n        self.root_element = kwargs.pop(\"root_element\", \"items\")\n        super().__init__(**kwargs)\n        if not self.encoding:\n            self.encoding = \"utf-8\"\n        self.xg = XMLGenerator(file, encoding=self.encoding)\n\n    def _beautify_newline(self, new_item=False):\n        if self.indent is not None and (self.indent > 0 or new_item):\n            self.xg.characters(\"\\n\")\n\n    def _beautify_indent(self, depth=1):\n        if self.indent:\n            self.xg.characters(\" \" * self.indent * depth)\n\n    def start_exporting(self):\n        self.xg.startDocument()\n        self.xg.startElement(self.root_element, {})\n        self._beautify_newline(new_item=True)\n\n    def export_item(self, item):\n        self._beautify_indent(depth=1)\n        self.xg.startElement(self.item_element, {})\n        self._beautify_newline()\n        for name, value in self._get_serialized_fields(item, default_value=\"\"):\n            self._export_xml_field(name, value, depth=2)\n        self._beautify_indent(depth=1)\n        self.xg.endElement(self.item_element)\n        self._beautify_newline(new_item=True)\n\n    def finish_exporting(self):\n        self.xg.endElement(self.root_element)\n        self.xg.endDocument()\n\n    def _export_xml_field(self, name, serialized_value, depth):\n        self._beautify_indent(depth=depth)\n        self.xg.startElement(name, {})\n        if hasattr(serialized_value, \"items\"):\n            self._beautify_newline()\n            for subname, value in serialized_value.items():\n                self._export_xml_field(subname, value, depth=depth + 1)\n            self._beautify_indent(depth=depth)\n        elif is_listlike(serialized_value):\n            self._beautify_newline()\n            for value in serialized_value:\n                self._export_xml_field(\"value\", value, depth=depth + 1)\n            self._beautify_indent(depth=depth)\n        elif isinstance(serialized_value, str):\n            self.xg.characters(serialized_value)\n        else:\n            self.xg.characters(str(serialized_value))\n        self.xg.endElement(name)\n        self._beautify_newline()\n\n\n[docs]class CsvItemExporter(BaseItemExporter):\n    def __init__(\n        self,\n        file,\n        include_headers_line=True,\n        join_multivalued=\",\",\n        errors=None,\n        **kwargs,\n    ):\n        super().__init__(dont_fail=True, **kwargs)\n        if not self.encoding:\n            self.encoding = \"utf-8\"\n        self.include_headers_line = include_headers_line\n        self.stream = io.TextIOWrapper(\n            file,\n            line_buffering=False,\n            write_through=True,\n            encoding=self.encoding,\n            newline=\"\",  # Windows needs this https://github.com/scrapy/scrapy/issues/3034\n            errors=errors,\n        )\n        self.csv_writer = csv.writer(self.stream, **self._kwargs)\n        self._headers_not_written = True\n        self._join_multivalued = join_multivalued\n\n    def serialize_field(self, field, name, value):\n        serializer = field.get(\"serializer\", self._join_if_needed)\n        return serializer(value)\n\n    def _join_if_needed(self, value):\n        if isinstance(value, (list, tuple)):\n            try:\n                return self._join_multivalued.join(value)\n            except TypeError:  # list in value may not contain strings\n                pass\n        return value\n\n    def export_item(self, item):\n        if self._headers_not_written:\n            self._headers_not_written = False\n            self._write_headers_and_set_fields_to_export(item)\n\n        fields = self._get_serialized_fields(item, default_value=\"\", include_empty=True)\n        values = list(self._build_row(x for _, x in fields))\n        self.csv_writer.writerow(values)\n\n    def finish_exporting(self):\n        self.stream.detach()  # Avoid closing the wrapped file.\n\n    def _build_row(self, values):\n        for s in values:\n            try:\n                yield to_unicode(s, self.encoding)\n            except TypeError:\n                yield s\n\n    def _write_headers_and_set_fields_to_export(self, item):\n        if self.include_headers_line:\n            if not self.fields_to_export:\n                # use declared field names, or keys if the item is a dict\n                self.fields_to_export = ItemAdapter(item).field_names()\n            if isinstance(self.fields_to_export, Mapping):\n                fields = self.fields_to_export.values()\n            else:\n                fields = self.fields_to_export\n            row = list(self._build_row(fields))\n            self.csv_writer.writerow(row)\n\n\n[docs]class PickleItemExporter(BaseItemExporter):\n    def __init__(self, file, protocol=4, **kwargs):\n        super().__init__(**kwargs)\n        self.file = file\n        self.protocol = protocol\n\n    def export_item(self, item):\n        d = dict(self._get_serialized_fields(item))\n        pickle.dump(d, self.file, self.protocol)\n\n\n[docs]class MarshalItemExporter(BaseItemExporter):\n    \"\"\"Exports items in a Python-specific binary format (see\n    :mod:`marshal`).\n\n    :param file: The file-like object to use for exporting the data. Its\n                 ``write`` method should accept :class:`bytes` (a disk file\n                 opened in binary mode, a :class:`~io.BytesIO` object, etc)\n    \"\"\"\n\n    def __init__(self, file, **kwargs):\n        super().__init__(**kwargs)\n        self.file = file\n\n    def export_item(self, item):\n        marshal.dump(dict(self._get_serialized_fields(item)), self.file)\n\n\n[docs]class PprintItemExporter(BaseItemExporter):\n    def __init__(self, file, **kwargs):\n        super().__init__(**kwargs)\n        self.file = file\n\n    def export_item(self, item):\n        itemdict = dict(self._get_serialized_fields(item))\n        self.file.write(to_bytes(pprint.pformat(itemdict) + \"\\n\"))\n\n\n[docs]class PythonItemExporter(BaseItemExporter):\n    \"\"\"This is a base class for item exporters that extends\n    :class:`BaseItemExporter` with support for nested items.\n\n    It serializes items to built-in Python types, so that any serialization\n    library (e.g. :mod:`json` or msgpack_) can be used on top of it.\n\n    .. _msgpack: https://pypi.org/project/msgpack/\n    \"\"\"\n\n    def _configure(self, options, dont_fail=False):\n        super()._configure(options, dont_fail)\n        if not self.encoding:\n            self.encoding = \"utf-8\"\n\n    def serialize_field(self, field, name, value):\n        serializer = field.get(\"serializer\", self._serialize_value)\n        return serializer(value)\n\n    def _serialize_value(self, value):\n        if isinstance(value, Item):\n            return self.export_item(value)\n        if is_item(value):\n            return dict(self._serialize_item(value))\n        if is_listlike(value):\n            return [self._serialize_value(v) for v in value]\n        if isinstance(value, (str, bytes)):\n            return to_unicode(value, encoding=self.encoding)\n        return value\n\n    def _serialize_item(self, item):\n        for key, value in ItemAdapter(item).items():\n            yield key, self._serialize_value(value)\n\n    def export_item(self, item):\n        result = dict(self._get_serialized_fields(item))\n        return result\n</pre>"], "links": [{"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.BaseItemExporter"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.BaseItemExporter.export_item"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.BaseItemExporter.serialize_field"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.BaseItemExporter.start_exporting"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.BaseItemExporter.finish_exporting"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.JsonLinesItemExporter"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.JsonItemExporter"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.XmlItemExporter"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.CsvItemExporter"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.PickleItemExporter"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.MarshalItemExporter"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.PprintItemExporter"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.PythonItemExporter"}], "timestamp": "2023-10-12T00:11:43.970611", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"}
][
{"url": "https://docs.scrapy.org/en/latest/", "content": {"sections": [], "paragraphs": ["<p>Scrapy is a fast high-level <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Web_crawler\">web crawling</a> and <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Web_scraping\">web scraping</a> framework, used\nto crawl websites and extract structured data from their pages. It can be used\nfor a wide range of purposes, from data mining to monitoring and automated\ntesting.</p>", "<p>Having trouble? We’d like to help!</p>", "<p>Try the <a class=\"reference internal\" href=\"faq.html\"><span class=\"doc\">FAQ</span></a> – it’s got answers to some common questions.</p>", "<p>Looking for specific information? Try the <a class=\"reference internal\" href=\"genindex.html\"><span class=\"std std-ref\">Index</span></a> or <a class=\"reference internal\" href=\"py-modindex.html\"><span class=\"std std-ref\">Module Index</span></a>.</p>", "<p>Ask or search questions in <a class=\"reference external\" href=\"https://stackoverflow.com/tags/scrapy\">StackOverflow using the scrapy tag</a>.</p>", "<p>Ask or search questions in the <a class=\"reference external\" href=\"https://www.reddit.com/r/scrapy/\">Scrapy subreddit</a>.</p>", "<p>Search for questions on the archives of the <a class=\"reference external\" href=\"https://groups.google.com/forum/#!forum/scrapy-users\">scrapy-users mailing list</a>.</p>", "<p>Ask a question in the <a class=\"reference external\" href=\"irc://irc.freenode.net/scrapy\">#scrapy IRC channel</a>,</p>", "<p>Report bugs with Scrapy in our <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues\">issue tracker</a>.</p>", "<p>Join the Discord community <a class=\"reference external\" href=\"https://discord.gg/mv3yErfpvq\">Scrapy Discord</a>.</p>", "<p>Understand what Scrapy is and how it can help you.</p>", "<p>Get Scrapy installed on your computer.</p>", "<p>Write your first Scrapy project.</p>", "<p>Learn more by playing with a pre-made Scrapy project.</p>", "<p>Learn about the command-line tool used to manage your Scrapy project.</p>", "<p>Write the rules to crawl your websites.</p>", "<p>Extract the data from web pages using XPath.</p>", "<p>Test your extraction code in an interactive environment.</p>", "<p>Define the data you want to scrape.</p>", "<p>Populate your items with the extracted data.</p>", "<p>Post-process and store your scraped data.</p>", "<p>Output your scraped data using different formats and storages.</p>", "<p>Understand the classes used to represent HTTP requests and responses.</p>", "<p>Convenient classes to extract links to follow from pages.</p>", "<p>Learn how to configure Scrapy and see all <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#topics-settings-ref\"><span class=\"std std-ref\">available settings</span></a>.</p>", "<p>See all available exceptions and their meaning.</p>", "<p>Learn how to use Python’s builtin logging on Scrapy.</p>", "<p>Collect statistics about your scraping crawler.</p>", "<p>Send email notifications when certain events occur.</p>", "<p>Inspect a running crawler using a built-in Python console.</p>", "<p>Get answers to most frequently asked questions.</p>", "<p>Learn how to debug common problems of your Scrapy spider.</p>", "<p>Learn how to use contracts for testing your spiders.</p>", "<p>Get familiar with some Scrapy common practices.</p>", "<p>Tune Scrapy for crawling a lot domains in parallel.</p>", "<p>Learn how to scrape with your browser’s developer tools.</p>", "<p>Read webpage data that is loaded dynamically.</p>", "<p>Learn how to find and get rid of memory leaks in your crawler.</p>", "<p>Download files and/or images associated with your scraped items.</p>", "<p>Deploying your Scrapy spiders and run them in a remote server.</p>", "<p>Adjust crawl rate dynamically based on load.</p>", "<p>Check how Scrapy performs on your hardware.</p>", "<p>Learn how to pause and resume crawls for large spiders.</p>", "<p>Use the <a class=\"reference external\" href=\"https://docs.python.org/3/reference/compound_stmts.html#async\" title=\"(in Python v3.11)\"><span class=\"xref std std-ref\">coroutine syntax</span></a>.</p>", "<p>Use <a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio.html#module-asyncio\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">asyncio</span></code></a> and <a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio.html#module-asyncio\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">asyncio</span></code></a>-powered libraries.</p>", "<p>Understand the Scrapy architecture.</p>", "<p>Enable and configure third-party extensions.</p>", "<p>Customize how pages get requested and downloaded.</p>", "<p>Customize the input and output of your spiders.</p>", "<p>Extend Scrapy with your custom functionality</p>", "<p>See all available signals and how to work with them.</p>", "<p>Understand the scheduler component.</p>", "<p>Quickly export your scraped items to a file (XML, CSV, etc).</p>", "<p>Learn the common API and some good practices when building custom Scrapy\ncomponents.</p>", "<p>Use it on extensions and middlewares to extend Scrapy functionality.</p>", "<p>See what has changed in recent Scrapy versions.</p>", "<p>Learn how to contribute to the Scrapy project.</p>", "<p>Understand Scrapy versioning and API stability.</p>"]}, "code_blocks": [], "links": [{"text": "web crawling", "href": "https://en.wikipedia.org/wiki/Web_crawler"}, {"text": "web scraping", "href": "https://en.wikipedia.org/wiki/Web_scraping"}, {"text": "FAQ", "href": "faq.html"}, {"text": "Index", "href": "genindex.html"}, {"text": "Module Index", "href": "py-modindex.html"}, {"text": "StackOverflow using the scrapy tag", "href": "https://stackoverflow.com/tags/scrapy"}, {"text": "Scrapy subreddit", "href": "https://www.reddit.com/r/scrapy/"}, {"text": "scrapy-users mailing list", "href": "https://groups.google.com/forum/#!forum/scrapy-users"}, {"text": "#scrapy IRC channel", "href": "irc://irc.freenode.net/scrapy"}, {"text": "issue tracker", "href": "https://github.com/scrapy/scrapy/issues"}, {"text": "Scrapy Discord", "href": "https://discord.gg/mv3yErfpvq"}, {"text": "Scrapy at a glance", "href": "intro/overview.html"}, {"text": "Installation guide", "href": "intro/install.html"}, {"text": "Scrapy Tutorial", "href": "intro/tutorial.html"}, {"text": "Examples", "href": "intro/examples.html"}, {"text": "Command line tool", "href": "topics/commands.html"}, {"text": "Spiders", "href": "topics/spiders.html"}, {"text": "Selectors", "href": "topics/selectors.html"}, {"text": "Scrapy shell", "href": "topics/shell.html"}, {"text": "Items", "href": "topics/items.html"}, {"text": "Item Loaders", "href": "topics/loaders.html"}, {"text": "Item Pipeline", "href": "topics/item-pipeline.html"}, {"text": "Feed exports", "href": "topics/feed-exports.html"}, {"text": "Requests and Responses", "href": "topics/request-response.html"}, {"text": "Link Extractors", "href": "topics/link-extractors.html"}, {"text": "Settings", "href": "topics/settings.html"}, {"text": "available settings", "href": "topics/settings.html#topics-settings-ref"}, {"text": "Exceptions", "href": "topics/exceptions.html"}, {"text": "Logging", "href": "topics/logging.html"}, {"text": "Stats Collection", "href": "topics/stats.html"}, {"text": "Sending e-mail", "href": "topics/email.html"}, {"text": "Telnet Console", "href": "topics/telnetconsole.html"}, {"text": "Frequently Asked Questions", "href": "faq.html"}, {"text": "Debugging Spiders", "href": "topics/debug.html"}, {"text": "Spiders Contracts", "href": "topics/contracts.html"}, {"text": "Common Practices", "href": "topics/practices.html"}, {"text": "Broad Crawls", "href": "topics/broad-crawls.html"}, {"text": "Using your browser’s Developer Tools for scraping", "href": "topics/developer-tools.html"}, {"text": "Selecting dynamically-loaded content", "href": "topics/dynamic-content.html"}, {"text": "Debugging memory leaks", "href": "topics/leaks.html"}, {"text": "Downloading and processing files and images", "href": "topics/media-pipeline.html"}, {"text": "Deploying Spiders", "href": "topics/deploy.html"}, {"text": "AutoThrottle extension", "href": "topics/autothrottle.html"}, {"text": "Benchmarking", "href": "topics/benchmarking.html"}, {"text": "Jobs: pausing and resuming crawls", "href": "topics/jobs.html"}, {"text": "Coroutines", "href": "topics/coroutines.html"}, {"text": "coroutine syntax", "href": "https://docs.python.org/3/reference/compound_stmts.html#async"}, {"text": "asyncio", "href": "topics/asyncio.html"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "Architecture overview", "href": "topics/architecture.html"}, {"text": "Add-ons", "href": "topics/addons.html"}, {"text": "Downloader Middleware", "href": "topics/downloader-middleware.html"}, {"text": "Spider Middleware", "href": "topics/spider-middleware.html"}, {"text": "Extensions", "href": "topics/extensions.html"}, {"text": "Signals", "href": "topics/signals.html"}, {"text": "Scheduler", "href": "topics/scheduler.html"}, {"text": "Item Exporters", "href": "topics/exporters.html"}, {"text": "Components", "href": "topics/components.html"}, {"text": "Core API", "href": "topics/api.html"}, {"text": "Release notes", "href": "news.html"}, {"text": "Contributing to Scrapy", "href": "contributing.html"}, {"text": "Versioning and API stability", "href": "versioning.html"}], "timestamp": "2023-10-12T21:19:08.260019", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/news.html", "content": {"sections": [], "paragraphs": ["<p>Highlights:</p>", "<p>Spiders can now modify <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#topics-settings\"><span class=\"std std-ref\">settings</span></a> in their\n<a class=\"reference internal\" href=\"topics/spiders.html#scrapy.Spider.from_crawler\" title=\"scrapy.Spider.from_crawler\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">from_crawler()</span></code></a> methods, e.g. based on <a class=\"hoverxref tooltip reference internal\" href=\"topics/spiders.html#spiderargs\"><span class=\"std std-ref\">spider\narguments</span></a>.</p>", "<p>Periodic logging of stats.</p>", "<p>Most of the initialization of <a class=\"reference internal\" href=\"topics/api.html#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.crawler.Crawler</span></code></a> instances is\nnow done in <a class=\"reference internal\" href=\"topics/api.html#scrapy.crawler.Crawler.crawl\" title=\"scrapy.crawler.Crawler.crawl\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">crawl()</span></code></a>, so the state of\ninstances before that method is called is now different compared to older\nScrapy versions. We do not recommend using the\n<a class=\"reference internal\" href=\"topics/api.html#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a> instances before\n<a class=\"reference internal\" href=\"topics/api.html#scrapy.crawler.Crawler.crawl\" title=\"scrapy.crawler.Crawler.crawl\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">crawl()</span></code></a> is called. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6038\">issue 6038</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/spiders.html#scrapy.Spider.from_crawler\" title=\"scrapy.Spider.from_crawler\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">scrapy.Spider.from_crawler()</span></code></a> is now called before the initialization\nof various components previously initialized in\n<code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">scrapy.crawler.Crawler.__init__()</span></code> and before the settings are\nfinalized and frozen. This change was needed to allow changing the settings\nin <a class=\"reference internal\" href=\"topics/spiders.html#scrapy.Spider.from_crawler\" title=\"scrapy.Spider.from_crawler\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">scrapy.Spider.from_crawler()</span></code></a>. If you want to access the final\nsetting values in the spider code as early as possible you can do this in\n<a class=\"reference internal\" href=\"topics/spiders.html#scrapy.Spider.start_requests\" title=\"scrapy.Spider.start_requests\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">start_requests()</span></code></a>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6038\">issue 6038</a>)</p>", "<p>The <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.TextResponse.json\" title=\"scrapy.http.TextResponse.json\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">TextResponse.json</span></code></a> method now\nrequires the response to be in a valid JSON encoding (UTF-8, UTF-16, or\nUTF-32). If you need to deal with JSON documents in an invalid encoding,\nuse <code class=\"docutils literal notranslate\"><span class=\"pre\">json.loads(response.text)</span></code> instead. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6016\">issue 6016</a>)</p>", "<p>Removed the binary export mode of\n<a class=\"reference internal\" href=\"topics/exporters.html#scrapy.exporters.PythonItemExporter\" title=\"scrapy.exporters.PythonItemExporter\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">PythonItemExporter</span></code></a>, deprecated in Scrapy 1.1.0.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6006\">issue 6006</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6007\">issue 6007</a>)</p>", "<p class=\"admonition-title\">Note</p>", "<p>If you are using this Scrapy version on Scrapy Cloud with a stack\nthat includes an older Scrapy version and get a “TypeError:\nUnexpected options: binary” error, you may need to add\n<code class=\"docutils literal notranslate\"><span class=\"pre\">scrapinghub-entrypoint-scrapy</span> <span class=\"pre\">&gt;=</span> <span class=\"pre\">0.14.1</span></code> to your project\nrequirements or switch to a stack that includes Scrapy 2.11.</p>", "<p>Removed the <code class=\"docutils literal notranslate\"><span class=\"pre\">CrawlerRunner.spiders</span></code> attribute, deprecated in Scrapy\n1.0.0, use <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">CrawlerRunner.spider_loader</span></code> instead. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6010\">issue 6010</a>)</p>", "<p>Running <a class=\"reference internal\" href=\"topics/api.html#scrapy.crawler.Crawler.crawl\" title=\"scrapy.crawler.Crawler.crawl\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">crawl()</span></code></a> more than once on the same\n<a class=\"reference internal\" href=\"topics/api.html#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.crawler.Crawler</span></code></a> instance is now deprecated. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1587\">issue 1587</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6040\">issue 6040</a>)</p>", "<p>Spiders can now modify settings in their\n<a class=\"reference internal\" href=\"topics/spiders.html#scrapy.Spider.from_crawler\" title=\"scrapy.Spider.from_crawler\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">from_crawler()</span></code></a> method, e.g. based on <a class=\"hoverxref tooltip reference internal\" href=\"topics/spiders.html#spiderargs\"><span class=\"std std-ref\">spider\narguments</span></a>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1305\">issue 1305</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1580\">issue 1580</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2392\">issue 2392</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3663\">issue 3663</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6038\">issue 6038</a>)</p>", "<p>Added the <a class=\"reference internal\" href=\"topics/extensions.html#scrapy.extensions.periodic_log.PeriodicLog\" title=\"scrapy.extensions.periodic_log.PeriodicLog\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">PeriodicLog</span></code></a> extension\nwhich can be enabled to log stats and/or their differences periodically.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5926\">issue 5926</a>)</p>", "<p>Optimized the memory usage in <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.TextResponse.json\" title=\"scrapy.http.TextResponse.json\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">TextResponse.json</span></code></a> by removing unnecessary body decoding.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5968\">issue 5968</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6016\">issue 6016</a>)</p>", "<p>Links to <code class=\"docutils literal notranslate\"><span class=\"pre\">.webp</span></code> files are now ignored by <a class=\"hoverxref tooltip reference internal\" href=\"topics/link-extractors.html#topics-link-extractors\"><span class=\"std std-ref\">link extractors</span></a>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6021\">issue 6021</a>)</p>", "<p>Fixed logging enabled add-ons. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6036\">issue 6036</a>)</p>", "<p>Fixed <a class=\"reference internal\" href=\"topics/email.html#scrapy.mail.MailSender\" title=\"scrapy.mail.MailSender\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MailSender</span></code></a> producing invalid message bodies\nwhen the <code class=\"docutils literal notranslate\"><span class=\"pre\">charset</span></code> argument is passed to\n<a class=\"reference internal\" href=\"topics/email.html#scrapy.mail.MailSender.send\" title=\"scrapy.mail.MailSender.send\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">send()</span></code></a>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5096\">issue 5096</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5118\">issue 5118</a>)</p>", "<p>Fixed an exception when accessing <code class=\"docutils literal notranslate\"><span class=\"pre\">self.EXCEPTIONS_TO_RETRY</span></code> from a\nsubclass of <a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.retry.RetryMiddleware\" title=\"scrapy.downloadermiddlewares.retry.RetryMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RetryMiddleware</span></code></a>.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6049\">issue 6049</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6050\">issue 6050</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/api.html#scrapy.settings.BaseSettings.getdictorlist\" title=\"scrapy.settings.BaseSettings.getdictorlist\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">scrapy.settings.BaseSettings.getdictorlist()</span></code></a>, used to parse\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEED_EXPORT_FIELDS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEED_EXPORT_FIELDS</span></code></a>, now handles tuple values. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6011\">issue 6011</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6013\">issue 6013</a>)</p>", "<p>Calls to <code class=\"docutils literal notranslate\"><span class=\"pre\">datetime.utcnow()</span></code>, no longer recommended to be used, have been\nreplaced with calls to <code class=\"docutils literal notranslate\"><span class=\"pre\">datetime.now()</span></code> with a timezone. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6014\">issue 6014</a>)</p>", "<p>Updated a deprecated function call in a pipeline example. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6008\">issue 6008</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6009\">issue 6009</a>)</p>", "<p>Extended typing hints. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6003\">issue 6003</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6005\">issue 6005</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6031\">issue 6031</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6034\">issue 6034</a>)</p>", "<p>Pinned <a class=\"reference external\" href=\"https://github.com/google/brotli\">brotli</a> to 1.0.9 for the PyPy tests as 1.1.0 breaks them.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6044\">issue 6044</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6045\">issue 6045</a>)</p>", "<p>Other CI and pre-commit improvements. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6002\">issue 6002</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6013\">issue 6013</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6046\">issue 6046</a>)</p>", "<p>Marked <code class=\"docutils literal notranslate\"><span class=\"pre\">Twisted</span> <span class=\"pre\">&gt;=</span> <span class=\"pre\">23.8.0</span></code> as unsupported. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6024\">issue 6024</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6026\">issue 6026</a>)</p>", "<p>Highlights:</p>", "<p>Added Python 3.12 support, dropped Python 3.7 support.</p>", "<p>The new add-ons framework simplifies configuring 3rd-party components that\nsupport it.</p>", "<p>Exceptions to retry can now be configured.</p>", "<p>Many fixes and improvements for feed exports.</p>", "<p>Dropped support for Python 3.7. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5953\">issue 5953</a>)</p>", "<p>Added support for the upcoming Python 3.12. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5984\">issue 5984</a>)</p>", "<p>Minimum versions increased for these dependencies:</p>", "<p><a class=\"reference external\" href=\"https://lxml.de/\">lxml</a>: 4.3.0 → 4.4.1</p>", "<p><a class=\"reference external\" href=\"https://cryptography.io/en/latest/\">cryptography</a>: 3.4.6 → 36.0.0</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">pkg_resources</span></code> is no longer used. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5956\">issue 5956</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5958\">issue 5958</a>)</p>", "<p><a class=\"reference external\" href=\"https://github.com/boto/boto3\">boto3</a> is now recommended instead of <a class=\"reference external\" href=\"https://github.com/boto/botocore\">botocore</a> for exporting to S3.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5833\">issue 5833</a>).</p>", "<p>The value of the <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEED_STORE_EMPTY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEED_STORE_EMPTY</span></code></a> setting is now <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code>\ninstead of <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code>. In earlier Scrapy versions empty files were created\neven when this setting was <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code> (which was a bug that is now fixed),\nso the new default should keep the old behavior. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/872\">issue 872</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5847\">issue 5847</a>)</p>", "<p>When a function is assigned to the <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEED_URI_PARAMS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEED_URI_PARAMS</span></code></a> setting,\nreturning <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> or modifying the <code class=\"docutils literal notranslate\"><span class=\"pre\">params</span></code> input parameter, deprecated\nin Scrapy 2.6, is no longer supported. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5994\">issue 5994</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5996\">issue 5996</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.reqser</span></code> module, deprecated in Scrapy 2.6, is removed.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5994\">issue 5994</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5996\">issue 5996</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.squeues</span></code> classes <code class=\"docutils literal notranslate\"><span class=\"pre\">PickleFifoDiskQueueNonRequest</span></code>,\n<code class=\"docutils literal notranslate\"><span class=\"pre\">PickleLifoDiskQueueNonRequest</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">MarshalFifoDiskQueueNonRequest</span></code>,\nand <code class=\"docutils literal notranslate\"><span class=\"pre\">MarshalLifoDiskQueueNonRequest</span></code>, deprecated in\nScrapy 2.6, are removed. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5994\">issue 5994</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5996\">issue 5996</a>)</p>", "<p>The property <code class=\"docutils literal notranslate\"><span class=\"pre\">open_spiders</span></code> and the methods <code class=\"docutils literal notranslate\"><span class=\"pre\">has_capacity</span></code> and\n<code class=\"docutils literal notranslate\"><span class=\"pre\">schedule</span></code> of <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.core.engine.ExecutionEngine</span></code>,\ndeprecated in Scrapy 2.6, are removed. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5994\">issue 5994</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5998\">issue 5998</a>)</p>", "<p>Passing a <code class=\"docutils literal notranslate\"><span class=\"pre\">spider</span></code> argument to the\n<code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">spider_is_idle()</span></code>,\n<code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">crawl()</span></code> and\n<code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">download()</span></code> methods of\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.core.engine.ExecutionEngine</span></code>, deprecated in Scrapy 2.6, is\nno longer supported. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5994\">issue 5994</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5998\">issue 5998</a>)</p>", "<p><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.utils.datatypes.CaselessDict</span></code> is deprecated, use\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.utils.datatypes.CaseInsensitiveDict</span></code> instead.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5146\">issue 5146</a>)</p>", "<p>Passing the <code class=\"docutils literal notranslate\"><span class=\"pre\">custom</span></code> argument to\n<code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.utils.conf.build_component_list()</span></code> is deprecated, it was used\nin the past to merge <code class=\"docutils literal notranslate\"><span class=\"pre\">FOO</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">FOO_BASE</span></code> setting values but now Scrapy\nuses <a class=\"reference internal\" href=\"topics/api.html#scrapy.settings.BaseSettings.getwithbase\" title=\"scrapy.settings.BaseSettings.getwithbase\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.settings.BaseSettings.getwithbase()</span></code></a> to do the same.\nCode that uses this argument and cannot be switched to <code class=\"docutils literal notranslate\"><span class=\"pre\">getwithbase()</span></code>\ncan be switched to merging the values explicitly. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5726\">issue 5726</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5923\">issue 5923</a>)</p>", "<p>Added support for <a class=\"hoverxref tooltip reference internal\" href=\"topics/addons.html#topics-addons\"><span class=\"std std-ref\">Scrapy add-ons</span></a>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5950\">issue 5950</a>)</p>", "<p>Added the <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-setting-RETRY_EXCEPTIONS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">RETRY_EXCEPTIONS</span></code></a> setting that configures which\nexceptions will be retried by\n<a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.retry.RetryMiddleware\" title=\"scrapy.downloadermiddlewares.retry.RetryMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RetryMiddleware</span></code></a>.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2701\">issue 2701</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5929\">issue 5929</a>)</p>", "<p>Added the possiiblity to close the spider if no items were produced in the\nspecified time, configured by <a class=\"hoverxref tooltip reference internal\" href=\"topics/extensions.html#std-setting-CLOSESPIDER_TIMEOUT_NO_ITEM\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">CLOSESPIDER_TIMEOUT_NO_ITEM</span></code></a>.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5979\">issue 5979</a>)</p>", "<p>Added support for the <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-AWS_REGION_NAME\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">AWS_REGION_NAME</span></code></a> setting to feed exports.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5980\">issue 5980</a>)</p>", "<p>Added support for using <a class=\"reference external\" href=\"https://docs.python.org/3/library/pathlib.html#pathlib.Path\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pathlib.Path</span></code></a> objects that refer to\nabsolute Windows paths in the <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEEDS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEEDS</span></code></a> setting. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5939\">issue 5939</a>)</p>", "<p>Fixed creating empty feeds even with <code class=\"docutils literal notranslate\"><span class=\"pre\">FEED_STORE_EMPTY=False</span></code>.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/872\">issue 872</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5847\">issue 5847</a>)</p>", "<p>Fixed using absolute Windows paths when specifying output files.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5969\">issue 5969</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5971\">issue 5971</a>)</p>", "<p>Fixed problems with uploading large files to S3 by switching to multipart\nuploads (requires <a class=\"reference external\" href=\"https://github.com/boto/boto3\">boto3</a>). (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/960\">issue 960</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5735\">issue 5735</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5833\">issue 5833</a>)</p>", "<p>Fixed the JSON exporter writing extra commas when some exceptions occur.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3090\">issue 3090</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5952\">issue 5952</a>)</p>", "<p>Fixed the “read of closed file” error in the CSV exporter. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5043\">issue 5043</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5705\">issue 5705</a>)</p>", "<p>Fixed an error when a component added by the class object throws\n<a class=\"reference internal\" href=\"topics/exceptions.html#scrapy.exceptions.NotConfigured\" title=\"scrapy.exceptions.NotConfigured\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">NotConfigured</span></code></a> with a message. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5950\">issue 5950</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5992\">issue 5992</a>)</p>", "<p>Added the missing <a class=\"reference internal\" href=\"topics/api.html#scrapy.settings.BaseSettings.pop\" title=\"scrapy.settings.BaseSettings.pop\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">scrapy.settings.BaseSettings.pop()</span></code></a> method.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5959\">issue 5959</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5960\">issue 5960</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5963\">issue 5963</a>)</p>", "<p>Added <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CaseInsensitiveDict</span></code> as a replacement\nfor <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CaselessDict</span></code> that fixes some API\ninconsistencies. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5146\">issue 5146</a>)</p>", "<p>Documented <a class=\"reference internal\" href=\"topics/spiders.html#scrapy.Spider.update_settings\" title=\"scrapy.Spider.update_settings\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">scrapy.Spider.update_settings()</span></code></a>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5745\">issue 5745</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5846\">issue 5846</a>)</p>", "<p>Documented possible problems with early Twisted reactor installation and\ntheir solutions. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5981\">issue 5981</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/6000\">issue 6000</a>)</p>", "<p>Added examples of making additional requests in callbacks. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5927\">issue 5927</a>)</p>", "<p>Improved the feed export docs. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5579\">issue 5579</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5931\">issue 5931</a>)</p>", "<p>Clarified the docs about request objects on redirection. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5707\">issue 5707</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5937\">issue 5937</a>)</p>", "<p>Added support for running tests against the installed Scrapy version.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4914\">issue 4914</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5949\">issue 5949</a>)</p>", "<p>Extended typing hints. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5925\">issue 5925</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5977\">issue 5977</a>)</p>", "<p>Fixed the <code class=\"docutils literal notranslate\"><span class=\"pre\">test_utils_asyncio.AsyncioTest.test_set_asyncio_event_loop</span></code>\ntest. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5951\">issue 5951</a>)</p>", "<p>Fixed the <code class=\"docutils literal notranslate\"><span class=\"pre\">test_feedexport.BatchDeliveriesTest.test_batch_path_differ</span></code>\ntest on Windows. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5847\">issue 5847</a>)</p>", "<p>Enabled CI runs for Python 3.11 on Windows. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5999\">issue 5999</a>)</p>", "<p>Simplified skipping tests that depend on <code class=\"docutils literal notranslate\"><span class=\"pre\">uvloop</span></code>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5984\">issue 5984</a>)</p>", "<p>Fixed the <code class=\"docutils literal notranslate\"><span class=\"pre\">extra-deps-pinned</span></code> tox env. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5948\">issue 5948</a>)</p>", "<p>Implemented cleanups. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5965\">issue 5965</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5986\">issue 5986</a>)</p>", "<p>Highlights:</p>", "<p>Per-domain download settings.</p>", "<p>Compatibility with new <a class=\"reference external\" href=\"https://cryptography.io/en/latest/\">cryptography</a> and new <a class=\"reference external\" href=\"https://github.com/scrapy/parsel\">parsel</a>.</p>", "<p>JMESPath selectors from the new <a class=\"reference external\" href=\"https://github.com/scrapy/parsel\">parsel</a>.</p>", "<p>Bug fixes.</p>", "<p><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.extensions.feedexport._FeedSlot</span></code> is renamed to\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.extensions.feedexport.FeedSlot</span></code> and the old name is\ndeprecated. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5876\">issue 5876</a>)</p>", "<p>Settings correponding to <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DOWNLOAD_DELAY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOAD_DELAY</span></code></a>,\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> and\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-RANDOMIZE_DOWNLOAD_DELAY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">RANDOMIZE_DOWNLOAD_DELAY</span></code></a> can now be set on a per-domain basis\nvia the new <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DOWNLOAD_SLOTS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOAD_SLOTS</span></code></a> setting. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5328\">issue 5328</a>)</p>", "<p>Added <code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">TextResponse.jmespath()</span></code>, a shortcut for JMESPath selectors\navailable since <a class=\"reference external\" href=\"https://github.com/scrapy/parsel\">parsel</a> 1.8.1. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5894\">issue 5894</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5915\">issue 5915</a>)</p>", "<p>Added <a class=\"hoverxref tooltip reference internal\" href=\"topics/signals.html#std-signal-feed_slot_closed\"><code class=\"xref std std-signal docutils literal notranslate\"><span class=\"pre\">feed_slot_closed</span></code></a> and <a class=\"hoverxref tooltip reference internal\" href=\"topics/signals.html#std-signal-feed_exporter_closed\"><code class=\"xref std std-signal docutils literal notranslate\"><span class=\"pre\">feed_exporter_closed</span></code></a>\nsignals. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5876\">issue 5876</a>)</p>", "<p>Added <code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.utils.request.request_to_curl()</span></code>, a function to produce a\ncurl command from a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> object. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5892\">issue 5892</a>)</p>", "<p>Values of <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#std-setting-FILES_STORE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FILES_STORE</span></code></a> and <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#std-setting-IMAGES_STORE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">IMAGES_STORE</span></code></a> can now be\n<a class=\"reference external\" href=\"https://docs.python.org/3/library/pathlib.html#pathlib.Path\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pathlib.Path</span></code></a> instances. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5801\">issue 5801</a>)</p>", "<p>Fixed a warning with Parsel 1.8.1+. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5903\">issue 5903</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5918\">issue 5918</a>)</p>", "<p>Fixed an error when using feed postprocessing with S3 storage.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5500\">issue 5500</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5581\">issue 5581</a>)</p>", "<p>Added the missing <a class=\"reference internal\" href=\"topics/api.html#scrapy.settings.BaseSettings.setdefault\" title=\"scrapy.settings.BaseSettings.setdefault\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">scrapy.settings.BaseSettings.setdefault()</span></code></a> method.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5811\">issue 5811</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5821\">issue 5821</a>)</p>", "<p>Fixed an error when using <a class=\"reference external\" href=\"https://cryptography.io/en/latest/\">cryptography</a> 40.0.0+ and\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING</span></code></a> is enabled.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5857\">issue 5857</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5858\">issue 5858</a>)</p>", "<p>The checksums returned by <a class=\"reference internal\" href=\"topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline\" title=\"scrapy.pipelines.files.FilesPipeline\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">FilesPipeline</span></code></a>\nfor files on Google Cloud Storage are no longer Base64-encoded.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5874\">issue 5874</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5891\">issue 5891</a>)</p>", "<p><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.utils.request.request_from_curl()</span></code> now supports $-prefixed\nstring values for the curl <code class=\"docutils literal notranslate\"><span class=\"pre\">--data-raw</span></code> argument, which are produced by\nbrowsers for data that includes certain symbols. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5899\">issue 5899</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5901\">issue 5901</a>)</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-parse\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">parse</span></code></a> command now also works with async generator callbacks.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5819\">issue 5819</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5824\">issue 5824</a>)</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-genspider\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">genspider</span></code></a> command now properly works with HTTPS URLs.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3553\">issue 3553</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5808\">issue 5808</a>)</p>", "<p>Improved handling of asyncio loops. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5831\">issue 5831</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5832\">issue 5832</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\" title=\"scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">LinkExtractor</span></code></a>\nnow skips certain malformed URLs instead of raising an exception.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5881\">issue 5881</a>)</p>", "<p><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.utils.python.get_func_args()</span></code> now supports more types of\ncallables. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5872\">issue 5872</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5885\">issue 5885</a>)</p>", "<p>Fixed an error when processing non-UTF8 values of <code class=\"docutils literal notranslate\"><span class=\"pre\">Content-Type</span></code> headers.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5914\">issue 5914</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5917\">issue 5917</a>)</p>", "<p>Fixed an error breaking user handling of send failures in\n<a class=\"reference internal\" href=\"topics/email.html#scrapy.mail.MailSender.send\" title=\"scrapy.mail.MailSender.send\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">scrapy.mail.MailSender.send()</span></code></a>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1611\">issue 1611</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5880\">issue 5880</a>)</p>", "<p>Expanded contributing docs. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5109\">issue 5109</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5851\">issue 5851</a>)</p>", "<p>Added <a class=\"reference external\" href=\"https://github.com/adamchainz/blacken-docs\">blacken-docs</a> to pre-commit and reformatted the docs with it.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5813\">issue 5813</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5816\">issue 5816</a>)</p>", "<p>Fixed a JS issue. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5875\">issue 5875</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5877\">issue 5877</a>)</p>", "<p>Fixed <code class=\"docutils literal notranslate\"><span class=\"pre\">make</span> <span class=\"pre\">htmlview</span></code>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5878\">issue 5878</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5879\">issue 5879</a>)</p>", "<p>Fixed typos and other small errors. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5827\">issue 5827</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5839\">issue 5839</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5883\">issue 5883</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5890\">issue 5890</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5895\">issue 5895</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5904\">issue 5904</a>)</p>", "<p>Extended typing hints. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5805\">issue 5805</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5889\">issue 5889</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5896\">issue 5896</a>)</p>", "<p>Tests for most of the examples in the docs are now run as a part of CI,\nfound problems were fixed. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5816\">issue 5816</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5826\">issue 5826</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5919\">issue 5919</a>)</p>", "<p>Removed usage of deprecated Python classes. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5849\">issue 5849</a>)</p>", "<p>Silenced <code class=\"docutils literal notranslate\"><span class=\"pre\">include-ignored</span></code> warnings from coverage. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5820\">issue 5820</a>)</p>", "<p>Fixed a random failure of the <code class=\"docutils literal notranslate\"><span class=\"pre\">test_feedexport.test_batch_path_differ</span></code>\ntest. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5855\">issue 5855</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5898\">issue 5898</a>)</p>", "<p>Updated docstrings to match output produced by <a class=\"reference external\" href=\"https://github.com/scrapy/parsel\">parsel</a> 1.8.1 so that they\ndon’t cause test failures. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5902\">issue 5902</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5919\">issue 5919</a>)</p>", "<p>Other CI and pre-commit improvements. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5802\">issue 5802</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5823\">issue 5823</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5908\">issue 5908</a>)</p>", "<p>This is a maintenance release, with minor features, bug fixes, and cleanups.</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.gz.read1</span></code> function, deprecated in Scrapy 2.0, has now\nbeen removed. Use the <a class=\"reference external\" href=\"https://docs.python.org/3/library/io.html#io.BufferedIOBase.read1\" title=\"(in Python v3.11)\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">read1()</span></code></a> method of\n<a class=\"reference external\" href=\"https://docs.python.org/3/library/gzip.html#gzip.GzipFile\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">GzipFile</span></code></a> instead.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5719\">issue 5719</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.python.to_native_str</span></code> function, deprecated in Scrapy\n2.0, has now been removed. Use <code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.utils.python.to_unicode()</span></code>\ninstead.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5719\">issue 5719</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.python.MutableChain.next</span></code> method, deprecated in Scrapy\n2.0, has now been removed. Use\n<code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">__next__()</span></code> instead.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5719\">issue 5719</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.linkextractors.FilteringLinkExtractor</span></code> class, deprecated\nin Scrapy 2.0, has now been removed. Use\n<a class=\"reference internal\" href=\"topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\" title=\"scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">LinkExtractor</span></code></a>\ninstead.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5720\">issue 5720</a>)</p>", "<p>Support for using environment variables prefixed with <code class=\"docutils literal notranslate\"><span class=\"pre\">SCRAPY_</span></code> to\noverride settings, deprecated in Scrapy 2.0, has now been removed.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5724\">issue 5724</a>)</p>", "<p>Support for the <code class=\"docutils literal notranslate\"><span class=\"pre\">noconnect</span></code> query string argument in proxy URLs,\ndeprecated in Scrapy 2.0, has now been removed. We expect proxies that used\nto need it to work fine without it.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5731\">issue 5731</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.python.retry_on_eintr</span></code> function, deprecated in Scrapy\n2.3, has now been removed.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5719\">issue 5719</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.python.WeakKeyCache</span></code> class, deprecated in Scrapy 2.4,\nhas now been removed.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5719\">issue 5719</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.boto.is_botocore()</span></code> function, deprecated in Scrapy 2.4,\nhas now been removed.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5719\">issue 5719</a>)</p>", "<p><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">scrapy.pipelines.images.NoimagesDrop</span></code> is now deprecated.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5368\">issue 5368</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5489\">issue 5489</a>)</p>", "<p><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">ImagesPipeline.convert_image</span></code> must now accept a\n<code class=\"docutils literal notranslate\"><span class=\"pre\">response_body</span></code> parameter.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3055\">issue 3055</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3689\">issue 3689</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4753\">issue 4753</a>)</p>", "<p>Applied <a class=\"reference external\" href=\"https://black.readthedocs.io/en/stable/\">black</a> coding style to files generated with the\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-genspider\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">genspider</span></code></a> and <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-startproject\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">startproject</span></code></a> commands.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5809\">issue 5809</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5814\">issue 5814</a>)</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEED_EXPORT_ENCODING\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEED_EXPORT_ENCODING</span></code></a> is now set to <code class=\"docutils literal notranslate\"><span class=\"pre\">\"utf-8\"</span></code> in the\n<code class=\"docutils literal notranslate\"><span class=\"pre\">settings.py</span></code> file that the <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-startproject\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">startproject</span></code></a> command generates.\nWith this value, JSON exports won’t force the use of escape sequences for\nnon-ASCII characters.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5797\">issue 5797</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5800\">issue 5800</a>)</p>", "<p>The <a class=\"reference internal\" href=\"topics/extensions.html#scrapy.extensions.memusage.MemoryUsage\" title=\"scrapy.extensions.memusage.MemoryUsage\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MemoryUsage</span></code></a> extension now logs the\npeak memory usage during checks, and the binary unit MiB is now used to\navoid confusion.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5717\">issue 5717</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5722\">issue 5722</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5727\">issue 5727</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">callback</span></code> parameter of <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> can now be set\nto <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.request.NO_CALLBACK\" title=\"scrapy.http.request.NO_CALLBACK\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.http.request.NO_CALLBACK()</span></code></a>, to distinguish it from\n<code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>, as the latter indicates that the default spider callback\n(<a class=\"reference internal\" href=\"topics/spiders.html#scrapy.Spider.parse\" title=\"scrapy.Spider.parse\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">parse()</span></code></a>) is to be used.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5798\">issue 5798</a>)</p>", "<p>Enabled unsafe legacy SSL renegotiation to fix access to some outdated\nwebsites.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5491\">issue 5491</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5790\">issue 5790</a>)</p>", "<p>Fixed STARTTLS-based email delivery not working with Twisted 21.2.0 and\nbetter.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5386\">issue 5386</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5406\">issue 5406</a>)</p>", "<p>Fixed the <code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">finish_exporting()</span></code> method of <a class=\"hoverxref tooltip reference internal\" href=\"topics/exporters.html#topics-exporters\"><span class=\"std std-ref\">item exporters</span></a> not being called for empty files.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5537\">issue 5537</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5758\">issue 5758</a>)</p>", "<p>Fixed HTTP/2 responses getting only the last value for a header when\nmultiple headers with the same name are received.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5777\">issue 5777</a>)</p>", "<p>Fixed an exception raised by the <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-shell\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">shell</span></code></a> command on some cases\nwhen <a class=\"hoverxref tooltip reference internal\" href=\"topics/asyncio.html#using-asyncio\"><span class=\"std std-ref\">using asyncio</span></a>.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5740\">issue 5740</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5742\">issue 5742</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5748\">issue 5748</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5759\">issue 5759</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5760\">issue 5760</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5771\">issue 5771</a>)</p>", "<p>When using <a class=\"reference internal\" href=\"topics/spiders.html#scrapy.spiders.CrawlSpider\" title=\"scrapy.spiders.CrawlSpider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CrawlSpider</span></code></a>, callback keyword arguments\n(<code class=\"docutils literal notranslate\"><span class=\"pre\">cb_kwargs</span></code>) added to a request in the <code class=\"docutils literal notranslate\"><span class=\"pre\">process_request</span></code> callback of a\n<a class=\"reference internal\" href=\"topics/spiders.html#scrapy.spiders.Rule\" title=\"scrapy.spiders.Rule\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Rule</span></code></a> will no longer be ignored.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5699\">issue 5699</a>)</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#images-pipeline\"><span class=\"std std-ref\">images pipeline</span></a> no longer re-encodes JPEG\nfiles.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3055\">issue 3055</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3689\">issue 3689</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4753\">issue 4753</a>)</p>", "<p>Fixed the handling of transparent WebP images by the <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#images-pipeline\"><span class=\"std std-ref\">images pipeline</span></a>.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3072\">issue 3072</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5766\">issue 5766</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5767\">issue 5767</a>)</p>", "<p><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.shell.inspect_response()</span></code> no longer inhibits <code class=\"docutils literal notranslate\"><span class=\"pre\">SIGINT</span></code>\n(Ctrl+C).\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2918\">issue 2918</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\" title=\"scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">LinkExtractor</span></code></a>\nwith <code class=\"docutils literal notranslate\"><span class=\"pre\">unique=False</span></code> no longer filters out links that have identical URL\n<em>and</em> text.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3798\">issue 3798</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3799\">issue 3799</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4695\">issue 4695</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5458\">issue 5458</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware\" title=\"scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RobotsTxtMiddleware</span></code></a> now\nignores URL protocols that do not support <code class=\"docutils literal notranslate\"><span class=\"pre\">robots.txt</span></code> (<code class=\"docutils literal notranslate\"><span class=\"pre\">data://</span></code>,\n<code class=\"docutils literal notranslate\"><span class=\"pre\">file://</span></code>).\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5807\">issue 5807</a>)</p>", "<p>Silenced the <code class=\"docutils literal notranslate\"><span class=\"pre\">filelock</span></code> debug log messages introduced in Scrapy 2.6.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5753\">issue 5753</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5754\">issue 5754</a>)</p>", "<p>Fixed the output of <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">-h</span></code> showing an unintended <code class=\"docutils literal notranslate\"><span class=\"pre\">**commands**</span></code>\nline.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5709\">issue 5709</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5711\">issue 5711</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5712\">issue 5712</a>)</p>", "<p>Made the active project indication in the output of <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#topics-commands\"><span class=\"std std-ref\">commands</span></a> more clear.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5715\">issue 5715</a>)</p>", "<p>Documented how to <a class=\"hoverxref tooltip reference internal\" href=\"topics/debug.html#debug-vscode\"><span class=\"std std-ref\">debug spiders from Visual Studio Code</span></a>.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5721\">issue 5721</a>)</p>", "<p>Documented how <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DOWNLOAD_DELAY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOAD_DELAY</span></code></a> affects per-domain concurrency.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5083\">issue 5083</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5540\">issue 5540</a>)</p>", "<p>Improved consistency.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5761\">issue 5761</a>)</p>", "<p>Fixed typos.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5714\">issue 5714</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5744\">issue 5744</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5764\">issue 5764</a>)</p>", "<p>Applied <a class=\"hoverxref tooltip reference internal\" href=\"contributing.html#coding-style\"><span class=\"std std-ref\">black coding style</span></a>, sorted import statements,\nand introduced <a class=\"hoverxref tooltip reference internal\" href=\"contributing.html#scrapy-pre-commit\"><span class=\"std std-ref\">pre-commit</span></a>.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4654\">issue 4654</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4658\">issue 4658</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5734\">issue 5734</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5737\">issue 5737</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5806\">issue 5806</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5810\">issue 5810</a>)</p>", "<p>Switched from <a class=\"reference external\" href=\"https://docs.python.org/3/library/os.path.html#module-os.path\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">os.path</span></code></a> to <a class=\"reference external\" href=\"https://docs.python.org/3/library/pathlib.html#module-pathlib\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">pathlib</span></code></a>.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4916\">issue 4916</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4497\">issue 4497</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5682\">issue 5682</a>)</p>", "<p>Addressed many issues reported by Pylint.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5677\">issue 5677</a>)</p>", "<p>Improved code readability.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5736\">issue 5736</a>)</p>", "<p>Improved package metadata.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5768\">issue 5768</a>)</p>", "<p>Removed direct invocations of <code class=\"docutils literal notranslate\"><span class=\"pre\">setup.py</span></code>.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5774\">issue 5774</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5776\">issue 5776</a>)</p>", "<p>Removed unnecessary <a class=\"reference external\" href=\"https://docs.python.org/3/library/collections.html#collections.OrderedDict\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">OrderedDict</span></code></a> usages.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5795\">issue 5795</a>)</p>", "<p>Removed unnecessary <code class=\"docutils literal notranslate\"><span class=\"pre\">__str__</span></code> definitions.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5150\">issue 5150</a>)</p>", "<p>Removed obsolete code and comments.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5725\">issue 5725</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5729\">issue 5729</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5730\">issue 5730</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5732\">issue 5732</a>)</p>", "<p>Fixed test and CI issues.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5749\">issue 5749</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5750\">issue 5750</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5756\">issue 5756</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5762\">issue 5762</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5765\">issue 5765</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5780\">issue 5780</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5781\">issue 5781</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5782\">issue 5782</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5783\">issue 5783</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5785\">issue 5785</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5786\">issue 5786</a>)</p>", "<p>Relaxed the restriction introduced in 2.6.2 so that the\n<code class=\"docutils literal notranslate\"><span class=\"pre\">Proxy-Authorization</span></code> header can again be set explicitly, as long as the\nproxy URL in the <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-proxy\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">proxy</span></code></a> metadata has no other credentials, and\nfor as long as that proxy URL remains the same; this restores compatibility\nwith scrapy-zyte-smartproxy 2.1.0 and older (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5626\">issue 5626</a>).</p>", "<p>Using <code class=\"docutils literal notranslate\"><span class=\"pre\">-O</span></code>/<code class=\"docutils literal notranslate\"><span class=\"pre\">--overwrite-output</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">-t</span></code>/<code class=\"docutils literal notranslate\"><span class=\"pre\">--output-format</span></code> options\ntogether now produces an error instead of ignoring the former option\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5516\">issue 5516</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5605\">issue 5605</a>).</p>", "<p>Replaced deprecated <a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio.html#module-asyncio\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">asyncio</span></code></a> APIs that implicitly use the current\nevent loop with code that explicitly requests a loop from the event loop\npolicy (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5685\">issue 5685</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5689\">issue 5689</a>).</p>", "<p>Fixed uses of deprecated Scrapy APIs in Scrapy itself (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5588\">issue 5588</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5589\">issue 5589</a>).</p>", "<p>Fixed uses of a deprecated Pillow API (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5684\">issue 5684</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5692\">issue 5692</a>).</p>", "<p>Improved code that checks if generators return values, so that it no longer\nfails on decorated methods and partial methods (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5323\">issue 5323</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5592\">issue 5592</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5599\">issue 5599</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5691\">issue 5691</a>).</p>", "<p>Upgraded the Code of Conduct to Contributor Covenant v2.1 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5698\">issue 5698</a>).</p>", "<p>Fixed typos (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5681\">issue 5681</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5694\">issue 5694</a>).</p>", "<p>Re-enabled some erroneously disabled flake8 checks (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5688\">issue 5688</a>).</p>", "<p>Ignored harmless deprecation warnings from <a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#module-typing\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">typing</span></code></a> in tests\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5686\">issue 5686</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5697\">issue 5697</a>).</p>", "<p>Modernized our CI configuration (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5695\">issue 5695</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5696\">issue 5696</a>).</p>", "<p>Highlights:</p>", "<p>Added Python 3.11 support, dropped Python 3.6 support</p>", "<p>Improved support for <a class=\"hoverxref tooltip reference internal\" href=\"topics/coroutines.html#topics-coroutines\"><span class=\"std std-ref\">asynchronous callbacks</span></a></p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/asyncio.html#using-asyncio\"><span class=\"std std-ref\">Asyncio support</span></a> is enabled by default on new\nprojects</p>", "<p>Output names of item fields can now be arbitrary strings</p>", "<p>Centralized <a class=\"hoverxref tooltip reference internal\" href=\"topics/request-response.html#request-fingerprints\"><span class=\"std std-ref\">request fingerprinting</span></a>\nconfiguration is now possible</p>", "<p>Python 3.7 or greater is now required; support for Python 3.6 has been dropped.\nSupport for the upcoming Python 3.11 has been added.</p>", "<p>The minimum required version of some dependencies has changed as well:</p>", "<p><a class=\"reference external\" href=\"https://lxml.de/\">lxml</a>: 3.5.0 → 4.3.0</p>", "<p><a class=\"reference external\" href=\"https://python-pillow.org/\">Pillow</a> (<a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#images-pipeline\"><span class=\"std std-ref\">images pipeline</span></a>): 4.0.0 → 7.1.0</p>", "<p><a class=\"reference external\" href=\"https://zopeinterface.readthedocs.io/en/latest/\">zope.interface</a>: 5.0.0 → 5.1.0</p>", "<p>(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5512\">issue 5512</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5514\">issue 5514</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5524\">issue 5524</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5563\">issue 5563</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5664\">issue 5664</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5670\">issue 5670</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5678\">issue 5678</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline.thumb_path\" title=\"scrapy.pipelines.images.ImagesPipeline.thumb_path\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">ImagesPipeline.thumb_path</span></code></a> must now accept an\n<code class=\"docutils literal notranslate\"><span class=\"pre\">item</span></code> parameter (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5504\">issue 5504</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5508\">issue 5508</a>).</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.downloadermiddlewares.decompression</span></code> module is now\ndeprecated (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5546\">issue 5546</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5547\">issue 5547</a>).</p>", "<p>The\n<a class=\"reference internal\" href=\"topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\" title=\"scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_spider_output()</span></code></a>\nmethod of <a class=\"hoverxref tooltip reference internal\" href=\"topics/spider-middleware.html#topics-spider-middleware\"><span class=\"std std-ref\">spider middlewares</span></a> can now be\ndefined as an <a class=\"reference external\" href=\"https://docs.python.org/3/glossary.html#term-asynchronous-generator\" title=\"(in Python v3.11)\"><span class=\"xref std std-term\">asynchronous generator</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4978\">issue 4978</a>).</p>", "<p>The output of <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> callbacks defined as\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/coroutines.html#topics-coroutines\"><span class=\"std std-ref\">coroutines</span></a> is now processed asynchronously\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4978\">issue 4978</a>).</p>", "<p><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CrawlSpider</span></code> now supports <a class=\"hoverxref tooltip reference internal\" href=\"topics/coroutines.html#topics-coroutines\"><span class=\"std std-ref\">asynchronous\ncallbacks</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5657\">issue 5657</a>).</p>", "<p>New projects created with the <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-startproject\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">startproject</span></code></a> command have\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/asyncio.html#using-asyncio\"><span class=\"std std-ref\">asyncio support</span></a> enabled by default (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5590\">issue 5590</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5679\">issue 5679</a>).</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEED_EXPORT_FIELDS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEED_EXPORT_FIELDS</span></code></a> setting can now be defined as a\ndictionary to customize the output name of item fields, lifting the\nrestriction that required output names to be valid Python identifiers, e.g.\npreventing them to have whitespace (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1008\">issue 1008</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3266\">issue 3266</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3696\">issue 3696</a>).</p>", "<p>You can now customize <a class=\"hoverxref tooltip reference internal\" href=\"topics/request-response.html#request-fingerprints\"><span class=\"std std-ref\">request fingerprinting</span></a>\nthrough the new <a class=\"hoverxref tooltip reference internal\" href=\"topics/request-response.html#std-setting-REQUEST_FINGERPRINTER_CLASS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">REQUEST_FINGERPRINTER_CLASS</span></code></a> setting, instead of\nhaving to change it on every Scrapy component that relies on request\nfingerprinting (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/900\">issue 900</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3420\">issue 3420</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4113\">issue 4113</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4762\">issue 4762</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4524\">issue 4524</a>).</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">jsonl</span></code> is now supported and encouraged as a file extension for <a class=\"reference external\" href=\"https://jsonlines.org/\">JSON\nLines</a> files (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4848\">issue 4848</a>).</p>", "<p><a class=\"reference internal\" href=\"topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline.thumb_path\" title=\"scrapy.pipelines.images.ImagesPipeline.thumb_path\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">ImagesPipeline.thumb_path</span></code></a> now receives the\nsource <a class=\"hoverxref tooltip reference internal\" href=\"topics/items.html#topics-items\"><span class=\"std std-ref\">item</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5504\">issue 5504</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5508\">issue 5508</a>).</p>", "<p>When using Google Cloud Storage with a <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#topics-media-pipeline\"><span class=\"std std-ref\">media pipeline</span></a>, <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#std-setting-FILES_EXPIRES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FILES_EXPIRES</span></code></a> now also works when\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#std-setting-FILES_STORE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FILES_STORE</span></code></a> does not point at the root of your Google Cloud\nStorage bucket (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5317\">issue 5317</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5318\">issue 5318</a>).</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-parse\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">parse</span></code></a> command now supports <a class=\"hoverxref tooltip reference internal\" href=\"topics/coroutines.html#topics-coroutines\"><span class=\"std std-ref\">asynchronous callbacks</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5424\">issue 5424</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5577\">issue 5577</a>).</p>", "<p>When using the <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-parse\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">parse</span></code></a> command with a URL for which there is no\navailable spider, an exception is no longer raised (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3264\">issue 3264</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3265\">issue 3265</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5375\">issue 5375</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5376\">issue 5376</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5497\">issue 5497</a>).</p>", "<p><a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.TextResponse\" title=\"scrapy.http.TextResponse\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">TextResponse</span></code></a> now gives higher priority to the <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Byte_order_mark\">byte\norder mark</a> when determining the text encoding of the response body,\nfollowing the <a class=\"reference external\" href=\"https://html.spec.whatwg.org/multipage/parsing.html#determining-the-character-encoding\">HTML living standard</a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5601\">issue 5601</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5611\">issue 5611</a>).</p>", "<p>MIME sniffing takes the response body into account in FTP and HTTP/1.0\nrequests, as well as in cached requests (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4873\">issue 4873</a>).</p>", "<p>MIME sniffing now detects valid HTML 5 documents even if the <code class=\"docutils literal notranslate\"><span class=\"pre\">html</span></code> tag\nis missing (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4873\">issue 4873</a>).</p>", "<p>An exception is now raised if <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-ASYNCIO_EVENT_LOOP\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">ASYNCIO_EVENT_LOOP</span></code></a> has a value\nthat does not match the asyncio event loop actually installed\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5529\">issue 5529</a>).</p>", "<p>Fixed <code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">Headers.getlist</span></code>\nreturning only the last header (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5515\">issue 5515</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5526\">issue 5526</a>).</p>", "<p>Fixed <a class=\"reference internal\" href=\"topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\" title=\"scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">LinkExtractor</span></code></a> not ignoring the\n<code class=\"docutils literal notranslate\"><span class=\"pre\">tar.gz</span></code> file extension by default (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1837\">issue 1837</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2067\">issue 2067</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4066\">issue 4066</a>)</p>", "<p>Clarified the return type of <a class=\"reference internal\" href=\"topics/spiders.html#scrapy.Spider.parse\" title=\"scrapy.Spider.parse\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">Spider.parse</span></code></a>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5602\">issue 5602</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5608\">issue 5608</a>).</p>", "<p>To enable\n<a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware\" title=\"scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">HttpCompressionMiddleware</span></code></a>\nto do <a class=\"reference external\" href=\"https://www.ietf.org/rfc/rfc7932.txt\">brotli compression</a>, installing <a class=\"reference external\" href=\"https://github.com/google/brotli\">brotli</a> is now recommended instead\nof installing <a class=\"reference external\" href=\"https://github.com/python-hyper/brotlipy/\">brotlipy</a>, as the former provides a more recent version of\nbrotli.</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/signals.html#topics-signals\"><span class=\"std std-ref\">Signal documentation</span></a> now mentions <a class=\"hoverxref tooltip reference internal\" href=\"topics/coroutines.html#topics-coroutines\"><span class=\"std std-ref\">coroutine\nsupport</span></a> and uses it in code examples (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4852\">issue 4852</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5358\">issue 5358</a>).</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/practices.html#bans\"><span class=\"std std-ref\">Avoiding getting banned</span></a> now recommends <a class=\"reference external\" href=\"https://commoncrawl.org/\">Common Crawl</a> instead of <a class=\"reference external\" href=\"http://www.googleguide.com/cached_pages.html\">Google cache</a>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3582\">issue 3582</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5432\">issue 5432</a>).</p>", "<p>The new <a class=\"hoverxref tooltip reference internal\" href=\"topics/components.html#topics-components\"><span class=\"std std-ref\">Components</span></a> topic covers enforcing requirements on\nScrapy components, like <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#topics-downloader-middleware\"><span class=\"std std-ref\">downloader middlewares</span></a>, <a class=\"hoverxref tooltip reference internal\" href=\"topics/extensions.html#topics-extensions\"><span class=\"std std-ref\">extensions</span></a>,\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/item-pipeline.html#topics-item-pipeline\"><span class=\"std std-ref\">item pipelines</span></a>, <a class=\"hoverxref tooltip reference internal\" href=\"topics/spider-middleware.html#topics-spider-middleware\"><span class=\"std std-ref\">spider middlewares</span></a>, and more; <a class=\"hoverxref tooltip reference internal\" href=\"topics/asyncio.html#enforce-asyncio-requirement\"><span class=\"std std-ref\">Enforcing asyncio as a requirement</span></a>\nhas also been added (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4978\">issue 4978</a>).</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#topics-settings\"><span class=\"std std-ref\">Settings</span></a> now indicates that setting values must be\n<a class=\"reference external\" href=\"https://docs.python.org/3/library/pickle.html#pickle-picklable\" title=\"(in Python v3.11)\"><span class=\"xref std std-ref\">picklable</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5607\">issue 5607</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5629\">issue 5629</a>).</p>", "<p>Removed outdated documentation (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5446\">issue 5446</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5373\">issue 5373</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5369\">issue 5369</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5370\">issue 5370</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5554\">issue 5554</a>).</p>", "<p>Fixed typos (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5442\">issue 5442</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5455\">issue 5455</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5457\">issue 5457</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5461\">issue 5461</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5538\">issue 5538</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5553\">issue 5553</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5558\">issue 5558</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5624\">issue 5624</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5631\">issue 5631</a>).</p>", "<p>Fixed other issues (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5283\">issue 5283</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5284\">issue 5284</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5559\">issue 5559</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5567\">issue 5567</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5648\">issue 5648</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5659\">issue 5659</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5665\">issue 5665</a>).</p>", "<p>Added a continuous integration job to run <a class=\"reference external\" href=\"https://twine.readthedocs.io/en/stable/#twine-check\">twine check</a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5655\">issue 5655</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5656\">issue 5656</a>).</p>", "<p>Addressed test issues and warnings (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5560\">issue 5560</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5561\">issue 5561</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5612\">issue 5612</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5617\">issue 5617</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5639\">issue 5639</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5645\">issue 5645</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5662\">issue 5662</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5671\">issue 5671</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5675\">issue 5675</a>).</p>", "<p>Cleaned up code (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4991\">issue 4991</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4995\">issue 4995</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5451\">issue 5451</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5487\">issue 5487</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5542\">issue 5542</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5667\">issue 5667</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5668\">issue 5668</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5672\">issue 5672</a>).</p>", "<p>Applied minor code improvements (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5661\">issue 5661</a>).</p>", "<p>Added support for <a class=\"reference external\" href=\"https://www.pyopenssl.org/en/stable/\">pyOpenSSL</a> 22.1.0, removing support for SSLv3\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5634\">issue 5634</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5635\">issue 5635</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5636\">issue 5636</a>).</p>", "<p>Upgraded the minimum versions of the following dependencies:</p>", "<p><a class=\"reference external\" href=\"https://cryptography.io/en/latest/\">cryptography</a>: 2.0 → 3.3</p>", "<p><a class=\"reference external\" href=\"https://www.pyopenssl.org/en/stable/\">pyOpenSSL</a>: 16.2.0 → 21.0.0</p>", "<p><a class=\"reference external\" href=\"https://service-identity.readthedocs.io/en/stable/\">service_identity</a>: 16.0.0 → 18.1.0</p>", "<p><a class=\"reference external\" href=\"https://twistedmatrix.com/trac/\">Twisted</a>: 17.9.0 → 18.9.0</p>", "<p><a class=\"reference external\" href=\"https://zopeinterface.readthedocs.io/en/latest/\">zope.interface</a>: 4.1.3 → 5.0.0</p>", "<p>(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5621\">issue 5621</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5632\">issue 5632</a>)</p>", "<p>Fixes test and documentation issues (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5612\">issue 5612</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5617\">issue 5617</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5631\">issue 5631</a>).</p>", "<p>When <a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\" title=\"scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">HttpProxyMiddleware</span></code></a>\nprocesses a request with <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-proxy\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">proxy</span></code></a> metadata, and that\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-proxy\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">proxy</span></code></a> metadata includes proxy credentials,\n<a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\" title=\"scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">HttpProxyMiddleware</span></code></a> sets\nthe <code class=\"docutils literal notranslate\"><span class=\"pre\">Proxy-Authorization</span></code> header, but only if that header is not already\nset.</p>", "<p>There are third-party proxy-rotation downloader middlewares that set\ndifferent <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-proxy\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">proxy</span></code></a> metadata every time they process a request.</p>", "<p>Because of request retries and redirects, the same request can be processed\nby downloader middlewares more than once, including both\n<a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\" title=\"scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">HttpProxyMiddleware</span></code></a> and\nany third-party proxy-rotation downloader middleware.</p>", "<p>These third-party proxy-rotation downloader middlewares could change the\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-proxy\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">proxy</span></code></a> metadata of a request to a new value, but fail to remove\nthe <code class=\"docutils literal notranslate\"><span class=\"pre\">Proxy-Authorization</span></code> header from the previous value of the\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-proxy\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">proxy</span></code></a> metadata, causing the credentials of one proxy to be sent\nto a different proxy.</p>", "<p>To prevent the unintended leaking of proxy credentials, the behavior of\n<a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\" title=\"scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">HttpProxyMiddleware</span></code></a> is now\nas follows when processing a request:</p>", "<p>If the request being processed defines <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-proxy\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">proxy</span></code></a> metadata that\nincludes credentials, the <code class=\"docutils literal notranslate\"><span class=\"pre\">Proxy-Authorization</span></code> header is always\nupdated to feature those credentials.</p>", "<p>If the request being processed defines <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-proxy\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">proxy</span></code></a> metadata\nwithout credentials, the <code class=\"docutils literal notranslate\"><span class=\"pre\">Proxy-Authorization</span></code> header is removed\n<em>unless</em> it was originally defined for the same proxy URL.</p>", "<p>To remove proxy credentials while keeping the same proxy URL, remove\nthe <code class=\"docutils literal notranslate\"><span class=\"pre\">Proxy-Authorization</span></code> header.</p>", "<p>If the request has no <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-proxy\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">proxy</span></code></a> metadata, or that metadata is a\nfalsy value (e.g. <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>), the <code class=\"docutils literal notranslate\"><span class=\"pre\">Proxy-Authorization</span></code> header is\nremoved.</p>", "<p>It is no longer possible to set a proxy URL through the\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-proxy\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">proxy</span></code></a> metadata but set the credentials through the\n<code class=\"docutils literal notranslate\"><span class=\"pre\">Proxy-Authorization</span></code> header. Set proxy credentials through the\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-proxy\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">proxy</span></code></a> metadata instead.</p>", "<p>Also fixes the following regressions introduced in 2.6.0:</p>", "<p><a class=\"reference internal\" href=\"topics/api.html#scrapy.crawler.CrawlerProcess\" title=\"scrapy.crawler.CrawlerProcess\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CrawlerProcess</span></code></a> supports again crawling multiple\nspiders (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5435\">issue 5435</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5436\">issue 5436</a>)</p>", "<p>Installing a Twisted reactor before Scrapy does (e.g. importing\n<a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html\" title=\"(in Twisted)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">twisted.internet.reactor</span></code></a> somewhere at the module level) no longer\nprevents Scrapy from starting, as long as a different reactor is not\nspecified in <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-TWISTED_REACTOR\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">TWISTED_REACTOR</span></code></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5525\">issue 5525</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5528\">issue 5528</a>)</p>", "<p>Fixed an exception that was being logged after the spider finished under\ncertain conditions (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5437\">issue 5437</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5440\">issue 5440</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">--output</span></code>/<code class=\"docutils literal notranslate\"><span class=\"pre\">-o</span></code> command-line parameter supports again a value\nstarting with a hyphen (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5444\">issue 5444</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5445\">issue 5445</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">parse</span> <span class=\"pre\">-h</span></code> command no longer throws an error (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5481\">issue 5481</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5482\">issue 5482</a>)</p>", "<p>Fixes a regression introduced in 2.6.0 that would unset the request method when\nfollowing redirects.</p>", "<p>Highlights:</p>", "<p>Python 3.10 support</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/asyncio.html#using-asyncio\"><span class=\"std std-ref\">asyncio support</span></a> is no longer considered\nexperimental, and works out-of-the-box on Windows regardless of your Python\nversion</p>", "<p>Feed exports now support <a class=\"reference external\" href=\"https://docs.python.org/3/library/pathlib.html#pathlib.Path\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pathlib.Path</span></code></a> output paths and per-feed\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#item-filter\"><span class=\"std std-ref\">item filtering</span></a> and\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#post-processing\"><span class=\"std std-ref\">post-processing</span></a></p>", "<p>When a <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> object with cookies defined gets a\nredirect response causing a new <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> object to be\nscheduled, the cookies defined in the original\n<a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> object are no longer copied into the new\n<a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> object.</p>", "<p>If you manually set the <code class=\"docutils literal notranslate\"><span class=\"pre\">Cookie</span></code> header on a\n<a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> object and the domain name of the redirect\nURL is not an exact match for the domain of the URL of the original\n<a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> object, your <code class=\"docutils literal notranslate\"><span class=\"pre\">Cookie</span></code> header is now dropped\nfrom the new <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> object.</p>", "<p>The old behavior could be exploited by an attacker to gain access to your\ncookies. Please, see the <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8\">cjvr-mfj7-j4j8 security advisory</a> for more\ninformation.</p>", "<p class=\"admonition-title\">Note</p>", "<p>It is still possible to enable the sharing of cookies between\ndifferent domains with a shared domain suffix (e.g.\n<code class=\"docutils literal notranslate\"><span class=\"pre\">example.com</span></code> and any subdomain) by defining the shared domain\nsuffix (e.g. <code class=\"docutils literal notranslate\"><span class=\"pre\">example.com</span></code>) as the cookie domain when defining\nyour cookies. See the documentation of the\n<a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> class for more information.</p>", "<p>When the domain of a cookie, either received in the <code class=\"docutils literal notranslate\"><span class=\"pre\">Set-Cookie</span></code> header\nof a response or defined in a <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> object, is set\nto a <a class=\"reference external\" href=\"https://publicsuffix.org/\">public suffix</a>, the cookie is now\nignored unless the cookie domain is the same as the request domain.</p>", "<p>The old behavior could be exploited by an attacker to inject cookies from a\ncontrolled domain into your cookiejar that could be sent to other domains\nnot controlled by the attacker. Please, see the <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/security/advisories/GHSA-mfjm-vh54-3f96\">mfjm-vh54-3f96 security\nadvisory</a> for more information.</p>", "<p>The <a class=\"reference external\" href=\"https://pypi.org/project/h2/\">h2</a> dependency is now optional, only needed to\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#http2\"><span class=\"std std-ref\">enable HTTP/2 support</span></a>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5113\">issue 5113</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">formdata</span></code> parameter of <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">FormRequest</span></code>, if specified\nfor a non-POST request, now overrides the URL query string, instead of\nbeing appended to it. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2919\">issue 2919</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3579\">issue 3579</a>)</p>", "<p>When a function is assigned to the <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEED_URI_PARAMS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEED_URI_PARAMS</span></code></a> setting, now\nthe return value of that function, and not the <code class=\"docutils literal notranslate\"><span class=\"pre\">params</span></code> input parameter,\nwill determine the feed URI parameters, unless that return value is\n<code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4962\">issue 4962</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4966\">issue 4966</a>)</p>", "<p>In <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.core.engine.ExecutionEngine</span></code>, methods\n<code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">crawl()</span></code>,\n<code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">download()</span></code>,\n<code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">schedule()</span></code>,\nand <code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">spider_is_idle()</span></code>\nnow raise <a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#RuntimeError\" title=\"(in Python v3.11)\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">RuntimeError</span></code></a> if called before\n<code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">open_spider()</span></code>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5090\">issue 5090</a>)</p>", "<p>These methods used to assume that\n<code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">ExecutionEngine.slot</span></code> had\nbeen defined by a prior call to\n<code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">open_spider()</span></code>, so they were\nraising <a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#AttributeError\" title=\"(in Python v3.11)\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">AttributeError</span></code></a> instead.</p>", "<p>If the API of the configured <a class=\"hoverxref tooltip reference internal\" href=\"topics/scheduler.html#topics-scheduler\"><span class=\"std std-ref\">scheduler</span></a> does not\nmeet expectations, <a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#TypeError\" title=\"(in Python v3.11)\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">TypeError</span></code></a> is now raised at startup time. Before,\nother exceptions would be raised at run time. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3559\">issue 3559</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">_encoding</span></code> field of serialized <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> objects\nis now named <code class=\"docutils literal notranslate\"><span class=\"pre\">encoding</span></code>, in line with all other fields (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5130\">issue 5130</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.http.TextResponse.body_as_unicode</span></code>, deprecated in Scrapy 2.2, has\nnow been removed. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5393\">issue 5393</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.item.BaseItem</span></code>, deprecated in Scrapy 2.2, has now been removed.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5398\">issue 5398</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.item.DictItem</span></code>, deprecated in Scrapy 1.8, has now been removed.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5398\">issue 5398</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.Spider.make_requests_from_url</span></code>, deprecated in Scrapy 1.4, has now\nbeen removed. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4178\">issue 4178</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4356\">issue 4356</a>)</p>", "<p>When a function is assigned to the <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEED_URI_PARAMS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEED_URI_PARAMS</span></code></a> setting,\nreturning <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> or modifying the <code class=\"docutils literal notranslate\"><span class=\"pre\">params</span></code> input parameter is now\ndeprecated. Return a new dictionary instead. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4962\">issue 4962</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4966\">issue 4966</a>)</p>", "<p><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">scrapy.utils.reqser</span></code> is deprecated. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5130\">issue 5130</a>)</p>", "<p>Instead of <code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">request_to_dict()</span></code>, use the new\n<a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request.to_dict\" title=\"scrapy.http.Request.to_dict\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">Request.to_dict</span></code></a> method.</p>", "<p>Instead of <code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">request_from_dict()</span></code>, use the new\n<a class=\"reference internal\" href=\"topics/request-response.html#scrapy.utils.request.request_from_dict\" title=\"scrapy.utils.request.request_from_dict\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.utils.request.request_from_dict()</span></code></a> function.</p>", "<p>In <code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">scrapy.squeues</span></code>, the following queue classes are deprecated:\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">PickleFifoDiskQueueNonRequest</span></code>,\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">PickleLifoDiskQueueNonRequest</span></code>,\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MarshalFifoDiskQueueNonRequest</span></code>,\nand <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MarshalLifoDiskQueueNonRequest</span></code>. You should\ninstead use:\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">PickleFifoDiskQueue</span></code>,\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">PickleLifoDiskQueue</span></code>,\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MarshalFifoDiskQueue</span></code>,\nand <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MarshalLifoDiskQueue</span></code>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5117\">issue 5117</a>)</p>", "<p>Many aspects of <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.core.engine.ExecutionEngine</span></code> that come from\na time when this class could handle multiple <a class=\"reference internal\" href=\"topics/spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a>\nobjects at a time have been deprecated. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5090\">issue 5090</a>)</p>", "<p>The <code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">has_capacity()</span></code> method\nis deprecated.</p>", "<p>The <code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">schedule()</span></code> method is\ndeprecated, use <code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">crawl()</span></code> or\n<code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">download()</span></code> instead.</p>", "<p>The <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">open_spiders</span></code> attribute\nis deprecated, use <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">spider</span></code>\ninstead.</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">spider</span></code> parameter is deprecated for the following methods:</p>", "<p>Instead, call <code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">open_spider()</span></code>\nfirst to set the <a class=\"reference internal\" href=\"topics/spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object.</p>", "<p>You can now use <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#item-filter\"><span class=\"std std-ref\">item filtering</span></a> to control which items\nare exported to each output feed. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4575\">issue 4575</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5178\">issue 5178</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5161\">issue 5161</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5203\">issue 5203</a>)</p>", "<p>You can now apply <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#post-processing\"><span class=\"std std-ref\">post-processing</span></a> to feeds, and\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#builtin-plugins\"><span class=\"std std-ref\">built-in post-processing plugins</span></a> are provided for\noutput file compression. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2174\">issue 2174</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5168\">issue 5168</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5190\">issue 5190</a>)</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEEDS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEEDS</span></code></a> setting now supports <a class=\"reference external\" href=\"https://docs.python.org/3/library/pathlib.html#pathlib.Path\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pathlib.Path</span></code></a> objects as\nkeys. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5383\">issue 5383</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5384\">issue 5384</a>)</p>", "<p>Enabling <a class=\"hoverxref tooltip reference internal\" href=\"topics/asyncio.html#using-asyncio\"><span class=\"std std-ref\">asyncio</span></a> while using Windows and Python 3.8\nor later will automatically switch the asyncio event loop to one that\nallows Scrapy to work. See <a class=\"hoverxref tooltip reference internal\" href=\"topics/asyncio.html#asyncio-windows\"><span class=\"std std-ref\">Windows-specific notes</span></a>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4976\">issue 4976</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5315\">issue 5315</a>)</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-genspider\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">genspider</span></code></a> command now supports a start URL instead of a\ndomain name. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4439\">issue 4439</a>)</p>", "<p><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">scrapy.utils.defer</span></code> gained 2 new functions,\n<a class=\"reference internal\" href=\"topics/asyncio.html#scrapy.utils.defer.deferred_to_future\" title=\"scrapy.utils.defer.deferred_to_future\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">deferred_to_future()</span></code></a> and\n<a class=\"reference internal\" href=\"topics/asyncio.html#scrapy.utils.defer.maybe_deferred_to_future\" title=\"scrapy.utils.defer.maybe_deferred_to_future\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">maybe_deferred_to_future()</span></code></a>, to help <a class=\"hoverxref tooltip reference internal\" href=\"topics/asyncio.html#asyncio-await-dfd\"><span class=\"std std-ref\">await\non Deferreds when using the asyncio reactor</span></a>.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5288\">issue 5288</a>)</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#topics-feed-storage-s3\"><span class=\"std std-ref\">Amazon S3 feed export storage</span></a> gained\nsupport for <a class=\"reference external\" href=\"https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#temporary-access-keys\">temporary security credentials</a>\n(<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-AWS_SESSION_TOKEN\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">AWS_SESSION_TOKEN</span></code></a>) and endpoint customization\n(<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-AWS_ENDPOINT_URL\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">AWS_ENDPOINT_URL</span></code></a>). (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4998\">issue 4998</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5210\">issue 5210</a>)</p>", "<p>New <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-LOG_FILE_APPEND\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">LOG_FILE_APPEND</span></code></a> setting to allow truncating the log file.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5279\">issue 5279</a>)</p>", "<p><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Request.cookies</span></code> values that are\n<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">bool</span></code></a>, <a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">float</span></code></a> or <a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">int</span></code></a> are cast to <a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">str</span></code></a>.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5252\">issue 5252</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5253\">issue 5253</a>)</p>", "<p>You may now raise <a class=\"reference internal\" href=\"topics/exceptions.html#scrapy.exceptions.CloseSpider\" title=\"scrapy.exceptions.CloseSpider\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">CloseSpider</span></code></a> from a handler of\nthe <a class=\"hoverxref tooltip reference internal\" href=\"topics/signals.html#std-signal-spider_idle\"><code class=\"xref std std-signal docutils literal notranslate\"><span class=\"pre\">spider_idle</span></code></a> signal to customize the reason why the spider is\nstopping. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5191\">issue 5191</a>)</p>", "<p>When using\n<a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\" title=\"scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">HttpProxyMiddleware</span></code></a>, the\nproxy URL for non-HTTPS HTTP/1.1 requests no longer needs to include a URL\nscheme. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4505\">issue 4505</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4649\">issue 4649</a>)</p>", "<p>All built-in queues now expose a <code class=\"docutils literal notranslate\"><span class=\"pre\">peek</span></code> method that returns the next\nqueue object (like <code class=\"docutils literal notranslate\"><span class=\"pre\">pop</span></code>) but does not remove the returned object from\nthe queue. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5112\">issue 5112</a>)</p>", "<p>If the underlying queue does not support peeking (e.g. because you are not\nusing <code class=\"docutils literal notranslate\"><span class=\"pre\">queuelib</span></code> 1.6.1 or later), the <code class=\"docutils literal notranslate\"><span class=\"pre\">peek</span></code> method raises\n<a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#NotImplementedError\" title=\"(in Python v3.11)\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">NotImplementedError</span></code></a>.</p>", "<p><a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> and <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Response\" title=\"scrapy.http.Response\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response</span></code></a> now have\nan <code class=\"docutils literal notranslate\"><span class=\"pre\">attributes</span></code> attribute that makes subclassing easier. For\n<a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a>, it also allows subclasses to work with\n<a class=\"reference internal\" href=\"topics/request-response.html#scrapy.utils.request.request_from_dict\" title=\"scrapy.utils.request.request_from_dict\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.utils.request.request_from_dict()</span></code></a>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1877\">issue 1877</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5130\">issue 5130</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5218\">issue 5218</a>)</p>", "<p>The <a class=\"reference internal\" href=\"topics/scheduler.html#scrapy.core.scheduler.BaseScheduler.open\" title=\"scrapy.core.scheduler.BaseScheduler.open\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">open()</span></code></a> and\n<a class=\"reference internal\" href=\"topics/scheduler.html#scrapy.core.scheduler.BaseScheduler.close\" title=\"scrapy.core.scheduler.BaseScheduler.close\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">close()</span></code></a> methods of the\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/scheduler.html#topics-scheduler\"><span class=\"std std-ref\">scheduler</span></a> are now optional. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3559\">issue 3559</a>)</p>", "<p>HTTP/1.1 <code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">TunnelError</span></code>\nexceptions now only truncate response bodies longer than 1000 characters,\ninstead of those longer than 32 characters, making it easier to debug such\nerrors. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4881\">issue 4881</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5007\">issue 5007</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/loaders.html#scrapy.loader.ItemLoader\" title=\"scrapy.loader.ItemLoader\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ItemLoader</span></code></a> now supports non-text responses.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5145\">issue 5145</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5269\">issue 5269</a>)</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-TWISTED_REACTOR\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">TWISTED_REACTOR</span></code></a> and <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-ASYNCIO_EVENT_LOOP\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">ASYNCIO_EVENT_LOOP</span></code></a> settings\nare no longer ignored if defined in <a class=\"reference internal\" href=\"topics/spiders.html#scrapy.Spider.custom_settings\" title=\"scrapy.Spider.custom_settings\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">custom_settings</span></code></a>.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4485\">issue 4485</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5352\">issue 5352</a>)</p>", "<p>Removed a module-level Twisted reactor import that could prevent\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/asyncio.html#using-asyncio\"><span class=\"std std-ref\">using the asyncio reactor</span></a>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5357\">issue 5357</a>)</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-startproject\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">startproject</span></code></a> command works with existing folders again.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4665\">issue 4665</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4676\">issue 4676</a>)</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEED_URI_PARAMS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEED_URI_PARAMS</span></code></a> setting now behaves as documented.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4962\">issue 4962</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4966\">issue 4966</a>)</p>", "<p><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Request.cb_kwargs</span></code> once again allows the\n<code class=\"docutils literal notranslate\"><span class=\"pre\">callback</span></code> keyword. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5237\">issue 5237</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5251\">issue 5251</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5264\">issue 5264</a>)</p>", "<p>Made <code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.utils.response.open_in_browser()</span></code> support more complex\nHTML. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5319\">issue 5319</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5320\">issue 5320</a>)</p>", "<p>Fixed <a class=\"reference internal\" href=\"topics/spiders.html#scrapy.spiders.CSVFeedSpider.quotechar\" title=\"scrapy.spiders.CSVFeedSpider.quotechar\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">CSVFeedSpider.quotechar</span></code></a> being interpreted as the CSV file\nencoding. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5391\">issue 5391</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5394\">issue 5394</a>)</p>", "<p>Added missing <a class=\"reference external\" href=\"https://pypi.org/project/setuptools/\">setuptools</a> to the list of dependencies. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5122\">issue 5122</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\" title=\"scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">LinkExtractor</span></code></a>\nnow also works as expected with links that have comma-separated <code class=\"docutils literal notranslate\"><span class=\"pre\">rel</span></code>\nattribute values including <code class=\"docutils literal notranslate\"><span class=\"pre\">nofollow</span></code>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5225\">issue 5225</a>)</p>", "<p>Fixed a <a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#TypeError\" title=\"(in Python v3.11)\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">TypeError</span></code></a> that could be raised during <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#topics-feed-exports\"><span class=\"std std-ref\">feed export</span></a> parameter parsing. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5359\">issue 5359</a>)</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/asyncio.html#using-asyncio\"><span class=\"std std-ref\">asyncio support</span></a> is no longer considered\nexperimental. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5332\">issue 5332</a>)</p>", "<p>Included <a class=\"hoverxref tooltip reference internal\" href=\"topics/asyncio.html#asyncio-windows\"><span class=\"std std-ref\">Windows-specific help for asyncio usage</span></a>.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4976\">issue 4976</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5315\">issue 5315</a>)</p>", "<p>Rewrote <a class=\"hoverxref tooltip reference internal\" href=\"topics/dynamic-content.html#topics-headless-browsing\"><span class=\"std std-ref\">Using a headless browser</span></a> with up-to-date best practices.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4484\">issue 4484</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4613\">issue 4613</a>)</p>", "<p>Documented <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#topics-file-naming\"><span class=\"std std-ref\">local file naming in media pipelines</span></a>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5069\">issue 5069</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5152\">issue 5152</a>)</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"faq.html#faq\"><span class=\"std std-ref\">Frequently Asked Questions</span></a> now covers spider file name collision issues. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2680\">issue 2680</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3669\">issue 3669</a>)</p>", "<p>Provided better context and instructions to disable the\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-URLLENGTH_LIMIT\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">URLLENGTH_LIMIT</span></code></a> setting. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5135\">issue 5135</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5250\">issue 5250</a>)</p>", "<p>Documented that <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#reppy-parser\"><span class=\"std std-ref\">Reppy parser</span></a> does not support Python 3.9+.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5226\">issue 5226</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5231\">issue 5231</a>)</p>", "<p>Documented <a class=\"hoverxref tooltip reference internal\" href=\"topics/scheduler.html#topics-scheduler\"><span class=\"std std-ref\">the scheduler component</span></a>.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3537\">issue 3537</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3559\">issue 3559</a>)</p>", "<p>Documented the method used by <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#topics-media-pipeline\"><span class=\"std std-ref\">media pipelines</span></a> to <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#file-expiration\"><span class=\"std std-ref\">determine if a file has expired</span></a>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5120\">issue 5120</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5254\">issue 5254</a>)</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/practices.html#run-multiple-spiders\"><span class=\"std std-ref\">Running multiple spiders in the same process</span></a> now features\n<code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.utils.project.get_project_settings()</span></code> usage. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5070\">issue 5070</a>)</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/practices.html#run-multiple-spiders\"><span class=\"std std-ref\">Running multiple spiders in the same process</span></a> now covers what happens when you define\ndifferent per-spider values for some settings that cannot differ at run\ntime. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4485\">issue 4485</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5352\">issue 5352</a>)</p>", "<p>Extended the documentation of the\n<a class=\"reference internal\" href=\"topics/extensions.html#scrapy.extensions.statsmailer.StatsMailer\" title=\"scrapy.extensions.statsmailer.StatsMailer\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">StatsMailer</span></code></a> extension.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5199\">issue 5199</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5217\">issue 5217</a>)</p>", "<p>Added <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-JOBDIR\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">JOBDIR</span></code></a> to <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#topics-settings\"><span class=\"std std-ref\">Settings</span></a>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5173\">issue 5173</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5224\">issue 5224</a>)</p>", "<p>Documented <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Spider.attribute</span></code>.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5174\">issue 5174</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5244\">issue 5244</a>)</p>", "<p>Documented <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.TextResponse.urljoin\" title=\"scrapy.http.TextResponse.urljoin\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">TextResponse.urljoin</span></code></a>.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1582\">issue 1582</a>)</p>", "<p>Added the <code class=\"docutils literal notranslate\"><span class=\"pre\">body_length</span></code> parameter to the documented signature of the\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/signals.html#std-signal-headers_received\"><code class=\"xref std std-signal docutils literal notranslate\"><span class=\"pre\">headers_received</span></code></a> signal. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5270\">issue 5270</a>)</p>", "<p>Clarified <a class=\"reference internal\" href=\"topics/selectors.html#scrapy.selector.SelectorList.get\" title=\"scrapy.selector.SelectorList.get\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">SelectorList.get</span></code></a> usage\nin the <a class=\"hoverxref tooltip reference internal\" href=\"intro/tutorial.html#intro-tutorial\"><span class=\"std std-ref\">tutorial</span></a>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5256\">issue 5256</a>)</p>", "<p>The documentation now features the shortest import path of classes with\nmultiple import paths. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2733\">issue 2733</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5099\">issue 5099</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">quotes.toscrape.com</span></code> references now use HTTPS instead of HTTP.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5395\">issue 5395</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5396\">issue 5396</a>)</p>", "<p>Added a link to <a class=\"reference external\" href=\"https://discord.gg/mv3yErfpvq\">our Discord server</a>\nto <a class=\"hoverxref tooltip reference internal\" href=\"index.html#getting-help\"><span class=\"std std-ref\">Getting help</span></a>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5421\">issue 5421</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5422\">issue 5422</a>)</p>", "<p>The pronunciation of the project name is now <a class=\"hoverxref tooltip reference internal\" href=\"intro/overview.html#intro-overview\"><span class=\"std std-ref\">officially</span></a> /ˈskreɪpaɪ/. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5280\">issue 5280</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5281\">issue 5281</a>)</p>", "<p>Added the Scrapy logo to the README. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5255\">issue 5255</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5258\">issue 5258</a>)</p>", "<p>Fixed issues and implemented minor improvements. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3155\">issue 3155</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4335\">issue 4335</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5074\">issue 5074</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5098\">issue 5098</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5134\">issue 5134</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5180\">issue 5180</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5194\">issue 5194</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5239\">issue 5239</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5266\">issue 5266</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5271\">issue 5271</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5273\">issue 5273</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5274\">issue 5274</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5276\">issue 5276</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5347\">issue 5347</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5356\">issue 5356</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5414\">issue 5414</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5415\">issue 5415</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5416\">issue 5416</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5419\">issue 5419</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5420\">issue 5420</a>)</p>", "<p>Added support for Python 3.10. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5212\">issue 5212</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5221\">issue 5221</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5265\">issue 5265</a>)</p>", "<p>Significantly reduced memory usage by\n<code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.utils.response.response_httprepr()</span></code>, used by the\n<a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.stats.DownloaderStats\" title=\"scrapy.downloadermiddlewares.stats.DownloaderStats\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">DownloaderStats</span></code></a> downloader\nmiddleware, which is enabled by default. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4964\">issue 4964</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4972\">issue 4972</a>)</p>", "<p>Removed uses of the deprecated <a class=\"reference external\" href=\"https://docs.python.org/3/library/optparse.html#module-optparse\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">optparse</span></code></a> module. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5366\">issue 5366</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5374\">issue 5374</a>)</p>", "<p>Extended typing hints. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5077\">issue 5077</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5090\">issue 5090</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5100\">issue 5100</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5108\">issue 5108</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5171\">issue 5171</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5215\">issue 5215</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5334\">issue 5334</a>)</p>", "<p>Improved tests, fixed CI issues, removed unused code. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5094\">issue 5094</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5157\">issue 5157</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5162\">issue 5162</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5198\">issue 5198</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5207\">issue 5207</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5208\">issue 5208</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5229\">issue 5229</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5298\">issue 5298</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5299\">issue 5299</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5310\">issue 5310</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5316\">issue 5316</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5333\">issue 5333</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5388\">issue 5388</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5389\">issue 5389</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5400\">issue 5400</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5401\">issue 5401</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5404\">issue 5404</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5405\">issue 5405</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5407\">issue 5407</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5410\">issue 5410</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5412\">issue 5412</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5425\">issue 5425</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5427\">issue 5427</a>)</p>", "<p>Implemented improvements for contributors. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5080\">issue 5080</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5082\">issue 5082</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5177\">issue 5177</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5200\">issue 5200</a>)</p>", "<p>Implemented cleanups. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5095\">issue 5095</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5106\">issue 5106</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5209\">issue 5209</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5228\">issue 5228</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5235\">issue 5235</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5245\">issue 5245</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5246\">issue 5246</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5292\">issue 5292</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5314\">issue 5314</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5322\">issue 5322</a>)</p>", "<p>If you use\n<a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware\" title=\"scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">HttpAuthMiddleware</span></code></a>\n(i.e. the <code class=\"docutils literal notranslate\"><span class=\"pre\">http_user</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">http_pass</span></code> spider attributes) for HTTP\nauthentication, any request exposes your credentials to the request target.</p>", "<p>To prevent unintended exposure of authentication credentials to unintended\ndomains, you must now additionally set a new, additional spider attribute,\n<code class=\"docutils literal notranslate\"><span class=\"pre\">http_auth_domain</span></code>, and point it to the specific domain to which the\nauthentication credentials must be sent.</p>", "<p>If the <code class=\"docutils literal notranslate\"><span class=\"pre\">http_auth_domain</span></code> spider attribute is not set, the domain of the\nfirst request will be considered the HTTP authentication target, and\nauthentication credentials will only be sent in requests targeting that\ndomain.</p>", "<p>If you need to send the same HTTP authentication credentials to multiple\ndomains, you can use <a class=\"reference external\" href=\"https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header\" title=\"(in w3lib v2.1)\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">w3lib.http.basic_auth_header()</span></code></a> instead to\nset the value of the <code class=\"docutils literal notranslate\"><span class=\"pre\">Authorization</span></code> header of your requests.</p>", "<p>If you <em>really</em> want your spider to send the same HTTP authentication\ncredentials to any domain, set the <code class=\"docutils literal notranslate\"><span class=\"pre\">http_auth_domain</span></code> spider attribute\nto <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>.</p>", "<p>Finally, if you are a user of <a class=\"reference external\" href=\"https://github.com/scrapy-plugins/scrapy-splash\">scrapy-splash</a>, know that this version of\nScrapy breaks compatibility with scrapy-splash 0.7.2 and earlier. You will\nneed to upgrade scrapy-splash to a greater version for it to continue to\nwork.</p>", "<p>Highlights:</p>", "<p>Official Python 3.9 support</p>", "<p>Experimental <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#http2\"><span class=\"std std-ref\">HTTP/2 support</span></a></p>", "<p>New <a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.retry.get_retry_request\" title=\"scrapy.downloadermiddlewares.retry.get_retry_request\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">get_retry_request()</span></code></a> function\nto retry requests from spider callbacks</p>", "<p>New <a class=\"reference internal\" href=\"topics/signals.html#scrapy.signals.headers_received\" title=\"scrapy.signals.headers_received\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">headers_received</span></code></a> signal that allows stopping\ndownloads early</p>", "<p>New <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Response.protocol\" title=\"scrapy.http.Response.protocol\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response.protocol</span></code></a> attribute</p>", "<p>Removed all code that <a class=\"hoverxref tooltip reference internal\" href=\"#id96\"><span class=\"std std-ref\">was deprecated in 1.7.0</span></a> and\nhad not <a class=\"hoverxref tooltip reference internal\" href=\"#id45\"><span class=\"std std-ref\">already been removed in 2.4.0</span></a>.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4901\">issue 4901</a>)</p>", "<p>Removed support for the <code class=\"docutils literal notranslate\"><span class=\"pre\">SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE</span></code> environment\nvariable, <a class=\"hoverxref tooltip reference internal\" href=\"#id88\"><span class=\"std std-ref\">deprecated in 1.8.0</span></a>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4912\">issue 4912</a>)</p>", "<p>The <code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">scrapy.utils.py36</span></code> module is now deprecated in favor of\n<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">scrapy.utils.asyncgen</span></code>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4900\">issue 4900</a>)</p>", "<p>Experimental <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#http2\"><span class=\"std std-ref\">HTTP/2 support</span></a> through a new download handler\nthat can be assigned to the <code class=\"docutils literal notranslate\"><span class=\"pre\">https</span></code> protocol in the\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DOWNLOAD_HANDLERS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOAD_HANDLERS</span></code></a> setting.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1854\">issue 1854</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4769\">issue 4769</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5058\">issue 5058</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5059\">issue 5059</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5066\">issue 5066</a>)</p>", "<p>The new <a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.retry.get_retry_request\" title=\"scrapy.downloadermiddlewares.retry.get_retry_request\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.downloadermiddlewares.retry.get_retry_request()</span></code></a>\nfunction may be used from spider callbacks or middlewares to handle the\nretrying of a request beyond the scenarios that\n<a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.retry.RetryMiddleware\" title=\"scrapy.downloadermiddlewares.retry.RetryMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RetryMiddleware</span></code></a> supports.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3590\">issue 3590</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3685\">issue 3685</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4902\">issue 4902</a>)</p>", "<p>The new <a class=\"reference internal\" href=\"topics/signals.html#scrapy.signals.headers_received\" title=\"scrapy.signals.headers_received\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">headers_received</span></code></a> signal gives early access\nto response headers and allows <a class=\"hoverxref tooltip reference internal\" href=\"topics/request-response.html#topics-stop-response-download\"><span class=\"std std-ref\">stopping downloads</span></a>.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1772\">issue 1772</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4897\">issue 4897</a>)</p>", "<p>The new <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Response.protocol\" title=\"scrapy.http.Response.protocol\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Response.protocol</span></code></a>\nattribute gives access to the string that identifies the protocol used to\ndownload a response. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4878\">issue 4878</a>)</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/stats.html#topics-stats\"><span class=\"std std-ref\">Stats</span></a> now include the following entries that indicate\nthe number of successes and failures in storing\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#topics-feed-exports\"><span class=\"std std-ref\">feeds</span></a>:</p>", "<p>Where <code class=\"docutils literal notranslate\"><span class=\"pre\">&lt;storage</span> <span class=\"pre\">type&gt;</span></code> is the feed storage backend class name, such as\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">FileFeedStorage</span></code> or\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">FTPFeedStorage</span></code>.</p>", "<p>(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3947\">issue 3947</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4850\">issue 4850</a>)</p>", "<p>The <a class=\"reference internal\" href=\"topics/spider-middleware.html#scrapy.spidermiddlewares.urllength.UrlLengthMiddleware\" title=\"scrapy.spidermiddlewares.urllength.UrlLengthMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">UrlLengthMiddleware</span></code></a> spider\nmiddleware now logs ignored URLs with <code class=\"docutils literal notranslate\"><span class=\"pre\">INFO</span></code> <a class=\"reference external\" href=\"https://docs.python.org/3/library/logging.html#levels\" title=\"(in Python v3.11)\"><span class=\"xref std std-ref\">logging level</span></a> instead of <code class=\"docutils literal notranslate\"><span class=\"pre\">DEBUG</span></code>, and it now includes the following entry\ninto <a class=\"hoverxref tooltip reference internal\" href=\"topics/stats.html#topics-stats\"><span class=\"std std-ref\">stats</span></a> to keep track of the number of ignored\nURLs:</p>", "<p>(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5036\">issue 5036</a>)</p>", "<p>The\n<a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware\" title=\"scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">HttpCompressionMiddleware</span></code></a>\ndownloader middleware now logs the number of decompressed responses and the\ntotal count of resulting bytes:</p>", "<p>(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4797\">issue 4797</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4799\">issue 4799</a>)</p>", "<p>Fixed installation on PyPy installing PyDispatcher in addition to\nPyPyDispatcher, which could prevent Scrapy from working depending on which\npackage got imported. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4710\">issue 4710</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4814\">issue 4814</a>)</p>", "<p>When inspecting a callback to check if it is a generator that also returns\na value, an exception is no longer raised if the callback has a docstring\nwith lower indentation than the following code.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4477\">issue 4477</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4935\">issue 4935</a>)</p>", "<p>The <a class=\"reference external\" href=\"https://tools.ietf.org/html/rfc2616#section-14.13\">Content-Length</a>\nheader is no longer omitted from responses when using the default, HTTP/1.1\ndownload handler (see <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DOWNLOAD_HANDLERS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOAD_HANDLERS</span></code></a>).\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5009\">issue 5009</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5034\">issue 5034</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5045\">issue 5045</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5057\">issue 5057</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5062\">issue 5062</a>)</p>", "<p>Setting the <a class=\"hoverxref tooltip reference internal\" href=\"topics/spider-middleware.html#std-reqmeta-handle_httpstatus_all\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">handle_httpstatus_all</span></code></a> request meta key to <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code>\nnow has the same effect as not setting it at all, instead of having the\nsame effect as setting it to <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code>.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3851\">issue 3851</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4694\">issue 4694</a>)</p>", "<p>Added instructions to <a class=\"hoverxref tooltip reference internal\" href=\"intro/install.html#intro-install-windows\"><span class=\"std std-ref\">install Scrapy in Windows using pip</span></a>.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4715\">issue 4715</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4736\">issue 4736</a>)</p>", "<p>Logging documentation now includes <a class=\"hoverxref tooltip reference internal\" href=\"topics/logging.html#topics-logging-advanced-customization\"><span class=\"std std-ref\">additional ways to filter logs</span></a>.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4216\">issue 4216</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4257\">issue 4257</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4965\">issue 4965</a>)</p>", "<p>Covered how to deal with long lists of allowed domains in the <a class=\"hoverxref tooltip reference internal\" href=\"faq.html#faq\"><span class=\"std std-ref\">FAQ</span></a>. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2263\">issue 2263</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3667\">issue 3667</a>)</p>", "<p>Covered <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy-bench\">scrapy-bench</a> in <a class=\"hoverxref tooltip reference internal\" href=\"topics/benchmarking.html#benchmarking\"><span class=\"std std-ref\">Benchmarking</span></a>.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4996\">issue 4996</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5016\">issue 5016</a>)</p>", "<p>Clarified that one <a class=\"hoverxref tooltip reference internal\" href=\"topics/extensions.html#topics-extensions\"><span class=\"std std-ref\">extension</span></a> instance is created\nper crawler.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5014\">issue 5014</a>)</p>", "<p>Fixed some errors in examples.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4829\">issue 4829</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4830\">issue 4830</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4907\">issue 4907</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4909\">issue 4909</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5008\">issue 5008</a>)</p>", "<p>Fixed some external links, typos, and so on.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4892\">issue 4892</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4899\">issue 4899</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4936\">issue 4936</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4942\">issue 4942</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5005\">issue 5005</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5063\">issue 5063</a>)</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"topics/request-response.html#topics-request-meta\"><span class=\"std std-ref\">list of Request.meta keys</span></a> is now sorted\nalphabetically.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5061\">issue 5061</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5065\">issue 5065</a>)</p>", "<p>Updated references to Scrapinghub, which is now called Zyte.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4973\">issue 4973</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5072\">issue 5072</a>)</p>", "<p>Added a mention to contributors in the README. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4956\">issue 4956</a>)</p>", "<p>Reduced the top margin of lists. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4974\">issue 4974</a>)</p>", "<p>Made Python 3.9 support official (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4757\">issue 4757</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4759\">issue 4759</a>)</p>", "<p>Extended typing hints (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4895\">issue 4895</a>)</p>", "<p>Fixed deprecated uses of the Twisted API.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4940\">issue 4940</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4950\">issue 4950</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5073\">issue 5073</a>)</p>", "<p>Made our tests run with the new pip resolver.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4710\">issue 4710</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4814\">issue 4814</a>)</p>", "<p>Added tests to ensure that <a class=\"hoverxref tooltip reference internal\" href=\"topics/coroutines.html#coroutine-support\"><span class=\"std std-ref\">coroutine support</span></a>\nis tested. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4987\">issue 4987</a>)</p>", "<p>Migrated from Travis CI to GitHub Actions. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4924\">issue 4924</a>)</p>", "<p>Fixed CI issues.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4986\">issue 4986</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5020\">issue 5020</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5022\">issue 5022</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5027\">issue 5027</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5052\">issue 5052</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5053\">issue 5053</a>)</p>", "<p>Implemented code refactorings, style fixes and cleanups.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4911\">issue 4911</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4982\">issue 4982</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5001\">issue 5001</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5002\">issue 5002</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/5076\">issue 5076</a>)</p>", "<p>Fixed <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#topics-feed-exports\"><span class=\"std std-ref\">feed exports</span></a> overwrite support (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4845\">issue 4845</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4857\">issue 4857</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4859\">issue 4859</a>)</p>", "<p>Fixed the AsyncIO event loop handling, which could make code hang\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4855\">issue 4855</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4872\">issue 4872</a>)</p>", "<p>Fixed the IPv6-capable DNS resolver\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CachingHostnameResolver</span></code> for download handlers\nthat call\n<a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.internet.interfaces.IReactorCore.html#resolve\" title=\"(in Twisted)\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">reactor.resolve</span></code></a>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4802\">issue 4802</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4803\">issue 4803</a>)</p>", "<p>Fixed the output of the <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-genspider\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">genspider</span></code></a> command showing placeholders\ninstead of the import path of the generated spider module (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4874\">issue 4874</a>)</p>", "<p>Migrated Windows CI from Azure Pipelines to GitHub Actions (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4869\">issue 4869</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4876\">issue 4876</a>)</p>", "<p>Highlights:</p>", "<p>Python 3.5 support has been dropped.</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">file_path</span></code> method of <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#topics-media-pipeline\"><span class=\"std std-ref\">media pipelines</span></a>\ncan now access the source <a class=\"hoverxref tooltip reference internal\" href=\"topics/items.html#topics-items\"><span class=\"std std-ref\">item</span></a>.</p>", "<p>This allows you to set a download file path based on item data.</p>", "<p>The new <code class=\"docutils literal notranslate\"><span class=\"pre\">item_export_kwargs</span></code> key of the <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEEDS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEEDS</span></code></a> setting allows\nto define keyword parameters to pass to <a class=\"hoverxref tooltip reference internal\" href=\"topics/exporters.html#topics-exporters\"><span class=\"std std-ref\">item exporter classes</span></a></p>", "<p>You can now choose whether <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#topics-feed-exports\"><span class=\"std std-ref\">feed exports</span></a>\noverwrite or append to the output file.</p>", "<p>For example, when using the <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-crawl\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">crawl</span></code></a> or <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-runspider\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">runspider</span></code></a>\ncommands, you can use the <code class=\"docutils literal notranslate\"><span class=\"pre\">-O</span></code> option instead of <code class=\"docutils literal notranslate\"><span class=\"pre\">-o</span></code> to overwrite the\noutput file.</p>", "<p>Zstd-compressed responses are now supported if <a class=\"reference external\" href=\"https://pypi.org/project/zstandard/\">zstandard</a> is installed.</p>", "<p>In settings, where the import path of a class is required, it is now\npossible to pass a class object instead.</p>", "<p>Python 3.6 or greater is now required; support for Python 3.5 has been\ndropped</p>", "<p>As a result:</p>", "<p>When using PyPy, PyPy 7.2.0 or greater <a class=\"hoverxref tooltip reference internal\" href=\"intro/install.html#faq-python-versions\"><span class=\"std std-ref\">is now required</span></a></p>", "<p>For Amazon S3 storage support in <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#topics-feed-storage-s3\"><span class=\"std std-ref\">feed exports</span></a> or <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#media-pipelines-s3\"><span class=\"std std-ref\">media pipelines</span></a>, <a class=\"reference external\" href=\"https://github.com/boto/botocore\">botocore</a> 1.4.87 or greater is now required</p>", "<p>To use the <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#images-pipeline\"><span class=\"std std-ref\">images pipeline</span></a>, <a class=\"reference external\" href=\"https://python-pillow.org/\">Pillow</a> 4.0.0 or\ngreater is now required</p>", "<p>(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4718\">issue 4718</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4732\">issue 4732</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4733\">issue 4733</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4742\">issue 4742</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4743\">issue 4743</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4764\">issue 4764</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware\" title=\"scrapy.downloadermiddlewares.cookies.CookiesMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CookiesMiddleware</span></code></a> once again\ndiscards cookies defined in <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request.headers\" title=\"scrapy.http.Request.headers\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Request.headers</span></code></a>.</p>", "<p>We decided to revert this bug fix, introduced in Scrapy 2.2.0, because it\nwas reported that the current implementation could break existing code.</p>", "<p>If you need to set cookies for a request, use the <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request.cookies</span></code></a> parameter.</p>", "<p>A future version of Scrapy will include a new, better implementation of the\nreverted bug fix.</p>", "<p>(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4717\">issue 4717</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4823\">issue 4823</a>)</p>", "<p><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.extensions.feedexport.S3FeedStorage</span></code> no longer reads the\nvalues of <code class=\"docutils literal notranslate\"><span class=\"pre\">access_key</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">secret_key</span></code> from the running project\nsettings when they are not passed to its <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method; you must\neither pass those parameters to its <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method or use\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">S3FeedStorage.from_crawler</span></code>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4356\">issue 4356</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4411\">issue 4411</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4688\">issue 4688</a>)</p>", "<p><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Rule.process_request</span></code>\nno longer admits callables which expect a single <code class=\"docutils literal notranslate\"><span class=\"pre\">request</span></code> parameter,\nrather than both <code class=\"docutils literal notranslate\"><span class=\"pre\">request</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">response</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4818\">issue 4818</a>)</p>", "<p>In custom <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#topics-media-pipeline\"><span class=\"std std-ref\">media pipelines</span></a>, signatures that\ndo not accept a keyword-only <code class=\"docutils literal notranslate\"><span class=\"pre\">item</span></code> parameter in any of the  methods that\n<a class=\"hoverxref tooltip reference internal\" href=\"#media-pipeline-item-parameter\"><span class=\"std std-ref\">now support this parameter</span></a> are now\ndeprecated (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4628\">issue 4628</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4686\">issue 4686</a>)</p>", "<p>In custom <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#topics-feed-storage\"><span class=\"std std-ref\">feed storage backend classes</span></a>,\n<code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method signatures that do not accept a keyword-only\n<code class=\"docutils literal notranslate\"><span class=\"pre\">feed_options</span></code> parameter are now deprecated (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/547\">issue 547</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/716\">issue 716</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4512\">issue 4512</a>)</p>", "<p>The <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.utils.python.WeakKeyCache</span></code> class is now deprecated\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4684\">issue 4684</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4701\">issue 4701</a>)</p>", "<p>The <code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.utils.boto.is_botocore()</span></code> function is now deprecated, use\n<code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.utils.boto.is_botocore_available()</span></code> instead (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4734\">issue 4734</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4776\">issue 4776</a>)</p>", "<p>The following methods of <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#topics-media-pipeline\"><span class=\"std std-ref\">media pipelines</span></a> now\naccept an <code class=\"docutils literal notranslate\"><span class=\"pre\">item</span></code> keyword-only parameter containing the source\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/items.html#topics-items\"><span class=\"std std-ref\">item</span></a>:</p>", "<p>In <a class=\"reference internal\" href=\"topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline\" title=\"scrapy.pipelines.files.FilesPipeline\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.pipelines.files.FilesPipeline</span></code></a>:</p>", "<p>In <a class=\"reference internal\" href=\"topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline\" title=\"scrapy.pipelines.images.ImagesPipeline\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.pipelines.images.ImagesPipeline</span></code></a>:</p>", "<p>(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4628\">issue 4628</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4686\">issue 4686</a>)</p>", "<p>The new <code class=\"docutils literal notranslate\"><span class=\"pre\">item_export_kwargs</span></code> key of the <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEEDS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEEDS</span></code></a> setting allows\nto define keyword parameters to pass to <a class=\"hoverxref tooltip reference internal\" href=\"topics/exporters.html#topics-exporters\"><span class=\"std std-ref\">item exporter classes</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4606\">issue 4606</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4768\">issue 4768</a>)</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#topics-feed-exports\"><span class=\"std std-ref\">Feed exports</span></a> gained overwrite support:</p>", "<p>When using the <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-crawl\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">crawl</span></code></a> or <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-runspider\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">runspider</span></code></a> commands, you\ncan use the <code class=\"docutils literal notranslate\"><span class=\"pre\">-O</span></code> option instead of <code class=\"docutils literal notranslate\"><span class=\"pre\">-o</span></code> to overwrite the output\nfile</p>", "<p>You can use the <code class=\"docutils literal notranslate\"><span class=\"pre\">overwrite</span></code> key in the <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEEDS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEEDS</span></code></a> setting to\nconfigure whether to overwrite the output file (<code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code>) or append to\nits content (<code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">from_crawler</span></code> methods of <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#topics-feed-storage\"><span class=\"std std-ref\">feed storage\nbackend classes</span></a> now receive a new keyword-only\nparameter, <code class=\"docutils literal notranslate\"><span class=\"pre\">feed_options</span></code>, which is a dictionary of <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#feed-options\"><span class=\"std std-ref\">feed\noptions</span></a></p>", "<p>(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/547\">issue 547</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/716\">issue 716</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4512\">issue 4512</a>)</p>", "<p>Zstd-compressed responses are now supported if <a class=\"reference external\" href=\"https://pypi.org/project/zstandard/\">zstandard</a> is installed\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4831\">issue 4831</a>)</p>", "<p>In settings, where the import path of a class is required, it is now\npossible to pass a class object instead (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3870\">issue 3870</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3873\">issue 3873</a>).</p>", "<p>This includes also settings where only part of its value is made of an\nimport path, such as <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DOWNLOADER_MIDDLEWARES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOADER_MIDDLEWARES</span></code></a> or\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DOWNLOAD_HANDLERS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOAD_HANDLERS</span></code></a>.</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#topics-downloader-middleware\"><span class=\"std std-ref\">Downloader middlewares</span></a> can now\noverride <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Response.request\" title=\"scrapy.http.Response.request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">response.request</span></code></a>.</p>", "<p>If a <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#topics-downloader-middleware\"><span class=\"std std-ref\">downloader middleware</span></a> returns\na <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Response\" title=\"scrapy.http.Response\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response</span></code></a> object from\n<a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_response\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_response()</span></code></a>\nor\n<a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_exception()</span></code></a>\nwith a custom <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> object assigned to\n<a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Response.request\" title=\"scrapy.http.Response.request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">response.request</span></code></a>:</p>", "<p>The response is handled by the callback of that custom\n<a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> object, instead of being handled by the\ncallback of the original <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> object</p>", "<p>That custom <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> object is now sent as the\n<code class=\"docutils literal notranslate\"><span class=\"pre\">request</span></code> argument to the <a class=\"hoverxref tooltip reference internal\" href=\"topics/signals.html#std-signal-response_received\"><code class=\"xref std std-signal docutils literal notranslate\"><span class=\"pre\">response_received</span></code></a> signal, instead\nof the original <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> object</p>", "<p>(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4529\">issue 4529</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4632\">issue 4632</a>)</p>", "<p>When using the <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#topics-feed-storage-ftp\"><span class=\"std std-ref\">FTP feed storage backend</span></a>:</p>", "<p>It is now possible to set the new <code class=\"docutils literal notranslate\"><span class=\"pre\">overwrite</span></code> <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#feed-options\"><span class=\"std std-ref\">feed option</span></a> to <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code> to append to an existing file instead of\noverwriting it</p>", "<p>The FTP password can now be omitted if it is not necessary</p>", "<p>(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/547\">issue 547</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/716\">issue 716</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4512\">issue 4512</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method of <a class=\"reference internal\" href=\"topics/exporters.html#scrapy.exporters.CsvItemExporter\" title=\"scrapy.exporters.CsvItemExporter\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CsvItemExporter</span></code></a> now\nsupports an <code class=\"docutils literal notranslate\"><span class=\"pre\">errors</span></code> parameter to indicate how to handle encoding errors\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4755\">issue 4755</a>)</p>", "<p>When <a class=\"hoverxref tooltip reference internal\" href=\"topics/asyncio.html#using-asyncio\"><span class=\"std std-ref\">using asyncio</span></a>, it is now possible to\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/asyncio.html#using-custom-loops\"><span class=\"std std-ref\">set a custom asyncio loop</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4306\">issue 4306</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4414\">issue 4414</a>)</p>", "<p>Serialized requests (see <a class=\"hoverxref tooltip reference internal\" href=\"topics/jobs.html#topics-jobs\"><span class=\"std std-ref\">Jobs: pausing and resuming crawls</span></a>) now support callbacks that are\nspider methods that delegate on other callable (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4756\">issue 4756</a>)</p>", "<p>When a response is larger than <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DOWNLOAD_MAXSIZE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOAD_MAXSIZE</span></code></a>, the logged\nmessage is now a warning, instead of an error (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3874\">issue 3874</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3886\">issue 3886</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4752\">issue 4752</a>)</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-genspider\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">genspider</span></code></a> command no longer overwrites existing files\nunless the <code class=\"docutils literal notranslate\"><span class=\"pre\">--force</span></code> option is used (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4561\">issue 4561</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4616\">issue 4616</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4623\">issue 4623</a>)</p>", "<p>Cookies with an empty value are no longer considered invalid cookies\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4772\">issue 4772</a>)</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-runspider\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">runspider</span></code></a> command now supports files with the <code class=\"docutils literal notranslate\"><span class=\"pre\">.pyw</span></code> file\nextension (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4643\">issue 4643</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4646\">issue 4646</a>)</p>", "<p>The <a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\" title=\"scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">HttpProxyMiddleware</span></code></a>\nmiddleware now simply ignores unsupported proxy values (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3331\">issue 3331</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4778\">issue 4778</a>)</p>", "<p>Checks for generator callbacks with a <code class=\"docutils literal notranslate\"><span class=\"pre\">return</span></code> statement no longer warn\nabout <code class=\"docutils literal notranslate\"><span class=\"pre\">return</span></code> statements in nested functions (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4720\">issue 4720</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4721\">issue 4721</a>)</p>", "<p>The system file mode creation mask no longer affects the permissions of\nfiles generated using the <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-startproject\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">startproject</span></code></a> command (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4722\">issue 4722</a>)</p>", "<p><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.utils.iterators.xmliter()</span></code> now supports namespaced node names\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/861\">issue 861</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4746\">issue 4746</a>)</p>", "<p><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> objects can now have <code class=\"docutils literal notranslate\"><span class=\"pre\">about:</span></code> URLs, which can\nwork when using a headless browser (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4835\">issue 4835</a>)</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEED_URI_PARAMS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEED_URI_PARAMS</span></code></a> setting is now documented (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4671\">issue 4671</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4724\">issue 4724</a>)</p>", "<p>Improved the documentation of\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/link-extractors.html#topics-link-extractors\"><span class=\"std std-ref\">link extractors</span></a> with an usage example from\na spider callback and reference documentation for the\n<a class=\"reference internal\" href=\"topics/link-extractors.html#scrapy.link.Link\" title=\"scrapy.link.Link\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Link</span></code></a> class (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4751\">issue 4751</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4775\">issue 4775</a>)</p>", "<p>Clarified the impact of <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-CONCURRENT_REQUESTS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">CONCURRENT_REQUESTS</span></code></a> when using the\n<a class=\"reference internal\" href=\"topics/extensions.html#scrapy.extensions.closespider.CloseSpider\" title=\"scrapy.extensions.closespider.CloseSpider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CloseSpider</span></code></a> extension\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4836\">issue 4836</a>)</p>", "<p>Removed references to Python 2’s <code class=\"docutils literal notranslate\"><span class=\"pre\">unicode</span></code> type (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4547\">issue 4547</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4703\">issue 4703</a>)</p>", "<p>We now have an <a class=\"hoverxref tooltip reference internal\" href=\"versioning.html#deprecation-policy\"><span class=\"std std-ref\">official deprecation policy</span></a>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4705\">issue 4705</a>)</p>", "<p>Our <a class=\"hoverxref tooltip reference internal\" href=\"contributing.html#documentation-policies\"><span class=\"std std-ref\">documentation policies</span></a> now cover usage\nof Sphinx’s <a class=\"reference external\" href=\"https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionadded\" title=\"(in Sphinx v7.3.0)\"><code class=\"xref rst rst-dir docutils literal notranslate\"><span class=\"pre\">versionadded</span></code></a> and <a class=\"reference external\" href=\"https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionchanged\" title=\"(in Sphinx v7.3.0)\"><code class=\"xref rst rst-dir docutils literal notranslate\"><span class=\"pre\">versionchanged</span></code></a>\ndirectives, and we have removed usages referencing Scrapy 1.4.0 and earlier\nversions (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3971\">issue 3971</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4310\">issue 4310</a>)</p>", "<p>Other documentation cleanups (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4090\">issue 4090</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4782\">issue 4782</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4800\">issue 4800</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4801\">issue 4801</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4809\">issue 4809</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4816\">issue 4816</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4825\">issue 4825</a>)</p>", "<p>Extended typing hints (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4243\">issue 4243</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4691\">issue 4691</a>)</p>", "<p>Added tests for the <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-check\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">check</span></code></a> command (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4663\">issue 4663</a>)</p>", "<p>Fixed test failures on Debian (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4726\">issue 4726</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4727\">issue 4727</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4735\">issue 4735</a>)</p>", "<p>Improved Windows test coverage (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4723\">issue 4723</a>)</p>", "<p>Switched to <a class=\"reference external\" href=\"https://docs.python.org/3/reference/lexical_analysis.html#f-strings\" title=\"(in Python v3.11)\"><span class=\"xref std std-ref\">formatted string literals</span></a> where possible\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4307\">issue 4307</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4324\">issue 4324</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4672\">issue 4672</a>)</p>", "<p>Modernized <code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">super()</span></code> usage (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4707\">issue 4707</a>)</p>", "<p>Other code and test cleanups (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1790\">issue 1790</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3288\">issue 3288</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4165\">issue 4165</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4564\">issue 4564</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4651\">issue 4651</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4714\">issue 4714</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4738\">issue 4738</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4745\">issue 4745</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4747\">issue 4747</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4761\">issue 4761</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4765\">issue 4765</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4804\">issue 4804</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4817\">issue 4817</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4820\">issue 4820</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4822\">issue 4822</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4839\">issue 4839</a>)</p>", "<p>Highlights:</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#topics-feed-exports\"><span class=\"std std-ref\">Feed exports</span></a> now support <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#topics-feed-storage-gcs\"><span class=\"std std-ref\">Google Cloud\nStorage</span></a> as a storage backend</p>", "<p>The new <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEED_EXPORT_BATCH_ITEM_COUNT</span></code></a> setting allows to deliver\noutput items in batches of up to the specified number of items.</p>", "<p>It also serves as a workaround for <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#delayed-file-delivery\"><span class=\"std std-ref\">delayed file delivery</span></a>, which causes Scrapy to only start item delivery\nafter the crawl has finished when using certain storage backends\n(<a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#topics-feed-storage-s3\"><span class=\"std std-ref\">S3</span></a>, <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#topics-feed-storage-ftp\"><span class=\"std std-ref\">FTP</span></a>,\nand now <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#topics-feed-storage-gcs\"><span class=\"std std-ref\">GCS</span></a>).</p>", "<p>The base implementation of <a class=\"hoverxref tooltip reference internal\" href=\"topics/loaders.html#topics-loaders\"><span class=\"std std-ref\">item loaders</span></a> has been\nmoved into a separate library, <a class=\"reference external\" href=\"https://itemloaders.readthedocs.io/en/latest/index.html\" title=\"(in itemloaders)\"><span class=\"xref std std-doc\">itemloaders</span></a>,\nallowing usage from outside Scrapy and a separate release schedule</p>", "<p>Removed the following classes and their parent modules from\n<code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.linkextractors</span></code>:</p>", "<p>Use\n<a class=\"reference internal\" href=\"topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\" title=\"scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">LinkExtractor</span></code></a>\ninstead (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4356\">issue 4356</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4679\">issue 4679</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.python.retry_on_eintr</span></code> function is now deprecated\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4683\">issue 4683</a>)</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#topics-feed-exports\"><span class=\"std std-ref\">Feed exports</span></a> support <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#topics-feed-storage-gcs\"><span class=\"std std-ref\">Google Cloud\nStorage</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/685\">issue 685</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3608\">issue 3608</a>)</p>", "<p>New <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEED_EXPORT_BATCH_ITEM_COUNT</span></code></a> setting for batch deliveries\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4250\">issue 4250</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4434\">issue 4434</a>)</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-parse\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">parse</span></code></a> command now allows specifying an output file\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4317\">issue 4317</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4377\">issue 4377</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request.from_curl\" title=\"scrapy.http.Request.from_curl\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">Request.from_curl</span></code></a> and\n<a class=\"reference internal\" href=\"topics/developer-tools.html#scrapy.utils.curl.curl_to_request_kwargs\" title=\"scrapy.utils.curl.curl_to_request_kwargs\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">curl_to_request_kwargs()</span></code></a> now also support\n<code class=\"docutils literal notranslate\"><span class=\"pre\">--data-raw</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4612\">issue 4612</a>)</p>", "<p>A <code class=\"docutils literal notranslate\"><span class=\"pre\">parse</span></code> callback may now be used in built-in spider subclasses, such\nas <a class=\"reference internal\" href=\"topics/spiders.html#scrapy.spiders.CrawlSpider\" title=\"scrapy.spiders.CrawlSpider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CrawlSpider</span></code></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/712\">issue 712</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/732\">issue 732</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/781\">issue 781</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4254\">issue 4254</a> )</p>", "<p>Fixed the <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#topics-feed-format-csv\"><span class=\"std std-ref\">CSV exporting</span></a> of\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/items.html#dataclass-items\"><span class=\"std std-ref\">dataclass items</span></a> and <a class=\"hoverxref tooltip reference internal\" href=\"topics/items.html#attrs-items\"><span class=\"std std-ref\">attr.s items</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4667\">issue 4667</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4668\">issue 4668</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request.from_curl\" title=\"scrapy.http.Request.from_curl\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">Request.from_curl</span></code></a> and\n<a class=\"reference internal\" href=\"topics/developer-tools.html#scrapy.utils.curl.curl_to_request_kwargs\" title=\"scrapy.utils.curl.curl_to_request_kwargs\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">curl_to_request_kwargs()</span></code></a> now set the request\nmethod to <code class=\"docutils literal notranslate\"><span class=\"pre\">POST</span></code> when a request body is specified and no request method\nis specified (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4612\">issue 4612</a>)</p>", "<p>The processing of ANSI escape sequences in enabled in Windows 10.0.14393\nand later, where it is required for colored output (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4393\">issue 4393</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4403\">issue 4403</a>)</p>", "<p>Updated the <a class=\"reference external\" href=\"https://www.openssl.org/docs/manmaster/man1/openssl-ciphers.html#CIPHER-LIST-FORMAT\">OpenSSL cipher list format</a> link in the documentation about\nthe <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOADER_CLIENT_TLS_CIPHERS</span></code></a> setting (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4653\">issue 4653</a>)</p>", "<p>Simplified the code example in <a class=\"hoverxref tooltip reference internal\" href=\"topics/loaders.html#topics-loaders-dataclass\"><span class=\"std std-ref\">Working with dataclass items</span></a>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4652\">issue 4652</a>)</p>", "<p>The base implementation of <a class=\"hoverxref tooltip reference internal\" href=\"topics/loaders.html#topics-loaders\"><span class=\"std std-ref\">item loaders</span></a> has been\nmoved into <a class=\"reference external\" href=\"https://itemloaders.readthedocs.io/en/latest/index.html\" title=\"(in itemloaders)\"><span class=\"xref std std-doc\">itemloaders</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4005\">issue 4005</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4516\">issue 4516</a>)</p>", "<p>Fixed a silenced error in some scheduler tests (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4644\">issue 4644</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4645\">issue 4645</a>)</p>", "<p>Renewed the localhost certificate used for SSL tests (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4650\">issue 4650</a>)</p>", "<p>Removed cookie-handling code specific to Python 2 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4682\">issue 4682</a>)</p>", "<p>Stopped using Python 2 unicode literal syntax (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4704\">issue 4704</a>)</p>", "<p>Stopped using a backlash for line continuation (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4673\">issue 4673</a>)</p>", "<p>Removed unneeded entries from the MyPy exception list (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4690\">issue 4690</a>)</p>", "<p>Automated tests now pass on Windows as part of our continuous integration\nsystem (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4458\">issue 4458</a>)</p>", "<p>Automated tests now pass on the latest PyPy version for supported Python\nversions in our continuous integration system (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4504\">issue 4504</a>)</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-startproject\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">startproject</span></code></a> command no longer makes unintended changes to\nthe permissions of files in the destination folder, such as removing\nexecution permissions (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4662\">issue 4662</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4666\">issue 4666</a>)</p>", "<p>Highlights:</p>", "<p>Python 3.5.2+ is required now</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/items.html#dataclass-items\"><span class=\"std std-ref\">dataclass objects</span></a> and\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/items.html#attrs-items\"><span class=\"std std-ref\">attrs objects</span></a> are now valid <a class=\"hoverxref tooltip reference internal\" href=\"topics/items.html#item-types\"><span class=\"std std-ref\">item types</span></a></p>", "<p>New <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.TextResponse.json\" title=\"scrapy.http.TextResponse.json\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">TextResponse.json</span></code></a> method</p>", "<p>New <a class=\"hoverxref tooltip reference internal\" href=\"topics/signals.html#std-signal-bytes_received\"><code class=\"xref std std-signal docutils literal notranslate\"><span class=\"pre\">bytes_received</span></code></a> signal that allows canceling response download</p>", "<p><a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware\" title=\"scrapy.downloadermiddlewares.cookies.CookiesMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CookiesMiddleware</span></code></a> fixes</p>", "<p>Support for Python 3.5.0 and 3.5.1 has been dropped; Scrapy now refuses to\nrun with a Python version lower than 3.5.2, which introduced\n<a class=\"reference external\" href=\"https://docs.python.org/3/library/typing.html#typing.Type\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">typing.Type</span></code></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4615\">issue 4615</a>)</p>", "<p><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">TextResponse.body_as_unicode</span></code> is now deprecated, use\n<a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.TextResponse.text\" title=\"scrapy.http.TextResponse.text\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">TextResponse.text</span></code></a> instead\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4546\">issue 4546</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4555\">issue 4555</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4579\">issue 4579</a>)</p>", "<p><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.item.BaseItem</span></code> is now deprecated, use\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.item.Item</span></code> instead (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4534\">issue 4534</a>)</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/items.html#dataclass-items\"><span class=\"std std-ref\">dataclass objects</span></a> and\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/items.html#attrs-items\"><span class=\"std std-ref\">attrs objects</span></a> are now valid <a class=\"hoverxref tooltip reference internal\" href=\"topics/items.html#item-types\"><span class=\"std std-ref\">item types</span></a>, and a new <a class=\"reference external\" href=\"https://github.com/scrapy/itemadapter\">itemadapter</a> library makes it easy to\nwrite code that <a class=\"hoverxref tooltip reference internal\" href=\"topics/items.html#supporting-item-types\"><span class=\"std std-ref\">supports any item type</span></a>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2749\">issue 2749</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2807\">issue 2807</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3761\">issue 3761</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3881\">issue 3881</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4642\">issue 4642</a>)</p>", "<p>A new <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.TextResponse.json\" title=\"scrapy.http.TextResponse.json\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">TextResponse.json</span></code></a> method\nallows to deserialize JSON responses (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2444\">issue 2444</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4460\">issue 4460</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4574\">issue 4574</a>)</p>", "<p>A new <a class=\"hoverxref tooltip reference internal\" href=\"topics/signals.html#std-signal-bytes_received\"><code class=\"xref std std-signal docutils literal notranslate\"><span class=\"pre\">bytes_received</span></code></a> signal allows monitoring response download\nprogress and <a class=\"hoverxref tooltip reference internal\" href=\"topics/request-response.html#topics-stop-response-download\"><span class=\"std std-ref\">stopping downloads</span></a>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4205\">issue 4205</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4559\">issue 4559</a>)</p>", "<p>The dictionaries in the result list of a <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#topics-media-pipeline\"><span class=\"std std-ref\">media pipeline</span></a> now include a new key, <code class=\"docutils literal notranslate\"><span class=\"pre\">status</span></code>, which indicates\nif the file was downloaded or, if the file was not downloaded, why it was\nnot downloaded; see <a class=\"reference internal\" href=\"topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.get_media_requests\" title=\"scrapy.pipelines.files.FilesPipeline.get_media_requests\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">FilesPipeline.get_media_requests</span></code></a> for more\ninformation (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2893\">issue 2893</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4486\">issue 4486</a>)</p>", "<p>When using <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#media-pipeline-gcs\"><span class=\"std std-ref\">Google Cloud Storage</span></a> for\na <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#topics-media-pipeline\"><span class=\"std std-ref\">media pipeline</span></a>, a warning is now logged if\nthe configured credentials do not grant the required permissions\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4346\">issue 4346</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4508\">issue 4508</a>)</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/link-extractors.html#topics-link-extractors\"><span class=\"std std-ref\">Link extractors</span></a> are now serializable,\nas long as you do not use <a class=\"reference external\" href=\"https://docs.python.org/3/reference/expressions.html#lambda\" title=\"(in Python v3.11)\"><span class=\"xref std std-ref\">lambdas</span></a> for parameters; for\nexample, you can now pass link extractors in <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request.cb_kwargs\" title=\"scrapy.http.Request.cb_kwargs\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Request.cb_kwargs</span></code></a> or\n<a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request.meta\" title=\"scrapy.http.Request.meta\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Request.meta</span></code></a> when <a class=\"hoverxref tooltip reference internal\" href=\"topics/jobs.html#topics-jobs\"><span class=\"std std-ref\">persisting\nscheduled requests</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4554\">issue 4554</a>)</p>", "<p>Upgraded the <a class=\"reference external\" href=\"https://docs.python.org/3/library/pickle.html#pickle-protocols\" title=\"(in Python v3.11)\"><span class=\"xref std std-ref\">pickle protocol</span></a> that Scrapy uses\nfrom protocol 2 to protocol 4, improving serialization capabilities and\nperformance (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4135\">issue 4135</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4541\">issue 4541</a>)</p>", "<p><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.utils.misc.create_instance()</span></code> now raises a <a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#TypeError\" title=\"(in Python v3.11)\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">TypeError</span></code></a>\nexception if the resulting instance is <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4528\">issue 4528</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4532\">issue 4532</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware\" title=\"scrapy.downloadermiddlewares.cookies.CookiesMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CookiesMiddleware</span></code></a> no longer\ndiscards cookies defined in <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request.headers\" title=\"scrapy.http.Request.headers\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Request.headers</span></code></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1992\">issue 1992</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2400\">issue 2400</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware\" title=\"scrapy.downloadermiddlewares.cookies.CookiesMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CookiesMiddleware</span></code></a> no longer\nre-encodes cookies defined as <a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#bytes\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">bytes</span></code></a> in the <code class=\"docutils literal notranslate\"><span class=\"pre\">cookies</span></code> parameter\nof the <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method of <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2400\">issue 2400</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3575\">issue 3575</a>)</p>", "<p>When <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEEDS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEEDS</span></code></a> defines multiple URIs, <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEED_STORE_EMPTY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEED_STORE_EMPTY</span></code></a> is\n<code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code> and the crawl yields no items, Scrapy no longer stops feed\nexports after the first URI (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4621\">issue 4621</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4626\">issue 4626</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/spiders.html#scrapy.spiders.Spider\" title=\"scrapy.spiders.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> callbacks defined using <a class=\"reference internal\" href=\"topics/coroutines.html\"><span class=\"doc\">coroutine\nsyntax</span></a> no longer need to return an iterable, and may\ninstead return a <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> object, an\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/items.html#topics-items\"><span class=\"std std-ref\">item</span></a>, or <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4609\">issue 4609</a>)</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-startproject\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">startproject</span></code></a> command now ensures that the generated project\nfolders and files have the right permissions (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4604\">issue 4604</a>)</p>", "<p>Fix a <a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#KeyError\" title=\"(in Python v3.11)\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">KeyError</span></code></a> exception being sometimes raised from\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.utils.datatypes.LocalWeakReferencedCache</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4597\">issue 4597</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4599\">issue 4599</a>)</p>", "<p>When <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEEDS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEEDS</span></code></a> defines multiple URIs, log messages about items being\nstored now contain information from the corresponding feed, instead of\nalways containing information about only one of the feeds (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4619\">issue 4619</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4629\">issue 4629</a>)</p>", "<p>Added a new section about <a class=\"hoverxref tooltip reference internal\" href=\"topics/request-response.html#errback-cb-kwargs\"><span class=\"std std-ref\">accessing cb_kwargs from errbacks</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4598\">issue 4598</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4634\">issue 4634</a>)</p>", "<p>Covered <a class=\"reference external\" href=\"https://github.com/Nykakin/chompjs\">chompjs</a> in <a class=\"hoverxref tooltip reference internal\" href=\"topics/dynamic-content.html#topics-parsing-javascript\"><span class=\"std std-ref\">Parsing JavaScript code</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4556\">issue 4556</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4562\">issue 4562</a>)</p>", "<p>Removed from <a class=\"reference internal\" href=\"topics/coroutines.html\"><span class=\"doc\">Coroutines</span></a> the warning about the API being\nexperimental (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4511\">issue 4511</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4513\">issue 4513</a>)</p>", "<p>Removed references to unsupported versions of <a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/index.html\" title=\"(in Twisted v23.8)\"><span class=\"xref std std-doc\">Twisted</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4533\">issue 4533</a>)</p>", "<p>Updated the description of the <a class=\"hoverxref tooltip reference internal\" href=\"topics/item-pipeline.html#screenshotpipeline\"><span class=\"std std-ref\">screenshot pipeline example</span></a>, which now uses <a class=\"reference internal\" href=\"topics/coroutines.html\"><span class=\"doc\">coroutine syntax</span></a> instead of returning a\n<a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html\" title=\"(in Twisted)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Deferred</span></code></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4514\">issue 4514</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4593\">issue 4593</a>)</p>", "<p>Removed a misleading import line from the\n<a class=\"reference internal\" href=\"topics/logging.html#scrapy.utils.log.configure_logging\" title=\"scrapy.utils.log.configure_logging\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.utils.log.configure_logging()</span></code></a> code example (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4510\">issue 4510</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4587\">issue 4587</a>)</p>", "<p>The display-on-hover behavior of internal documentation references now also\ncovers links to <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#topics-commands\"><span class=\"std std-ref\">commands</span></a>, <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request.meta\" title=\"scrapy.http.Request.meta\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Request.meta</span></code></a> keys, <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#topics-settings\"><span class=\"std std-ref\">settings</span></a> and\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/signals.html#topics-signals\"><span class=\"std std-ref\">signals</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4495\">issue 4495</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4563\">issue 4563</a>)</p>", "<p>It is again possible to download the documentation for offline reading\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4578\">issue 4578</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4585\">issue 4585</a>)</p>", "<p>Removed backslashes preceding <code class=\"docutils literal notranslate\"><span class=\"pre\">*args</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">**kwargs</span></code> in some function\nand method signatures (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4592\">issue 4592</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4596\">issue 4596</a>)</p>", "<p>Adjusted the code base further to our <a class=\"hoverxref tooltip reference internal\" href=\"contributing.html#coding-style\"><span class=\"std std-ref\">style guidelines</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4237\">issue 4237</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4525\">issue 4525</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4538\">issue 4538</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4539\">issue 4539</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4540\">issue 4540</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4542\">issue 4542</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4543\">issue 4543</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4544\">issue 4544</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4545\">issue 4545</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4557\">issue 4557</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4558\">issue 4558</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4566\">issue 4566</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4568\">issue 4568</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4572\">issue 4572</a>)</p>", "<p>Removed remnants of Python 2 support (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4550\">issue 4550</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4553\">issue 4553</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4568\">issue 4568</a>)</p>", "<p>Improved code sharing between the <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-crawl\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">crawl</span></code></a> and <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-runspider\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">runspider</span></code></a>\ncommands (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4548\">issue 4548</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4552\">issue 4552</a>)</p>", "<p>Replaced <code class=\"docutils literal notranslate\"><span class=\"pre\">chain(*iterable)</span></code> with <code class=\"docutils literal notranslate\"><span class=\"pre\">chain.from_iterable(iterable)</span></code>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4635\">issue 4635</a>)</p>", "<p>You may now run the <a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio.html#module-asyncio\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">asyncio</span></code></a> tests with Tox on any Python version\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4521\">issue 4521</a>)</p>", "<p>Updated test requirements to reflect an incompatibility with pytest 5.4 and\n5.4.1 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4588\">issue 4588</a>)</p>", "<p>Improved <a class=\"reference internal\" href=\"topics/api.html#scrapy.spiderloader.SpiderLoader\" title=\"scrapy.spiderloader.SpiderLoader\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">SpiderLoader</span></code></a> test coverage for\nscenarios involving duplicate spider names (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4549\">issue 4549</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4560\">issue 4560</a>)</p>", "<p>Configured Travis CI to also run the tests with Python 3.5.2\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4518\">issue 4518</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4615\">issue 4615</a>)</p>", "<p>Added a <a class=\"reference external\" href=\"https://www.pylint.org/\">Pylint</a> job to Travis CI\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3727\">issue 3727</a>)</p>", "<p>Added a <a class=\"reference external\" href=\"http://mypy-lang.org/\">Mypy</a> job to Travis CI (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4637\">issue 4637</a>)</p>", "<p>Made use of set literals in tests (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4573\">issue 4573</a>)</p>", "<p>Cleaned up the Travis CI configuration (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4517\">issue 4517</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4519\">issue 4519</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4522\">issue 4522</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4537\">issue 4537</a>)</p>", "<p>Highlights:</p>", "<p>New <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEEDS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEEDS</span></code></a> setting to export to multiple feeds</p>", "<p>New <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Response.ip_address\" title=\"scrapy.http.Response.ip_address\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Response.ip_address</span></code></a> attribute</p>", "<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#AssertionError\" title=\"(in Python v3.11)\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">AssertionError</span></code></a> exceptions triggered by <a class=\"reference external\" href=\"https://docs.python.org/3/reference/simple_stmts.html#assert\" title=\"(in Python v3.11)\"><span class=\"xref std std-ref\">assert</span></a>\nstatements have been replaced by new exception types, to support running\nPython in optimized mode (see <a class=\"reference external\" href=\"https://docs.python.org/3/using/cmdline.html#cmdoption-O\" title=\"(in Python v3.11)\"><code class=\"xref std std-option docutils literal notranslate\"><span class=\"pre\">-O</span></code></a>) without changing Scrapy’s\nbehavior in any unexpected ways.</p>", "<p>If you catch an <a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#AssertionError\" title=\"(in Python v3.11)\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">AssertionError</span></code></a> exception from Scrapy, update your\ncode to catch the corresponding new exception.</p>", "<p>(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4440\">issue 4440</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">LOG_UNSERIALIZABLE_REQUESTS</span></code> setting is no longer supported, use\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-SCHEDULER_DEBUG\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SCHEDULER_DEBUG</span></code></a> instead (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4385\">issue 4385</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">REDIRECT_MAX_METAREFRESH_DELAY</span></code> setting is no longer supported, use\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-setting-METAREFRESH_MAXDELAY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">METAREFRESH_MAXDELAY</span></code></a> instead (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4385\">issue 4385</a>)</p>", "<p>The <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ChunkedTransferMiddleware</span></code>\nmiddleware has been removed, including the entire\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.downloadermiddlewares.chunked</span></code> module; chunked transfers\nwork out of the box (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4431\">issue 4431</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">spiders</span></code> property has been removed from\n<a class=\"reference internal\" href=\"topics/api.html#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a>, use <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CrawlerRunner.spider_loader</span></code> or instantiate\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-SPIDER_LOADER_CLASS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SPIDER_LOADER_CLASS</span></code></a> with your settings instead (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4398\">issue 4398</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">MultiValueDict</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">MultiValueDictKeyError</span></code>, and <code class=\"docutils literal notranslate\"><span class=\"pre\">SiteNode</span></code>\nclasses have been removed from <code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">scrapy.utils.datatypes</span></code>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4400\">issue 4400</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">FEED_FORMAT</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">FEED_URI</span></code> settings have been deprecated in\nfavor of the new <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEEDS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEEDS</span></code></a> setting (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1336\">issue 1336</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3858\">issue 3858</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4507\">issue 4507</a>)</p>", "<p>A new setting, <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEEDS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEEDS</span></code></a>, allows configuring multiple output feeds\nwith different settings each (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1336\">issue 1336</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3858\">issue 3858</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4507\">issue 4507</a>)</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-crawl\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">crawl</span></code></a> and <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-runspider\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">runspider</span></code></a> commands now support multiple\n<code class=\"docutils literal notranslate\"><span class=\"pre\">-o</span></code> parameters (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1336\">issue 1336</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3858\">issue 3858</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4507\">issue 4507</a>)</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-crawl\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">crawl</span></code></a> and <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-runspider\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">runspider</span></code></a> commands now support\nspecifying an output format by appending <code class=\"docutils literal notranslate\"><span class=\"pre\">:&lt;format&gt;</span></code> to the output file\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1336\">issue 1336</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3858\">issue 3858</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4507\">issue 4507</a>)</p>", "<p>The new <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Response.ip_address\" title=\"scrapy.http.Response.ip_address\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Response.ip_address</span></code></a>\nattribute gives access to the IP address that originated a response\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3903\">issue 3903</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3940\">issue 3940</a>)</p>", "<p>A warning is now issued when a value in\n<code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">allowed_domains</span></code> includes a port\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/50\">issue 50</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3198\">issue 3198</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4413\">issue 4413</a>)</p>", "<p>Zsh completion now excludes used option aliases from the completion list\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4438\">issue 4438</a>)</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/jobs.html#request-serialization\"><span class=\"std std-ref\">Request serialization</span></a> no longer breaks for\ncallbacks that are spider attributes which are assigned a function with a\ndifferent name (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4500\">issue 4500</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> values in <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">allowed_domains</span></code> no longer\ncause a <a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#TypeError\" title=\"(in Python v3.11)\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">TypeError</span></code></a> exception (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4410\">issue 4410</a>)</p>", "<p>Zsh completion no longer allows options after arguments (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4438\">issue 4438</a>)</p>", "<p>zope.interface 5.0.0 and later versions are now supported\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4447\">issue 4447</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4448\">issue 4448</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">Spider.make_requests_from_url</span></code>, deprecated in Scrapy 1.4.0, now issues a\nwarning when used (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4412\">issue 4412</a>)</p>", "<p>Improved the documentation about signals that allow their handlers to\nreturn a <a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html\" title=\"(in Twisted)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Deferred</span></code></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4295\">issue 4295</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4390\">issue 4390</a>)</p>", "<p>Our PyPI entry now includes links for our documentation, our source code\nrepository and our issue tracker (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4456\">issue 4456</a>)</p>", "<p>Covered the <a class=\"reference external\" href=\"https://michael-shub.github.io/curl2scrapy/\">curl2scrapy</a>\nservice in the documentation (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4206\">issue 4206</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4455\">issue 4455</a>)</p>", "<p>Removed references to the Guppy library, which only works in Python 2\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4285\">issue 4285</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4343\">issue 4343</a>)</p>", "<p>Extended use of InterSphinx to link to Python 3 documentation\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4444\">issue 4444</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4445\">issue 4445</a>)</p>", "<p>Added support for Sphinx 3.0 and later (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4475\">issue 4475</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4480\">issue 4480</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4496\">issue 4496</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4503\">issue 4503</a>)</p>", "<p>Removed warnings about using old, removed settings (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4404\">issue 4404</a>)</p>", "<p>Removed a warning about importing\n<a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.internet.testing.StringTransport.html\" title=\"(in Twisted)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">StringTransport</span></code></a> from\n<code class=\"docutils literal notranslate\"><span class=\"pre\">twisted.test.proto_helpers</span></code> in Twisted 19.7.0 or newer (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4409\">issue 4409</a>)</p>", "<p>Removed outdated Debian package build files (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4384\">issue 4384</a>)</p>", "<p>Removed <a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#object\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">object</span></code></a> usage as a base class (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4430\">issue 4430</a>)</p>", "<p>Removed code that added support for old versions of Twisted that we no\nlonger support (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4472\">issue 4472</a>)</p>", "<p>Fixed code style issues (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4468\">issue 4468</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4469\">issue 4469</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4471\">issue 4471</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4481\">issue 4481</a>)</p>", "<p>Removed <a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.internet.defer.html#returnValue\" title=\"(in Twisted)\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">twisted.internet.defer.returnValue()</span></code></a> calls (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4443\">issue 4443</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4446\">issue 4446</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4489\">issue 4489</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Response.follow_all\" title=\"scrapy.http.Response.follow_all\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">Response.follow_all</span></code></a> now supports\nan empty URL iterable as input (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4408\">issue 4408</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4420\">issue 4420</a>)</p>", "<p>Removed top-level <a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html\" title=\"(in Twisted)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">reactor</span></code></a> imports to prevent\nerrors about the wrong Twisted reactor being installed when setting a\ndifferent Twisted reactor using <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-TWISTED_REACTOR\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">TWISTED_REACTOR</span></code></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4401\">issue 4401</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4406\">issue 4406</a>)</p>", "<p>Fixed tests (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4422\">issue 4422</a>)</p>", "<p>Highlights:</p>", "<p>Python 2 support has been removed</p>", "<p><a class=\"reference internal\" href=\"topics/coroutines.html\"><span class=\"doc\">Partial</span></a> <a class=\"reference external\" href=\"https://docs.python.org/3/reference/compound_stmts.html#async\" title=\"(in Python v3.11)\"><span class=\"xref std std-ref\">coroutine syntax</span></a> support\nand <a class=\"reference internal\" href=\"topics/asyncio.html\"><span class=\"doc\">experimental</span></a> <a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio.html#module-asyncio\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">asyncio</span></code></a> support</p>", "<p>New <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Response.follow_all\" title=\"scrapy.http.Response.follow_all\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">Response.follow_all</span></code></a> method</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#media-pipeline-ftp\"><span class=\"std std-ref\">FTP support</span></a> for media pipelines</p>", "<p>New <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Response.certificate\" title=\"scrapy.http.Response.certificate\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Response.certificate</span></code></a>\nattribute</p>", "<p>IPv6 support through <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DNS_RESOLVER\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DNS_RESOLVER</span></code></a></p>", "<p>Python 2 support has been removed, following <a class=\"reference external\" href=\"https://www.python.org/doc/sunset-python-2/\">Python 2 end-of-life on\nJanuary 1, 2020</a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4091\">issue 4091</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4114\">issue 4114</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4115\">issue 4115</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4121\">issue 4121</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4138\">issue 4138</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4231\">issue 4231</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4242\">issue 4242</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4304\">issue 4304</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4309\">issue 4309</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4373\">issue 4373</a>)</p>", "<p>Retry gaveups (see <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-setting-RETRY_TIMES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">RETRY_TIMES</span></code></a>) are now logged as errors instead\nof as debug information (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3171\">issue 3171</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3566\">issue 3566</a>)</p>", "<p>File extensions that\n<a class=\"reference internal\" href=\"topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\" title=\"scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">LinkExtractor</span></code></a>\nignores by default now also include <code class=\"docutils literal notranslate\"><span class=\"pre\">7z</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">7zip</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">apk</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">bz2</span></code>,\n<code class=\"docutils literal notranslate\"><span class=\"pre\">cdr</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">dmg</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">ico</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">iso</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">tar</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">tar.gz</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">webm</span></code>, and\n<code class=\"docutils literal notranslate\"><span class=\"pre\">xz</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1837\">issue 1837</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2067\">issue 2067</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4066\">issue 4066</a>)</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-setting-METAREFRESH_IGNORE_TAGS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">METAREFRESH_IGNORE_TAGS</span></code></a> setting is now an empty list by\ndefault, following web browser behavior (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3844\">issue 3844</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4311\">issue 4311</a>)</p>", "<p>The\n<a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware\" title=\"scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">HttpCompressionMiddleware</span></code></a>\nnow includes spaces after commas in the value of the <code class=\"docutils literal notranslate\"><span class=\"pre\">Accept-Encoding</span></code>\nheader that it sets, following web browser behavior (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4293\">issue 4293</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method of custom download handlers (see\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DOWNLOAD_HANDLERS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOAD_HANDLERS</span></code></a>) or subclasses of the following downloader\nhandlers  no longer receives a <code class=\"docutils literal notranslate\"><span class=\"pre\">settings</span></code> parameter:</p>", "<p>Use the <code class=\"docutils literal notranslate\"><span class=\"pre\">from_settings</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">from_crawler</span></code> class methods to expose such\na parameter to your custom download handlers.</p>", "<p>(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4126\">issue 4126</a>)</p>", "<p>We have refactored the <a class=\"reference internal\" href=\"topics/scheduler.html#scrapy.core.scheduler.Scheduler\" title=\"scrapy.core.scheduler.Scheduler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.core.scheduler.Scheduler</span></code></a> class and\nrelated queue classes (see <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-SCHEDULER_PRIORITY_QUEUE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SCHEDULER_PRIORITY_QUEUE</span></code></a>,\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-SCHEDULER_DISK_QUEUE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SCHEDULER_DISK_QUEUE</span></code></a> and <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-SCHEDULER_MEMORY_QUEUE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SCHEDULER_MEMORY_QUEUE</span></code></a>) to\nmake it easier to implement custom scheduler queue classes. See\n<a class=\"hoverxref tooltip reference internal\" href=\"#scheduler-queue-changes\"><span class=\"std std-ref\">Changes to scheduler queue classes</span></a> below for details.</p>", "<p>Overridden settings are now logged in a different format. This is more in\nline with similar information logged at startup (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4199\">issue 4199</a>)</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"topics/shell.html#topics-shell\"><span class=\"std std-ref\">Scrapy shell</span></a> no longer provides a <cite>sel</cite> proxy\nobject, use <code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">response.selector</span></code>\ninstead (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4347\">issue 4347</a>)</p>", "<p>LevelDB support has been removed (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4112\">issue 4112</a>)</p>", "<p>The following functions have been removed from <code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">scrapy.utils.python</span></code>:\n<code class=\"docutils literal notranslate\"><span class=\"pre\">isbinarytext</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">is_writable</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">setattr_default</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">stringify_dict</span></code>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4362\">issue 4362</a>)</p>", "<p>Using environment variables prefixed with <code class=\"docutils literal notranslate\"><span class=\"pre\">SCRAPY_</span></code> to override settings\nis deprecated (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4300\">issue 4300</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4374\">issue 4374</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4375\">issue 4375</a>)</p>", "<p><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.linkextractors.FilteringLinkExtractor</span></code> is deprecated, use\n<a class=\"reference internal\" href=\"topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\" title=\"scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.linkextractors.LinkExtractor</span></code></a> instead (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4045\">issue 4045</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">noconnect</span></code> query string argument of proxy URLs is deprecated and\nshould be removed from proxy URLs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4198\">issue 4198</a>)</p>", "<p>The <code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">next</span></code> method of\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.utils.python.MutableChain</span></code> is deprecated, use the global\n<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#next\" title=\"(in Python v3.11)\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">next()</span></code></a> function or <code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">MutableChain.__next__</span></code> instead (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4153\">issue 4153</a>)</p>", "<p>Added <a class=\"reference internal\" href=\"topics/coroutines.html\"><span class=\"doc\">partial support</span></a> for Python’s\n<a class=\"reference external\" href=\"https://docs.python.org/3/reference/compound_stmts.html#async\" title=\"(in Python v3.11)\"><span class=\"xref std std-ref\">coroutine syntax</span></a> and <a class=\"reference internal\" href=\"topics/asyncio.html\"><span class=\"doc\">experimental support</span></a> for <a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio.html#module-asyncio\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">asyncio</span></code></a> and <a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio.html#module-asyncio\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">asyncio</span></code></a>-powered libraries\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4010\">issue 4010</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4259\">issue 4259</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4269\">issue 4269</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4270\">issue 4270</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4271\">issue 4271</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4316\">issue 4316</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4318\">issue 4318</a>)</p>", "<p>The new <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Response.follow_all\" title=\"scrapy.http.Response.follow_all\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">Response.follow_all</span></code></a>\nmethod offers the same functionality as\n<a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Response.follow\" title=\"scrapy.http.Response.follow\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">Response.follow</span></code></a> but supports an\niterable of URLs as input and returns an iterable of requests\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2582\">issue 2582</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4057\">issue 4057</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4286\">issue 4286</a>)</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#topics-media-pipeline\"><span class=\"std std-ref\">Media pipelines</span></a> now support <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#media-pipeline-ftp\"><span class=\"std std-ref\">FTP\nstorage</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3928\">issue 3928</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3961\">issue 3961</a>)</p>", "<p>The new <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Response.certificate\" title=\"scrapy.http.Response.certificate\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Response.certificate</span></code></a>\nattribute exposes the SSL certificate of the server as a\n<a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.internet.ssl.Certificate.html\" title=\"(in Twisted)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">twisted.internet.ssl.Certificate</span></code></a> object for HTTPS responses\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2726\">issue 2726</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4054\">issue 4054</a>)</p>", "<p>A new <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DNS_RESOLVER\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DNS_RESOLVER</span></code></a> setting allows enabling IPv6 support\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1031\">issue 1031</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4227\">issue 4227</a>)</p>", "<p>A new <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-SCRAPER_SLOT_MAX_ACTIVE_SIZE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SCRAPER_SLOT_MAX_ACTIVE_SIZE</span></code></a> setting allows configuring\nthe existing soft limit that pauses request downloads when the total\nresponse data being processed is too high (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1410\">issue 1410</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3551\">issue 3551</a>)</p>", "<p>A new <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-TWISTED_REACTOR\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">TWISTED_REACTOR</span></code></a> setting allows customizing the\n<a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html\" title=\"(in Twisted)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">reactor</span></code></a> that Scrapy uses, allowing to\n<a class=\"reference internal\" href=\"topics/asyncio.html\"><span class=\"doc\">enable asyncio support</span></a> or deal with a\n<a class=\"hoverxref tooltip reference internal\" href=\"faq.html#faq-specific-reactor\"><span class=\"std std-ref\">common macOS issue</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2905\">issue 2905</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4294\">issue 4294</a>)</p>", "<p>Scheduler disk and memory queues may now use the class methods\n<code class=\"docutils literal notranslate\"><span class=\"pre\">from_crawler</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">from_settings</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3884\">issue 3884</a>)</p>", "<p>The new <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Response.cb_kwargs\" title=\"scrapy.http.Response.cb_kwargs\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Response.cb_kwargs</span></code></a>\nattribute serves as a shortcut for <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request.cb_kwargs\" title=\"scrapy.http.Request.cb_kwargs\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Response.request.cb_kwargs</span></code></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4331\">issue 4331</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Response.follow\" title=\"scrapy.http.Response.follow\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">Response.follow</span></code></a> now supports a\n<code class=\"docutils literal notranslate\"><span class=\"pre\">flags</span></code> parameter, for consistency with <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4277\">issue 4277</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4279\">issue 4279</a>)</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/loaders.html#topics-loaders-processors\"><span class=\"std std-ref\">Item loader processors</span></a> can now be\nregular functions, they no longer need to be methods (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3899\">issue 3899</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/spiders.html#scrapy.spiders.Rule\" title=\"scrapy.spiders.Rule\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Rule</span></code></a> now accepts an <code class=\"docutils literal notranslate\"><span class=\"pre\">errback</span></code> parameter\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4000\">issue 4000</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> no longer requires a <code class=\"docutils literal notranslate\"><span class=\"pre\">callback</span></code> parameter\nwhen an <code class=\"docutils literal notranslate\"><span class=\"pre\">errback</span></code> parameter is specified (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3586\">issue 3586</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4008\">issue 4008</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/logging.html#scrapy.logformatter.LogFormatter\" title=\"scrapy.logformatter.LogFormatter\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">LogFormatter</span></code></a> now supports some additional\nmethods:</p>", "<p><a class=\"reference internal\" href=\"topics/logging.html#scrapy.logformatter.LogFormatter.download_error\" title=\"scrapy.logformatter.LogFormatter.download_error\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">download_error</span></code></a> for\ndownload errors</p>", "<p><a class=\"reference internal\" href=\"topics/logging.html#scrapy.logformatter.LogFormatter.item_error\" title=\"scrapy.logformatter.LogFormatter.item_error\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">item_error</span></code></a> for exceptions\nraised during item processing by <a class=\"hoverxref tooltip reference internal\" href=\"topics/item-pipeline.html#topics-item-pipeline\"><span class=\"std std-ref\">item pipelines</span></a></p>", "<p><a class=\"reference internal\" href=\"topics/logging.html#scrapy.logformatter.LogFormatter.spider_error\" title=\"scrapy.logformatter.LogFormatter.spider_error\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">spider_error</span></code></a> for exceptions\nraised from <a class=\"hoverxref tooltip reference internal\" href=\"topics/spiders.html#topics-spiders\"><span class=\"std std-ref\">spider callbacks</span></a></p>", "<p>(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/374\">issue 374</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3986\">issue 3986</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3989\">issue 3989</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4176\">issue 4176</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4188\">issue 4188</a>)</p>", "<p>The <code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEED_URI</span></code> setting now supports <a class=\"reference external\" href=\"https://docs.python.org/3/library/pathlib.html#pathlib.Path\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">pathlib.Path</span></code></a> values\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3731\">issue 3731</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4074\">issue 4074</a>)</p>", "<p>A new <a class=\"hoverxref tooltip reference internal\" href=\"topics/signals.html#std-signal-request_left_downloader\"><code class=\"xref std std-signal docutils literal notranslate\"><span class=\"pre\">request_left_downloader</span></code></a> signal is sent when a request\nleaves the downloader (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4303\">issue 4303</a>)</p>", "<p>Scrapy logs a warning when it detects a request callback or errback that\nuses <code class=\"docutils literal notranslate\"><span class=\"pre\">yield</span></code> but also returns a value, since the returned value would be\nlost (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3484\">issue 3484</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3869\">issue 3869</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/spiders.html#scrapy.spiders.Spider\" title=\"scrapy.spiders.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> objects now raise an <a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#AttributeError\" title=\"(in Python v3.11)\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">AttributeError</span></code></a>\nexception if they do not have a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">start_urls</span></code>\nattribute nor reimplement <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">start_requests</span></code>,\nbut have a <code class=\"docutils literal notranslate\"><span class=\"pre\">start_url</span></code> attribute (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4133\">issue 4133</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4170\">issue 4170</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/exporters.html#scrapy.exporters.BaseItemExporter\" title=\"scrapy.exporters.BaseItemExporter\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">BaseItemExporter</span></code></a> subclasses may now use\n<code class=\"docutils literal notranslate\"><span class=\"pre\">super().__init__(**kwargs)</span></code> instead of <code class=\"docutils literal notranslate\"><span class=\"pre\">self._configure(kwargs)</span></code> in\ntheir <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method, passing <code class=\"docutils literal notranslate\"><span class=\"pre\">dont_fail=True</span></code> to the parent\n<code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method if needed, and accessing <code class=\"docutils literal notranslate\"><span class=\"pre\">kwargs</span></code> at <code class=\"docutils literal notranslate\"><span class=\"pre\">self._kwargs</span></code>\nafter calling their parent <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4193\">issue 4193</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4370\">issue 4370</a>)</p>", "<p>A new <code class=\"docutils literal notranslate\"><span class=\"pre\">keep_fragments</span></code> parameter of\n<code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.request.request_fingerprint</span></code> allows to generate\ndifferent fingerprints for requests with different fragments in their URL\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4104\">issue 4104</a>)</p>", "<p>Download handlers (see <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DOWNLOAD_HANDLERS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOAD_HANDLERS</span></code></a>) may now use the\n<code class=\"docutils literal notranslate\"><span class=\"pre\">from_settings</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">from_crawler</span></code> class methods that other Scrapy\ncomponents already supported (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4126\">issue 4126</a>)</p>", "<p><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.utils.python.MutableChain.__iter__</span></code> now returns <code class=\"docutils literal notranslate\"><span class=\"pre\">self</span></code>,\n<a class=\"reference external\" href=\"https://lgtm.com/rules/4850080/\">allowing it to be used as a sequence</a>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4153\">issue 4153</a>)</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-crawl\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">crawl</span></code></a> command now also exits with exit code 1 when an\nexception happens before the crawling starts (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4175\">issue 4175</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4207\">issue 4207</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links\" title=\"scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">LinkExtractor.extract_links</span></code></a> no longer\nre-encodes the query string or URLs from non-UTF-8 responses in UTF-8\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/998\">issue 998</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1403\">issue 1403</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1949\">issue 1949</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4321\">issue 4321</a>)</p>", "<p>The first spider middleware (see <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-SPIDER_MIDDLEWARES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SPIDER_MIDDLEWARES</span></code></a>) now also\nprocesses exceptions raised from callbacks that are generators\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4260\">issue 4260</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4272\">issue 4272</a>)</p>", "<p>Redirects to URLs starting with 3 slashes (<code class=\"docutils literal notranslate\"><span class=\"pre\">///</span></code>) are now supported\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4032\">issue 4032</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4042\">issue 4042</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> no longer accepts strings as <code class=\"docutils literal notranslate\"><span class=\"pre\">url</span></code> simply\nbecause they have a colon (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2552\">issue 2552</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4094\">issue 4094</a>)</p>", "<p>The correct encoding is now used for attach names in\n<a class=\"reference internal\" href=\"topics/email.html#scrapy.mail.MailSender\" title=\"scrapy.mail.MailSender\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MailSender</span></code></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4229\">issue 4229</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4239\">issue 4239</a>)</p>", "<p><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RFPDupeFilter</span></code>, the default\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DUPEFILTER_CLASS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DUPEFILTER_CLASS</span></code></a>, no longer writes an extra <code class=\"docutils literal notranslate\"><span class=\"pre\">\\r</span></code> character on\neach line in Windows, which made the size of the <code class=\"docutils literal notranslate\"><span class=\"pre\">requests.seen</span></code> file\nunnecessarily large on that platform (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4283\">issue 4283</a>)</p>", "<p>Z shell auto-completion now looks for <code class=\"docutils literal notranslate\"><span class=\"pre\">.html</span></code> files, not <code class=\"docutils literal notranslate\"><span class=\"pre\">.http</span></code> files,\nand covers the <code class=\"docutils literal notranslate\"><span class=\"pre\">-h</span></code> command-line switch (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4122\">issue 4122</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4291\">issue 4291</a>)</p>", "<p>Adding items to a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.utils.datatypes.LocalCache</span></code> object\nwithout a <code class=\"docutils literal notranslate\"><span class=\"pre\">limit</span></code> defined no longer raises a <a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#TypeError\" title=\"(in Python v3.11)\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">TypeError</span></code></a> exception\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4123\">issue 4123</a>)</p>", "<p>Fixed a typo in the message of the <a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#ValueError\" title=\"(in Python v3.11)\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">ValueError</span></code></a> exception raised when\n<code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.utils.misc.create_instance()</span></code> gets both <code class=\"docutils literal notranslate\"><span class=\"pre\">settings</span></code> and\n<code class=\"docutils literal notranslate\"><span class=\"pre\">crawler</span></code> set to <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4128\">issue 4128</a>)</p>", "<p>API documentation now links to an online, syntax-highlighted view of the\ncorresponding source code (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4148\">issue 4148</a>)</p>", "<p>Links to unexisting documentation pages now allow access to the sidebar\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4152\">issue 4152</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4169\">issue 4169</a>)</p>", "<p>Cross-references within our documentation now display a tooltip when\nhovered (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4173\">issue 4173</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4183\">issue 4183</a>)</p>", "<p>Improved the documentation about <a class=\"reference internal\" href=\"topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links\" title=\"scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">LinkExtractor.extract_links</span></code></a> and\nsimplified <a class=\"hoverxref tooltip reference internal\" href=\"topics/link-extractors.html#topics-link-extractors\"><span class=\"std std-ref\">Link Extractors</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4045\">issue 4045</a>)</p>", "<p>Clarified how <a class=\"reference internal\" href=\"topics/loaders.html#scrapy.loader.ItemLoader.item\" title=\"scrapy.loader.ItemLoader.item\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ItemLoader.item</span></code></a>\nworks (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3574\">issue 3574</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4099\">issue 4099</a>)</p>", "<p>Clarified that <a class=\"reference external\" href=\"https://docs.python.org/3/library/logging.html#logging.basicConfig\" title=\"(in Python v3.11)\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">logging.basicConfig()</span></code></a> should not be used when also\nusing <a class=\"reference internal\" href=\"topics/api.html#scrapy.crawler.CrawlerProcess\" title=\"scrapy.crawler.CrawlerProcess\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CrawlerProcess</span></code></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2149\">issue 2149</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2352\">issue 2352</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3146\">issue 3146</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3960\">issue 3960</a>)</p>", "<p>Clarified the requirements for <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> objects\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/jobs.html#request-serialization\"><span class=\"std std-ref\">when using persistence</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4124\">issue 4124</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4139\">issue 4139</a>)</p>", "<p>Clarified how to install a <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#media-pipeline-example\"><span class=\"std std-ref\">custom image pipeline</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4034\">issue 4034</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4252\">issue 4252</a>)</p>", "<p>Fixed the signatures of the <code class=\"docutils literal notranslate\"><span class=\"pre\">file_path</span></code> method in <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#topics-media-pipeline\"><span class=\"std std-ref\">media pipeline</span></a> examples (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4290\">issue 4290</a>)</p>", "<p>Covered a backward-incompatible change in Scrapy 1.7.0 affecting custom\n<a class=\"reference internal\" href=\"topics/scheduler.html#scrapy.core.scheduler.Scheduler\" title=\"scrapy.core.scheduler.Scheduler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.core.scheduler.Scheduler</span></code></a> subclasses (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4274\">issue 4274</a>)</p>", "<p>Improved the <code class=\"docutils literal notranslate\"><span class=\"pre\">README.rst</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">CODE_OF_CONDUCT.md</span></code> files\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4059\">issue 4059</a>)</p>", "<p>Documentation examples are now checked as part of our test suite and we\nhave fixed some of the issues detected (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4142\">issue 4142</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4146\">issue 4146</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4171\">issue 4171</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4184\">issue 4184</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4190\">issue 4190</a>)</p>", "<p>Fixed logic issues, broken links and typos (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4247\">issue 4247</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4258\">issue 4258</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4282\">issue 4282</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4288\">issue 4288</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4305\">issue 4305</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4308\">issue 4308</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4323\">issue 4323</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4338\">issue 4338</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4359\">issue 4359</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4361\">issue 4361</a>)</p>", "<p>Improved consistency when referring to the <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method of an object\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4086\">issue 4086</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4088\">issue 4088</a>)</p>", "<p>Fixed an inconsistency between code and output in <a class=\"hoverxref tooltip reference internal\" href=\"intro/overview.html#intro-overview\"><span class=\"std std-ref\">Scrapy at a glance</span></a>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4213\">issue 4213</a>)</p>", "<p>Extended <a class=\"reference external\" href=\"https://www.sphinx-doc.org/en/master/usage/extensions/intersphinx.html#module-sphinx.ext.intersphinx\" title=\"(in Sphinx v7.3.0)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">intersphinx</span></code></a> usage (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4147\">issue 4147</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4172\">issue 4172</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4185\">issue 4185</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4194\">issue 4194</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4197\">issue 4197</a>)</p>", "<p>We now use a recent version of Python to build the documentation\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4140\">issue 4140</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4249\">issue 4249</a>)</p>", "<p>Cleaned up documentation (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4143\">issue 4143</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4275\">issue 4275</a>)</p>", "<p>Re-enabled proxy <code class=\"docutils literal notranslate\"><span class=\"pre\">CONNECT</span></code> tests (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2545\">issue 2545</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4114\">issue 4114</a>)</p>", "<p>Added <a class=\"reference external\" href=\"https://bandit.readthedocs.io/\">Bandit</a> security checks to our test suite (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4162\">issue 4162</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4181\">issue 4181</a>)</p>", "<p>Added <a class=\"reference external\" href=\"https://flake8.pycqa.org/en/latest/\">Flake8</a> style checks to our test suite and applied many of the\ncorresponding changes (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3944\">issue 3944</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3945\">issue 3945</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4137\">issue 4137</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4157\">issue 4157</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4167\">issue 4167</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4174\">issue 4174</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4186\">issue 4186</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4195\">issue 4195</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4238\">issue 4238</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4246\">issue 4246</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4355\">issue 4355</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4360\">issue 4360</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4365\">issue 4365</a>)</p>", "<p>Improved test coverage (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4097\">issue 4097</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4218\">issue 4218</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4236\">issue 4236</a>)</p>", "<p>Started reporting slowest tests, and improved the performance of some of\nthem (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4163\">issue 4163</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4164\">issue 4164</a>)</p>", "<p>Fixed broken tests and refactored some tests (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4014\">issue 4014</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4095\">issue 4095</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4244\">issue 4244</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4268\">issue 4268</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4372\">issue 4372</a>)</p>", "<p>Modified the <a class=\"reference external\" href=\"https://tox.wiki/en/latest/index.html\" title=\"(in Python v4.11)\"><span class=\"xref std std-doc\">tox</span></a> configuration to allow running tests\nwith any Python version, run <a class=\"reference external\" href=\"https://bandit.readthedocs.io/\">Bandit</a> and <a class=\"reference external\" href=\"https://flake8.pycqa.org/en/latest/\">Flake8</a> tests by default, and\nenforce a minimum tox version programmatically (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4179\">issue 4179</a>)</p>", "<p>Cleaned up code (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3937\">issue 3937</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4208\">issue 4208</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4209\">issue 4209</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4210\">issue 4210</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4212\">issue 4212</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4369\">issue 4369</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4376\">issue 4376</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4378\">issue 4378</a>)</p>", "<p>The following changes may impact any custom queue classes of all types:</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">push</span></code> method no longer receives a second positional parameter\ncontaining <code class=\"docutils literal notranslate\"><span class=\"pre\">request.priority</span> <span class=\"pre\">*</span> <span class=\"pre\">-1</span></code>. If you need that value, get it\nfrom the first positional parameter, <code class=\"docutils literal notranslate\"><span class=\"pre\">request</span></code>, instead, or use\nthe new <code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">priority()</span></code>\nmethod in <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.core.scheduler.ScrapyPriorityQueue</span></code>\nsubclasses.</p>", "<p>The following changes may impact custom priority queue classes:</p>", "<p>In the <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method or the <code class=\"docutils literal notranslate\"><span class=\"pre\">from_crawler</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">from_settings</span></code>\nclass methods:</p>", "<p>The parameter that used to contain a factory function,\n<code class=\"docutils literal notranslate\"><span class=\"pre\">qfactory</span></code>, is now passed as a keyword parameter named\n<code class=\"docutils literal notranslate\"><span class=\"pre\">downstream_queue_cls</span></code>.</p>", "<p>A new keyword parameter has been added: <code class=\"docutils literal notranslate\"><span class=\"pre\">key</span></code>. It is a string\nthat is always an empty string for memory queues and indicates the\n<code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">JOB_DIR</span></code> value for disk queues.</p>", "<p>The parameter for disk queues that contains data from the previous\ncrawl, <code class=\"docutils literal notranslate\"><span class=\"pre\">startprios</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">slot_startprios</span></code>, is now passed as a\nkeyword parameter named <code class=\"docutils literal notranslate\"><span class=\"pre\">startprios</span></code>.</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">serialize</span></code> parameter is no longer passed. The disk queue\nclass must take care of request serialization on its own before\nwriting to disk, using the\n<code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">request_to_dict()</span></code> and\n<code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">request_from_dict()</span></code> functions from the\n<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">scrapy.utils.reqser</span></code> module.</p>", "<p>The following changes may impact custom disk and memory queue classes:</p>", "<p>The signature of the <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method is now\n<code class=\"docutils literal notranslate\"><span class=\"pre\">__init__(self,</span> <span class=\"pre\">crawler,</span> <span class=\"pre\">key)</span></code>.</p>", "<p>The following changes affect specifically the\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ScrapyPriorityQueue</span></code> and\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">DownloaderAwarePriorityQueue</span></code> classes from\n<a class=\"reference internal\" href=\"topics/scheduler.html#module-scrapy.core.scheduler\" title=\"scrapy.core.scheduler\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">scrapy.core.scheduler</span></code></a> and may affect subclasses:</p>", "<p>In the <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method, most of the changes described above apply.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> may still receive all parameters as positional parameters,\nhowever:</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">downstream_queue_cls</span></code>, which replaced <code class=\"docutils literal notranslate\"><span class=\"pre\">qfactory</span></code>, must be\ninstantiated differently.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">qfactory</span></code> was instantiated with a priority value (integer).</p>", "<p>Instances of <code class=\"docutils literal notranslate\"><span class=\"pre\">downstream_queue_cls</span></code> should be created using\nthe new\n<code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">ScrapyPriorityQueue.qfactory</span></code>\nor\n<code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">DownloaderAwarePriorityQueue.pqfactory</span></code>\nmethods.</p>", "<p>The new <code class=\"docutils literal notranslate\"><span class=\"pre\">key</span></code> parameter displaced the <code class=\"docutils literal notranslate\"><span class=\"pre\">startprios</span></code>\nparameter 1 position to the right.</p>", "<p>The following class attributes have been added:</p>", "<p><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">downstream_queue_cls</span></code>\n(details above)</p>", "<p><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">key</span></code> (details above)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">serialize</span></code> attribute has been removed (details above)</p>", "<p>The following changes affect specifically the\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ScrapyPriorityQueue</span></code> class and may affect\nsubclasses:</p>", "<p>A new <code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">priority()</span></code>\nmethod has been added which, given a request, returns\n<code class=\"docutils literal notranslate\"><span class=\"pre\">request.priority</span> <span class=\"pre\">*</span> <span class=\"pre\">-1</span></code>.</p>", "<p>It is used in <code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">push()</span></code>\nto make up for the removal of its <code class=\"docutils literal notranslate\"><span class=\"pre\">priority</span></code> parameter.</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">spider</span></code> attribute has been removed. Use\n<code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">crawler.spider</span></code>\ninstead.</p>", "<p>The following changes affect specifically the\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">DownloaderAwarePriorityQueue</span></code> class and may\naffect subclasses:</p>", "<p>A new <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">pqueues</span></code>\nattribute offers a mapping of downloader slot names to the\ncorresponding instances of\n<code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">downstream_queue_cls</span></code>.</p>", "<p>(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3884\">issue 3884</a>)</p>", "<p>When <a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\" title=\"scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">HttpProxyMiddleware</span></code></a>\nprocesses a request with <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-proxy\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">proxy</span></code></a> metadata, and that\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-proxy\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">proxy</span></code></a> metadata includes proxy credentials,\n<a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\" title=\"scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">HttpProxyMiddleware</span></code></a> sets\nthe <code class=\"docutils literal notranslate\"><span class=\"pre\">Proxy-Authorization</span></code> header, but only if that header is not already\nset.</p>", "<p>There are third-party proxy-rotation downloader middlewares that set\ndifferent <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-proxy\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">proxy</span></code></a> metadata every time they process a request.</p>", "<p>Because of request retries and redirects, the same request can be processed\nby downloader middlewares more than once, including both\n<a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\" title=\"scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">HttpProxyMiddleware</span></code></a> and\nany third-party proxy-rotation downloader middleware.</p>", "<p>These third-party proxy-rotation downloader middlewares could change the\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-proxy\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">proxy</span></code></a> metadata of a request to a new value, but fail to remove\nthe <code class=\"docutils literal notranslate\"><span class=\"pre\">Proxy-Authorization</span></code> header from the previous value of the\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-proxy\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">proxy</span></code></a> metadata, causing the credentials of one proxy to be sent\nto a different proxy.</p>", "<p>To prevent the unintended leaking of proxy credentials, the behavior of\n<a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\" title=\"scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">HttpProxyMiddleware</span></code></a> is now\nas follows when processing a request:</p>", "<p>If the request being processed defines <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-proxy\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">proxy</span></code></a> metadata that\nincludes credentials, the <code class=\"docutils literal notranslate\"><span class=\"pre\">Proxy-Authorization</span></code> header is always\nupdated to feature those credentials.</p>", "<p>If the request being processed defines <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-proxy\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">proxy</span></code></a> metadata\nwithout credentials, the <code class=\"docutils literal notranslate\"><span class=\"pre\">Proxy-Authorization</span></code> header is removed\n<em>unless</em> it was originally defined for the same proxy URL.</p>", "<p>To remove proxy credentials while keeping the same proxy URL, remove\nthe <code class=\"docutils literal notranslate\"><span class=\"pre\">Proxy-Authorization</span></code> header.</p>", "<p>If the request has no <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-proxy\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">proxy</span></code></a> metadata, or that metadata is a\nfalsy value (e.g. <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>), the <code class=\"docutils literal notranslate\"><span class=\"pre\">Proxy-Authorization</span></code> header is\nremoved.</p>", "<p>It is no longer possible to set a proxy URL through the\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-proxy\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">proxy</span></code></a> metadata but set the credentials through the\n<code class=\"docutils literal notranslate\"><span class=\"pre\">Proxy-Authorization</span></code> header. Set proxy credentials through the\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-proxy\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">proxy</span></code></a> metadata instead.</p>", "<p>When a <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> object with cookies defined gets a\nredirect response causing a new <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> object to be\nscheduled, the cookies defined in the original\n<a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> object are no longer copied into the new\n<a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> object.</p>", "<p>If you manually set the <code class=\"docutils literal notranslate\"><span class=\"pre\">Cookie</span></code> header on a\n<a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> object and the domain name of the redirect\nURL is not an exact match for the domain of the URL of the original\n<a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> object, your <code class=\"docutils literal notranslate\"><span class=\"pre\">Cookie</span></code> header is now dropped\nfrom the new <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> object.</p>", "<p>The old behavior could be exploited by an attacker to gain access to your\ncookies. Please, see the <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8\">cjvr-mfj7-j4j8 security advisory</a> for more\ninformation.</p>", "<p class=\"admonition-title\">Note</p>", "<p>It is still possible to enable the sharing of cookies between\ndifferent domains with a shared domain suffix (e.g.\n<code class=\"docutils literal notranslate\"><span class=\"pre\">example.com</span></code> and any subdomain) by defining the shared domain\nsuffix (e.g. <code class=\"docutils literal notranslate\"><span class=\"pre\">example.com</span></code>) as the cookie domain when defining\nyour cookies. See the documentation of the\n<a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> class for more information.</p>", "<p>When the domain of a cookie, either received in the <code class=\"docutils literal notranslate\"><span class=\"pre\">Set-Cookie</span></code> header\nof a response or defined in a <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> object, is set\nto a <a class=\"reference external\" href=\"https://publicsuffix.org/\">public suffix</a>, the cookie is now\nignored unless the cookie domain is the same as the request domain.</p>", "<p>The old behavior could be exploited by an attacker to inject cookies into\nyour requests to some other domains. Please, see the <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/security/advisories/GHSA-mfjm-vh54-3f96\">mfjm-vh54-3f96\nsecurity advisory</a> for more information.</p>", "<p>If you use\n<a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware\" title=\"scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">HttpAuthMiddleware</span></code></a>\n(i.e. the <code class=\"docutils literal notranslate\"><span class=\"pre\">http_user</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">http_pass</span></code> spider attributes) for HTTP\nauthentication, any request exposes your credentials to the request target.</p>", "<p>To prevent unintended exposure of authentication credentials to unintended\ndomains, you must now additionally set a new, additional spider attribute,\n<code class=\"docutils literal notranslate\"><span class=\"pre\">http_auth_domain</span></code>, and point it to the specific domain to which the\nauthentication credentials must be sent.</p>", "<p>If the <code class=\"docutils literal notranslate\"><span class=\"pre\">http_auth_domain</span></code> spider attribute is not set, the domain of the\nfirst request will be considered the HTTP authentication target, and\nauthentication credentials will only be sent in requests targeting that\ndomain.</p>", "<p>If you need to send the same HTTP authentication credentials to multiple\ndomains, you can use <a class=\"reference external\" href=\"https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header\" title=\"(in w3lib v2.1)\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">w3lib.http.basic_auth_header()</span></code></a> instead to\nset the value of the <code class=\"docutils literal notranslate\"><span class=\"pre\">Authorization</span></code> header of your requests.</p>", "<p>If you <em>really</em> want your spider to send the same HTTP authentication\ncredentials to any domain, set the <code class=\"docutils literal notranslate\"><span class=\"pre\">http_auth_domain</span></code> spider attribute\nto <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>.</p>", "<p>Finally, if you are a user of <a class=\"reference external\" href=\"https://github.com/scrapy-plugins/scrapy-splash\">scrapy-splash</a>, know that this version of\nScrapy breaks compatibility with scrapy-splash 0.7.2 and earlier. You will\nneed to upgrade scrapy-splash to a greater version for it to continue to\nwork.</p>", "<p>Highlights:</p>", "<p>Dropped Python 3.4 support and updated minimum requirements; made Python 3.8\nsupport official</p>", "<p>New <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request.from_curl\" title=\"scrapy.http.Request.from_curl\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">Request.from_curl</span></code></a> class method</p>", "<p>New <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-ROBOTSTXT_PARSER\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">ROBOTSTXT_PARSER</span></code></a> and <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-ROBOTSTXT_USER_AGENT\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">ROBOTSTXT_USER_AGENT</span></code></a> settings</p>", "<p>New <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOADER_CLIENT_TLS_CIPHERS</span></code></a> and\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING</span></code></a> settings</p>", "<p>Python 3.4 is no longer supported, and some of the minimum requirements of\nScrapy have also changed:</p>", "<p><a class=\"reference external\" href=\"https://cssselect.readthedocs.io/en/latest/index.html\" title=\"(in cssselect v1.2.0)\"><span class=\"xref std std-doc\">cssselect</span></a> 0.9.1</p>", "<p><a class=\"reference external\" href=\"https://cryptography.io/en/latest/\">cryptography</a> 2.0</p>", "<p><a class=\"reference external\" href=\"https://lxml.de/\">lxml</a> 3.5.0</p>", "<p><a class=\"reference external\" href=\"https://www.pyopenssl.org/en/stable/\">pyOpenSSL</a> 16.2.0</p>", "<p><a class=\"reference external\" href=\"https://github.com/scrapy/queuelib\">queuelib</a> 1.4.2</p>", "<p><a class=\"reference external\" href=\"https://service-identity.readthedocs.io/en/stable/\">service_identity</a> 16.0.0</p>", "<p><a class=\"reference external\" href=\"https://six.readthedocs.io/\">six</a> 1.10.0</p>", "<p><a class=\"reference external\" href=\"https://twistedmatrix.com/trac/\">Twisted</a> 17.9.0 (16.0.0 with Python 2)</p>", "<p><a class=\"reference external\" href=\"https://zopeinterface.readthedocs.io/en/latest/\">zope.interface</a> 4.1.3</p>", "<p>(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3892\">issue 3892</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">JSONRequest</span></code> is now called <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.JsonRequest\" title=\"scrapy.http.JsonRequest\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">JsonRequest</span></code></a> for\nconsistency with similar classes (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3929\">issue 3929</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3982\">issue 3982</a>)</p>", "<p>If you are using a custom context factory\n(<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DOWNLOADER_CLIENTCONTEXTFACTORY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOADER_CLIENTCONTEXTFACTORY</span></code></a>), its <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method must\naccept two new parameters: <code class=\"docutils literal notranslate\"><span class=\"pre\">tls_verbose_logging</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">tls_ciphers</span></code>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2111\">issue 2111</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3392\">issue 3392</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3442\">issue 3442</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3450\">issue 3450</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/loaders.html#scrapy.loader.ItemLoader\" title=\"scrapy.loader.ItemLoader\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ItemLoader</span></code></a> now turns the values of its input item\ninto lists:</p>", "<p>This is needed to allow adding values to existing fields\n(<code class=\"docutils literal notranslate\"><span class=\"pre\">loader.add_value('field',</span> <span class=\"pre\">'value2')</span></code>).</p>", "<p>(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3804\">issue 3804</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3819\">issue 3819</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3897\">issue 3897</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3976\">issue 3976</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3998\">issue 3998</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4036\">issue 4036</a>)</p>", "<p>See also <a class=\"hoverxref tooltip reference internal\" href=\"#id86\"><span class=\"std std-ref\">Deprecation removals</span></a> below.</p>", "<p>A new <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request.from_curl\" title=\"scrapy.http.Request.from_curl\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">Request.from_curl</span></code></a> class\nmethod allows <a class=\"hoverxref tooltip reference internal\" href=\"topics/developer-tools.html#requests-from-curl\"><span class=\"std std-ref\">creating a request from a cURL command</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2985\">issue 2985</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3862\">issue 3862</a>)</p>", "<p>A new <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-ROBOTSTXT_PARSER\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">ROBOTSTXT_PARSER</span></code></a> setting allows choosing which <a class=\"reference external\" href=\"https://www.robotstxt.org/\">robots.txt</a>\nparser to use. It includes built-in support for\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#python-robotfileparser\"><span class=\"std std-ref\">RobotFileParser</span></a>,\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#protego-parser\"><span class=\"std std-ref\">Protego</span></a> (default), <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#reppy-parser\"><span class=\"std std-ref\">Reppy</span></a>, and\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#rerp-parser\"><span class=\"std std-ref\">Robotexclusionrulesparser</span></a>, and allows you to\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#support-for-new-robots-parser\"><span class=\"std std-ref\">implement support for additional parsers</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/754\">issue 754</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2669\">issue 2669</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3796\">issue 3796</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3935\">issue 3935</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3969\">issue 3969</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4006\">issue 4006</a>)</p>", "<p>A new <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-ROBOTSTXT_USER_AGENT\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">ROBOTSTXT_USER_AGENT</span></code></a> setting allows defining a separate\nuser agent string to use for <a class=\"reference external\" href=\"https://www.robotstxt.org/\">robots.txt</a> parsing (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3931\">issue 3931</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3966\">issue 3966</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/spiders.html#scrapy.spiders.Rule\" title=\"scrapy.spiders.Rule\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Rule</span></code></a> no longer requires a <a class=\"reference internal\" href=\"topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\" title=\"scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">LinkExtractor</span></code></a> parameter\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/781\">issue 781</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4016\">issue 4016</a>)</p>", "<p>Use the new <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOADER_CLIENT_TLS_CIPHERS</span></code></a> setting to customize\nthe TLS/SSL ciphers used by the default HTTP/1.1 downloader (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3392\">issue 3392</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3442\">issue 3442</a>)</p>", "<p>Set the new <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING</span></code></a> setting to\n<code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code> to enable debug-level messages about TLS connection parameters\nafter establishing HTTPS connections (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2111\">issue 2111</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3450\">issue 3450</a>)</p>", "<p>Callbacks that receive keyword arguments\n(see <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request.cb_kwargs\" title=\"scrapy.http.Request.cb_kwargs\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Request.cb_kwargs</span></code></a>) can now be\ntested using the new <a class=\"reference internal\" href=\"topics/contracts.html#scrapy.contracts.default.CallbackKeywordArgumentsContract\" title=\"scrapy.contracts.default.CallbackKeywordArgumentsContract\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">@cb_kwargs</span></code></a>\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/contracts.html#topics-contracts\"><span class=\"std std-ref\">spider contract</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3985\">issue 3985</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3988\">issue 3988</a>)</p>", "<p>When a <a class=\"reference internal\" href=\"topics/contracts.html#scrapy.contracts.default.ScrapesContract\" title=\"scrapy.contracts.default.ScrapesContract\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">@scrapes</span></code></a> spider\ncontract fails, all missing fields are now reported (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/766\">issue 766</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3939\">issue 3939</a>)</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/logging.html#custom-log-formats\"><span class=\"std std-ref\">Custom log formats</span></a> can now drop messages by\nhaving the corresponding methods of the configured <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-LOG_FORMATTER\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">LOG_FORMATTER</span></code></a>\nreturn <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3984\">issue 3984</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3987\">issue 3987</a>)</p>", "<p>A much improved completion definition is now available for <a class=\"reference external\" href=\"https://www.zsh.org/\">Zsh</a>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4069\">issue 4069</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/loaders.html#scrapy.loader.ItemLoader.load_item\" title=\"scrapy.loader.ItemLoader.load_item\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">ItemLoader.load_item()</span></code></a> no\nlonger makes later calls to <a class=\"reference internal\" href=\"topics/loaders.html#scrapy.loader.ItemLoader.get_output_value\" title=\"scrapy.loader.ItemLoader.get_output_value\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">ItemLoader.get_output_value()</span></code></a> or\n<a class=\"reference internal\" href=\"topics/loaders.html#scrapy.loader.ItemLoader.load_item\" title=\"scrapy.loader.ItemLoader.load_item\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">ItemLoader.load_item()</span></code></a> return\nempty data (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3804\">issue 3804</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3819\">issue 3819</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3897\">issue 3897</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3976\">issue 3976</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3998\">issue 3998</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4036\">issue 4036</a>)</p>", "<p>Fixed <a class=\"reference internal\" href=\"topics/stats.html#scrapy.statscollectors.DummyStatsCollector\" title=\"scrapy.statscollectors.DummyStatsCollector\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">DummyStatsCollector</span></code></a> raising a\n<a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#TypeError\" title=\"(in Python v3.11)\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">TypeError</span></code></a> exception (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4007\">issue 4007</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4052\">issue 4052</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.file_path\" title=\"scrapy.pipelines.files.FilesPipeline.file_path\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">FilesPipeline.file_path</span></code></a> and\n<a class=\"reference internal\" href=\"topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline.file_path\" title=\"scrapy.pipelines.images.ImagesPipeline.file_path\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">ImagesPipeline.file_path</span></code></a> no longer choose\nfile extensions that are not <a class=\"reference external\" href=\"https://www.iana.org/assignments/media-types/media-types.xhtml\">registered with IANA</a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1287\">issue 1287</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3953\">issue 3953</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3954\">issue 3954</a>)</p>", "<p>When using <a class=\"reference external\" href=\"https://github.com/boto/botocore\">botocore</a> to persist files in S3, all botocore-supported headers\nare properly mapped now (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3904\">issue 3904</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3905\">issue 3905</a>)</p>", "<p>FTP passwords in <code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEED_URI</span></code> containing percent-escaped characters\nare now properly decoded (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3941\">issue 3941</a>)</p>", "<p>A memory-handling and error-handling issue in\n<code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.utils.ssl.get_temp_key_info()</span></code> has been fixed (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3920\">issue 3920</a>)</p>", "<p>The documentation now covers how to define and configure a <a class=\"hoverxref tooltip reference internal\" href=\"topics/logging.html#custom-log-formats\"><span class=\"std std-ref\">custom log\nformat</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3616\">issue 3616</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3660\">issue 3660</a>)</p>", "<p>API documentation added for <a class=\"reference internal\" href=\"topics/exporters.html#scrapy.exporters.MarshalItemExporter\" title=\"scrapy.exporters.MarshalItemExporter\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MarshalItemExporter</span></code></a>\nand <a class=\"reference internal\" href=\"topics/exporters.html#scrapy.exporters.PythonItemExporter\" title=\"scrapy.exporters.PythonItemExporter\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">PythonItemExporter</span></code></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3973\">issue 3973</a>)</p>", "<p>API documentation added for <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">BaseItem</span></code> and\n<a class=\"reference internal\" href=\"topics/items.html#scrapy.item.ItemMeta\" title=\"scrapy.item.ItemMeta\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ItemMeta</span></code></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3999\">issue 3999</a>)</p>", "<p>Minor documentation fixes (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2998\">issue 2998</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3398\">issue 3398</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3597\">issue 3597</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3894\">issue 3894</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3934\">issue 3934</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3978\">issue 3978</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3993\">issue 3993</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4022\">issue 4022</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4028\">issue 4028</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4033\">issue 4033</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4046\">issue 4046</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4050\">issue 4050</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4055\">issue 4055</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4056\">issue 4056</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4061\">issue 4061</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4072\">issue 4072</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4071\">issue 4071</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4079\">issue 4079</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4081\">issue 4081</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4089\">issue 4089</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4093\">issue 4093</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.xlib</span></code> has been removed (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4015\">issue 4015</a>)</p>", "<p>The <a class=\"reference external\" href=\"https://github.com/google/leveldb\">LevelDB</a> storage backend\n(<code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.extensions.httpcache.LeveldbCacheStorage</span></code>) of\n<a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware\" title=\"scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">HttpCacheMiddleware</span></code></a> is\ndeprecated (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4085\">issue 4085</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4092\">issue 4092</a>)</p>", "<p>Use of the undocumented <code class=\"docutils literal notranslate\"><span class=\"pre\">SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE</span></code> environment\nvariable is deprecated (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3910\">issue 3910</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.item.DictItem</span></code> is deprecated, use <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Item</span></code>\ninstead (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3999\">issue 3999</a>)</p>", "<p>Minimum versions of optional Scrapy requirements that are covered by\ncontinuous integration tests have been updated:</p>", "<p><a class=\"reference external\" href=\"https://github.com/boto/botocore\">botocore</a> 1.3.23</p>", "<p><a class=\"reference external\" href=\"https://python-pillow.org/\">Pillow</a> 3.4.2</p>", "<p>Lower versions of these optional requirements may work, but it is not\nguaranteed (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3892\">issue 3892</a>)</p>", "<p>GitHub templates for bug reports and feature requests (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3126\">issue 3126</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3471\">issue 3471</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3749\">issue 3749</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3754\">issue 3754</a>)</p>", "<p>Continuous integration fixes (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3923\">issue 3923</a>)</p>", "<p>Code cleanup (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3391\">issue 3391</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3907\">issue 3907</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3946\">issue 3946</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3950\">issue 3950</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4023\">issue 4023</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/4031\">issue 4031</a>)</p>", "<p>Revert the fix for <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3804\">issue 3804</a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3819\">issue 3819</a>), which has a few undesired\nside effects (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3897\">issue 3897</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3976\">issue 3976</a>).</p>", "<p>As a result, when an item loader is initialized with an item,\n<a class=\"reference internal\" href=\"topics/loaders.html#scrapy.loader.ItemLoader.load_item\" title=\"scrapy.loader.ItemLoader.load_item\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">ItemLoader.load_item()</span></code></a> once again\nmakes later calls to <a class=\"reference internal\" href=\"topics/loaders.html#scrapy.loader.ItemLoader.get_output_value\" title=\"scrapy.loader.ItemLoader.get_output_value\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">ItemLoader.get_output_value()</span></code></a> or <a class=\"reference internal\" href=\"topics/loaders.html#scrapy.loader.ItemLoader.load_item\" title=\"scrapy.loader.ItemLoader.load_item\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">ItemLoader.load_item()</span></code></a> return empty data.</p>", "<p>Enforce lxml 4.3.5 or lower for Python 3.4 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3912\">issue 3912</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3918\">issue 3918</a>).</p>", "<p>Fix Python 2 support (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3889\">issue 3889</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3893\">issue 3893</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3896\">issue 3896</a>).</p>", "<p>Re-packaging of Scrapy 1.7.0, which was missing some changes in PyPI.</p>", "<p class=\"admonition-title\">Note</p>", "<p>Make sure you install Scrapy 1.7.1. The Scrapy 1.7.0 package in PyPI\nis the result of an erroneous commit tagging and does not include all\nthe changes described below.</p>", "<p>Highlights:</p>", "<p>Improvements for crawls targeting multiple domains</p>", "<p>A cleaner way to pass arguments to callbacks</p>", "<p>A new class for JSON requests</p>", "<p>Improvements for rule-based spiders</p>", "<p>New features for feed exports</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">429</span></code> is now part of the <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-setting-RETRY_HTTP_CODES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">RETRY_HTTP_CODES</span></code></a> setting by default</p>", "<p>This change is <strong>backward incompatible</strong>. If you don’t want to retry\n<code class=\"docutils literal notranslate\"><span class=\"pre\">429</span></code>, you must override <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-setting-RETRY_HTTP_CODES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">RETRY_HTTP_CODES</span></code></a> accordingly.</p>", "<p><a class=\"reference internal\" href=\"topics/api.html#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a>,\n<a class=\"reference internal\" href=\"topics/api.html#scrapy.crawler.CrawlerRunner.crawl\" title=\"scrapy.crawler.CrawlerRunner.crawl\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CrawlerRunner.crawl</span></code></a> and\n<a class=\"reference internal\" href=\"topics/api.html#scrapy.crawler.CrawlerRunner.create_crawler\" title=\"scrapy.crawler.CrawlerRunner.create_crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CrawlerRunner.create_crawler</span></code></a>\nno longer accept a <a class=\"reference internal\" href=\"topics/spiders.html#scrapy.spiders.Spider\" title=\"scrapy.spiders.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> subclass instance, they\nonly accept a <a class=\"reference internal\" href=\"topics/spiders.html#scrapy.spiders.Spider\" title=\"scrapy.spiders.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> subclass now.</p>", "<p><a class=\"reference internal\" href=\"topics/spiders.html#scrapy.spiders.Spider\" title=\"scrapy.spiders.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> subclass instances were never meant to\nwork, and they were not working as one would expect: instead of using the\npassed <a class=\"reference internal\" href=\"topics/spiders.html#scrapy.spiders.Spider\" title=\"scrapy.spiders.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> subclass instance, their\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">from_crawler</span></code> method was called to generate\na new instance.</p>", "<p>Non-default values for the <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-SCHEDULER_PRIORITY_QUEUE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SCHEDULER_PRIORITY_QUEUE</span></code></a> setting\nmay stop working. Scheduler priority queue classes now need to handle\n<a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> objects instead of arbitrary Python data\nstructures.</p>", "<p>An additional <code class=\"docutils literal notranslate\"><span class=\"pre\">crawler</span></code> parameter has been added to the <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code>\nmethod of the <a class=\"reference internal\" href=\"topics/scheduler.html#scrapy.core.scheduler.Scheduler\" title=\"scrapy.core.scheduler.Scheduler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Scheduler</span></code></a> class. Custom\nscheduler subclasses which don’t accept arbitrary parameters in their\n<code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method might break because of this change.</p>", "<p>For more information, see <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-SCHEDULER\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SCHEDULER</span></code></a>.</p>", "<p>See also <a class=\"hoverxref tooltip reference internal\" href=\"#id94\"><span class=\"std std-ref\">Deprecation removals</span></a> below.</p>", "<p>A new scheduler priority queue,\n<code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.pqueues.DownloaderAwarePriorityQueue</span></code>, may be\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/broad-crawls.html#broad-crawls-scheduler-priority-queue\"><span class=\"std std-ref\">enabled</span></a> for a significant\nscheduling improvement on crawls targeting multiple web domains, at the\ncost of no <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-CONCURRENT_REQUESTS_PER_IP\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">CONCURRENT_REQUESTS_PER_IP</span></code></a> support (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3520\">issue 3520</a>)</p>", "<p>A new <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request.cb_kwargs\" title=\"scrapy.http.Request.cb_kwargs\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Request.cb_kwargs</span></code></a> attribute\nprovides a cleaner way to pass keyword arguments to callback methods\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1138\">issue 1138</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3563\">issue 3563</a>)</p>", "<p>A new <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.JsonRequest\" title=\"scrapy.http.JsonRequest\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">JSONRequest</span></code></a> class offers a more\nconvenient way to build JSON requests (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3504\">issue 3504</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3505\">issue 3505</a>)</p>", "<p>A <code class=\"docutils literal notranslate\"><span class=\"pre\">process_request</span></code> callback passed to the <a class=\"reference internal\" href=\"topics/spiders.html#scrapy.spiders.Rule\" title=\"scrapy.spiders.Rule\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Rule</span></code></a>\n<code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method now receives the <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Response\" title=\"scrapy.http.Response\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response</span></code></a> object that\noriginated the request as its second argument (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3682\">issue 3682</a>)</p>", "<p>A new <code class=\"docutils literal notranslate\"><span class=\"pre\">restrict_text</span></code> parameter for the\n<a class=\"reference internal\" href=\"topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\" title=\"scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">LinkExtractor</span></code></a>\n<code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method allows filtering links by linking text (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3622\">issue 3622</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3635\">issue 3635</a>)</p>", "<p>A new <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEED_STORAGE_S3_ACL\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEED_STORAGE_S3_ACL</span></code></a> setting allows defining a custom ACL\nfor feeds exported to Amazon S3 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3607\">issue 3607</a>)</p>", "<p>A new <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEED_STORAGE_FTP_ACTIVE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEED_STORAGE_FTP_ACTIVE</span></code></a> setting allows using FTP’s active\nconnection mode for feeds exported to FTP servers (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3829\">issue 3829</a>)</p>", "<p>A new <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-setting-METAREFRESH_IGNORE_TAGS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">METAREFRESH_IGNORE_TAGS</span></code></a> setting allows overriding which\nHTML tags are ignored when searching a response for HTML meta tags that\ntrigger a redirect (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1422\">issue 1422</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3768\">issue 3768</a>)</p>", "<p>A new <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-redirect_reasons\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">redirect_reasons</span></code></a> request meta key exposes the reason\n(status code, meta refresh) behind every followed redirect (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3581\">issue 3581</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3687\">issue 3687</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">SCRAPY_CHECK</span></code> variable is now set to the <code class=\"docutils literal notranslate\"><span class=\"pre\">true</span></code> string during runs\nof the <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-check\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">check</span></code></a> command, which allows <a class=\"hoverxref tooltip reference internal\" href=\"topics/contracts.html#detecting-contract-check-runs\"><span class=\"std std-ref\">detecting contract\ncheck runs from code</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3704\">issue 3704</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3739\">issue 3739</a>)</p>", "<p>A new <code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">Item.deepcopy()</span></code> method makes it\neasier to <a class=\"hoverxref tooltip reference internal\" href=\"topics/items.html#copying-items\"><span class=\"std std-ref\">deep-copy items</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1493\">issue 1493</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3671\">issue 3671</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/extensions.html#scrapy.extensions.corestats.CoreStats\" title=\"scrapy.extensions.corestats.CoreStats\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CoreStats</span></code></a> also logs\n<code class=\"docutils literal notranslate\"><span class=\"pre\">elapsed_time_seconds</span></code> now (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3638\">issue 3638</a>)</p>", "<p>Exceptions from <a class=\"reference internal\" href=\"topics/loaders.html#scrapy.loader.ItemLoader\" title=\"scrapy.loader.ItemLoader\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ItemLoader</span></code></a> <a class=\"hoverxref tooltip reference internal\" href=\"topics/loaders.html#topics-loaders-processors\"><span class=\"std std-ref\">input and output\nprocessors</span></a> are now more verbose\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3836\">issue 3836</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3840\">issue 3840</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/api.html#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a>,\n<a class=\"reference internal\" href=\"topics/api.html#scrapy.crawler.CrawlerRunner.crawl\" title=\"scrapy.crawler.CrawlerRunner.crawl\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CrawlerRunner.crawl</span></code></a> and\n<a class=\"reference internal\" href=\"topics/api.html#scrapy.crawler.CrawlerRunner.create_crawler\" title=\"scrapy.crawler.CrawlerRunner.create_crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CrawlerRunner.create_crawler</span></code></a>\nnow fail gracefully if they receive a <a class=\"reference internal\" href=\"topics/spiders.html#scrapy.spiders.Spider\" title=\"scrapy.spiders.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a>\nsubclass instance instead of the subclass itself (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2283\">issue 2283</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3610\">issue 3610</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3872\">issue 3872</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception\" title=\"scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_spider_exception()</span></code></a>\nis now also invoked for generators (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/220\">issue 220</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2061\">issue 2061</a>)</p>", "<p>System exceptions like <a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#KeyboardInterrupt\">KeyboardInterrupt</a> are no longer caught\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3726\">issue 3726</a>)</p>", "<p><a class=\"reference internal\" href=\"topics/loaders.html#scrapy.loader.ItemLoader.load_item\" title=\"scrapy.loader.ItemLoader.load_item\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">ItemLoader.load_item()</span></code></a> no\nlonger makes later calls to <a class=\"reference internal\" href=\"topics/loaders.html#scrapy.loader.ItemLoader.get_output_value\" title=\"scrapy.loader.ItemLoader.get_output_value\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">ItemLoader.get_output_value()</span></code></a> or\n<a class=\"reference internal\" href=\"topics/loaders.html#scrapy.loader.ItemLoader.load_item\" title=\"scrapy.loader.ItemLoader.load_item\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">ItemLoader.load_item()</span></code></a> return\nempty data (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3804\">issue 3804</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3819\">issue 3819</a>)</p>", "<p>The images pipeline (<a class=\"reference internal\" href=\"topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline\" title=\"scrapy.pipelines.images.ImagesPipeline\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ImagesPipeline</span></code></a>) no\nlonger ignores these Amazon S3 settings: <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-AWS_ENDPOINT_URL\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">AWS_ENDPOINT_URL</span></code></a>,\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-AWS_REGION_NAME\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">AWS_REGION_NAME</span></code></a>, <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-AWS_USE_SSL\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">AWS_USE_SSL</span></code></a>, <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-AWS_VERIFY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">AWS_VERIFY</span></code></a>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3625\">issue 3625</a>)</p>", "<p>Fixed a memory leak in <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.pipelines.media.MediaPipeline</span></code> affecting,\nfor example, non-200 responses and exceptions from custom middlewares\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3813\">issue 3813</a>)</p>", "<p>Requests with private callbacks are now correctly unserialized from disk\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3790\">issue 3790</a>)</p>", "<p><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">FormRequest.from_response()</span></code>\nnow handles invalid methods like major web browsers (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3777\">issue 3777</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3794\">issue 3794</a>)</p>", "<p>A new topic, <a class=\"hoverxref tooltip reference internal\" href=\"topics/dynamic-content.html#topics-dynamic-content\"><span class=\"std std-ref\">Selecting dynamically-loaded content</span></a>, covers recommended approaches\nto read dynamically-loaded data (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3703\">issue 3703</a>)</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/broad-crawls.html#topics-broad-crawls\"><span class=\"std std-ref\">Broad Crawls</span></a> now features information about memory usage\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1264\">issue 1264</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3866\">issue 3866</a>)</p>", "<p>The documentation of <a class=\"reference internal\" href=\"topics/spiders.html#scrapy.spiders.Rule\" title=\"scrapy.spiders.Rule\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Rule</span></code></a> now covers how to access\nthe text of a link when using <a class=\"reference internal\" href=\"topics/spiders.html#scrapy.spiders.CrawlSpider\" title=\"scrapy.spiders.CrawlSpider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CrawlSpider</span></code></a>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3711\">issue 3711</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3712\">issue 3712</a>)</p>", "<p>A new section, <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#httpcache-storage-custom\"><span class=\"std std-ref\">Writing your own storage backend</span></a>, covers writing a custom\ncache storage backend for\n<a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware\" title=\"scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">HttpCacheMiddleware</span></code></a>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3683\">issue 3683</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3692\">issue 3692</a>)</p>", "<p>A new <a class=\"hoverxref tooltip reference internal\" href=\"faq.html#faq\"><span class=\"std std-ref\">FAQ</span></a> entry, <a class=\"hoverxref tooltip reference internal\" href=\"faq.html#faq-split-item\"><span class=\"std std-ref\">How to split an item into multiple items in an item pipeline?</span></a>, explains what to do\nwhen you want to split an item into multiple items from an item pipeline\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2240\">issue 2240</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3672\">issue 3672</a>)</p>", "<p>Updated the <a class=\"hoverxref tooltip reference internal\" href=\"faq.html#faq-bfo-dfo\"><span class=\"std std-ref\">FAQ entry about crawl order</span></a> to explain why\nthe first few requests rarely follow the desired order (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1739\">issue 1739</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3621\">issue 3621</a>)</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-LOGSTATS_INTERVAL\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">LOGSTATS_INTERVAL</span></code></a> setting (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3730\">issue 3730</a>), the\n<a class=\"reference internal\" href=\"topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.file_path\" title=\"scrapy.pipelines.files.FilesPipeline.file_path\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">FilesPipeline.file_path</span></code></a>\nand\n<a class=\"reference internal\" href=\"topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline.file_path\" title=\"scrapy.pipelines.images.ImagesPipeline.file_path\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">ImagesPipeline.file_path</span></code></a>\nmethods (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2253\">issue 2253</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3609\">issue 3609</a>) and the\n<a class=\"reference internal\" href=\"topics/api.html#scrapy.crawler.Crawler.stop\" title=\"scrapy.crawler.Crawler.stop\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">Crawler.stop()</span></code></a> method (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3842\">issue 3842</a>)\nare now documented</p>", "<p>Some parts of the documentation that were confusing or misleading are now\nclearer (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1347\">issue 1347</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1789\">issue 1789</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2289\">issue 2289</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3069\">issue 3069</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3615\">issue 3615</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3626\">issue 3626</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3668\">issue 3668</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3670\">issue 3670</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3673\">issue 3673</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3728\">issue 3728</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3762\">issue 3762</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3861\">issue 3861</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3882\">issue 3882</a>)</p>", "<p>Minor documentation fixes (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3648\">issue 3648</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3649\">issue 3649</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3662\">issue 3662</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3674\">issue 3674</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3676\">issue 3676</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3694\">issue 3694</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3724\">issue 3724</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3764\">issue 3764</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3767\">issue 3767</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3791\">issue 3791</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3797\">issue 3797</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3806\">issue 3806</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3812\">issue 3812</a>)</p>", "<p>The following deprecated APIs have been removed (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3578\">issue 3578</a>):</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.conf</span></code> (use <a class=\"reference internal\" href=\"topics/api.html#scrapy.crawler.Crawler.settings\" title=\"scrapy.crawler.Crawler.settings\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Crawler.settings</span></code></a>)</p>", "<p>From <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.core.downloader.handlers</span></code>:</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">http.HttpDownloadHandler</span></code> (use <code class=\"docutils literal notranslate\"><span class=\"pre\">http10.HTTP10DownloadHandler</span></code>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.loader.ItemLoader._get_values</span></code> (use <code class=\"docutils literal notranslate\"><span class=\"pre\">_get_xpathvalues</span></code>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.loader.XPathItemLoader</span></code> (use <a class=\"reference internal\" href=\"topics/loaders.html#scrapy.loader.ItemLoader\" title=\"scrapy.loader.ItemLoader\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ItemLoader</span></code></a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.log</span></code> (see <a class=\"hoverxref tooltip reference internal\" href=\"topics/logging.html#topics-logging\"><span class=\"std std-ref\">Logging</span></a>)</p>", "<p>From <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.pipelines</span></code>:</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">files.FilesPipeline.file_key</span></code> (use <code class=\"docutils literal notranslate\"><span class=\"pre\">file_path</span></code>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">images.ImagesPipeline.file_key</span></code> (use <code class=\"docutils literal notranslate\"><span class=\"pre\">file_path</span></code>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">images.ImagesPipeline.image_key</span></code> (use <code class=\"docutils literal notranslate\"><span class=\"pre\">file_path</span></code>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">images.ImagesPipeline.thumb_key</span></code> (use <code class=\"docutils literal notranslate\"><span class=\"pre\">thumb_path</span></code>)</p>", "<p>From both <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.selector</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.selector.lxmlsel</span></code>:</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">HtmlXPathSelector</span></code> (use <a class=\"reference internal\" href=\"topics/selectors.html#scrapy.selector.Selector\" title=\"scrapy.selector.Selector\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Selector</span></code></a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">XmlXPathSelector</span></code> (use <a class=\"reference internal\" href=\"topics/selectors.html#scrapy.selector.Selector\" title=\"scrapy.selector.Selector\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Selector</span></code></a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">XPathSelector</span></code> (use <a class=\"reference internal\" href=\"topics/selectors.html#scrapy.selector.Selector\" title=\"scrapy.selector.Selector\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Selector</span></code></a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">XPathSelectorList</span></code> (use <a class=\"reference internal\" href=\"topics/selectors.html#scrapy.selector.Selector\" title=\"scrapy.selector.Selector\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Selector</span></code></a>)</p>", "<p>From <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.selector.csstranslator</span></code>:</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">ScrapyGenericTranslator</span></code> (use <a class=\"reference external\" href=\"https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.GenericTranslator\">parsel.csstranslator.GenericTranslator</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">ScrapyHTMLTranslator</span></code> (use <a class=\"reference external\" href=\"https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.HTMLTranslator\">parsel.csstranslator.HTMLTranslator</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">ScrapyXPathExpr</span></code> (use <a class=\"reference external\" href=\"https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.XPathExpr\">parsel.csstranslator.XPathExpr</a>)</p>", "<p>From <a class=\"reference internal\" href=\"topics/selectors.html#scrapy.selector.Selector\" title=\"scrapy.selector.Selector\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Selector</span></code></a>:</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">_root</span></code> (both the <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method argument and the object property, use\n<code class=\"docutils literal notranslate\"><span class=\"pre\">root</span></code>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">extract_unquoted</span></code> (use <code class=\"docutils literal notranslate\"><span class=\"pre\">getall</span></code>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">select</span></code> (use <code class=\"docutils literal notranslate\"><span class=\"pre\">xpath</span></code>)</p>", "<p>From <a class=\"reference internal\" href=\"topics/selectors.html#scrapy.selector.SelectorList\" title=\"scrapy.selector.SelectorList\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">SelectorList</span></code></a>:</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">extract_unquoted</span></code> (use <code class=\"docutils literal notranslate\"><span class=\"pre\">getall</span></code>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">select</span></code> (use <code class=\"docutils literal notranslate\"><span class=\"pre\">xpath</span></code>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">x</span></code> (use <code class=\"docutils literal notranslate\"><span class=\"pre\">xpath</span></code>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.spiders.BaseSpider</span></code> (use <a class=\"reference internal\" href=\"topics/spiders.html#scrapy.spiders.Spider\" title=\"scrapy.spiders.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a>)</p>", "<p>From <a class=\"reference internal\" href=\"topics/spiders.html#scrapy.spiders.Spider\" title=\"scrapy.spiders.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> (and subclasses):</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">DOWNLOAD_DELAY</span></code> (use <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#spider-download-delay-attribute\"><span class=\"std std-ref\">download_delay</span></a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">set_crawler</span></code> (use <code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">from_crawler()</span></code>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.spiders.spiders</span></code> (use <a class=\"reference internal\" href=\"topics/api.html#scrapy.spiderloader.SpiderLoader\" title=\"scrapy.spiderloader.SpiderLoader\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">SpiderLoader</span></code></a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.telnet</span></code> (use <a class=\"reference internal\" href=\"topics/extensions.html#module-scrapy.extensions.telnet\" title=\"scrapy.extensions.telnet: Telnet console\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">scrapy.extensions.telnet</span></code></a>)</p>", "<p>From <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.python</span></code>:</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">str_to_unicode</span></code> (use <code class=\"docutils literal notranslate\"><span class=\"pre\">to_unicode</span></code>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">unicode_to_str</span></code> (use <code class=\"docutils literal notranslate\"><span class=\"pre\">to_bytes</span></code>)</p>", "<p>The following deprecated settings have also been removed (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3578\">issue 3578</a>):</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">SPIDER_MANAGER_CLASS</span></code> (use <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-SPIDER_LOADER_CLASS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SPIDER_LOADER_CLASS</span></code></a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">queuelib.PriorityQueue</span></code> value for the\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-SCHEDULER_PRIORITY_QUEUE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SCHEDULER_PRIORITY_QUEUE</span></code></a> setting is deprecated. Use\n<code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.pqueues.ScrapyPriorityQueue</span></code> instead.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">process_request</span></code> callbacks passed to <a class=\"reference internal\" href=\"topics/spiders.html#scrapy.spiders.Rule\" title=\"scrapy.spiders.Rule\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Rule</span></code></a> that\ndo not accept two arguments are deprecated.</p>", "<p>The following modules are deprecated:</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.http</span></code> (use <a class=\"reference external\" href=\"https://w3lib.readthedocs.io/en/latest/w3lib.html#module-w3lib.http\">w3lib.http</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.markup</span></code> (use <a class=\"reference external\" href=\"https://w3lib.readthedocs.io/en/latest/w3lib.html#module-w3lib.html\">w3lib.html</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.multipart</span></code> (use <a class=\"reference external\" href=\"https://urllib3.readthedocs.io/en/latest/index.html\">urllib3</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.datatypes.MergeDict</span></code> class is deprecated for Python 3\ncode bases. Use <a class=\"reference external\" href=\"https://docs.python.org/3/library/collections.html#collections.ChainMap\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ChainMap</span></code></a> instead. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3878\">issue 3878</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.gz.is_gzipped</span></code> function is deprecated. Use\n<code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.gz.gzip_magic_number</span></code> instead.</p>", "<p>It is now possible to run all tests from the same <a class=\"reference external\" href=\"https://pypi.org/project/tox/\">tox</a> environment in\nparallel; the documentation now covers <a class=\"hoverxref tooltip reference internal\" href=\"contributing.html#running-tests\"><span class=\"std std-ref\">this and other ways to run\ntests</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3707\">issue 3707</a>)</p>", "<p>It is now possible to generate an API documentation coverage report\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3806\">issue 3806</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3810\">issue 3810</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3860\">issue 3860</a>)</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"contributing.html#documentation-policies\"><span class=\"std std-ref\">documentation policies</span></a> now require\n<a class=\"reference external\" href=\"https://docs.python.org/3/glossary.html#term-docstring\">docstrings</a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3701\">issue 3701</a>) that follow <a class=\"reference external\" href=\"https://www.python.org/dev/peps/pep-0257/\">PEP 257</a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3748\">issue 3748</a>)</p>", "<p>Internal fixes and cleanup (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3629\">issue 3629</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3643\">issue 3643</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3684\">issue 3684</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3698\">issue 3698</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3734\">issue 3734</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3735\">issue 3735</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3736\">issue 3736</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3737\">issue 3737</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3809\">issue 3809</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3821\">issue 3821</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3825\">issue 3825</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3827\">issue 3827</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3833\">issue 3833</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3857\">issue 3857</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3877\">issue 3877</a>)</p>", "<p>Highlights:</p>", "<p>better Windows support;</p>", "<p>Python 3.7 compatibility;</p>", "<p>big documentation improvements, including a switch\nfrom <code class=\"docutils literal notranslate\"><span class=\"pre\">.extract_first()</span></code> + <code class=\"docutils literal notranslate\"><span class=\"pre\">.extract()</span></code> API to <code class=\"docutils literal notranslate\"><span class=\"pre\">.get()</span></code> + <code class=\"docutils literal notranslate\"><span class=\"pre\">.getall()</span></code>\nAPI;</p>", "<p>feed exports, FilePipeline and MediaPipeline improvements;</p>", "<p>better extensibility: <a class=\"hoverxref tooltip reference internal\" href=\"topics/signals.html#std-signal-item_error\"><code class=\"xref std std-signal docutils literal notranslate\"><span class=\"pre\">item_error</span></code></a> and\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/signals.html#std-signal-request_reached_downloader\"><code class=\"xref std std-signal docutils literal notranslate\"><span class=\"pre\">request_reached_downloader</span></code></a> signals; <code class=\"docutils literal notranslate\"><span class=\"pre\">from_crawler</span></code> support\nfor feed exporters, feed storages and dupefilters.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.contracts</span></code> fixes and new features;</p>", "<p>telnet console security improvements, first released as a\nbackport in <a class=\"hoverxref tooltip reference internal\" href=\"#release-1-5-2\"><span class=\"std std-ref\">Scrapy 1.5.2 (2019-01-22)</span></a>;</p>", "<p>clean-up of the deprecated code;</p>", "<p>various bug fixes, small new features and usability improvements across\nthe codebase.</p>", "<p>While these are not changes in Scrapy itself, but rather in the <a class=\"reference external\" href=\"https://github.com/scrapy/parsel\">parsel</a>\nlibrary which Scrapy uses for xpath/css selectors, these changes are\nworth mentioning here. Scrapy now depends on parsel &gt;= 1.5, and\nScrapy documentation is updated to follow recent <code class=\"docutils literal notranslate\"><span class=\"pre\">parsel</span></code> API conventions.</p>", "<p>Most visible change is that <code class=\"docutils literal notranslate\"><span class=\"pre\">.get()</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">.getall()</span></code> selector\nmethods are now preferred over <code class=\"docutils literal notranslate\"><span class=\"pre\">.extract_first()</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">.extract()</span></code>.\nWe feel that these new methods result in a more concise and readable code.\nSee <a class=\"hoverxref tooltip reference internal\" href=\"topics/selectors.html#old-extraction-api\"><span class=\"std std-ref\">extract() and extract_first()</span></a> for more details.</p>", "<p class=\"admonition-title\">Note</p>", "<p>There are currently <strong>no plans</strong> to deprecate <code class=\"docutils literal notranslate\"><span class=\"pre\">.extract()</span></code>\nand <code class=\"docutils literal notranslate\"><span class=\"pre\">.extract_first()</span></code> methods.</p>", "<p>Another useful new feature is the introduction of <code class=\"docutils literal notranslate\"><span class=\"pre\">Selector.attrib</span></code> and\n<code class=\"docutils literal notranslate\"><span class=\"pre\">SelectorList.attrib</span></code> properties, which make it easier to get\nattributes of HTML elements. See <a class=\"hoverxref tooltip reference internal\" href=\"topics/selectors.html#selecting-attributes\"><span class=\"std std-ref\">Selecting element attributes</span></a>.</p>", "<p>CSS selectors are cached in parsel &gt;= 1.5, which makes them faster\nwhen the same CSS path is used many times. This is very common in\ncase of Scrapy spiders: callbacks are usually called several times,\non different pages.</p>", "<p>If you’re using custom <code class=\"docutils literal notranslate\"><span class=\"pre\">Selector</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">SelectorList</span></code> subclasses,\na <strong>backward incompatible</strong> change in parsel may affect your code.\nSee <a class=\"reference external\" href=\"https://parsel.readthedocs.io/en/latest/history.html\">parsel changelog</a> for a detailed description, as well as for the\nfull list of improvements.</p>", "<p><strong>Backward incompatible</strong>: Scrapy’s telnet console now requires username\nand password. See <a class=\"hoverxref tooltip reference internal\" href=\"topics/telnetconsole.html#topics-telnetconsole\"><span class=\"std std-ref\">Telnet Console</span></a> for more details. This change\nfixes a <strong>security issue</strong>; see <a class=\"hoverxref tooltip reference internal\" href=\"#release-1-5-2\"><span class=\"std std-ref\">Scrapy 1.5.2 (2019-01-22)</span></a> release notes for details.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">from_crawler</span></code> support is added to feed exporters and feed storages. This,\namong other things, allows to access Scrapy settings from custom feed\nstorages and exporters (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1605\">issue 1605</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3348\">issue 3348</a>).</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">from_crawler</span></code> support is added to dupefilters (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2956\">issue 2956</a>); this allows\nto access e.g. settings or a spider from a dupefilter.</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/signals.html#std-signal-item_error\"><code class=\"xref std std-signal docutils literal notranslate\"><span class=\"pre\">item_error</span></code></a> is fired when an error happens in a pipeline\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3256\">issue 3256</a>);</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/signals.html#std-signal-request_reached_downloader\"><code class=\"xref std std-signal docutils literal notranslate\"><span class=\"pre\">request_reached_downloader</span></code></a> is fired when Downloader gets\na new Request; this signal can be useful e.g. for custom Schedulers\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3393\">issue 3393</a>).</p>", "<p>new SitemapSpider <a class=\"reference internal\" href=\"topics/spiders.html#scrapy.spiders.SitemapSpider.sitemap_filter\" title=\"scrapy.spiders.SitemapSpider.sitemap_filter\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">sitemap_filter()</span></code></a> method which allows\nto select sitemap entries based on their attributes in SitemapSpider\nsubclasses (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3512\">issue 3512</a>).</p>", "<p>Lazy loading of Downloader Handlers is now optional; this enables better\ninitialization error handling in custom Downloader Handlers (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3394\">issue 3394</a>).</p>", "<p>Expose more options for S3FilesStore: <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-AWS_ENDPOINT_URL\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">AWS_ENDPOINT_URL</span></code></a>,\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-AWS_USE_SSL\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">AWS_USE_SSL</span></code></a>, <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-AWS_VERIFY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">AWS_VERIFY</span></code></a>, <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-AWS_REGION_NAME\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">AWS_REGION_NAME</span></code></a>.\nFor example, this allows to use alternative or self-hosted\nAWS-compatible providers (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2609\">issue 2609</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3548\">issue 3548</a>).</p>", "<p>ACL support for Google Cloud Storage: <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#std-setting-FILES_STORE_GCS_ACL\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FILES_STORE_GCS_ACL</span></code></a> and\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#std-setting-IMAGES_STORE_GCS_ACL\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">IMAGES_STORE_GCS_ACL</span></code></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3199\">issue 3199</a>).</p>", "<p>Exceptions in contracts code are handled better (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3377\">issue 3377</a>);</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">dont_filter=True</span></code> is used for contract requests, which allows to test\ndifferent callbacks with the same URL (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3381\">issue 3381</a>);</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">request_cls</span></code> attribute in Contract subclasses allow to use different\nRequest classes in contracts, for example FormRequest (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3383\">issue 3383</a>).</p>", "<p>Fixed errback handling in contracts, e.g. for cases where a contract\nis executed for URL which returns non-200 response (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3371\">issue 3371</a>).</p>", "<p>more stats for RobotsTxtMiddleware (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3100\">issue 3100</a>)</p>", "<p>INFO log level is used to show telnet host/port (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3115\">issue 3115</a>)</p>", "<p>a message is added to IgnoreRequest in RobotsTxtMiddleware (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3113\">issue 3113</a>)</p>", "<p>better validation of <code class=\"docutils literal notranslate\"><span class=\"pre\">url</span></code> argument in <code class=\"docutils literal notranslate\"><span class=\"pre\">Response.follow</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3131\">issue 3131</a>)</p>", "<p>non-zero exit code is returned from Scrapy commands when error happens\non spider initialization (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3226\">issue 3226</a>)</p>", "<p>Link extraction improvements: “ftp” is added to scheme list (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3152\">issue 3152</a>);\n“flv” is added to common video extensions (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3165\">issue 3165</a>)</p>", "<p>better error message when an exporter is disabled (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3358\">issue 3358</a>);</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">shell</span> <span class=\"pre\">--help</span></code> mentions syntax required for local files\n(<code class=\"docutils literal notranslate\"><span class=\"pre\">./file.html</span></code>) - <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3496\">issue 3496</a>.</p>", "<p>Referer header value is added to RFPDupeFilter log messages (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3588\">issue 3588</a>)</p>", "<p>fixed issue with extra blank lines in .csv exports under Windows\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3039\">issue 3039</a>);</p>", "<p>proper handling of pickling errors in Python 3 when serializing objects\nfor disk queues (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3082\">issue 3082</a>)</p>", "<p>flags are now preserved when copying Requests (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3342\">issue 3342</a>);</p>", "<p>FormRequest.from_response clickdata shouldn’t ignore elements with\n<code class=\"docutils literal notranslate\"><span class=\"pre\">input[type=image]</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3153\">issue 3153</a>).</p>", "<p>FormRequest.from_response should preserve duplicate keys (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3247\">issue 3247</a>)</p>", "<p>Docs are re-written to suggest .get/.getall API instead of\n.extract/.extract_first. Also, <a class=\"hoverxref tooltip reference internal\" href=\"topics/selectors.html#topics-selectors\"><span class=\"std std-ref\">Selectors</span></a> docs are updated\nand re-structured to match latest parsel docs; they now contain more topics,\nsuch as <a class=\"hoverxref tooltip reference internal\" href=\"topics/selectors.html#selecting-attributes\"><span class=\"std std-ref\">Selecting element attributes</span></a> or <a class=\"hoverxref tooltip reference internal\" href=\"topics/selectors.html#topics-selectors-css-extensions\"><span class=\"std std-ref\">Extensions to CSS Selectors</span></a>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3390\">issue 3390</a>).</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/developer-tools.html#topics-developer-tools\"><span class=\"std std-ref\">Using your browser’s Developer Tools for scraping</span></a> is a new tutorial which replaces\nold Firefox and Firebug tutorials (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3400\">issue 3400</a>).</p>", "<p>SCRAPY_PROJECT environment variable is documented (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3518\">issue 3518</a>);</p>", "<p>troubleshooting section is added to install instructions (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3517\">issue 3517</a>);</p>", "<p>improved links to beginner resources in the tutorial\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3367\">issue 3367</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3468\">issue 3468</a>);</p>", "<p>fixed <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-setting-RETRY_HTTP_CODES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">RETRY_HTTP_CODES</span></code></a> default values in docs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3335\">issue 3335</a>);</p>", "<p>remove unused <code class=\"docutils literal notranslate\"><span class=\"pre\">DEPTH_STATS</span></code> option from docs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3245\">issue 3245</a>);</p>", "<p>other cleanups (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3347\">issue 3347</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3350\">issue 3350</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3445\">issue 3445</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3544\">issue 3544</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3605\">issue 3605</a>).</p>", "<p>Compatibility shims for pre-1.0 Scrapy module names are removed\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3318\">issue 3318</a>):</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.contrib</span></code> (with all submodules)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.contrib_exp</span></code> (with all submodules)</p>", "<p>See <a class=\"hoverxref tooltip reference internal\" href=\"#module-relocations\"><span class=\"std std-ref\">Module Relocations</span></a> for more information, or use suggestions\nfrom Scrapy 1.5.x deprecation warnings to update your code.</p>", "<p>Other deprecation removals:</p>", "<p>Deprecated scrapy.interfaces.ISpiderManager is removed; please use\nscrapy.interfaces.ISpiderLoader.</p>", "<p>Deprecated <code class=\"docutils literal notranslate\"><span class=\"pre\">CrawlerSettings</span></code> class is removed (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3327\">issue 3327</a>).</p>", "<p>Deprecated <code class=\"docutils literal notranslate\"><span class=\"pre\">Settings.overrides</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">Settings.defaults</span></code> attributes\nare removed (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3327\">issue 3327</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3359\">issue 3359</a>).</p>", "<p>All Scrapy tests now pass on Windows; Scrapy testing suite is executed\nin a Windows environment on CI (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3315\">issue 3315</a>).</p>", "<p>Python 3.7 support (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3326\">issue 3326</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3150\">issue 3150</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3547\">issue 3547</a>).</p>", "<p>Testing and CI fixes (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3526\">issue 3526</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3538\">issue 3538</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3308\">issue 3308</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3311\">issue 3311</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3309\">issue 3309</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3305\">issue 3305</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3210\">issue 3210</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3299\">issue 3299</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.http.cookies.CookieJar.clear</span></code> accepts “domain”, “path” and “name”\noptional arguments (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3231\">issue 3231</a>).</p>", "<p>additional files are included to sdist (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3495\">issue 3495</a>);</p>", "<p>code style fixes (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3405\">issue 3405</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3304\">issue 3304</a>);</p>", "<p>unneeded .strip() call is removed (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3519\">issue 3519</a>);</p>", "<p>collections.deque is used to store MiddlewareManager methods instead\nof a list (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3476\">issue 3476</a>)</p>", "<p><em>Security bugfix</em>: Telnet console extension can be easily exploited by rogue\nwebsites POSTing content to <a class=\"reference external\" href=\"http://localhost:6023\">http://localhost:6023</a>, we haven’t found a way to\nexploit it from Scrapy, but it is very easy to trick a browser to do so and\nelevates the risk for local development environment.</p>", "<p><em>The fix is backward incompatible</em>, it enables telnet user-password\nauthentication by default with a random generated password. If you can’t\nupgrade right away, please consider setting <a class=\"hoverxref tooltip reference internal\" href=\"topics/telnetconsole.html#std-setting-TELNETCONSOLE_PORT\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">TELNETCONSOLE_PORT</span></code></a>\nout of its default value.</p>", "<p>See <a class=\"hoverxref tooltip reference internal\" href=\"topics/telnetconsole.html#topics-telnetconsole\"><span class=\"std std-ref\">telnet console</span></a> documentation for more info</p>", "<p>Backport CI build failure under GCE environment due to boto import error.</p>", "<p>This is a maintenance release with important bug fixes, but no new features:</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">O(N^2)</span></code> gzip decompression issue which affected Python 3 and PyPy\nis fixed (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3281\">issue 3281</a>);</p>", "<p>skipping of TLS validation errors is improved (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3166\">issue 3166</a>);</p>", "<p>Ctrl-C handling is fixed in Python 3.5+ (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3096\">issue 3096</a>);</p>", "<p>testing fixes (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3092\">issue 3092</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3263\">issue 3263</a>);</p>", "<p>documentation improvements (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3058\">issue 3058</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3059\">issue 3059</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3089\">issue 3089</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3123\">issue 3123</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3127\">issue 3127</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3189\">issue 3189</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3224\">issue 3224</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3280\">issue 3280</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3279\">issue 3279</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3201\">issue 3201</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3260\">issue 3260</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3284\">issue 3284</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3298\">issue 3298</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3294\">issue 3294</a>).</p>", "<p>This release brings small new features and improvements across the codebase.\nSome highlights:</p>", "<p>Google Cloud Storage is supported in FilesPipeline and ImagesPipeline.</p>", "<p>Crawling with proxy servers becomes more efficient, as connections\nto proxies can be reused now.</p>", "<p>Warnings, exception and logging messages are improved to make debugging\neasier.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">parse</span></code> command now allows to set custom request meta via\n<code class=\"docutils literal notranslate\"><span class=\"pre\">--meta</span></code> argument.</p>", "<p>Compatibility with Python 3.6, PyPy and PyPy3 is improved;\nPyPy and PyPy3 are now supported officially, by running tests on CI.</p>", "<p>Better default handling of HTTP 308, 522 and 524 status codes.</p>", "<p>Documentation is improved, as usual.</p>", "<p>Scrapy 1.5 drops support for Python 3.3.</p>", "<p>Default Scrapy User-Agent now uses https link to scrapy.org (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2983\">issue 2983</a>).\n<strong>This is technically backward-incompatible</strong>; override\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-USER_AGENT\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">USER_AGENT</span></code></a> if you relied on old value.</p>", "<p>Logging of settings overridden by <code class=\"docutils literal notranslate\"><span class=\"pre\">custom_settings</span></code> is fixed;\n<strong>this is technically backward-incompatible</strong> because the logger\nchanges from <code class=\"docutils literal notranslate\"><span class=\"pre\">[scrapy.utils.log]</span></code> to <code class=\"docutils literal notranslate\"><span class=\"pre\">[scrapy.crawler]</span></code>. If you’re\nparsing Scrapy logs, please update your log parsers (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1343\">issue 1343</a>).</p>", "<p>LinkExtractor now ignores <code class=\"docutils literal notranslate\"><span class=\"pre\">m4v</span></code> extension by default, this is change\nin behavior.</p>", "<p>522 and 524 status codes are added to <code class=\"docutils literal notranslate\"><span class=\"pre\">RETRY_HTTP_CODES</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2851\">issue 2851</a>)</p>", "<p>Support <code class=\"docutils literal notranslate\"><span class=\"pre\">&lt;link&gt;</span></code> tags in <code class=\"docutils literal notranslate\"><span class=\"pre\">Response.follow</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2785\">issue 2785</a>)</p>", "<p>Support for <code class=\"docutils literal notranslate\"><span class=\"pre\">ptpython</span></code> REPL (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2654\">issue 2654</a>)</p>", "<p>Google Cloud Storage support for FilesPipeline and ImagesPipeline\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2923\">issue 2923</a>).</p>", "<p>New <code class=\"docutils literal notranslate\"><span class=\"pre\">--meta</span></code> option of the “scrapy parse” command allows to pass additional\nrequest.meta (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2883\">issue 2883</a>)</p>", "<p>Populate spider variable when using <code class=\"docutils literal notranslate\"><span class=\"pre\">shell.inspect_response</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2812\">issue 2812</a>)</p>", "<p>Handle HTTP 308 Permanent Redirect (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2844\">issue 2844</a>)</p>", "<p>Add 522 and 524 to <code class=\"docutils literal notranslate\"><span class=\"pre\">RETRY_HTTP_CODES</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2851\">issue 2851</a>)</p>", "<p>Log versions information at startup (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2857\">issue 2857</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.mail.MailSender</span></code> now works in Python 3 (it requires Twisted 17.9.0)</p>", "<p>Connections to proxy servers are reused (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2743\">issue 2743</a>)</p>", "<p>Add template for a downloader middleware (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2755\">issue 2755</a>)</p>", "<p>Explicit message for NotImplementedError when parse callback not defined\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2831\">issue 2831</a>)</p>", "<p>CrawlerProcess got an option to disable installation of root log handler\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2921\">issue 2921</a>)</p>", "<p>LinkExtractor now ignores <code class=\"docutils literal notranslate\"><span class=\"pre\">m4v</span></code> extension by default</p>", "<p>Better log messages for responses over <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DOWNLOAD_WARNSIZE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOAD_WARNSIZE</span></code></a> and\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DOWNLOAD_MAXSIZE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOAD_MAXSIZE</span></code></a> limits (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2927\">issue 2927</a>)</p>", "<p>Show warning when a URL is put to <code class=\"docutils literal notranslate\"><span class=\"pre\">Spider.allowed_domains</span></code> instead of\na domain (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2250\">issue 2250</a>).</p>", "<p>Fix logging of settings overridden by <code class=\"docutils literal notranslate\"><span class=\"pre\">custom_settings</span></code>;\n<strong>this is technically backward-incompatible</strong> because the logger\nchanges from <code class=\"docutils literal notranslate\"><span class=\"pre\">[scrapy.utils.log]</span></code> to <code class=\"docutils literal notranslate\"><span class=\"pre\">[scrapy.crawler]</span></code>, so please\nupdate your log parsers if needed (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1343\">issue 1343</a>)</p>", "<p>Default Scrapy User-Agent now uses https link to scrapy.org (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2983\">issue 2983</a>).\n<strong>This is technically backward-incompatible</strong>; override\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-USER_AGENT\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">USER_AGENT</span></code></a> if you relied on old value.</p>", "<p>Fix PyPy and PyPy3 test failures, support them officially\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2793\">issue 2793</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2935\">issue 2935</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2990\">issue 2990</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3050\">issue 3050</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2213\">issue 2213</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3048\">issue 3048</a>)</p>", "<p>Fix DNS resolver when <code class=\"docutils literal notranslate\"><span class=\"pre\">DNSCACHE_ENABLED=False</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2811\">issue 2811</a>)</p>", "<p>Add <code class=\"docutils literal notranslate\"><span class=\"pre\">cryptography</span></code> for Debian Jessie tox test env (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2848\">issue 2848</a>)</p>", "<p>Add verification to check if Request callback is callable (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2766\">issue 2766</a>)</p>", "<p>Port <code class=\"docutils literal notranslate\"><span class=\"pre\">extras/qpsclient.py</span></code> to Python 3 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2849\">issue 2849</a>)</p>", "<p>Use getfullargspec under the scenes for Python 3 to stop DeprecationWarning\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2862\">issue 2862</a>)</p>", "<p>Update deprecated test aliases (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2876\">issue 2876</a>)</p>", "<p>Fix <code class=\"docutils literal notranslate\"><span class=\"pre\">SitemapSpider</span></code> support for alternate links (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2853\">issue 2853</a>)</p>", "<p>Added missing bullet point for the <code class=\"docutils literal notranslate\"><span class=\"pre\">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code>\nsetting. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2756\">issue 2756</a>)</p>", "<p>Update Contributing docs, document new support channels\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2762\">issue 2762</a>, issue:<cite>3038</cite>)</p>", "<p>Include references to Scrapy subreddit in the docs</p>", "<p>Fix broken links; use <a class=\"reference external\" href=\"https://\">https://</a> for external links\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2978\">issue 2978</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2982\">issue 2982</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2958\">issue 2958</a>)</p>", "<p>Document CloseSpider extension better (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2759\">issue 2759</a>)</p>", "<p>Use <code class=\"docutils literal notranslate\"><span class=\"pre\">pymongo.collection.Collection.insert_one()</span></code> in MongoDB example\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2781\">issue 2781</a>)</p>", "<p>Spelling mistake and typos\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2828\">issue 2828</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2837\">issue 2837</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2884\">issue 2884</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2924\">issue 2924</a>)</p>", "<p>Clarify <code class=\"docutils literal notranslate\"><span class=\"pre\">CSVFeedSpider.headers</span></code> documentation (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2826\">issue 2826</a>)</p>", "<p>Document <code class=\"docutils literal notranslate\"><span class=\"pre\">DontCloseSpider</span></code> exception and clarify <code class=\"docutils literal notranslate\"><span class=\"pre\">spider_idle</span></code>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2791\">issue 2791</a>)</p>", "<p>Update “Releases” section in README (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2764\">issue 2764</a>)</p>", "<p>Fix rst syntax in <code class=\"docutils literal notranslate\"><span class=\"pre\">DOWNLOAD_FAIL_ON_DATALOSS</span></code> docs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2763\">issue 2763</a>)</p>", "<p>Small fix in description of startproject arguments (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2866\">issue 2866</a>)</p>", "<p>Clarify data types in Response.body docs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2922\">issue 2922</a>)</p>", "<p>Add a note about <code class=\"docutils literal notranslate\"><span class=\"pre\">request.meta['depth']</span></code> to DepthMiddleware docs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2374\">issue 2374</a>)</p>", "<p>Add a note about <code class=\"docutils literal notranslate\"><span class=\"pre\">request.meta['dont_merge_cookies']</span></code> to CookiesMiddleware\ndocs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2999\">issue 2999</a>)</p>", "<p>Up-to-date example of project structure (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2964\">issue 2964</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2976\">issue 2976</a>)</p>", "<p>A better example of ItemExporters usage (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2989\">issue 2989</a>)</p>", "<p>Document <code class=\"docutils literal notranslate\"><span class=\"pre\">from_crawler</span></code> methods for spider and downloader middlewares\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3019\">issue 3019</a>)</p>", "<p>Scrapy 1.4 does not bring that many breathtaking new features\nbut quite a few handy improvements nonetheless.</p>", "<p>Scrapy now supports anonymous FTP sessions with customizable user and\npassword via the new <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-FTP_USER\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FTP_USER</span></code></a> and <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-FTP_PASSWORD\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FTP_PASSWORD</span></code></a> settings.\nAnd if you’re using Twisted version 17.1.0 or above, FTP is now available\nwith Python 3.</p>", "<p>There’s a new <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.TextResponse.follow\" title=\"scrapy.http.TextResponse.follow\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">response.follow</span></code></a> method\nfor creating requests; <strong>it is now a recommended way to create Requests\nin Scrapy spiders</strong>. This method makes it easier to write correct\nspiders; <code class=\"docutils literal notranslate\"><span class=\"pre\">response.follow</span></code> has several advantages over creating\n<code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.Request</span></code> objects directly:</p>", "<p>it handles relative URLs;</p>", "<p>it works properly with non-ascii URLs on non-UTF8 pages;</p>", "<p>in addition to absolute and relative URLs it supports Selectors;\nfor <code class=\"docutils literal notranslate\"><span class=\"pre\">&lt;a&gt;</span></code> elements it can also extract their href values.</p>", "<p>For example, instead of this:</p>", "<p>One can now write this:</p>", "<p>Link extractors are also improved. They work similarly to what a regular\nmodern browser would do: leading and trailing whitespace are removed\nfrom attributes (think <code class=\"docutils literal notranslate\"><span class=\"pre\">href=\"</span>   <span class=\"pre\">http://example.com\"</span></code>) when building\n<code class=\"docutils literal notranslate\"><span class=\"pre\">Link</span></code> objects. This whitespace-stripping also happens for <code class=\"docutils literal notranslate\"><span class=\"pre\">action</span></code>\nattributes with <code class=\"docutils literal notranslate\"><span class=\"pre\">FormRequest</span></code>.</p>", "<p><strong>Please also note that link extractors do not canonicalize URLs by default\nanymore.</strong> This was puzzling users every now and then, and it’s not what\nbrowsers do in fact, so we removed that extra transformation on extracted\nlinks.</p>", "<p>For those of you wanting more control on the <code class=\"docutils literal notranslate\"><span class=\"pre\">Referer:</span></code> header that Scrapy\nsends when following links, you can set your own <code class=\"docutils literal notranslate\"><span class=\"pre\">Referrer</span> <span class=\"pre\">Policy</span></code>.\nPrior to Scrapy 1.4, the default <code class=\"docutils literal notranslate\"><span class=\"pre\">RefererMiddleware</span></code> would simply and\nblindly set it to the URL of the response that generated the HTTP request\n(which could leak information on your URL seeds).\nBy default, Scrapy now behaves much like your regular browser does.\nAnd this policy is fully customizable with W3C standard values\n(or with something really custom of your own if you wish).\nSee <a class=\"hoverxref tooltip reference internal\" href=\"topics/spider-middleware.html#std-setting-REFERRER_POLICY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">REFERRER_POLICY</span></code></a> for details.</p>", "<p>To make Scrapy spiders easier to debug, Scrapy logs more stats by default\nin 1.4: memory usage stats, detailed retry stats, detailed HTTP error code\nstats. A similar change is that HTTP cache path is also visible in logs now.</p>", "<p>Last but not least, Scrapy now has the option to make JSON and XML items\nmore human-readable, with newlines between items and even custom indenting\noffset, using the new <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEED_EXPORT_INDENT\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEED_EXPORT_INDENT</span></code></a> setting.</p>", "<p>Enjoy! (Or read on for the rest of changes in this release.)</p>", "<p>Default to <code class=\"docutils literal notranslate\"><span class=\"pre\">canonicalize=False</span></code> in\n<a class=\"reference internal\" href=\"topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\" title=\"scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.linkextractors.LinkExtractor</span></code></a>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2537\">issue 2537</a>, fixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1941\">issue 1941</a> and <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1982\">issue 1982</a>):\n<strong>warning, this is technically backward-incompatible</strong></p>", "<p>Enable memusage extension by default (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2539\">issue 2539</a>, fixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2187\">issue 2187</a>);\n<strong>this is technically backward-incompatible</strong> so please check if you have\nany non-default <code class=\"docutils literal notranslate\"><span class=\"pre\">MEMUSAGE_***</span></code> options set.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">EDITOR</span></code> environment variable now takes precedence over <code class=\"docutils literal notranslate\"><span class=\"pre\">EDITOR</span></code>\noption defined in settings.py (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1829\">issue 1829</a>); Scrapy default settings\nno longer depend on environment variables. <strong>This is technically a backward\nincompatible change</strong>.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">Spider.make_requests_from_url</span></code> is deprecated\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1728\">issue 1728</a>, fixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1495\">issue 1495</a>).</p>", "<p>Accept proxy credentials in <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-proxy\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">proxy</span></code></a> request meta key (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2526\">issue 2526</a>)</p>", "<p>Support <a class=\"reference external\" href=\"https://www.ietf.org/rfc/rfc7932.txt\">brotli-compressed</a> content; requires optional <a class=\"reference external\" href=\"https://github.com/python-hyper/brotlipy/\">brotlipy</a>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2535\">issue 2535</a>)</p>", "<p>New <a class=\"hoverxref tooltip reference internal\" href=\"intro/tutorial.html#response-follow-example\"><span class=\"std std-ref\">response.follow</span></a> shortcut\nfor creating requests (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1940\">issue 1940</a>)</p>", "<p>Added <code class=\"docutils literal notranslate\"><span class=\"pre\">flags</span></code> argument and attribute to <a class=\"reference internal\" href=\"topics/request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a>\nobjects (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2047\">issue 2047</a>)</p>", "<p>Support Anonymous FTP (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2342\">issue 2342</a>)</p>", "<p>Added <code class=\"docutils literal notranslate\"><span class=\"pre\">retry/count</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">retry/max_reached</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">retry/reason_count/&lt;reason&gt;</span></code>\nstats to <a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.retry.RetryMiddleware\" title=\"scrapy.downloadermiddlewares.retry.RetryMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RetryMiddleware</span></code></a>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2543\">issue 2543</a>)</p>", "<p>Added <code class=\"docutils literal notranslate\"><span class=\"pre\">httperror/response_ignored_count</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">httperror/response_ignored_status_count/&lt;status&gt;</span></code>\nstats to <a class=\"reference internal\" href=\"topics/spider-middleware.html#scrapy.spidermiddlewares.httperror.HttpErrorMiddleware\" title=\"scrapy.spidermiddlewares.httperror.HttpErrorMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">HttpErrorMiddleware</span></code></a>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2566\">issue 2566</a>)</p>", "<p>Customizable <a class=\"hoverxref tooltip reference internal\" href=\"topics/spider-middleware.html#std-setting-REFERRER_POLICY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">Referrer</span> <span class=\"pre\">policy</span></code></a> in\n<a class=\"reference internal\" href=\"topics/spider-middleware.html#scrapy.spidermiddlewares.referer.RefererMiddleware\" title=\"scrapy.spidermiddlewares.referer.RefererMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RefererMiddleware</span></code></a>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2306\">issue 2306</a>)</p>", "<p>New <code class=\"docutils literal notranslate\"><span class=\"pre\">data:</span></code> URI download handler (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2334\">issue 2334</a>, fixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2156\">issue 2156</a>)</p>", "<p>Log cache directory when HTTP Cache is used (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2611\">issue 2611</a>, fixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2604\">issue 2604</a>)</p>", "<p>Warn users when project contains duplicate spider names (fixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2181\">issue 2181</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.datatypes.CaselessDict</span></code> now accepts <code class=\"docutils literal notranslate\"><span class=\"pre\">Mapping</span></code> instances and\nnot only dicts (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2646\">issue 2646</a>)</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#topics-media-pipeline\"><span class=\"std std-ref\">Media downloads</span></a>, with\n<a class=\"reference internal\" href=\"topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline\" title=\"scrapy.pipelines.files.FilesPipeline\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">FilesPipeline</span></code></a> or\n<a class=\"reference internal\" href=\"topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline\" title=\"scrapy.pipelines.images.ImagesPipeline\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ImagesPipeline</span></code></a>, can now optionally handle\nHTTP redirects using the new <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#std-setting-MEDIA_ALLOW_REDIRECTS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">MEDIA_ALLOW_REDIRECTS</span></code></a> setting\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2616\">issue 2616</a>, fixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2004\">issue 2004</a>)</p>", "<p>Accept non-complete responses from websites using a new\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DOWNLOAD_FAIL_ON_DATALOSS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOAD_FAIL_ON_DATALOSS</span></code></a> setting (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2590\">issue 2590</a>, fixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2586\">issue 2586</a>)</p>", "<p>Optional pretty-printing of JSON and XML items via\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEED_EXPORT_INDENT\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEED_EXPORT_INDENT</span></code></a> setting (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2456\">issue 2456</a>, fixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1327\">issue 1327</a>)</p>", "<p>Allow dropping fields in <code class=\"docutils literal notranslate\"><span class=\"pre\">FormRequest.from_response</span></code> formdata when\n<code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> value is passed (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/667\">issue 667</a>)</p>", "<p>Per-request retry times with the new <a class=\"hoverxref tooltip reference internal\" href=\"topics/request-response.html#std-reqmeta-max_retry_times\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">max_retry_times</span></code></a> meta key\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2642\">issue 2642</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">python</span> <span class=\"pre\">-m</span> <span class=\"pre\">scrapy</span></code> as a more explicit alternative to <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span></code> command\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2740\">issue 2740</a>)</p>", "<p>LinkExtractor now strips leading and trailing whitespaces from attributes\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2547\">issue 2547</a>, fixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1614\">issue 1614</a>)</p>", "<p>Properly handle whitespaces in action attribute in\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">FormRequest</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2548\">issue 2548</a>)</p>", "<p>Buffer CONNECT response bytes from proxy until all HTTP headers are received\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2495\">issue 2495</a>, fixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2491\">issue 2491</a>)</p>", "<p>FTP downloader now works on Python 3, provided you use Twisted&gt;=17.1\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2599\">issue 2599</a>)</p>", "<p>Use body to choose response type after decompressing content (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2393\">issue 2393</a>,\nfixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2145\">issue 2145</a>)</p>", "<p>Always decompress <code class=\"docutils literal notranslate\"><span class=\"pre\">Content-Encoding:</span> <span class=\"pre\">gzip</span></code> at <a class=\"reference internal\" href=\"topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware\" title=\"scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">HttpCompressionMiddleware</span></code></a> stage (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2391\">issue 2391</a>)</p>", "<p>Respect custom log level in <code class=\"docutils literal notranslate\"><span class=\"pre\">Spider.custom_settings</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2581\">issue 2581</a>,\nfixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1612\">issue 1612</a>)</p>", "<p>‘make htmlview’ fix for macOS (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2661\">issue 2661</a>)</p>", "<p>Remove “commands” from the command list  (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2695\">issue 2695</a>)</p>", "<p>Fix duplicate Content-Length header for POST requests with empty body (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2677\">issue 2677</a>)</p>", "<p>Properly cancel large downloads, i.e. above <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DOWNLOAD_MAXSIZE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOAD_MAXSIZE</span></code></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1616\">issue 1616</a>)</p>", "<p>ImagesPipeline: fixed processing of transparent PNG images with palette\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2675\">issue 2675</a>)</p>", "<p>Tests: remove temp files and folders (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2570\">issue 2570</a>),\nfixed ProjectUtilsTest on macOS (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2569\">issue 2569</a>),\nuse portable pypy for Linux on Travis CI (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2710\">issue 2710</a>)</p>", "<p>Separate building request from <code class=\"docutils literal notranslate\"><span class=\"pre\">_requests_to_follow</span></code> in CrawlSpider (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2562\">issue 2562</a>)</p>", "<p>Remove “Python 3 progress” badge (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2567\">issue 2567</a>)</p>", "<p>Add a couple more lines to <code class=\"docutils literal notranslate\"><span class=\"pre\">.gitignore</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2557\">issue 2557</a>)</p>", "<p>Remove bumpversion prerelease configuration (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2159\">issue 2159</a>)</p>", "<p>Add codecov.yml file (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2750\">issue 2750</a>)</p>", "<p>Set context factory implementation based on Twisted version (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2577\">issue 2577</a>,\nfixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2560\">issue 2560</a>)</p>", "<p>Add omitted <code class=\"docutils literal notranslate\"><span class=\"pre\">self</span></code> arguments in default project middleware template (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2595\">issue 2595</a>)</p>", "<p>Remove redundant <code class=\"docutils literal notranslate\"><span class=\"pre\">slot.add_request()</span></code> call in ExecutionEngine (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2617\">issue 2617</a>)</p>", "<p>Catch more specific <code class=\"docutils literal notranslate\"><span class=\"pre\">os.error</span></code> exception in\n<code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.pipelines.files.FSFilesStore</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2644\">issue 2644</a>)</p>", "<p>Change “localhost” test server certificate (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2720\">issue 2720</a>)</p>", "<p>Remove unused <code class=\"docutils literal notranslate\"><span class=\"pre\">MEMUSAGE_REPORT</span></code> setting (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2576\">issue 2576</a>)</p>", "<p>Binary mode is required for exporters (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2564\">issue 2564</a>, fixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2553\">issue 2553</a>)</p>", "<p>Mention issue with <code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">FormRequest.from_response</span></code> due to bug in lxml (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2572\">issue 2572</a>)</p>", "<p>Use single quotes uniformly in templates (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2596\">issue 2596</a>)</p>", "<p>Document <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-reqmeta-ftp_user\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">ftp_user</span></code></a> and <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-reqmeta-ftp_password\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">ftp_password</span></code></a> meta keys (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2587\">issue 2587</a>)</p>", "<p>Removed section on deprecated <code class=\"docutils literal notranslate\"><span class=\"pre\">contrib/</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2636\">issue 2636</a>)</p>", "<p>Recommend Anaconda when installing Scrapy on Windows\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2477\">issue 2477</a>, fixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2475\">issue 2475</a>)</p>", "<p>FAQ: rewrite note on Python 3 support on Windows (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2690\">issue 2690</a>)</p>", "<p>Rearrange selector sections (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2705\">issue 2705</a>)</p>", "<p>Remove <code class=\"docutils literal notranslate\"><span class=\"pre\">__nonzero__</span></code> from <a class=\"reference internal\" href=\"topics/selectors.html#scrapy.selector.SelectorList\" title=\"scrapy.selector.SelectorList\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">SelectorList</span></code></a>\ndocs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2683\">issue 2683</a>)</p>", "<p>Mention how to disable request filtering in documentation of\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DUPEFILTER_CLASS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DUPEFILTER_CLASS</span></code></a> setting (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2714\">issue 2714</a>)</p>", "<p>Add sphinx_rtd_theme to docs setup readme (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2668\">issue 2668</a>)</p>", "<p>Open file in text mode in JSON item writer example (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2729\">issue 2729</a>)</p>", "<p>Clarify <code class=\"docutils literal notranslate\"><span class=\"pre\">allowed_domains</span></code> example (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2670\">issue 2670</a>)</p>", "<p>Make <code class=\"docutils literal notranslate\"><span class=\"pre\">SpiderLoader</span></code> raise <code class=\"docutils literal notranslate\"><span class=\"pre\">ImportError</span></code> again by default for missing\ndependencies and wrong <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-SPIDER_MODULES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SPIDER_MODULES</span></code></a>.\nThese exceptions were silenced as warnings since 1.3.0.\nA new setting is introduced to toggle between warning or exception if needed ;\nsee <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-SPIDER_LOADER_WARN_ONLY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SPIDER_LOADER_WARN_ONLY</span></code></a> for details.</p>", "<p>Preserve request class when converting to/from dicts (utils.reqser) (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2510\">issue 2510</a>).</p>", "<p>Use consistent selectors for author field in tutorial (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2551\">issue 2551</a>).</p>", "<p>Fix TLS compatibility in Twisted 17+ (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2558\">issue 2558</a>)</p>", "<p>Support <code class=\"docutils literal notranslate\"><span class=\"pre\">'True'</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">'False'</span></code> string values for boolean settings (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2519\">issue 2519</a>);\nyou can now do something like <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">crawl</span> <span class=\"pre\">myspider</span> <span class=\"pre\">-s</span> <span class=\"pre\">REDIRECT_ENABLED=False</span></code>.</p>", "<p>Support kwargs with <code class=\"docutils literal notranslate\"><span class=\"pre\">response.xpath()</span></code> to use <a class=\"hoverxref tooltip reference internal\" href=\"topics/selectors.html#topics-selectors-xpath-variables\"><span class=\"std std-ref\">XPath variables</span></a>\nand ad-hoc namespaces declarations ;\nthis requires at least Parsel v1.1 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2457\">issue 2457</a>).</p>", "<p>Add support for Python 3.6 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2485\">issue 2485</a>).</p>", "<p>Run tests on PyPy (warning: some tests still fail, so PyPy is not supported yet).</p>", "<p>Enforce <code class=\"docutils literal notranslate\"><span class=\"pre\">DNS_TIMEOUT</span></code> setting (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2496\">issue 2496</a>).</p>", "<p>Fix <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-view\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">view</span></code></a> command ; it was a regression in v1.3.0 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2503\">issue 2503</a>).</p>", "<p>Fix tests regarding <code class=\"docutils literal notranslate\"><span class=\"pre\">*_EXPIRES</span> <span class=\"pre\">settings</span></code> with Files/Images pipelines (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2460\">issue 2460</a>).</p>", "<p>Fix name of generated pipeline class when using basic project template (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2466\">issue 2466</a>).</p>", "<p>Fix compatibility with Twisted 17+ (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2496\">issue 2496</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2528\">issue 2528</a>).</p>", "<p>Fix <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.Item</span></code> inheritance on Python 3.6 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2511\">issue 2511</a>).</p>", "<p>Enforce numeric values for components order in <code class=\"docutils literal notranslate\"><span class=\"pre\">SPIDER_MIDDLEWARES</span></code>,\n<code class=\"docutils literal notranslate\"><span class=\"pre\">DOWNLOADER_MIDDLEWARES</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">EXTENSIONS</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">SPIDER_CONTRACTS</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2420\">issue 2420</a>).</p>", "<p>Reword Code of Conduct section and upgrade to Contributor Covenant v1.4\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2469\">issue 2469</a>).</p>", "<p>Clarify that passing spider arguments converts them to spider attributes\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2483\">issue 2483</a>).</p>", "<p>Document <code class=\"docutils literal notranslate\"><span class=\"pre\">formid</span></code> argument on <code class=\"docutils literal notranslate\"><span class=\"pre\">FormRequest.from_response()</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2497\">issue 2497</a>).</p>", "<p>Add .rst extension to README files (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2507\">issue 2507</a>).</p>", "<p>Mention LevelDB cache storage backend (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2525\">issue 2525</a>).</p>", "<p>Use <code class=\"docutils literal notranslate\"><span class=\"pre\">yield</span></code> in sample callback code (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2533\">issue 2533</a>).</p>", "<p>Add note about HTML entities decoding with <code class=\"docutils literal notranslate\"><span class=\"pre\">.re()/.re_first()</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1704\">issue 1704</a>).</p>", "<p>Typos (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2512\">issue 2512</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2534\">issue 2534</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2531\">issue 2531</a>).</p>", "<p>Remove redundant check in <code class=\"docutils literal notranslate\"><span class=\"pre\">MetaRefreshMiddleware</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2542\">issue 2542</a>).</p>", "<p>Faster checks in <code class=\"docutils literal notranslate\"><span class=\"pre\">LinkExtractor</span></code> for allow/deny patterns (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2538\">issue 2538</a>).</p>", "<p>Remove dead code supporting old Twisted versions (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2544\">issue 2544</a>).</p>", "<p>This release comes rather soon after 1.2.2 for one main reason:\nit was found out that releases since 0.18 up to 1.2.2 (included) use\nsome backported code from Twisted (<code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.xlib.tx.*</span></code>),\neven if newer Twisted modules are available.\nScrapy now uses <code class=\"docutils literal notranslate\"><span class=\"pre\">twisted.web.client</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">twisted.internet.endpoints</span></code> directly.\n(See also cleanups below.)</p>", "<p>As it is a major change, we wanted to get the bug fix out quickly\nwhile not breaking any projects using the 1.2 series.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">MailSender</span></code> now accepts single strings as values for <code class=\"docutils literal notranslate\"><span class=\"pre\">to</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">cc</span></code>\narguments (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2272\">issue 2272</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">fetch</span> <span class=\"pre\">url</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">shell</span> <span class=\"pre\">url</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">fetch(url)</span></code> inside\nScrapy shell now follow HTTP redirections by default (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2290\">issue 2290</a>);\nSee <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-fetch\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">fetch</span></code></a> and <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-shell\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">shell</span></code></a> for details.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">HttpErrorMiddleware</span></code> now logs errors with <code class=\"docutils literal notranslate\"><span class=\"pre\">INFO</span></code> level instead of <code class=\"docutils literal notranslate\"><span class=\"pre\">DEBUG</span></code>;\nthis is technically <strong>backward incompatible</strong> so please check your log parsers.</p>", "<p>By default, logger names now use a long-form path, e.g. <code class=\"docutils literal notranslate\"><span class=\"pre\">[scrapy.extensions.logstats]</span></code>,\ninstead of the shorter “top-level” variant of prior releases (e.g. <code class=\"docutils literal notranslate\"><span class=\"pre\">[scrapy]</span></code>);\nthis is <strong>backward incompatible</strong> if you have log parsers expecting the short\nlogger name part. You can switch back to short logger names using <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-LOG_SHORT_NAMES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">LOG_SHORT_NAMES</span></code></a>\nset to <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code>.</p>", "<p>Scrapy now requires Twisted &gt;= 13.1 which is the case for many Linux\ndistributions already.</p>", "<p>As a consequence, we got rid of <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.xlib.tx.*</span></code> modules, which\ncopied some of Twisted code for users stuck with an “old” Twisted version</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">ChunkedTransferMiddleware</span></code> is deprecated and removed from the default\ndownloader middlewares.</p>", "<p>Packaging fix: disallow unsupported Twisted versions in setup.py</p>", "<p>Fix a cryptic traceback when a pipeline fails on <code class=\"docutils literal notranslate\"><span class=\"pre\">open_spider()</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2011\">issue 2011</a>)</p>", "<p>Fix embedded IPython shell variables (fixing <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/396\">issue 396</a> that re-appeared\nin 1.2.0, fixed in <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2418\">issue 2418</a>)</p>", "<p>A couple of patches when dealing with robots.txt:</p>", "<p>handle (non-standard) relative sitemap URLs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2390\">issue 2390</a>)</p>", "<p>handle non-ASCII URLs and User-Agents in Python 2 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2373\">issue 2373</a>)</p>", "<p>Document <code class=\"docutils literal notranslate\"><span class=\"pre\">\"download_latency\"</span></code> key in <code class=\"docutils literal notranslate\"><span class=\"pre\">Request</span></code>’s <code class=\"docutils literal notranslate\"><span class=\"pre\">meta</span></code> dict (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2033\">issue 2033</a>)</p>", "<p>Remove page on (deprecated &amp; unsupported) Ubuntu packages from ToC (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2335\">issue 2335</a>)</p>", "<p>A few fixed typos (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2346\">issue 2346</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2369\">issue 2369</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2369\">issue 2369</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2380\">issue 2380</a>)\nand clarifications (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2354\">issue 2354</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2325\">issue 2325</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2414\">issue 2414</a>)</p>", "<p>Advertize <a class=\"reference external\" href=\"https://anaconda.org/conda-forge/scrapy\">conda-forge</a> as Scrapy’s official conda channel (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2387\">issue 2387</a>)</p>", "<p>More helpful error messages when trying to use <code class=\"docutils literal notranslate\"><span class=\"pre\">.css()</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">.xpath()</span></code>\non non-Text Responses (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2264\">issue 2264</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">startproject</span></code> command now generates a sample <code class=\"docutils literal notranslate\"><span class=\"pre\">middlewares.py</span></code> file (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2335\">issue 2335</a>)</p>", "<p>Add more dependencies’ version info in <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">version</span></code> verbose output (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2404\">issue 2404</a>)</p>", "<p>Remove all <code class=\"docutils literal notranslate\"><span class=\"pre\">*.pyc</span></code> files from source distribution (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2386\">issue 2386</a>)</p>", "<p>Include OpenSSL’s more permissive default ciphers when establishing\nTLS/SSL connections (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2314\">issue 2314</a>).</p>", "<p>Fix “Location” HTTP header decoding on non-ASCII URL redirects (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2321\">issue 2321</a>).</p>", "<p>Fix JsonWriterPipeline example (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2302\">issue 2302</a>).</p>", "<p>Various notes: <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2330\">issue 2330</a> on spider names,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2329\">issue 2329</a> on middleware methods processing order,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2327\">issue 2327</a> on getting multi-valued HTTP headers as lists.</p>", "<p>Removed <code class=\"docutils literal notranslate\"><span class=\"pre\">www.</span></code> from <code class=\"docutils literal notranslate\"><span class=\"pre\">start_urls</span></code> in built-in spider templates (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2299\">issue 2299</a>).</p>", "<p>New <a class=\"hoverxref tooltip reference internal\" href=\"topics/feed-exports.html#std-setting-FEED_EXPORT_ENCODING\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEED_EXPORT_ENCODING</span></code></a> setting to customize the encoding\nused when writing items to a file.\nThis can be used to turn off <code class=\"docutils literal notranslate\"><span class=\"pre\">\\uXXXX</span></code> escapes in JSON output.\nThis is also useful for those wanting something else than UTF-8\nfor XML or CSV output (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2034\">issue 2034</a>).</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">startproject</span></code> command now supports an optional destination directory\nto override the default one based on the project name (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2005\">issue 2005</a>).</p>", "<p>New <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-SCHEDULER_DEBUG\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SCHEDULER_DEBUG</span></code></a> setting to log requests serialization\nfailures (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1610\">issue 1610</a>).</p>", "<p>JSON encoder now supports serialization of <code class=\"docutils literal notranslate\"><span class=\"pre\">set</span></code> instances (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2058\">issue 2058</a>).</p>", "<p>Interpret <code class=\"docutils literal notranslate\"><span class=\"pre\">application/json-amazonui-streaming</span></code> as <code class=\"docutils literal notranslate\"><span class=\"pre\">TextResponse</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1503\">issue 1503</a>).</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span></code> is imported by default when using shell tools (<a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-shell\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">shell</span></code></a>,\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/shell.html#topics-shell-inspect-response\"><span class=\"std std-ref\">inspect_response</span></a>) (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2248\">issue 2248</a>).</p>", "<p>DefaultRequestHeaders middleware now runs before UserAgent middleware\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2088\">issue 2088</a>). <strong>Warning: this is technically backward incompatible</strong>,\nthough we consider this a bug fix.</p>", "<p>HTTP cache extension and plugins that use the <code class=\"docutils literal notranslate\"><span class=\"pre\">.scrapy</span></code> data directory now\nwork outside projects (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1581\">issue 1581</a>).  <strong>Warning: this is technically\nbackward incompatible</strong>, though we consider this a bug fix.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">Selector</span></code> does not allow passing both <code class=\"docutils literal notranslate\"><span class=\"pre\">response</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">text</span></code> anymore\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2153\">issue 2153</a>).</p>", "<p>Fixed logging of wrong callback name with <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">parse</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2169\">issue 2169</a>).</p>", "<p>Fix for an odd gzip decompression bug (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1606\">issue 1606</a>).</p>", "<p>Fix for selected callbacks when using <code class=\"docutils literal notranslate\"><span class=\"pre\">CrawlSpider</span></code> with <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-parse\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">parse</span></code></a>\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2225\">issue 2225</a>).</p>", "<p>Fix for invalid JSON and XML files when spider yields no items (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/872\">issue 872</a>).</p>", "<p>Implement <code class=\"docutils literal notranslate\"><span class=\"pre\">flush()</span></code> for <code class=\"docutils literal notranslate\"><span class=\"pre\">StreamLogger</span></code> avoiding a warning in logs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2125\">issue 2125</a>).</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">canonicalize_url</span></code> has been moved to <a class=\"reference external\" href=\"https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.url.canonicalize_url\">w3lib.url</a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2168\">issue 2168</a>).</p>", "<p>Scrapy’s new requirements baseline is Debian 8 “Jessie”. It was previously\nUbuntu 12.04 Precise.\nWhat this means in practice is that we run continuous integration tests\nwith these (main) packages versions at a minimum:\nTwisted 14.0, pyOpenSSL 0.14, lxml 3.4.</p>", "<p>Scrapy may very well work with older versions of these packages\n(the code base still has switches for older Twisted versions for example)\nbut it is not guaranteed (because it’s not tested anymore).</p>", "<p>Grammar fixes: <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2128\">issue 2128</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1566\">issue 1566</a>.</p>", "<p>Download stats badge removed from README (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2160\">issue 2160</a>).</p>", "<p>New Scrapy <a class=\"hoverxref tooltip reference internal\" href=\"topics/architecture.html#topics-architecture\"><span class=\"std std-ref\">architecture diagram</span></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2165\">issue 2165</a>).</p>", "<p>Updated <code class=\"docutils literal notranslate\"><span class=\"pre\">Response</span></code> parameters documentation (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2197\">issue 2197</a>).</p>", "<p>Reworded misleading <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-RANDOMIZE_DOWNLOAD_DELAY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">RANDOMIZE_DOWNLOAD_DELAY</span></code></a> description (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2190\">issue 2190</a>).</p>", "<p>Add StackOverflow as a support channel (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2257\">issue 2257</a>).</p>", "<p>Packaging fix: disallow unsupported Twisted versions in setup.py</p>", "<p>Class attributes for subclasses of <code class=\"docutils literal notranslate\"><span class=\"pre\">ImagesPipeline</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">FilesPipeline</span></code>\nwork as they did before 1.1.1 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2243\">issue 2243</a>, fixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2198\">issue 2198</a>)</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"intro/overview.html#intro-overview\"><span class=\"std std-ref\">Overview</span></a> and <a class=\"hoverxref tooltip reference internal\" href=\"intro/tutorial.html#intro-tutorial\"><span class=\"std std-ref\">tutorial</span></a>\nrewritten to use <a class=\"reference external\" href=\"http://toscrape.com\">http://toscrape.com</a> websites\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2236\">issue 2236</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2249\">issue 2249</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2252\">issue 2252</a>).</p>", "<p>Introduce a missing <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#std-setting-IMAGES_STORE_S3_ACL\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">IMAGES_STORE_S3_ACL</span></code></a> setting to override\nthe default ACL policy in <code class=\"docutils literal notranslate\"><span class=\"pre\">ImagesPipeline</span></code> when uploading images to S3\n(note that default ACL policy is “private” – instead of “public-read” –\nsince Scrapy 1.1.0)</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#std-setting-IMAGES_EXPIRES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">IMAGES_EXPIRES</span></code></a> default value set back to 90\n(the regression was introduced in 1.1.1)</p>", "<p>Add “Host” header in CONNECT requests to HTTPS proxies (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2069\">issue 2069</a>)</p>", "<p>Use response <code class=\"docutils literal notranslate\"><span class=\"pre\">body</span></code> when choosing response class\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2001\">issue 2001</a>, fixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2000\">issue 2000</a>)</p>", "<p>Do not fail on canonicalizing URLs with wrong netlocs\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2038\">issue 2038</a>, fixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2010\">issue 2010</a>)</p>", "<p>a few fixes for <code class=\"docutils literal notranslate\"><span class=\"pre\">HttpCompressionMiddleware</span></code> (and <code class=\"docutils literal notranslate\"><span class=\"pre\">SitemapSpider</span></code>):</p>", "<p>Do not decode HEAD responses (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2008\">issue 2008</a>, fixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1899\">issue 1899</a>)</p>", "<p>Handle charset parameter in gzip Content-Type header\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2050\">issue 2050</a>, fixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2049\">issue 2049</a>)</p>", "<p>Do not decompress gzip octet-stream responses\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2065\">issue 2065</a>, fixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2063\">issue 2063</a>)</p>", "<p>Catch (and ignore with a warning) exception when verifying certificate\nagainst IP-address hosts (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2094\">issue 2094</a>, fixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2092\">issue 2092</a>)</p>", "<p>Make <code class=\"docutils literal notranslate\"><span class=\"pre\">FilesPipeline</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">ImagesPipeline</span></code> backward compatible again\nregarding the use of legacy class attributes for customization\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1989\">issue 1989</a>, fixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1985\">issue 1985</a>)</p>", "<p>Enable genspider command outside project folder (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2052\">issue 2052</a>)</p>", "<p>Retry HTTPS CONNECT <code class=\"docutils literal notranslate\"><span class=\"pre\">TunnelError</span></code> by default (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1974\">issue 1974</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">FEED_TEMPDIR</span></code> setting at lexicographical position (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/9b3c72c\">commit 9b3c72c</a>)</p>", "<p>Use idiomatic <code class=\"docutils literal notranslate\"><span class=\"pre\">.extract_first()</span></code> in overview (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1994\">issue 1994</a>)</p>", "<p>Update years in copyright notice (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/c2c8036\">commit c2c8036</a>)</p>", "<p>Add information and example on errbacks (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1995\">issue 1995</a>)</p>", "<p>Use “url” variable in downloader middleware example (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2015\">issue 2015</a>)</p>", "<p>Grammar fixes (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2054\">issue 2054</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2120\">issue 2120</a>)</p>", "<p>New FAQ entry on using BeautifulSoup in spider callbacks (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2048\">issue 2048</a>)</p>", "<p>Add notes about Scrapy not working on Windows with Python 3 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2060\">issue 2060</a>)</p>", "<p>Encourage complete titles in pull requests (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2026\">issue 2026</a>)</p>", "<p>Upgrade py.test requirement on Travis CI and Pin pytest-cov to 2.2.1 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/2095\">issue 2095</a>)</p>", "<p>This 1.1 release brings a lot of interesting features and bug fixes:</p>", "<p>Scrapy 1.1 has beta Python 3 support (requires Twisted &gt;= 15.5). See\n<a class=\"hoverxref tooltip reference internal\" href=\"#news-betapy3\"><span class=\"std std-ref\">Beta Python 3 Support</span></a> for more details and some limitations.</p>", "<p>Hot new features:</p>", "<p>Item loaders now support nested loaders (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1467\">issue 1467</a>).</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">FormRequest.from_response</span></code> improvements (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1382\">issue 1382</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1137\">issue 1137</a>).</p>", "<p>Added setting <a class=\"hoverxref tooltip reference internal\" href=\"topics/autothrottle.html#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code></a> and improved\nAutoThrottle docs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1324\">issue 1324</a>).</p>", "<p>Added <code class=\"docutils literal notranslate\"><span class=\"pre\">response.text</span></code> to get body as unicode (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1730\">issue 1730</a>).</p>", "<p>Anonymous S3 connections (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1358\">issue 1358</a>).</p>", "<p>Deferreds in downloader middlewares (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1473\">issue 1473</a>). This enables better\nrobots.txt handling (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1471\">issue 1471</a>).</p>", "<p>HTTP caching now follows RFC2616 more closely, added settings\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-setting-HTTPCACHE_ALWAYS_STORE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">HTTPCACHE_ALWAYS_STORE</span></code></a> and\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-setting-HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS</span></code></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1151\">issue 1151</a>).</p>", "<p>Selectors were extracted to the <a class=\"reference external\" href=\"https://github.com/scrapy/parsel\">parsel</a> library (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1409\">issue 1409</a>). This means\nyou can use Scrapy Selectors without Scrapy and also upgrade the\nselectors engine without needing to upgrade Scrapy.</p>", "<p>HTTPS downloader now does TLS protocol negotiation by default,\ninstead of forcing TLS 1.0. You can also set the SSL/TLS method\nusing the new <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DOWNLOADER_CLIENT_TLS_METHOD\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOADER_CLIENT_TLS_METHOD</span></code></a>.</p>", "<p>These bug fixes may require your attention:</p>", "<p>Don’t retry bad requests (HTTP 400) by default (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1289\">issue 1289</a>).\nIf you need the old behavior, add <code class=\"docutils literal notranslate\"><span class=\"pre\">400</span></code> to <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-setting-RETRY_HTTP_CODES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">RETRY_HTTP_CODES</span></code></a>.</p>", "<p>Fix shell files argument handling (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1710\">issue 1710</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1550\">issue 1550</a>).\nIf you try <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">shell</span> <span class=\"pre\">index.html</span></code> it will try to load the URL <a class=\"reference external\" href=\"http://index.html\">http://index.html</a>,\nuse <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">shell</span> <span class=\"pre\">./index.html</span></code> to load a local file.</p>", "<p>Robots.txt compliance is now enabled by default for newly-created projects\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1724\">issue 1724</a>). Scrapy will also wait for robots.txt to be downloaded\nbefore proceeding with the crawl (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1735\">issue 1735</a>). If you want to disable\nthis behavior, update <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-ROBOTSTXT_OBEY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">ROBOTSTXT_OBEY</span></code></a> in <code class=\"docutils literal notranslate\"><span class=\"pre\">settings.py</span></code> file\nafter creating a new project.</p>", "<p>Exporters now work on unicode, instead of bytes by default (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1080\">issue 1080</a>).\nIf you use <a class=\"reference internal\" href=\"topics/exporters.html#scrapy.exporters.PythonItemExporter\" title=\"scrapy.exporters.PythonItemExporter\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">PythonItemExporter</span></code></a>, you may want to\nupdate your code to disable binary mode which is now deprecated.</p>", "<p>Accept XML node names containing dots as valid (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1533\">issue 1533</a>).</p>", "<p>When uploading files or images to S3 (with <code class=\"docutils literal notranslate\"><span class=\"pre\">FilesPipeline</span></code> or\n<code class=\"docutils literal notranslate\"><span class=\"pre\">ImagesPipeline</span></code>), the default ACL policy is now “private” instead\nof “public” <strong>Warning: backward incompatible!</strong>.\nYou can use <a class=\"hoverxref tooltip reference internal\" href=\"topics/media-pipeline.html#std-setting-FILES_STORE_S3_ACL\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FILES_STORE_S3_ACL</span></code></a> to change it.</p>", "<p>We’ve reimplemented <code class=\"docutils literal notranslate\"><span class=\"pre\">canonicalize_url()</span></code> for more correct output,\nespecially for URLs with non-ASCII characters (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1947\">issue 1947</a>).\nThis could change link extractors output compared to previous Scrapy versions.\nThis may also invalidate some cache entries you could still have from pre-1.1 runs.\n<strong>Warning: backward incompatible!</strong>.</p>", "<p>Keep reading for more details on other improvements and bug fixes.</p>", "<p>We have been <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/wiki/Python-3-Porting\">hard at work to make Scrapy run on Python 3</a>. As a result, now\nyou can run spiders on Python 3.3, 3.4 and 3.5 (Twisted &gt;= 15.5 required). Some\nfeatures are still missing (and some may never be ported).</p>", "<p>Almost all builtin extensions/middlewares are expected to work.\nHowever, we are aware of some limitations in Python 3:</p>", "<p>Scrapy does not work on Windows with Python 3</p>", "<p>Sending emails is not supported</p>", "<p>FTP download handler is not supported</p>", "<p>Telnet console is not supported</p>", "<p>Scrapy now has a <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\">Code of Conduct</a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1681\">issue 1681</a>).</p>", "<p>Command line tool now has completion for zsh (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/934\">issue 934</a>).</p>", "<p>Improvements to <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">shell</span></code>:</p>", "<p>Support for bpython and configure preferred Python shell via\n<code class=\"docutils literal notranslate\"><span class=\"pre\">SCRAPY_PYTHON_SHELL</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1100\">issue 1100</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1444\">issue 1444</a>).</p>", "<p>Support URLs without scheme (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1498\">issue 1498</a>)\n<strong>Warning: backward incompatible!</strong></p>", "<p>Bring back support for relative file path (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1710\">issue 1710</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1550\">issue 1550</a>).</p>", "<p>Added <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-MEMUSAGE_CHECK_INTERVAL_SECONDS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">MEMUSAGE_CHECK_INTERVAL_SECONDS</span></code></a> setting to change default check\ninterval (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1282\">issue 1282</a>).</p>", "<p>Download handlers are now lazy-loaded on first request using their\nscheme (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1390\">issue 1390</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1421\">issue 1421</a>).</p>", "<p>HTTPS download handlers do not force TLS 1.0 anymore; instead,\nOpenSSL’s <code class=\"docutils literal notranslate\"><span class=\"pre\">SSLv23_method()/TLS_method()</span></code> is used allowing to try\nnegotiating with the remote hosts the highest TLS protocol version\nit can (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1794\">issue 1794</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1629\">issue 1629</a>).</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">RedirectMiddleware</span></code> now skips the status codes from\n<code class=\"docutils literal notranslate\"><span class=\"pre\">handle_httpstatus_list</span></code> on spider attribute\nor in <code class=\"docutils literal notranslate\"><span class=\"pre\">Request</span></code>’s <code class=\"docutils literal notranslate\"><span class=\"pre\">meta</span></code> key (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1334\">issue 1334</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1364\">issue 1364</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1447\">issue 1447</a>).</p>", "<p>Form submission:</p>", "<p>now works with <code class=\"docutils literal notranslate\"><span class=\"pre\">&lt;button&gt;</span></code> elements too (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1469\">issue 1469</a>).</p>", "<p>an empty string is now used for submit buttons without a value\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1472\">issue 1472</a>)</p>", "<p>Dict-like settings now have per-key priorities\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1135\">issue 1135</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1149\">issue 1149</a> and <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1586\">issue 1586</a>).</p>", "<p>Sending non-ASCII emails (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1662\">issue 1662</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">CloseSpider</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">SpiderState</span></code> extensions now get disabled if no relevant\nsetting is set (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1723\">issue 1723</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1725\">issue 1725</a>).</p>", "<p>Added method <code class=\"docutils literal notranslate\"><span class=\"pre\">ExecutionEngine.close</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1423\">issue 1423</a>).</p>", "<p>Added method <code class=\"docutils literal notranslate\"><span class=\"pre\">CrawlerRunner.create_crawler</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1528\">issue 1528</a>).</p>", "<p>Scheduler priority queue can now be customized via\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-SCHEDULER_PRIORITY_QUEUE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SCHEDULER_PRIORITY_QUEUE</span></code></a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1822\">issue 1822</a>).</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">.pps</span></code> links are now ignored by default in link extractors (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1835\">issue 1835</a>).</p>", "<p>temporary data folder for FTP and S3 feed storages can be customized\nusing a new <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-FEED_TEMPDIR\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">FEED_TEMPDIR</span></code></a> setting (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1847\">issue 1847</a>).</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">FilesPipeline</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">ImagesPipeline</span></code> settings are now instance attributes\ninstead of class attributes, enabling spider-specific behaviors (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1891\">issue 1891</a>).</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">JsonItemExporter</span></code> now formats opening and closing square brackets\non their own line (first and last lines of output file) (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1950\">issue 1950</a>).</p>", "<p>If available, <code class=\"docutils literal notranslate\"><span class=\"pre\">botocore</span></code> is used for <code class=\"docutils literal notranslate\"><span class=\"pre\">S3FeedStorage</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">S3DownloadHandler</span></code>\nand <code class=\"docutils literal notranslate\"><span class=\"pre\">S3FilesStore</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1761\">issue 1761</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1883\">issue 1883</a>).</p>", "<p>Tons of documentation updates and related fixes (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1291\">issue 1291</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1302\">issue 1302</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1335\">issue 1335</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1683\">issue 1683</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1660\">issue 1660</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1642\">issue 1642</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1721\">issue 1721</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1727\">issue 1727</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1879\">issue 1879</a>).</p>", "<p>Other refactoring, optimizations and cleanup (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1476\">issue 1476</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1481\">issue 1481</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1477\">issue 1477</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1315\">issue 1315</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1290\">issue 1290</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1750\">issue 1750</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1881\">issue 1881</a>).</p>", "<p>Added <code class=\"docutils literal notranslate\"><span class=\"pre\">to_bytes</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">to_unicode</span></code>, deprecated <code class=\"docutils literal notranslate\"><span class=\"pre\">str_to_unicode</span></code> and\n<code class=\"docutils literal notranslate\"><span class=\"pre\">unicode_to_str</span></code> functions (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/778\">issue 778</a>).</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">binary_is_text</span></code> is introduced, to replace use of <code class=\"docutils literal notranslate\"><span class=\"pre\">isbinarytext</span></code>\n(but with inverse return value) (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1851\">issue 1851</a>)</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">optional_features</span></code> set has been removed (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1359\">issue 1359</a>).</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">--lsprof</span></code> command line option has been removed (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1689\">issue 1689</a>).\n<strong>Warning: backward incompatible</strong>, but doesn’t break user code.</p>", "<p>The following datatypes were deprecated (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1720\">issue 1720</a>):</p>", "<p>The previously bundled <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.xlib.pydispatch</span></code> library was deprecated and\nreplaced by <a class=\"reference external\" href=\"https://pypi.org/project/PyDispatcher/\">pydispatcher</a>.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">telnetconsole</span></code> was relocated to <code class=\"docutils literal notranslate\"><span class=\"pre\">extensions/</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1524\">issue 1524</a>).</p>", "<p>Note: telnet is not enabled on Python 3\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/pull/1524#issuecomment-146985595\">https://github.com/scrapy/scrapy/pull/1524#issuecomment-146985595</a>)</p>", "<p>Scrapy does not retry requests that got a <code class=\"docutils literal notranslate\"><span class=\"pre\">HTTP</span> <span class=\"pre\">400</span> <span class=\"pre\">Bad</span> <span class=\"pre\">Request</span></code>\nresponse anymore (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1289\">issue 1289</a>). <strong>Warning: backward incompatible!</strong></p>", "<p>Support empty password for http_proxy config (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1274\">issue 1274</a>).</p>", "<p>Interpret <code class=\"docutils literal notranslate\"><span class=\"pre\">application/x-json</span></code> as <code class=\"docutils literal notranslate\"><span class=\"pre\">TextResponse</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1333\">issue 1333</a>).</p>", "<p>Support link rel attribute with multiple values (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1201\">issue 1201</a>).</p>", "<p>Fixed <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.http.FormRequest.from_response</span></code> when there is a <code class=\"docutils literal notranslate\"><span class=\"pre\">&lt;base&gt;</span></code>\ntag (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1564\">issue 1564</a>).</p>", "<p>Fixed <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-TEMPLATES_DIR\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">TEMPLATES_DIR</span></code></a> handling (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1575\">issue 1575</a>).</p>", "<p>Various <code class=\"docutils literal notranslate\"><span class=\"pre\">FormRequest</span></code> fixes (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1595\">issue 1595</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1596\">issue 1596</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1597\">issue 1597</a>).</p>", "<p>Makes <code class=\"docutils literal notranslate\"><span class=\"pre\">_monkeypatches</span></code> more robust (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1634\">issue 1634</a>).</p>", "<p>Fixed bug on <code class=\"docutils literal notranslate\"><span class=\"pre\">XMLItemExporter</span></code> with non-string fields in\nitems (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1738\">issue 1738</a>).</p>", "<p>Fixed startproject command in macOS (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1635\">issue 1635</a>).</p>", "<p>Fixed <a class=\"reference internal\" href=\"topics/exporters.html#scrapy.exporters.PythonItemExporter\" title=\"scrapy.exporters.PythonItemExporter\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">PythonItemExporter</span></code></a> and CSVExporter for\nnon-string item types (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1737\">issue 1737</a>).</p>", "<p>Various logging related fixes (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1294\">issue 1294</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1419\">issue 1419</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1263\">issue 1263</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1624\">issue 1624</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1654\">issue 1654</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1722\">issue 1722</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1726\">issue 1726</a> and <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1303\">issue 1303</a>).</p>", "<p>Fixed bug in <code class=\"docutils literal notranslate\"><span class=\"pre\">utils.template.render_templatefile()</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1212\">issue 1212</a>).</p>", "<p>sitemaps extraction from <code class=\"docutils literal notranslate\"><span class=\"pre\">robots.txt</span></code> is now case-insensitive (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1902\">issue 1902</a>).</p>", "<p>HTTPS+CONNECT tunnels could get mixed up when using multiple proxies\nto same remote host (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1912\">issue 1912</a>).</p>", "<p>Packaging fix: disallow unsupported Twisted versions in setup.py</p>", "<p>FIX: RetryMiddleware is now robust to non-standard HTTP status codes (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1857\">issue 1857</a>)</p>", "<p>FIX: Filestorage HTTP cache was checking wrong modified time (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1875\">issue 1875</a>)</p>", "<p>DOC: Support for Sphinx 1.4+ (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1893\">issue 1893</a>)</p>", "<p>DOC: Consistency in selectors examples (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1869\">issue 1869</a>)</p>", "<p>FIX: [Backport] Ignore bogus links in LinkExtractors (fixes <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/907\">issue 907</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/108195e\">commit 108195e</a>)</p>", "<p>TST: Changed buildbot makefile to use ‘pytest’ (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/1f3d90a\">commit 1f3d90a</a>)</p>", "<p>DOC: Fixed typos in tutorial and media-pipeline (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/808a9ea\">commit 808a9ea</a> and <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/803bd87\">commit 803bd87</a>)</p>", "<p>DOC: Add AjaxCrawlMiddleware to DOWNLOADER_MIDDLEWARES_BASE in settings docs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/aa94121\">commit aa94121</a>)</p>", "<p>Ignoring xlib/tx folder, depending on Twisted version. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/7dfa979\">commit 7dfa979</a>)</p>", "<p>Run on new travis-ci infra (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/6e42f0b\">commit 6e42f0b</a>)</p>", "<p>Spelling fixes (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/823a1cc\">commit 823a1cc</a>)</p>", "<p>escape nodename in xmliter regex (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/da3c155\">commit da3c155</a>)</p>", "<p>test xml nodename with dots (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/4418fc3\">commit 4418fc3</a>)</p>", "<p>TST don’t use broken Pillow version in tests (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/a55078c\">commit a55078c</a>)</p>", "<p>disable log on version command. closes #1426 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/86fc330\">commit 86fc330</a>)</p>", "<p>disable log on startproject command (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/db4c9fe\">commit db4c9fe</a>)</p>", "<p>Add PyPI download stats badge (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/df2b944\">commit df2b944</a>)</p>", "<p>don’t run tests twice on Travis if a PR is made from a scrapy/scrapy branch (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/a83ab41\">commit a83ab41</a>)</p>", "<p>Add Python 3 porting status badge to the README (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/73ac80d\">commit 73ac80d</a>)</p>", "<p>fixed RFPDupeFilter persistence (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/97d080e\">commit 97d080e</a>)</p>", "<p>TST a test to show that dupefilter persistence is not working (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/97f2fb3\">commit 97f2fb3</a>)</p>", "<p>explicit close file on <a class=\"reference external\" href=\"file://\">file://</a> scheme handler (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/d9b4850\">commit d9b4850</a>)</p>", "<p>Disable dupefilter in shell (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/c0d0734\">commit c0d0734</a>)</p>", "<p>DOC: Add captions to toctrees which appear in sidebar (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/aa239ad\">commit aa239ad</a>)</p>", "<p>DOC Removed pywin32 from install instructions as it’s already declared as dependency. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/10eb400\">commit 10eb400</a>)</p>", "<p>Added installation notes about using Conda for Windows and other OSes. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/1c3600a\">commit 1c3600a</a>)</p>", "<p>Fixed minor grammar issues. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/7f4ddd5\">commit 7f4ddd5</a>)</p>", "<p>fixed a typo in the documentation. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/b71f677\">commit b71f677</a>)</p>", "<p>Version 1 now exists (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/5456c0e\">commit 5456c0e</a>)</p>", "<p>fix another invalid xpath error (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/0a1366e\">commit 0a1366e</a>)</p>", "<p>fix ValueError: Invalid XPath: //div/[id=”not-exists”]/text() on selectors.rst (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/ca8d60f\">commit ca8d60f</a>)</p>", "<p>Typos corrections (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/7067117\">commit 7067117</a>)</p>", "<p>fix typos in downloader-middleware.rst and exceptions.rst, middlware -&gt; middleware (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/32f115c\">commit 32f115c</a>)</p>", "<p>Add note to Ubuntu install section about Debian compatibility (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/23fda69\">commit 23fda69</a>)</p>", "<p>Replace alternative macOS install workaround with virtualenv (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/98b63ee\">commit 98b63ee</a>)</p>", "<p>Reference Homebrew’s homepage for installation instructions (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/1925db1\">commit 1925db1</a>)</p>", "<p>Add oldest supported tox version to contributing docs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/5d10d6d\">commit 5d10d6d</a>)</p>", "<p>Note in install docs about pip being already included in python&gt;=2.7.9 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/85c980e\">commit 85c980e</a>)</p>", "<p>Add non-python dependencies to Ubuntu install section in the docs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/fbd010d\">commit fbd010d</a>)</p>", "<p>Add macOS installation section to docs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/d8f4cba\">commit d8f4cba</a>)</p>", "<p>DOC(ENH): specify path to rtd theme explicitly (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/de73b1a\">commit de73b1a</a>)</p>", "<p>minor: scrapy.Spider docs grammar (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/1ddcc7b\">commit 1ddcc7b</a>)</p>", "<p>Make common practices sample code match the comments (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/1b85bcf\">commit 1b85bcf</a>)</p>", "<p>nextcall repetitive calls (heartbeats). (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/55f7104\">commit 55f7104</a>)</p>", "<p>Backport fix compatibility with Twisted 15.4.0 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/b262411\">commit b262411</a>)</p>", "<p>pin pytest to 2.7.3 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/a6535c2\">commit a6535c2</a>)</p>", "<p>Merge pull request #1512 from mgedmin/patch-1 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/8876111\">commit 8876111</a>)</p>", "<p>Merge pull request #1513 from mgedmin/patch-2 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/5d4daf8\">commit 5d4daf8</a>)</p>", "<p>Typo (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/f8d0682\">commit f8d0682</a>)</p>", "<p>Fix list formatting (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/5f83a93\">commit 5f83a93</a>)</p>", "<p>fix Scrapy squeue tests after recent changes to queuelib (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/3365c01\">commit 3365c01</a>)</p>", "<p>Merge pull request #1475 from rweindl/patch-1 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/2d688cd\">commit 2d688cd</a>)</p>", "<p>Update tutorial.rst (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/fbc1f25\">commit fbc1f25</a>)</p>", "<p>Merge pull request #1449 from rhoekman/patch-1 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/7d6538c\">commit 7d6538c</a>)</p>", "<p>Small grammatical change (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/8752294\">commit 8752294</a>)</p>", "<p>Add openssl version to version command (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/13c45ac\">commit 13c45ac</a>)</p>", "<p>add service_identity to Scrapy install_requires (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/cbc2501\">commit cbc2501</a>)</p>", "<p>Workaround for travis#296 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/66af9cd\">commit 66af9cd</a>)</p>", "<p>Twisted 15.3.0 does not raises PicklingError serializing lambda functions (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/b04dd7d\">commit b04dd7d</a>)</p>", "<p>Minor method name fix (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/6f85c7f\">commit 6f85c7f</a>)</p>", "<p>minor: scrapy.Spider grammar and clarity (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/9c9d2e0\">commit 9c9d2e0</a>)</p>", "<p>Put a blurb about support channels in CONTRIBUTING (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/c63882b\">commit c63882b</a>)</p>", "<p>Fixed typos (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/a9ae7b0\">commit a9ae7b0</a>)</p>", "<p>Fix doc reference. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/7c8a4fe\">commit 7c8a4fe</a>)</p>", "<p>Unquote request path before passing to FTPClient, it already escape paths (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/cc00ad2\">commit cc00ad2</a>)</p>", "<p>include tests/ to source distribution in MANIFEST.in (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/eca227e\">commit eca227e</a>)</p>", "<p>DOC Fix SelectJmes documentation (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/b8567bc\">commit b8567bc</a>)</p>", "<p>DOC Bring Ubuntu and Archlinux outside of Windows subsection (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/392233f\">commit 392233f</a>)</p>", "<p>DOC remove version suffix from Ubuntu package (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/5303c66\">commit 5303c66</a>)</p>", "<p>DOC Update release date for 1.0 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/c89fa29\">commit c89fa29</a>)</p>", "<p>You will find a lot of new features and bugfixes in this major release.  Make\nsure to check our updated <a class=\"hoverxref tooltip reference internal\" href=\"intro/overview.html#intro-overview\"><span class=\"std std-ref\">overview</span></a> to get a glance of\nsome of the changes, along with our brushed <a class=\"hoverxref tooltip reference internal\" href=\"intro/tutorial.html#intro-tutorial\"><span class=\"std std-ref\">tutorial</span></a>.</p>", "<p>Declaring and returning Scrapy Items is no longer necessary to collect the\nscraped data from your spider, you can now return explicit dictionaries\ninstead.</p>", "<p>Last Google Summer of Code project accomplished an important redesign of the\nmechanism used for populating settings, introducing explicit priorities to\noverride any given setting. As an extension of that goal, we included a new\nlevel of priority for settings that act exclusively for a single spider,\nallowing them to redefine project settings.</p>", "<p>Start using it by defining a <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">custom_settings</span></code>\nclass variable in your spider:</p>", "<p>Read more about settings population: <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#topics-settings\"><span class=\"std std-ref\">Settings</span></a></p>", "<p>Scrapy 1.0 has moved away from Twisted logging to support Python built in’s\nas default logging system. We’re maintaining backward compatibility for most\nof the old custom interface to call logging functions, but you’ll get\nwarnings to switch to the Python logging API entirely.</p>", "<p>Logging with spiders remains the same, but on top of the\n<code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">log()</span></code> method you’ll have access to a custom\n<code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">logger</span></code> created for the spider to issue log\nevents:</p>", "<p>Read more in the logging documentation: <a class=\"hoverxref tooltip reference internal\" href=\"topics/logging.html#topics-logging\"><span class=\"std std-ref\">Logging</span></a></p>", "<p>Another milestone for last Google Summer of Code was a refactoring of the\ninternal API, seeking a simpler and easier usage. Check new core interface\nin: <a class=\"hoverxref tooltip reference internal\" href=\"topics/api.html#topics-api\"><span class=\"std std-ref\">Core API</span></a></p>", "<p>A common situation where you will face these changes is while running Scrapy\nfrom scripts. Here’s a quick example of how to run a Spider manually with the\nnew API:</p>", "<p>Bear in mind this feature is still under development and its API may change\nuntil it reaches a stable status.</p>", "<p>See more examples for scripts running Scrapy: <a class=\"hoverxref tooltip reference internal\" href=\"topics/practices.html#topics-practices\"><span class=\"std std-ref\">Common Practices</span></a></p>", "<p>There’s been a large rearrangement of modules trying to improve the general\nstructure of Scrapy. Main changes were separating various subpackages into\nnew projects and dissolving both <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.contrib</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.contrib_exp</span></code>\ninto top level packages. Backward compatibility was kept among internal\nrelocations, while importing deprecated modules expect warnings indicating\ntheir new place.</p>", "<p>Outsourced packages</p>", "<p class=\"admonition-title\">Note</p>", "<p>These extensions went through some minor changes, e.g. some setting names\nwere changed. Please check the documentation in each new repository to\nget familiar with the new usage.</p>", "<p>Old location</p>", "<p>New location</p>", "<p>scrapy.commands.deploy</p>", "<p><a class=\"reference external\" href=\"https://github.com/scrapy/scrapyd-client\">scrapyd-client</a>\n(See other alternatives here:\n<a class=\"hoverxref tooltip reference internal\" href=\"topics/deploy.html#topics-deploy\"><span class=\"std std-ref\">Deploying Spiders</span></a>)</p>", "<p>scrapy.contrib.djangoitem</p>", "<p>scrapy.webservice</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.contrib_exp</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.contrib</span></code> dissolutions</p>", "<p>Old location</p>", "<p>New location</p>", "<p>scrapy.contrib_exp.downloadermiddleware.decompression</p>", "<p>scrapy.downloadermiddlewares.decompression</p>", "<p>scrapy.contrib_exp.iterators</p>", "<p>scrapy.utils.iterators</p>", "<p>scrapy.contrib.downloadermiddleware</p>", "<p>scrapy.downloadermiddlewares</p>", "<p>scrapy.contrib.exporter</p>", "<p>scrapy.exporters</p>", "<p>scrapy.contrib.linkextractors</p>", "<p>scrapy.linkextractors</p>", "<p>scrapy.contrib.loader</p>", "<p>scrapy.loader</p>", "<p>scrapy.contrib.loader.processor</p>", "<p>scrapy.loader.processors</p>", "<p>scrapy.contrib.pipeline</p>", "<p>scrapy.pipelines</p>", "<p>scrapy.contrib.spidermiddleware</p>", "<p>scrapy.spidermiddlewares</p>", "<p>scrapy.contrib.spiders</p>", "<p>scrapy.spiders</p>", "<p>scrapy.contrib.closespider</p>", "<p>scrapy.contrib.corestats</p>", "<p>scrapy.contrib.debug</p>", "<p>scrapy.contrib.feedexport</p>", "<p>scrapy.contrib.httpcache</p>", "<p>scrapy.contrib.logstats</p>", "<p>scrapy.contrib.memdebug</p>", "<p>scrapy.contrib.memusage</p>", "<p>scrapy.contrib.spiderstate</p>", "<p>scrapy.contrib.statsmailer</p>", "<p>scrapy.contrib.throttle</p>", "<p>scrapy.extensions.*</p>", "<p>Plural renames and Modules unification</p>", "<p>Old location</p>", "<p>New location</p>", "<p>scrapy.command</p>", "<p>scrapy.commands</p>", "<p>scrapy.dupefilter</p>", "<p>scrapy.dupefilters</p>", "<p>scrapy.linkextractor</p>", "<p>scrapy.linkextractors</p>", "<p>scrapy.spider</p>", "<p>scrapy.spiders</p>", "<p>scrapy.squeue</p>", "<p>scrapy.squeues</p>", "<p>scrapy.statscol</p>", "<p>scrapy.statscollectors</p>", "<p>scrapy.utils.decorator</p>", "<p>scrapy.utils.decorators</p>", "<p>Class renames</p>", "<p>Old location</p>", "<p>New location</p>", "<p>scrapy.spidermanager.SpiderManager</p>", "<p>scrapy.spiderloader.SpiderLoader</p>", "<p>Settings renames</p>", "<p>Old location</p>", "<p>New location</p>", "<p>SPIDER_MANAGER_CLASS</p>", "<p>SPIDER_LOADER_CLASS</p>", "<p>New Features and Enhancements</p>", "<p>Python logging (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1060\">issue 1060</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1235\">issue 1235</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1236\">issue 1236</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1240\">issue 1240</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1259\">issue 1259</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1278\">issue 1278</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1286\">issue 1286</a>)</p>", "<p>FEED_EXPORT_FIELDS option (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1159\">issue 1159</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1224\">issue 1224</a>)</p>", "<p>Dns cache size and timeout options (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1132\">issue 1132</a>)</p>", "<p>support namespace prefix in xmliter_lxml (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/963\">issue 963</a>)</p>", "<p>Reactor threadpool max size setting (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1123\">issue 1123</a>)</p>", "<p>Allow spiders to return dicts. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1081\">issue 1081</a>)</p>", "<p>Add Response.urljoin() helper (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1086\">issue 1086</a>)</p>", "<p>look in ~/.config/scrapy.cfg for user config (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1098\">issue 1098</a>)</p>", "<p>handle TLS SNI (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1101\">issue 1101</a>)</p>", "<p>Selectorlist extract first (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/624\">issue 624</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1145\">issue 1145</a>)</p>", "<p>Added JmesSelect (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1016\">issue 1016</a>)</p>", "<p>add gzip compression to filesystem http cache backend (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1020\">issue 1020</a>)</p>", "<p>CSS support in link extractors (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/983\">issue 983</a>)</p>", "<p>httpcache dont_cache meta #19 #689 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/821\">issue 821</a>)</p>", "<p>add signal to be sent when request is dropped by the scheduler\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/961\">issue 961</a>)</p>", "<p>avoid download large response (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/946\">issue 946</a>)</p>", "<p>Allow to specify the quotechar in CSVFeedSpider (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/882\">issue 882</a>)</p>", "<p>Add referer to “Spider error processing” log message (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/795\">issue 795</a>)</p>", "<p>process robots.txt once (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/896\">issue 896</a>)</p>", "<p>GSoC Per-spider settings (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/854\">issue 854</a>)</p>", "<p>Add project name validation (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/817\">issue 817</a>)</p>", "<p>GSoC API cleanup (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/816\">issue 816</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1128\">issue 1128</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1147\">issue 1147</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1148\">issue 1148</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1156\">issue 1156</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1185\">issue 1185</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1187\">issue 1187</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1258\">issue 1258</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1268\">issue 1268</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1276\">issue 1276</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1285\">issue 1285</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1284\">issue 1284</a>)</p>", "<p>Be more responsive with IO operations (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1074\">issue 1074</a> and <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1075\">issue 1075</a>)</p>", "<p>Do leveldb compaction for httpcache on closing (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1297\">issue 1297</a>)</p>", "<p>Deprecations and Removals</p>", "<p>Deprecate htmlparser link extractor (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1205\">issue 1205</a>)</p>", "<p>remove deprecated code from FeedExporter (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1155\">issue 1155</a>)</p>", "<p>a leftover for.15 compatibility (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/925\">issue 925</a>)</p>", "<p>drop support for CONCURRENT_REQUESTS_PER_SPIDER (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/895\">issue 895</a>)</p>", "<p>Drop old engine code (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/911\">issue 911</a>)</p>", "<p>Deprecate SgmlLinkExtractor (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/777\">issue 777</a>)</p>", "<p>Relocations</p>", "<p>Move exporters/__init__.py to exporters.py (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1242\">issue 1242</a>)</p>", "<p>Move base classes to their packages (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1218\">issue 1218</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1233\">issue 1233</a>)</p>", "<p>Module relocation (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1181\">issue 1181</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1210\">issue 1210</a>)</p>", "<p>rename SpiderManager to SpiderLoader (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1166\">issue 1166</a>)</p>", "<p>Remove djangoitem (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1177\">issue 1177</a>)</p>", "<p>remove scrapy deploy command (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1102\">issue 1102</a>)</p>", "<p>dissolve contrib_exp (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1134\">issue 1134</a>)</p>", "<p>Deleted bin folder from root, fixes #913 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/914\">issue 914</a>)</p>", "<p>Remove jsonrpc based webservice (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/859\">issue 859</a>)</p>", "<p>Move Test cases under project root dir (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/827\">issue 827</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/841\">issue 841</a>)</p>", "<p>Fix backward incompatibility for relocated paths in settings\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1267\">issue 1267</a>)</p>", "<p>Documentation</p>", "<p>CrawlerProcess documentation (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1190\">issue 1190</a>)</p>", "<p>Favoring web scraping over screen scraping in the descriptions\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1188\">issue 1188</a>)</p>", "<p>Some improvements for Scrapy tutorial (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1180\">issue 1180</a>)</p>", "<p>Documenting Files Pipeline together with Images Pipeline (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1150\">issue 1150</a>)</p>", "<p>deployment docs tweaks (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1164\">issue 1164</a>)</p>", "<p>Added deployment section covering scrapyd-deploy and shub (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1124\">issue 1124</a>)</p>", "<p>Adding more settings to project template (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1073\">issue 1073</a>)</p>", "<p>some improvements to overview page (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1106\">issue 1106</a>)</p>", "<p>Updated link in docs/topics/architecture.rst (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/647\">issue 647</a>)</p>", "<p>DOC reorder topics (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1022\">issue 1022</a>)</p>", "<p>updating list of Request.meta special keys (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1071\">issue 1071</a>)</p>", "<p>DOC document download_timeout (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/898\">issue 898</a>)</p>", "<p>DOC simplify extension docs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/893\">issue 893</a>)</p>", "<p>Leaks docs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/894\">issue 894</a>)</p>", "<p>DOC document from_crawler method for item pipelines (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/904\">issue 904</a>)</p>", "<p>Spider_error doesn’t support deferreds (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1292\">issue 1292</a>)</p>", "<p>Corrections &amp; Sphinx related fixes (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1220\">issue 1220</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1219\">issue 1219</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1196\">issue 1196</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1172\">issue 1172</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1171\">issue 1171</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1169\">issue 1169</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1160\">issue 1160</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1154\">issue 1154</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1127\">issue 1127</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1112\">issue 1112</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1105\">issue 1105</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1041\">issue 1041</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1082\">issue 1082</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1033\">issue 1033</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/944\">issue 944</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/866\">issue 866</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/864\">issue 864</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/796\">issue 796</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1260\">issue 1260</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1271\">issue 1271</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1293\">issue 1293</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1298\">issue 1298</a>)</p>", "<p>Bugfixes</p>", "<p>Item multi inheritance fix (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/353\">issue 353</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1228\">issue 1228</a>)</p>", "<p>ItemLoader.load_item: iterate over copy of fields (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/722\">issue 722</a>)</p>", "<p>Fix Unhandled error in Deferred (RobotsTxtMiddleware) (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1131\">issue 1131</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1197\">issue 1197</a>)</p>", "<p>Force to read DOWNLOAD_TIMEOUT as int (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/954\">issue 954</a>)</p>", "<p>scrapy.utils.misc.load_object should print full traceback (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/902\">issue 902</a>)</p>", "<p>Fix bug for “.local” host name (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/878\">issue 878</a>)</p>", "<p>Fix for Enabled extensions, middlewares, pipelines info not printed\nanymore (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/879\">issue 879</a>)</p>", "<p>fix dont_merge_cookies bad behaviour when set to false on meta\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/846\">issue 846</a>)</p>", "<p>Python 3 In Progress Support</p>", "<p>disable scrapy.telnet if twisted.conch is not available (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1161\">issue 1161</a>)</p>", "<p>fix Python 3 syntax errors in ajaxcrawl.py (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1162\">issue 1162</a>)</p>", "<p>more python3 compatibility changes for urllib (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1121\">issue 1121</a>)</p>", "<p>assertItemsEqual was renamed to assertCountEqual in Python 3.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1070\">issue 1070</a>)</p>", "<p>Import unittest.mock if available. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1066\">issue 1066</a>)</p>", "<p>updated deprecated cgi.parse_qsl to use six’s parse_qsl (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/909\">issue 909</a>)</p>", "<p>Prevent Python 3 port regressions (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/830\">issue 830</a>)</p>", "<p>PY3: use MutableMapping for python 3 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/810\">issue 810</a>)</p>", "<p>PY3: use six.BytesIO and six.moves.cStringIO (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/803\">issue 803</a>)</p>", "<p>PY3: fix xmlrpclib and email imports (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/801\">issue 801</a>)</p>", "<p>PY3: use six for robotparser and urlparse (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/800\">issue 800</a>)</p>", "<p>PY3: use six.iterkeys, six.iteritems, and tempfile (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/799\">issue 799</a>)</p>", "<p>PY3: fix has_key and use six.moves.configparser (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/798\">issue 798</a>)</p>", "<p>PY3: use six.moves.cPickle (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/797\">issue 797</a>)</p>", "<p>PY3 make it possible to run some tests in Python3 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/776\">issue 776</a>)</p>", "<p>Tests</p>", "<p>remove unnecessary lines from py3-ignores (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1243\">issue 1243</a>)</p>", "<p>Fix remaining warnings from pytest while collecting tests (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1206\">issue 1206</a>)</p>", "<p>Add docs build to travis (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1234\">issue 1234</a>)</p>", "<p>TST don’t collect tests from deprecated modules. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1165\">issue 1165</a>)</p>", "<p>install service_identity package in tests to prevent warnings\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1168\">issue 1168</a>)</p>", "<p>Fix deprecated settings API in tests (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1152\">issue 1152</a>)</p>", "<p>Add test for webclient with POST method and no body given (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1089\">issue 1089</a>)</p>", "<p>py3-ignores.txt supports comments (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1044\">issue 1044</a>)</p>", "<p>modernize some of the asserts (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/835\">issue 835</a>)</p>", "<p>selector.__repr__ test (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/779\">issue 779</a>)</p>", "<p>Code refactoring</p>", "<p>CSVFeedSpider cleanup: use iterate_spider_output (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1079\">issue 1079</a>)</p>", "<p>remove unnecessary check from scrapy.utils.spider.iter_spider_output\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/1078\">issue 1078</a>)</p>", "<p>Pydispatch pep8 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/992\">issue 992</a>)</p>", "<p>Removed unused ‘load=False’ parameter from walk_modules() (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/871\">issue 871</a>)</p>", "<p>For consistency, use <code class=\"docutils literal notranslate\"><span class=\"pre\">job_dir</span></code> helper in <code class=\"docutils literal notranslate\"><span class=\"pre\">SpiderState</span></code> extension.\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/805\">issue 805</a>)</p>", "<p>rename “sflo” local variables to less cryptic “log_observer” (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/775\">issue 775</a>)</p>", "<p>encode invalid xpath with unicode_escape under PY2 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/07cb3e5\">commit 07cb3e5</a>)</p>", "<p>fix IPython shell scope issue and load IPython user config (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/2c8e573\">commit 2c8e573</a>)</p>", "<p>Fix small typo in the docs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/d694019\">commit d694019</a>)</p>", "<p>Fix small typo (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/f92fa83\">commit f92fa83</a>)</p>", "<p>Converted sel.xpath() calls to response.xpath() in Extracting the data (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/c2c6d15\">commit c2c6d15</a>)</p>", "<p>Support new _getEndpoint Agent signatures on Twisted 15.0.0 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/540b9bc\">commit 540b9bc</a>)</p>", "<p>DOC a couple more references are fixed (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/b4c454b\">commit b4c454b</a>)</p>", "<p>DOC fix a reference (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/e3c1260\">commit e3c1260</a>)</p>", "<p>t.i.b.ThreadedResolver is now a new-style class (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/9e13f42\">commit 9e13f42</a>)</p>", "<p>S3DownloadHandler: fix auth for requests with quoted paths/query params (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/cdb9a0b\">commit cdb9a0b</a>)</p>", "<p>fixed the variable types in mailsender documentation (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/bb3a848\">commit bb3a848</a>)</p>", "<p>Reset items_scraped instead of item_count (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/edb07a4\">commit edb07a4</a>)</p>", "<p>Tentative attention message about what document to read for contributions (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/7ee6f7a\">commit 7ee6f7a</a>)</p>", "<p>mitmproxy 0.10.1 needs netlib 0.10.1 too (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/874fcdd\">commit 874fcdd</a>)</p>", "<p>pin mitmproxy 0.10.1 as &gt;0.11 does not work with tests (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/c6b21f0\">commit c6b21f0</a>)</p>", "<p>Test the parse command locally instead of against an external url (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/c3a6628\">commit c3a6628</a>)</p>", "<p>Patches Twisted issue while closing the connection pool on HTTPDownloadHandler (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/d0bf957\">commit d0bf957</a>)</p>", "<p>Updates documentation on dynamic item classes. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/eeb589a\">commit eeb589a</a>)</p>", "<p>Merge pull request #943 from Lazar-T/patch-3 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/5fdab02\">commit 5fdab02</a>)</p>", "<p>typo (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/b0ae199\">commit b0ae199</a>)</p>", "<p>pywin32 is required by Twisted. closes #937 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/5cb0cfb\">commit 5cb0cfb</a>)</p>", "<p>Update install.rst (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/781286b\">commit 781286b</a>)</p>", "<p>Merge pull request #928 from Lazar-T/patch-1 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/b415d04\">commit b415d04</a>)</p>", "<p>comma instead of fullstop (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/627b9ba\">commit 627b9ba</a>)</p>", "<p>Merge pull request #885 from jsma/patch-1 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/de909ad\">commit de909ad</a>)</p>", "<p>Update request-response.rst (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/3f3263d\">commit 3f3263d</a>)</p>", "<p>SgmlLinkExtractor - fix for parsing &lt;area&gt; tag with Unicode present (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/49b40f0\">commit 49b40f0</a>)</p>", "<p>pem file is used by mockserver and required by scrapy bench (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/5eddc68\">commit 5eddc68</a>)</p>", "<p>scrapy bench needs scrapy.tests* (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/d6cb999\">commit d6cb999</a>)</p>", "<p>no need to waste travis-ci time on py3 for 0.24 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/8e080c1\">commit 8e080c1</a>)</p>", "<p>Update installation docs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/1d0c096\">commit 1d0c096</a>)</p>", "<p>There is a trove classifier for Scrapy framework! (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/4c701d7\">commit 4c701d7</a>)</p>", "<p>update other places where w3lib version is mentioned (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/d109c13\">commit d109c13</a>)</p>", "<p>Update w3lib requirement to 1.8.0 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/39d2ce5\">commit 39d2ce5</a>)</p>", "<p>Use w3lib.html.replace_entities() (remove_entities() is deprecated) (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/180d3ad\">commit 180d3ad</a>)</p>", "<p>set zip_safe=False (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/a51ee8b\">commit a51ee8b</a>)</p>", "<p>do not ship tests package (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/ee3b371\">commit ee3b371</a>)</p>", "<p>scrapy.bat is not needed anymore (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/c3861cf\">commit c3861cf</a>)</p>", "<p>Modernize setup.py (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/362e322\">commit 362e322</a>)</p>", "<p>headers can not handle non-string values (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/94a5c65\">commit 94a5c65</a>)</p>", "<p>fix ftp test cases (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/a274a7f\">commit a274a7f</a>)</p>", "<p>The sum up of travis-ci builds are taking like 50min to complete (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/ae1e2cc\">commit ae1e2cc</a>)</p>", "<p>Update shell.rst typo (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/e49c96a\">commit e49c96a</a>)</p>", "<p>removes weird indentation in the shell results (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/1ca489d\">commit 1ca489d</a>)</p>", "<p>improved explanations, clarified blog post as source, added link for XPath string functions in the spec (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/65c8f05\">commit 65c8f05</a>)</p>", "<p>renamed UserTimeoutError and ServerTimeouterror #583 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/037f6ab\">commit 037f6ab</a>)</p>", "<p>adding some xpath tips to selectors docs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/2d103e0\">commit 2d103e0</a>)</p>", "<p>fix tests to account for <a class=\"reference external\" href=\"https://github.com/scrapy/w3lib/pull/23\">https://github.com/scrapy/w3lib/pull/23</a> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/f8d366a\">commit f8d366a</a>)</p>", "<p>get_func_args maximum recursion fix #728 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/81344ea\">commit 81344ea</a>)</p>", "<p>Updated input/output processor example according to #560. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/f7c4ea8\">commit f7c4ea8</a>)</p>", "<p>Fixed Python syntax in tutorial. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/db59ed9\">commit db59ed9</a>)</p>", "<p>Add test case for tunneling proxy (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/f090260\">commit f090260</a>)</p>", "<p>Bugfix for leaking Proxy-Authorization header to remote host when using tunneling (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/d8793af\">commit d8793af</a>)</p>", "<p>Extract links from XHTML documents with MIME-Type “application/xml” (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/ed1f376\">commit ed1f376</a>)</p>", "<p>Merge pull request #793 from roysc/patch-1 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/91a1106\">commit 91a1106</a>)</p>", "<p>Fix typo in commands.rst (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/743e1e2\">commit 743e1e2</a>)</p>", "<p>better testcase for settings.overrides.setdefault (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/e22daaf\">commit e22daaf</a>)</p>", "<p>Using CRLF as line marker according to http 1.1 definition (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/5ec430b\">commit 5ec430b</a>)</p>", "<p>Use a mutable mapping to proxy deprecated settings.overrides and settings.defaults attribute (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/e5e8133\">commit e5e8133</a>)</p>", "<p>there is not support for python3 yet (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/3cd6146\">commit 3cd6146</a>)</p>", "<p>Update python compatible version set to Debian packages (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/fa5d76b\">commit fa5d76b</a>)</p>", "<p>DOC fix formatting in release notes (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/c6a9e20\">commit c6a9e20</a>)</p>", "<p>Fix deprecated CrawlerSettings and increase backward compatibility with\n.defaults attribute (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/8e3f20a\">commit 8e3f20a</a>)</p>", "<p>Improve Scrapy top-level namespace (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/494\">issue 494</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/684\">issue 684</a>)</p>", "<p>Add selector shortcuts to responses (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/554\">issue 554</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/690\">issue 690</a>)</p>", "<p>Add new lxml based LinkExtractor to replace unmaintained SgmlLinkExtractor\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/559\">issue 559</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/761\">issue 761</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/763\">issue 763</a>)</p>", "<p>Cleanup settings API - part of per-spider settings <strong>GSoC project</strong> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/737\">issue 737</a>)</p>", "<p>Add UTF8 encoding header to templates (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/688\">issue 688</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/762\">issue 762</a>)</p>", "<p>Telnet console now binds to 127.0.0.1 by default (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/699\">issue 699</a>)</p>", "<p>Update Debian/Ubuntu install instructions (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/509\">issue 509</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/549\">issue 549</a>)</p>", "<p>Disable smart strings in lxml XPath evaluations (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/535\">issue 535</a>)</p>", "<p>Restore filesystem based cache as default for http\ncache middleware (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/541\">issue 541</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/500\">issue 500</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/571\">issue 571</a>)</p>", "<p>Expose current crawler in Scrapy shell (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/557\">issue 557</a>)</p>", "<p>Improve testsuite comparing CSV and XML exporters (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/570\">issue 570</a>)</p>", "<p>New <code class=\"docutils literal notranslate\"><span class=\"pre\">offsite/filtered</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">offsite/domains</span></code> stats (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/566\">issue 566</a>)</p>", "<p>Support process_links as generator in CrawlSpider (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/555\">issue 555</a>)</p>", "<p>Verbose logging and new stats counters for DupeFilter (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/553\">issue 553</a>)</p>", "<p>Add a mimetype parameter to <code class=\"docutils literal notranslate\"><span class=\"pre\">MailSender.send()</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/602\">issue 602</a>)</p>", "<p>Generalize file pipeline log messages (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/622\">issue 622</a>)</p>", "<p>Replace unencodeable codepoints with html entities in SGMLLinkExtractor (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/565\">issue 565</a>)</p>", "<p>Converted SEP documents to rst format (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/629\">issue 629</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/630\">issue 630</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/638\">issue 638</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/632\">issue 632</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/636\">issue 636</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/640\">issue 640</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/635\">issue 635</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/634\">issue 634</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/639\">issue 639</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/637\">issue 637</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/631\">issue 631</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/633\">issue 633</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/641\">issue 641</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/642\">issue 642</a>)</p>", "<p>Tests and docs for clickdata’s nr index in FormRequest (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/646\">issue 646</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/645\">issue 645</a>)</p>", "<p>Allow to disable a downloader handler just like any other component (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/650\">issue 650</a>)</p>", "<p>Log when a request is discarded after too many redirections (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/654\">issue 654</a>)</p>", "<p>Log error responses if they are not handled by spider callbacks\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/612\">issue 612</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/656\">issue 656</a>)</p>", "<p>Add content-type check to http compression mw (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/193\">issue 193</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/660\">issue 660</a>)</p>", "<p>Run pypy tests using latest pypi from ppa (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/674\">issue 674</a>)</p>", "<p>Run test suite using pytest instead of trial (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/679\">issue 679</a>)</p>", "<p>Build docs and check for dead links in tox environment (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/687\">issue 687</a>)</p>", "<p>Make scrapy.version_info a tuple of integers (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/681\">issue 681</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/692\">issue 692</a>)</p>", "<p>Infer exporter’s output format from filename extensions\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/546\">issue 546</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/659\">issue 659</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/760\">issue 760</a>)</p>", "<p>Support case-insensitive domains in <code class=\"docutils literal notranslate\"><span class=\"pre\">url_is_from_any_domain()</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/693\">issue 693</a>)</p>", "<p>Remove pep8 warnings in project and spider templates (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/698\">issue 698</a>)</p>", "<p>Tests and docs for <code class=\"docutils literal notranslate\"><span class=\"pre\">request_fingerprint</span></code> function (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/597\">issue 597</a>)</p>", "<p>Update SEP-19 for GSoC project <code class=\"docutils literal notranslate\"><span class=\"pre\">per-spider</span> <span class=\"pre\">settings</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/705\">issue 705</a>)</p>", "<p>Set exit code to non-zero when contracts fails (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/727\">issue 727</a>)</p>", "<p>Add a setting to control what class is instantiated as Downloader component\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/738\">issue 738</a>)</p>", "<p>Pass response in <code class=\"docutils literal notranslate\"><span class=\"pre\">item_dropped</span></code> signal (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/724\">issue 724</a>)</p>", "<p>Improve <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">check</span></code> contracts command (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/733\">issue 733</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/752\">issue 752</a>)</p>", "<p>Document <code class=\"docutils literal notranslate\"><span class=\"pre\">spider.closed()</span></code> shortcut (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/719\">issue 719</a>)</p>", "<p>Document <code class=\"docutils literal notranslate\"><span class=\"pre\">request_scheduled</span></code> signal (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/746\">issue 746</a>)</p>", "<p>Add a note about reporting security issues (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/697\">issue 697</a>)</p>", "<p>Add LevelDB http cache storage backend (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/626\">issue 626</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/500\">issue 500</a>)</p>", "<p>Sort spider list output of <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">list</span></code> command (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/742\">issue 742</a>)</p>", "<p>Multiple documentation enhancements and fixes\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/575\">issue 575</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/587\">issue 587</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/590\">issue 590</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/596\">issue 596</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/610\">issue 610</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/617\">issue 617</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/618\">issue 618</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/627\">issue 627</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/613\">issue 613</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/643\">issue 643</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/654\">issue 654</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/675\">issue 675</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/663\">issue 663</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/711\">issue 711</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/714\">issue 714</a>)</p>", "<p>Encode unicode URL value when creating Links in RegexLinkExtractor (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/561\">issue 561</a>)</p>", "<p>Ignore None values in ItemLoader processors (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/556\">issue 556</a>)</p>", "<p>Fix link text when there is an inner tag in SGMLLinkExtractor and\nHtmlParserLinkExtractor (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/485\">issue 485</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/574\">issue 574</a>)</p>", "<p>Fix wrong checks on subclassing of deprecated classes\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/581\">issue 581</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/584\">issue 584</a>)</p>", "<p>Handle errors caused by inspect.stack() failures (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/582\">issue 582</a>)</p>", "<p>Fix a reference to unexistent engine attribute (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/593\">issue 593</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/594\">issue 594</a>)</p>", "<p>Fix dynamic itemclass example usage of type() (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/603\">issue 603</a>)</p>", "<p>Use lucasdemarchi/codespell to fix typos (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/628\">issue 628</a>)</p>", "<p>Fix default value of attrs argument in SgmlLinkExtractor to be tuple (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/661\">issue 661</a>)</p>", "<p>Fix XXE flaw in sitemap reader (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/676\">issue 676</a>)</p>", "<p>Fix engine to support filtered start requests (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/707\">issue 707</a>)</p>", "<p>Fix offsite middleware case on urls with no hostnames (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/745\">issue 745</a>)</p>", "<p>Testsuite doesn’t require PIL anymore (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/585\">issue 585</a>)</p>", "<p>fix a reference to unexistent engine.slots. closes #593 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/13c099a\">commit 13c099a</a>)</p>", "<p>downloaderMW doc typo (spiderMW doc copy remnant) (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/8ae11bf\">commit 8ae11bf</a>)</p>", "<p>Correct typos (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/1346037\">commit 1346037</a>)</p>", "<p>localhost666 can resolve under certain circumstances (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/2ec2279\">commit 2ec2279</a>)</p>", "<p>test inspect.stack failure (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/cc3eda3\">commit cc3eda3</a>)</p>", "<p>Handle cases when inspect.stack() fails (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/8cb44f9\">commit 8cb44f9</a>)</p>", "<p>Fix wrong checks on subclassing of deprecated classes. closes #581 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/46d98d6\">commit 46d98d6</a>)</p>", "<p>Docs: 4-space indent for final spider example (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/13846de\">commit 13846de</a>)</p>", "<p>Fix HtmlParserLinkExtractor and tests after #485 merge (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/368a946\">commit 368a946</a>)</p>", "<p>BaseSgmlLinkExtractor: Fixed the missing space when the link has an inner tag (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/b566388\">commit b566388</a>)</p>", "<p>BaseSgmlLinkExtractor: Added unit test of a link with an inner tag (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/c1cb418\">commit c1cb418</a>)</p>", "<p>BaseSgmlLinkExtractor: Fixed unknown_endtag() so that it only set current_link=None when the end tag match the opening tag (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/7e4d627\">commit 7e4d627</a>)</p>", "<p>Fix tests for Travis-CI build (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/76c7e20\">commit 76c7e20</a>)</p>", "<p>replace unencodeable codepoints with html entities. fixes #562 and #285 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/5f87b17\">commit 5f87b17</a>)</p>", "<p>RegexLinkExtractor: encode URL unicode value when creating Links (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/d0ee545\">commit d0ee545</a>)</p>", "<p>Updated the tutorial crawl output with latest output. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/8da65de\">commit 8da65de</a>)</p>", "<p>Updated shell docs with the crawler reference and fixed the actual shell output. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/875b9ab\">commit 875b9ab</a>)</p>", "<p>PEP8 minor edits. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/f89efaf\">commit f89efaf</a>)</p>", "<p>Expose current crawler in the Scrapy shell. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/5349cec\">commit 5349cec</a>)</p>", "<p>Unused re import and PEP8 minor edits. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/387f414\">commit 387f414</a>)</p>", "<p>Ignore None’s values when using the ItemLoader. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/0632546\">commit 0632546</a>)</p>", "<p>DOC Fixed HTTPCACHE_STORAGE typo in the default value which is now Filesystem instead Dbm. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/cde9a8c\">commit cde9a8c</a>)</p>", "<p>show Ubuntu setup instructions as literal code (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/fb5c9c5\">commit fb5c9c5</a>)</p>", "<p>Update Ubuntu installation instructions (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/70fb105\">commit 70fb105</a>)</p>", "<p>Merge pull request #550 from stray-leone/patch-1 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/6f70b6a\">commit 6f70b6a</a>)</p>", "<p>modify the version of Scrapy Ubuntu package (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/725900d\">commit 725900d</a>)</p>", "<p>fix 0.22.0 release date (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/af0219a\">commit af0219a</a>)</p>", "<p>fix typos in news.rst and remove (not released yet) header (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/b7f58f4\">commit b7f58f4</a>)</p>", "<p>[<strong>Backward incompatible</strong>] Switched HTTPCacheMiddleware backend to filesystem (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/541\">issue 541</a>)\nTo restore old backend set <code class=\"docutils literal notranslate\"><span class=\"pre\">HTTPCACHE_STORAGE</span></code> to <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.contrib.httpcache.DbmCacheStorage</span></code></p>", "<p>Proxy https:// urls using CONNECT method (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/392\">issue 392</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/397\">issue 397</a>)</p>", "<p>Add a middleware to crawl ajax crawlable pages as defined by google (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/343\">issue 343</a>)</p>", "<p>Rename scrapy.spider.BaseSpider to scrapy.spider.Spider (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/510\">issue 510</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/519\">issue 519</a>)</p>", "<p>Selectors register EXSLT namespaces by default (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/472\">issue 472</a>)</p>", "<p>Unify item loaders similar to selectors renaming (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/461\">issue 461</a>)</p>", "<p>Make <code class=\"docutils literal notranslate\"><span class=\"pre\">RFPDupeFilter</span></code> class easily subclassable (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/533\">issue 533</a>)</p>", "<p>Improve test coverage and forthcoming Python 3 support (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/525\">issue 525</a>)</p>", "<p>Promote startup info on settings and middleware to INFO level (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/520\">issue 520</a>)</p>", "<p>Support partials in <code class=\"docutils literal notranslate\"><span class=\"pre\">get_func_args</span></code> util (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/506\">issue 506</a>, issue:<cite>504</cite>)</p>", "<p>Allow running individual tests via tox (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/503\">issue 503</a>)</p>", "<p>Update extensions ignored by link extractors (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/498\">issue 498</a>)</p>", "<p>Add middleware methods to get files/images/thumbs paths (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/490\">issue 490</a>)</p>", "<p>Improve offsite middleware tests (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/478\">issue 478</a>)</p>", "<p>Add a way to skip default Referer header set by RefererMiddleware (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/475\">issue 475</a>)</p>", "<p>Do not send <code class=\"docutils literal notranslate\"><span class=\"pre\">x-gzip</span></code> in default <code class=\"docutils literal notranslate\"><span class=\"pre\">Accept-Encoding</span></code> header (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/469\">issue 469</a>)</p>", "<p>Support defining http error handling using settings (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/466\">issue 466</a>)</p>", "<p>Use modern python idioms wherever you find legacies (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/497\">issue 497</a>)</p>", "<p>Improve and correct documentation\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/527\">issue 527</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/524\">issue 524</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/521\">issue 521</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/517\">issue 517</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/512\">issue 512</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/505\">issue 505</a>,\n<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/502\">issue 502</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/489\">issue 489</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/465\">issue 465</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/460\">issue 460</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/425\">issue 425</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/536\">issue 536</a>)</p>", "<p>Update Selector class imports in CrawlSpider template (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/484\">issue 484</a>)</p>", "<p>Fix unexistent reference to <code class=\"docutils literal notranslate\"><span class=\"pre\">engine.slots</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/464\">issue 464</a>)</p>", "<p>Do not try to call <code class=\"docutils literal notranslate\"><span class=\"pre\">body_as_unicode()</span></code> on a non-TextResponse instance (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/462\">issue 462</a>)</p>", "<p>Warn when subclassing XPathItemLoader, previously it only warned on\ninstantiation. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/523\">issue 523</a>)</p>", "<p>Warn when subclassing XPathSelector, previously it only warned on\ninstantiation. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/537\">issue 537</a>)</p>", "<p>Multiple fixes to memory stats (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/531\">issue 531</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/530\">issue 530</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/529\">issue 529</a>)</p>", "<p>Fix overriding url in <code class=\"docutils literal notranslate\"><span class=\"pre\">FormRequest.from_response()</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/507\">issue 507</a>)</p>", "<p>Fix tests runner under pip 1.5 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/513\">issue 513</a>)</p>", "<p>Fix logging error when spider name is unicode (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/479\">issue 479</a>)</p>", "<p>Update CrawlSpider Template with Selector changes (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/6d1457d\">commit 6d1457d</a>)</p>", "<p>fix method name in tutorial. closes GH-480 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/b4fc359\">commit b4fc359</a></p>", "<p>include_package_data is required to build wheels from published sources (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/5ba1ad5\">commit 5ba1ad5</a>)</p>", "<p>process_parallel was leaking the failures on its internal deferreds.  closes #458 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/419a780\">commit 419a780</a>)</p>", "<p>New Selector’s API including CSS selectors (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/395\">issue 395</a> and <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/426\">issue 426</a>),</p>", "<p>Request/Response url/body attributes are now immutable\n(modifying them had been deprecated for a long time)</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-ITEM_PIPELINES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">ITEM_PIPELINES</span></code></a> is now defined as a dict (instead of a list)</p>", "<p>Sitemap spider can fetch alternate URLs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/360\">issue 360</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">Selector.remove_namespaces()</span></code> now remove namespaces from element’s attributes. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/416\">issue 416</a>)</p>", "<p>Paved the road for Python 3.3+ (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/435\">issue 435</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/436\">issue 436</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/431\">issue 431</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/452\">issue 452</a>)</p>", "<p>New item exporter using native python types with nesting support (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/366\">issue 366</a>)</p>", "<p>Tune HTTP1.1 pool size so it matches concurrency defined by settings (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/b43b5f575\">commit b43b5f575</a>)</p>", "<p>scrapy.mail.MailSender now can connect over TLS or upgrade using STARTTLS (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/327\">issue 327</a>)</p>", "<p>New FilesPipeline with functionality factored out from ImagesPipeline (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/370\">issue 370</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/409\">issue 409</a>)</p>", "<p>Recommend Pillow instead of PIL for image handling (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/317\">issue 317</a>)</p>", "<p>Added Debian packages for Ubuntu Quantal and Raring (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/86230c0\">commit 86230c0</a>)</p>", "<p>Mock server (used for tests) can listen for HTTPS requests (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/410\">issue 410</a>)</p>", "<p>Remove multi spider support from multiple core components\n(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/422\">issue 422</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/421\">issue 421</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/420\">issue 420</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/419\">issue 419</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/423\">issue 423</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/418\">issue 418</a>)</p>", "<p>Travis-CI now tests Scrapy changes against development versions of <code class=\"docutils literal notranslate\"><span class=\"pre\">w3lib</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">queuelib</span></code> python packages.</p>", "<p>Add pypy 2.1 to continuous integration tests (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/ecfa7431\">commit ecfa7431</a>)</p>", "<p>Pylinted, pep8 and removed old-style exceptions from source (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/430\">issue 430</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/432\">issue 432</a>)</p>", "<p>Use importlib for parametric imports (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/445\">issue 445</a>)</p>", "<p>Handle a regression introduced in Python 2.7.5 that affects XmlItemExporter (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/372\">issue 372</a>)</p>", "<p>Bugfix crawling shutdown on SIGINT (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/450\">issue 450</a>)</p>", "<p>Do not submit <code class=\"docutils literal notranslate\"><span class=\"pre\">reset</span></code> type inputs in FormRequest.from_response (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/b326b87\">commit b326b87</a>)</p>", "<p>Do not silence download errors when request errback raises an exception (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/684cfc0\">commit 684cfc0</a>)</p>", "<p>Fix tests under Django 1.6 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/b6bed44c\">commit b6bed44c</a>)</p>", "<p>Lot of bugfixes to retry middleware under disconnections using HTTP 1.1 download handler</p>", "<p>Fix inconsistencies among Twisted releases (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/406\">issue 406</a>)</p>", "<p>Fix Scrapy shell bugs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/418\">issue 418</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/407\">issue 407</a>)</p>", "<p>Fix invalid variable name in setup.py (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/429\">issue 429</a>)</p>", "<p>Fix tutorial references (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/387\">issue 387</a>)</p>", "<p>Improve request-response docs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/391\">issue 391</a>)</p>", "<p>Improve best practices docs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/399\">issue 399</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/400\">issue 400</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/401\">issue 401</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/402\">issue 402</a>)</p>", "<p>Improve django integration docs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/404\">issue 404</a>)</p>", "<p>Document <code class=\"docutils literal notranslate\"><span class=\"pre\">bindaddress</span></code> request meta (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/37c24e01d7\">commit 37c24e01d7</a>)</p>", "<p>Improve <code class=\"docutils literal notranslate\"><span class=\"pre\">Request</span></code> class documentation (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/226\">issue 226</a>)</p>", "<p>Dropped Python 2.6 support (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/448\">issue 448</a>)</p>", "<p>Add <a class=\"reference external\" href=\"https://cssselect.readthedocs.io/en/latest/index.html\" title=\"(in cssselect v1.2.0)\"><span class=\"xref std std-doc\">cssselect</span></a> python package as install dependency</p>", "<p>Drop libxml2 and multi selector’s backend support, <a class=\"reference external\" href=\"https://lxml.de/\">lxml</a> is required from now on.</p>", "<p>Minimum Twisted version increased to 10.0.0, dropped Twisted 8.0 support.</p>", "<p>Running test suite now requires <code class=\"docutils literal notranslate\"><span class=\"pre\">mock</span></code> python library (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/390\">issue 390</a>)</p>", "<p>Thanks to everyone who contribute to this release!</p>", "<p>List of contributors sorted by number of commits:</p>", "<p>IPython refuses to update the namespace. fix #396 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/3d32c4f\">commit 3d32c4f</a>)</p>", "<p>Fix AlreadyCalledError replacing a request in shell command. closes #407 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/b1d8919\">commit b1d8919</a>)</p>", "<p>Fix start_requests laziness and early hangs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/89faf52\">commit 89faf52</a>)</p>", "<p>fix regression on lazy evaluation of start requests (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/12693a5\">commit 12693a5</a>)</p>", "<p>forms: do not submit reset inputs (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/e429f63\">commit e429f63</a>)</p>", "<p>increase unittest timeouts to decrease travis false positive failures (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/912202e\">commit 912202e</a>)</p>", "<p>backport master fixes to json exporter (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/cfc2d46\">commit cfc2d46</a>)</p>", "<p>Fix permission and set umask before generating sdist tarball (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/06149e0\">commit 06149e0</a>)</p>", "<p>Backport <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">check</span></code> command fixes and backward compatible multi\ncrawler process(<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/339\">issue 339</a>)</p>", "<p>remove extra import added by cherry picked changes (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/d20304e\">commit d20304e</a>)</p>", "<p>fix crawling tests under twisted pre 11.0.0 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/1994f38\">commit 1994f38</a>)</p>", "<p>py26 can not format zero length fields {} (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/abf756f\">commit abf756f</a>)</p>", "<p>test PotentiaDataLoss errors on unbound responses (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/b15470d\">commit b15470d</a>)</p>", "<p>Treat responses without content-length or Transfer-Encoding as good responses (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/c4bf324\">commit c4bf324</a>)</p>", "<p>do no include ResponseFailed if http11 handler is not enabled (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/6cbe684\">commit 6cbe684</a>)</p>", "<p>New HTTP client wraps connection lost in ResponseFailed exception. fix #373 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/1a20bba\">commit 1a20bba</a>)</p>", "<p>limit travis-ci build matrix (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/3b01bb8\">commit 3b01bb8</a>)</p>", "<p>Merge pull request #375 from peterarenot/patch-1 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/fa766d7\">commit fa766d7</a>)</p>", "<p>Fixed so it refers to the correct folder (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/3283809\">commit 3283809</a>)</p>", "<p>added Quantal &amp; Raring to support Ubuntu releases (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/1411923\">commit 1411923</a>)</p>", "<p>fix retry middleware which didn’t retry certain connection errors after the upgrade to http1 client, closes GH-373 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/bb35ed0\">commit bb35ed0</a>)</p>", "<p>fix XmlItemExporter in Python 2.7.4 and 2.7.5 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/de3e451\">commit de3e451</a>)</p>", "<p>minor updates to 0.18 release notes (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/c45e5f1\">commit c45e5f1</a>)</p>", "<p>fix contributors list format (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/0b60031\">commit 0b60031</a>)</p>", "<p>Lot of improvements to testsuite run using Tox, including a way to test on pypi</p>", "<p>Handle GET parameters for AJAX crawlable urls (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/3fe2a32\">commit 3fe2a32</a>)</p>", "<p>Use lxml recover option to parse sitemaps (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/347\">issue 347</a>)</p>", "<p>Bugfix cookie merging by hostname and not by netloc (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/352\">issue 352</a>)</p>", "<p>Support disabling <code class=\"docutils literal notranslate\"><span class=\"pre\">HttpCompressionMiddleware</span></code> using a flag setting (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/359\">issue 359</a>)</p>", "<p>Support xml namespaces using <code class=\"docutils literal notranslate\"><span class=\"pre\">iternodes</span></code> parser in <code class=\"docutils literal notranslate\"><span class=\"pre\">XMLFeedSpider</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/12\">issue 12</a>)</p>", "<p>Support <code class=\"docutils literal notranslate\"><span class=\"pre\">dont_cache</span></code> request meta flag (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/19\">issue 19</a>)</p>", "<p>Bugfix <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.gz.gunzip</span></code> broken by changes in python 2.7.4 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/4dc76e\">commit 4dc76e</a>)</p>", "<p>Bugfix url encoding on <code class=\"docutils literal notranslate\"><span class=\"pre\">SgmlLinkExtractor</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/24\">issue 24</a>)</p>", "<p>Bugfix <code class=\"docutils literal notranslate\"><span class=\"pre\">TakeFirst</span></code> processor shouldn’t discard zero (0) value (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/59\">issue 59</a>)</p>", "<p>Support nested items in xml exporter (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/66\">issue 66</a>)</p>", "<p>Improve cookies handling performance (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/77\">issue 77</a>)</p>", "<p>Log dupe filtered requests once (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/105\">issue 105</a>)</p>", "<p>Split redirection middleware into status and meta based middlewares (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/78\">issue 78</a>)</p>", "<p>Use HTTP1.1 as default downloader handler (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/109\">issue 109</a> and <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/318\">issue 318</a>)</p>", "<p>Support xpath form selection on <code class=\"docutils literal notranslate\"><span class=\"pre\">FormRequest.from_response</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/185\">issue 185</a>)</p>", "<p>Bugfix unicode decoding error on <code class=\"docutils literal notranslate\"><span class=\"pre\">SgmlLinkExtractor</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/199\">issue 199</a>)</p>", "<p>Bugfix signal dispatching on pypi interpreter (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/205\">issue 205</a>)</p>", "<p>Improve request delay and concurrency handling (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/206\">issue 206</a>)</p>", "<p>Add RFC2616 cache policy to <code class=\"docutils literal notranslate\"><span class=\"pre\">HttpCacheMiddleware</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/212\">issue 212</a>)</p>", "<p>Allow customization of messages logged by engine (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/214\">issue 214</a>)</p>", "<p>Multiples improvements to <code class=\"docutils literal notranslate\"><span class=\"pre\">DjangoItem</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/217\">issue 217</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/218\">issue 218</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/221\">issue 221</a>)</p>", "<p>Extend Scrapy commands using setuptools entry points (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/260\">issue 260</a>)</p>", "<p>Allow spider <code class=\"docutils literal notranslate\"><span class=\"pre\">allowed_domains</span></code> value to be set/tuple (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/261\">issue 261</a>)</p>", "<p>Support <code class=\"docutils literal notranslate\"><span class=\"pre\">settings.getdict</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/269\">issue 269</a>)</p>", "<p>Simplify internal <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.core.scraper</span></code> slot handling (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/271\">issue 271</a>)</p>", "<p>Added <code class=\"docutils literal notranslate\"><span class=\"pre\">Item.copy</span></code> (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/290\">issue 290</a>)</p>", "<p>Collect idle downloader slots (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/297\">issue 297</a>)</p>", "<p>Add <code class=\"docutils literal notranslate\"><span class=\"pre\">ftp://</span></code> scheme downloader handler (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/329\">issue 329</a>)</p>", "<p>Added downloader benchmark webserver and spider tools <a class=\"hoverxref tooltip reference internal\" href=\"topics/benchmarking.html#benchmarking\"><span class=\"std std-ref\">Benchmarking</span></a></p>", "<p>Moved persistent (on disk) queues to a separate project (<a class=\"reference external\" href=\"https://github.com/scrapy/queuelib\">queuelib</a>) which Scrapy now depends on</p>", "<p>Add Scrapy commands using external libraries (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/260\">issue 260</a>)</p>", "<p>Added <code class=\"docutils literal notranslate\"><span class=\"pre\">--pdb</span></code> option to <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span></code> command line tool</p>", "<p>Added <a class=\"reference internal\" href=\"topics/selectors.html#scrapy.selector.Selector.remove_namespaces\" title=\"scrapy.selector.Selector.remove_namespaces\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">XPathSelector.remove_namespaces</span></code></a> which allows to remove all namespaces from XML documents for convenience (to work with namespace-less XPaths). Documented in <a class=\"hoverxref tooltip reference internal\" href=\"topics/selectors.html#topics-selectors\"><span class=\"std std-ref\">Selectors</span></a>.</p>", "<p>Several improvements to spider contracts</p>", "<p>New default middleware named MetaRefreshMiddleware that handles meta-refresh html tag redirections,</p>", "<p>MetaRefreshMiddleware and RedirectMiddleware have different priorities to address #62</p>", "<p>added from_crawler method to spiders</p>", "<p>added system tests with mock server</p>", "<p>more improvements to macOS compatibility (thanks Alex Cepoi)</p>", "<p>several more cleanups to singletons and multi-spider support (thanks Nicolas Ramirez)</p>", "<p>support custom download slots</p>", "<p>added –spider option to “shell” command.</p>", "<p>log overridden settings when Scrapy starts</p>", "<p>Thanks to everyone who contribute to this release. Here is a list of\ncontributors sorted by number of commits:</p>", "<p>obey request method when Scrapy deploy is redirected to a new endpoint (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/8c4fcee\">commit 8c4fcee</a>)</p>", "<p>fix inaccurate downloader middleware documentation. refs #280 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/40667cb\">commit 40667cb</a>)</p>", "<p>doc: remove links to diveintopython.org, which is no longer available. closes #246 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/bd58bfa\">commit bd58bfa</a>)</p>", "<p>Find form nodes in invalid html5 documents (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/e3d6945\">commit e3d6945</a>)</p>", "<p>Fix typo labeling attrs type bool instead of list (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/a274276\">commit a274276</a>)</p>", "<p>fixes spelling errors in documentation (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/6d2b3aa\">commit 6d2b3aa</a>)</p>", "<p>add doc about disabling an extension. refs #132 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/c90de33\">commit c90de33</a>)</p>", "<p>Fixed error message formatting. log.err() doesn’t support cool formatting and when error occurred, the message was:    “ERROR: Error processing %(item)s” (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/c16150c\">commit c16150c</a>)</p>", "<p>lint and improve images pipeline error logging (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/56b45fc\">commit 56b45fc</a>)</p>", "<p>fixed doc typos (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/243be84\">commit 243be84</a>)</p>", "<p>add documentation topics: Broad Crawls &amp; Common Practices (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/1fbb715\">commit 1fbb715</a>)</p>", "<p>fix bug in Scrapy parse command when spider is not specified explicitly. closes #209 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/c72e682\">commit c72e682</a>)</p>", "<p>Update docs/topics/commands.rst (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/28eac7a\">commit 28eac7a</a>)</p>", "<p>Remove concurrency limitation when using download delays and still ensure inter-request delays are enforced (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/487b9b5\">commit 487b9b5</a>)</p>", "<p>add error details when image pipeline fails (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/8232569\">commit 8232569</a>)</p>", "<p>improve macOS compatibility (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/8dcf8aa\">commit 8dcf8aa</a>)</p>", "<p>setup.py: use README.rst to populate long_description (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/7b5310d\">commit 7b5310d</a>)</p>", "<p>doc: removed obsolete references to ClientForm (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/80f9bb6\">commit 80f9bb6</a>)</p>", "<p>correct docs for default storage backend (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/2aa491b\">commit 2aa491b</a>)</p>", "<p>doc: removed broken proxyhub link from FAQ (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/bdf61c4\">commit bdf61c4</a>)</p>", "<p>Fixed docs typo in SpiderOpenCloseLogging example (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/7184094\">commit 7184094</a>)</p>", "<p>Scrapy contracts: python2.6 compat (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/a4a9199\">commit a4a9199</a>)</p>", "<p>Scrapy contracts verbose option (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/ec41673\">commit ec41673</a>)</p>", "<p>proper unittest-like output for Scrapy contracts (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/86635e4\">commit 86635e4</a>)</p>", "<p>added open_in_browser to debugging doc (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/c9b690d\">commit c9b690d</a>)</p>", "<p>removed reference to global Scrapy stats from settings doc (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/dd55067\">commit dd55067</a>)</p>", "<p>Fix SpiderState bug in Windows platforms (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/58998f4\">commit 58998f4</a>)</p>", "<p>fixed LogStats extension, which got broken after a wrong merge before the 0.16 release (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/8c780fd\">commit 8c780fd</a>)</p>", "<p>better backward compatibility for scrapy.conf.settings (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/3403089\">commit 3403089</a>)</p>", "<p>extended documentation on how to access crawler stats from extensions (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/c4da0b5\">commit c4da0b5</a>)</p>", "<p>removed .hgtags (no longer needed now that Scrapy uses git) (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/d52c188\">commit d52c188</a>)</p>", "<p>fix dashes under rst headers (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/fa4f7f9\">commit fa4f7f9</a>)</p>", "<p>set release date for 0.16.0 in news (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/e292246\">commit e292246</a>)</p>", "<p>Scrapy changes:</p>", "<p>added <a class=\"hoverxref tooltip reference internal\" href=\"topics/contracts.html#topics-contracts\"><span class=\"std std-ref\">Spiders Contracts</span></a>, a mechanism for testing spiders in a formal/reproducible way</p>", "<p>added options <code class=\"docutils literal notranslate\"><span class=\"pre\">-o</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">-t</span></code> to the <a class=\"hoverxref tooltip reference internal\" href=\"topics/commands.html#std-command-runspider\"><code class=\"xref std std-command docutils literal notranslate\"><span class=\"pre\">runspider</span></code></a> command</p>", "<p>documented <a class=\"reference internal\" href=\"topics/autothrottle.html\"><span class=\"doc\">AutoThrottle extension</span></a> and added to extensions installed by default. You still need to enable it with <a class=\"hoverxref tooltip reference internal\" href=\"topics/autothrottle.html#std-setting-AUTOTHROTTLE_ENABLED\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">AUTOTHROTTLE_ENABLED</span></code></a></p>", "<p>major Stats Collection refactoring: removed separation of global/per-spider stats, removed stats-related signals (<code class=\"docutils literal notranslate\"><span class=\"pre\">stats_spider_opened</span></code>, etc). Stats are much simpler now, backward compatibility is kept on the Stats Collector API and signals.</p>", "<p>added <a class=\"reference internal\" href=\"topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_start_requests\" title=\"scrapy.spidermiddlewares.SpiderMiddleware.process_start_requests\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_start_requests()</span></code></a> method to spider middlewares</p>", "<p>dropped Signals singleton. Signals should now be accessed through the Crawler.signals attribute. See the signals documentation for more info.</p>", "<p>dropped Stats Collector singleton. Stats can now be accessed through the Crawler.stats attribute. See the stats collection documentation for more info.</p>", "<p>documented <a class=\"hoverxref tooltip reference internal\" href=\"topics/api.html#topics-api\"><span class=\"std std-ref\">Core API</span></a></p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">lxml</span></code> is now the default selectors backend instead of <code class=\"docutils literal notranslate\"><span class=\"pre\">libxml2</span></code></p>", "<p>ported FormRequest.from_response() to use <a class=\"reference external\" href=\"https://lxml.de/\">lxml</a> instead of <a class=\"reference external\" href=\"http://wwwsearch.sourceforge.net/old/ClientForm/\">ClientForm</a></p>", "<p>removed modules: <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.xlib.BeautifulSoup</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.xlib.ClientForm</span></code></p>", "<p>SitemapSpider: added support for sitemap urls ending in .xml and .xml.gz, even if they advertise a wrong content type (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/10ed28b\">commit 10ed28b</a>)</p>", "<p>StackTraceDump extension: also dump trackref live references (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/fe2ce93\">commit fe2ce93</a>)</p>", "<p>nested items now fully supported in JSON and JSONLines exporters</p>", "<p>added <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-reqmeta-cookiejar\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">cookiejar</span></code></a> Request meta key to support multiple cookie sessions per spider</p>", "<p>decoupled encoding detection code to <a class=\"reference external\" href=\"https://github.com/scrapy/w3lib/blob/master/w3lib/encoding.py\">w3lib.encoding</a>, and ported Scrapy code to use that module</p>", "<p>dropped support for Python 2.5. See <a class=\"reference external\" href=\"https://blog.scrapinghub.com/2012/02/27/scrapy-0-15-dropping-support-for-python-2-5/\">https://blog.scrapinghub.com/2012/02/27/scrapy-0-15-dropping-support-for-python-2-5/</a></p>", "<p>dropped support for Twisted 2.5</p>", "<p>added <a class=\"hoverxref tooltip reference internal\" href=\"topics/spider-middleware.html#std-setting-REFERER_ENABLED\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">REFERER_ENABLED</span></code></a> setting, to control referer middleware</p>", "<p>changed default user agent to: <code class=\"docutils literal notranslate\"><span class=\"pre\">Scrapy/VERSION</span> <span class=\"pre\">(+http://scrapy.org)</span></code></p>", "<p>removed (undocumented) <code class=\"docutils literal notranslate\"><span class=\"pre\">HTMLImageLinkExtractor</span></code> class from <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.contrib.linkextractors.image</span></code></p>", "<p>removed per-spider settings (to be replaced by instantiating multiple crawler objects)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">USER_AGENT</span></code> spider attribute will no longer work, use <code class=\"docutils literal notranslate\"><span class=\"pre\">user_agent</span></code> attribute instead</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">DOWNLOAD_TIMEOUT</span></code> spider attribute will no longer work, use <code class=\"docutils literal notranslate\"><span class=\"pre\">download_timeout</span></code> attribute instead</p>", "<p>removed <code class=\"docutils literal notranslate\"><span class=\"pre\">ENCODING_ALIASES</span></code> setting, as encoding auto-detection has been moved to the <a class=\"reference external\" href=\"https://github.com/scrapy/w3lib\">w3lib</a> library</p>", "<p>promoted <a class=\"hoverxref tooltip reference internal\" href=\"topics/djangoitem.html#topics-djangoitem\"><span class=\"std std-ref\">DjangoItem</span></a> to main contrib</p>", "<p>LogFormatter method now return dicts(instead of strings) to support lazy formatting (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/164\">issue 164</a>, <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/dcef7b0\">commit dcef7b0</a>)</p>", "<p>downloader handlers (<a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-DOWNLOAD_HANDLERS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOAD_HANDLERS</span></code></a> setting) now receive settings as the first argument of the <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method</p>", "<p>replaced memory usage accounting with (more portable) <a class=\"reference external\" href=\"https://docs.python.org/2/library/resource.html\">resource</a> module, removed <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.memory</span></code> module</p>", "<p>removed signal: <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.mail.mail_sent</span></code></p>", "<p>removed <code class=\"docutils literal notranslate\"><span class=\"pre\">TRACK_REFS</span></code> setting, now <a class=\"hoverxref tooltip reference internal\" href=\"topics/leaks.html#topics-leaks-trackrefs\"><span class=\"std std-ref\">trackrefs</span></a> is always enabled</p>", "<p>DBM is now the default storage backend for HTTP cache middleware</p>", "<p>number of log messages (per level) are now tracked through Scrapy stats (stat name: <code class=\"docutils literal notranslate\"><span class=\"pre\">log_count/LEVEL</span></code>)</p>", "<p>number received responses are now tracked through Scrapy stats (stat name: <code class=\"docutils literal notranslate\"><span class=\"pre\">response_received_count</span></code>)</p>", "<p>removed <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.log.started</span></code> attribute</p>", "<p>added precise to supported Ubuntu distros (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/b7e46df\">commit b7e46df</a>)</p>", "<p>fixed bug in json-rpc webservice reported in <a class=\"reference external\" href=\"https://groups.google.com/forum/#!topic/scrapy-users/qgVBmFybNAQ/discussion\">https://groups.google.com/forum/#!topic/scrapy-users/qgVBmFybNAQ/discussion</a>. also removed no longer supported ‘run’ command from extras/scrapy-ws.py (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/340fbdb\">commit 340fbdb</a>)</p>", "<p>meta tag attributes for content-type http equiv can be in any order. #123 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/0cb68af\">commit 0cb68af</a>)</p>", "<p>replace “import Image” by more standard “from PIL import Image”. closes #88 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/4d17048\">commit 4d17048</a>)</p>", "<p>return trial status as bin/runtests.sh exit value. #118 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/b7b2e7f\">commit b7b2e7f</a>)</p>", "<p>forgot to include pydispatch license. #118 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/fd85f9c\">commit fd85f9c</a>)</p>", "<p>include egg files used by testsuite in source distribution. #118 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/c897793\">commit c897793</a>)</p>", "<p>update docstring in project template to avoid confusion with genspider command, which may be considered as an advanced feature. refs #107 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/2548dcc\">commit 2548dcc</a>)</p>", "<p>added note to docs/topics/firebug.rst about google directory being shut down (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/668e352\">commit 668e352</a>)</p>", "<p>don’t discard slot when empty, just save in another dict in order to recycle if needed again. (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/8e9f607\">commit 8e9f607</a>)</p>", "<p>do not fail handling unicode xpaths in libxml2 backed selectors (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/b830e95\">commit b830e95</a>)</p>", "<p>fixed minor mistake in Request objects documentation (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/bf3c9ee\">commit bf3c9ee</a>)</p>", "<p>fixed minor defect in link extractors documentation (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/ba14f38\">commit ba14f38</a>)</p>", "<p>removed some obsolete remaining code related to sqlite support in Scrapy (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/0665175\">commit 0665175</a>)</p>", "<p>move buffer pointing to start of file before computing checksum. refs #92 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/6a5bef2\">commit 6a5bef2</a>)</p>", "<p>Compute image checksum before persisting images. closes #92 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/9817df1\">commit 9817df1</a>)</p>", "<p>remove leaking references in cached failures (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/673a120\">commit 673a120</a>)</p>", "<p>fixed bug in MemoryUsage extension: get_engine_status() takes exactly 1 argument (0 given) (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/11133e9\">commit 11133e9</a>)</p>", "<p>fixed struct.error on http compression middleware. closes #87 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/1423140\">commit 1423140</a>)</p>", "<p>ajax crawling wasn’t expanding for unicode urls (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/0de3fb4\">commit 0de3fb4</a>)</p>", "<p>Catch start_requests iterator errors. refs #83 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/454a21d\">commit 454a21d</a>)</p>", "<p>Speed-up libxml2 XPathSelector (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/2fbd662\">commit 2fbd662</a>)</p>", "<p>updated versioning doc according to recent changes (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/0a070f5\">commit 0a070f5</a>)</p>", "<p>scrapyd: fixed documentation link (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/2b4e4c3\">commit 2b4e4c3</a>)</p>", "<p>extras/makedeb.py: no longer obtaining version from git (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/caffe0e\">commit caffe0e</a>)</p>", "<p>extras/makedeb.py: no longer obtaining version from git (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/caffe0e\">commit caffe0e</a>)</p>", "<p>bumped version to 0.14.1 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/6cb9e1c\">commit 6cb9e1c</a>)</p>", "<p>fixed reference to tutorial directory (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/4b86bd6\">commit 4b86bd6</a>)</p>", "<p>doc: removed duplicated callback argument from Request.replace() (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/1aeccdd\">commit 1aeccdd</a>)</p>", "<p>fixed formatting of scrapyd doc (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/8bf19e6\">commit 8bf19e6</a>)</p>", "<p>Dump stacks for all running threads and fix engine status dumped by StackTraceDump extension (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/14a8e6e\">commit 14a8e6e</a>)</p>", "<p>added comment about why we disable ssl on boto images upload (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/5223575\">commit 5223575</a>)</p>", "<p>SSL handshaking hangs when doing too many parallel connections to S3 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/63d583d\">commit 63d583d</a>)</p>", "<p>change tutorial to follow changes on dmoz site (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/bcb3198\">commit bcb3198</a>)</p>", "<p>Avoid _disconnectedDeferred AttributeError exception in Twisted&gt;=11.1.0 (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/98f3f87\">commit 98f3f87</a>)</p>", "<p>allow spider to set autothrottle max concurrency (<a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/commit/175a4b5\">commit 175a4b5</a>)</p>", "<p>Support for <a class=\"reference external\" href=\"https://developers.google.com/search/docs/ajax-crawling/docs/getting-started?csw=1\">AJAX crawlable urls</a></p>", "<p>New persistent scheduler that stores requests on disk, allowing to suspend and resume crawls (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2737\">r2737</a>)</p>", "<p>added <code class=\"docutils literal notranslate\"><span class=\"pre\">-o</span></code> option to <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">crawl</span></code>, a shortcut for dumping scraped items into a file (or standard output using <code class=\"docutils literal notranslate\"><span class=\"pre\">-</span></code>)</p>", "<p>Added support for passing custom settings to Scrapyd <code class=\"docutils literal notranslate\"><span class=\"pre\">schedule.json</span></code> api (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2779\">r2779</a>, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2783\">r2783</a>)</p>", "<p>New <code class=\"docutils literal notranslate\"><span class=\"pre\">ChunkedTransferMiddleware</span></code> (enabled by default) to support <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Chunked_transfer_encoding\">chunked transfer encoding</a> (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2769\">r2769</a>)</p>", "<p>Add boto 2.0 support for S3 downloader handler (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2763\">r2763</a>)</p>", "<p>Added <a class=\"reference external\" href=\"https://docs.python.org/2/library/marshal.html\">marshal</a> to formats supported by feed exports (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2744\">r2744</a>)</p>", "<p>In request errbacks, offending requests are now received in <code class=\"docutils literal notranslate\"><span class=\"pre\">failure.request</span></code> attribute (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2738\">r2738</a>)</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-CONCURRENT_REQUESTS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">CONCURRENT_REQUESTS</span></code></a>, <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a>, <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-CONCURRENT_REQUESTS_PER_IP\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">CONCURRENT_REQUESTS_PER_IP</span></code></a></p>", "<p>check the documentation for more details</p>", "<p>Added builtin caching DNS resolver (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2728\">r2728</a>)</p>", "<p>Moved Amazon AWS-related components/extensions (SQS spider queue, SimpleDB stats collector) to a separate project: [scaws](<a class=\"reference external\" href=\"https://github.com/scrapinghub/scaws\">https://github.com/scrapinghub/scaws</a>) (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2706\">r2706</a>, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2714\">r2714</a>)</p>", "<p>Moved spider queues to scrapyd: <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.spiderqueue</span></code> -&gt; <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapyd.spiderqueue</span></code> (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2708\">r2708</a>)</p>", "<p>Moved sqlite utils to scrapyd: <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.sqlite</span></code> -&gt; <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapyd.sqlite</span></code> (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2781\">r2781</a>)</p>", "<p>Real support for returning iterators on <code class=\"docutils literal notranslate\"><span class=\"pre\">start_requests()</span></code> method. The iterator is now consumed during the crawl when the spider is getting idle (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2704\">r2704</a>)</p>", "<p>Added <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-setting-REDIRECT_ENABLED\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">REDIRECT_ENABLED</span></code></a> setting to quickly enable/disable the redirect middleware (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2697\">r2697</a>)</p>", "<p>Added <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-setting-RETRY_ENABLED\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">RETRY_ENABLED</span></code></a> setting to quickly enable/disable the retry middleware (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2694\">r2694</a>)</p>", "<p>Added <code class=\"docutils literal notranslate\"><span class=\"pre\">CloseSpider</span></code> exception to manually close spiders (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2691\">r2691</a>)</p>", "<p>Improved encoding detection by adding support for HTML5 meta charset declaration (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2690\">r2690</a>)</p>", "<p>Refactored close spider behavior to wait for all downloads to finish and be processed by spiders, before closing the spider (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2688\">r2688</a>)</p>", "<p>Added <code class=\"docutils literal notranslate\"><span class=\"pre\">SitemapSpider</span></code> (see documentation in Spiders page) (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2658\">r2658</a>)</p>", "<p>Added <code class=\"docutils literal notranslate\"><span class=\"pre\">LogStats</span></code> extension for periodically logging basic stats (like crawled pages and scraped items) (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2657\">r2657</a>)</p>", "<p>Make handling of gzipped responses more robust (#319, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2643\">r2643</a>). Now Scrapy will try and decompress as much as possible from a gzipped response, instead of failing with an <code class=\"docutils literal notranslate\"><span class=\"pre\">IOError</span></code>.</p>", "<p>Simplified !MemoryDebugger extension to use stats for dumping memory debugging info (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2639\">r2639</a>)</p>", "<p>Added new command to edit spiders: <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">edit</span></code> (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2636\">r2636</a>) and <code class=\"docutils literal notranslate\"><span class=\"pre\">-e</span></code> flag to <code class=\"docutils literal notranslate\"><span class=\"pre\">genspider</span></code> command that uses it (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2653\">r2653</a>)</p>", "<p>Changed default representation of items to pretty-printed dicts. (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2631\">r2631</a>). This improves default logging by making log more readable in the default case, for both Scraped and Dropped lines.</p>", "<p>Added <a class=\"hoverxref tooltip reference internal\" href=\"topics/signals.html#std-signal-spider_error\"><code class=\"xref std std-signal docutils literal notranslate\"><span class=\"pre\">spider_error</span></code></a> signal (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2628\">r2628</a>)</p>", "<p>Added <a class=\"hoverxref tooltip reference internal\" href=\"topics/downloader-middleware.html#std-setting-COOKIES_ENABLED\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">COOKIES_ENABLED</span></code></a> setting (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2625\">r2625</a>)</p>", "<p>Stats are now dumped to Scrapy log (default value of <a class=\"hoverxref tooltip reference internal\" href=\"topics/settings.html#std-setting-STATS_DUMP\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">STATS_DUMP</span></code></a> setting has been changed to <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code>). This is to make Scrapy users more aware of Scrapy stats and the data that is collected there.</p>", "<p>Added support for dynamically adjusting download delay and maximum concurrent requests (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2599\">r2599</a>)</p>", "<p>Added new DBM HTTP cache storage backend (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2576\">r2576</a>)</p>", "<p>Added <code class=\"docutils literal notranslate\"><span class=\"pre\">listjobs.json</span></code> API to Scrapyd (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2571\">r2571</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">CsvItemExporter</span></code>: added <code class=\"docutils literal notranslate\"><span class=\"pre\">join_multivalued</span></code> parameter (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2578\">r2578</a>)</p>", "<p>Added namespace support to <code class=\"docutils literal notranslate\"><span class=\"pre\">xmliter_lxml</span></code> (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2552\">r2552</a>)</p>", "<p>Improved cookies middleware by making <code class=\"docutils literal notranslate\"><span class=\"pre\">COOKIES_DEBUG</span></code> nicer and documenting it (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2579\">r2579</a>)</p>", "<p>Several improvements to Scrapyd and Link extractors</p>", "<p>original item_scraped signal was removed</p>", "<p>original item_passed signal was renamed to item_scraped</p>", "<p>old log lines <code class=\"docutils literal notranslate\"><span class=\"pre\">Scraped</span> <span class=\"pre\">Item...</span></code> were removed</p>", "<p>old log lines <code class=\"docutils literal notranslate\"><span class=\"pre\">Passed</span> <span class=\"pre\">Item...</span></code> were renamed to <code class=\"docutils literal notranslate\"><span class=\"pre\">Scraped</span> <span class=\"pre\">Item...</span></code> lines and downgraded to <code class=\"docutils literal notranslate\"><span class=\"pre\">DEBUG</span></code> level</p>", "<p><a class=\"reference external\" href=\"https://github.com/scrapy/w3lib\">w3lib</a> (several functions from <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.{http,markup,multipart,response,url}</span></code>, done in <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2584\">r2584</a>)</p>", "<p><a class=\"reference external\" href=\"https://github.com/scrapy/scrapely\">scrapely</a> (was <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.contrib.ibl</span></code>, done in <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2586\">r2586</a>)</p>", "<p>Removed unused function: <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.request.request_info()</span></code> (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2577\">r2577</a>)</p>", "<p>Removed googledir project from <code class=\"docutils literal notranslate\"><span class=\"pre\">examples/googledir</span></code>. There’s now a new example project called <code class=\"docutils literal notranslate\"><span class=\"pre\">dirbot</span></code> available on GitHub: <a class=\"reference external\" href=\"https://github.com/scrapy/dirbot\">https://github.com/scrapy/dirbot</a></p>", "<p>Removed support for default field values in Scrapy items (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2616\">r2616</a>)</p>", "<p>Removed experimental crawlspider v2 (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2632\">r2632</a>)</p>", "<p>Removed scheduler middleware to simplify architecture. Duplicates filter is now done in the scheduler itself, using the same dupe filtering class as before (<code class=\"docutils literal notranslate\"><span class=\"pre\">DUPEFILTER_CLASS</span></code> setting) (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2640\">r2640</a>)</p>", "<p>Removed support for passing urls to <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">crawl</span></code> command (use <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">parse</span></code> instead) (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2704\">r2704</a>)</p>", "<p>Removed deprecated Execution Queue (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2704\">r2704</a>)</p>", "<p>Removed (undocumented) spider context extension (from scrapy.contrib.spidercontext) (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2780\">r2780</a>)</p>", "<p>removed <code class=\"docutils literal notranslate\"><span class=\"pre\">CONCURRENT_SPIDERS</span></code> setting (use scrapyd maxproc instead) (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2789\">r2789</a>)</p>", "<p>Renamed attributes of core components: downloader.sites -&gt; downloader.slots, scraper.sites -&gt; scraper.slots (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2717\">r2717</a>, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2718\">r2718</a>)</p>", "<p>Renamed setting <code class=\"docutils literal notranslate\"><span class=\"pre\">CLOSESPIDER_ITEMPASSED</span></code> to <a class=\"hoverxref tooltip reference internal\" href=\"topics/extensions.html#std-setting-CLOSESPIDER_ITEMCOUNT\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">CLOSESPIDER_ITEMCOUNT</span></code></a> (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2655\">r2655</a>). Backward compatibility kept.</p>", "<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>", "<p>Passed item is now sent in the <code class=\"docutils literal notranslate\"><span class=\"pre\">item</span></code> argument of the <a class=\"hoverxref tooltip reference internal\" href=\"topics/signals.html#std-signal-item_scraped\"><code class=\"xref std std-signal docutils literal notranslate\"><span class=\"pre\">item_passed</span></code></a> (#273)</p>", "<p>Added verbose option to <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">version</span></code> command, useful for bug reports (#298)</p>", "<p>HTTP cache now stored by default in the project data dir (#279)</p>", "<p>Added project data storage directory (#276, #277)</p>", "<p>Documented file structure of Scrapy projects (see command-line tool doc)</p>", "<p>New lxml backend for XPath selectors (#147)</p>", "<p>Per-spider settings (#245)</p>", "<p>Support exit codes to signal errors in Scrapy commands (#248)</p>", "<p>Added <code class=\"docutils literal notranslate\"><span class=\"pre\">-c</span></code> argument to <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">shell</span></code> command</p>", "<p>Made <code class=\"docutils literal notranslate\"><span class=\"pre\">libxml2</span></code> optional (#260)</p>", "<p>New <code class=\"docutils literal notranslate\"><span class=\"pre\">deploy</span></code> command (#261)</p>", "<p>Added <a class=\"hoverxref tooltip reference internal\" href=\"topics/extensions.html#std-setting-CLOSESPIDER_PAGECOUNT\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">CLOSESPIDER_PAGECOUNT</span></code></a> setting (#253)</p>", "<p>Added <a class=\"hoverxref tooltip reference internal\" href=\"topics/extensions.html#std-setting-CLOSESPIDER_ERRORCOUNT\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">CLOSESPIDER_ERRORCOUNT</span></code></a> setting (#254)</p>", "<p>Scrapyd now uses one process per spider</p>", "<p>It stores one log file per spider run, and rotate them keeping the latest 5 logs per spider (by default)</p>", "<p>A minimal web ui was added, available at <a class=\"reference external\" href=\"http://localhost:6800\">http://localhost:6800</a> by default</p>", "<p>There is now a <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span> <span class=\"pre\">server</span></code> command to start a Scrapyd server of the current project</p>", "<p>added <code class=\"docutils literal notranslate\"><span class=\"pre\">HTTPCACHE_ENABLED</span></code> setting (False by default) to enable HTTP cache middleware</p>", "<p>changed <code class=\"docutils literal notranslate\"><span class=\"pre\">HTTPCACHE_EXPIRATION_SECS</span></code> semantics: now zero means “never expire”.</p>", "<p>Deprecated <code class=\"docutils literal notranslate\"><span class=\"pre\">runserver</span></code> command in favor of <code class=\"docutils literal notranslate\"><span class=\"pre\">server</span></code> command which starts a Scrapyd server. See also: Scrapyd changes</p>", "<p>Deprecated <code class=\"docutils literal notranslate\"><span class=\"pre\">queue</span></code> command in favor of using Scrapyd <code class=\"docutils literal notranslate\"><span class=\"pre\">schedule.json</span></code> API. See also: Scrapyd changes</p>", "<p>Removed the !LxmlItemLoader (experimental contrib which never graduated to main contrib)</p>", "<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>", "<p>New Scrapy service called <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapyd</span></code> for deploying Scrapy crawlers in production (#218) (documentation available)</p>", "<p>Simplified Images pipeline usage which doesn’t require subclassing your own images pipeline now (#217)</p>", "<p>Scrapy shell now shows the Scrapy log by default (#206)</p>", "<p>Refactored execution queue in a common base code and pluggable backends called “spider queues” (#220)</p>", "<p>New persistent spider queue (based on SQLite) (#198), available by default, which allows to start Scrapy in server mode and then schedule spiders to run.</p>", "<p>Added documentation for Scrapy command-line tool and all its available sub-commands. (documentation available)</p>", "<p>Feed exporters with pluggable backends (#197) (documentation available)</p>", "<p>Deferred signals (#193)</p>", "<p>Added two new methods to item pipeline open_spider(), close_spider() with deferred support (#195)</p>", "<p>Support for overriding default request headers per spider (#181)</p>", "<p>Replaced default Spider Manager with one with similar functionality but not depending on Twisted Plugins (#186)</p>", "<p>Split Debian package into two packages - the library and the service (#187)</p>", "<p>Scrapy log refactoring (#188)</p>", "<p>New extension for keeping persistent spider contexts among different runs (#203)</p>", "<p>Added <code class=\"docutils literal notranslate\"><span class=\"pre\">dont_redirect</span></code> request.meta key for avoiding redirects (#233)</p>", "<p>Added <code class=\"docutils literal notranslate\"><span class=\"pre\">dont_retry</span></code> request.meta key for avoiding retries (#234)</p>", "<p>New <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span></code> command which replaces the old <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy-ctl.py</span></code> (#199)\n- there is only one global <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy</span></code> command now, instead of one <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy-ctl.py</span></code> per project\n- Added <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.bat</span></code> script for running more conveniently from Windows</p>", "<p>Added bash completion to command-line tool (#210)</p>", "<p>Renamed command <code class=\"docutils literal notranslate\"><span class=\"pre\">start</span></code> to <code class=\"docutils literal notranslate\"><span class=\"pre\">runserver</span></code> (#209)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">url</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">body</span></code> attributes of Request objects are now read-only (#230)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">Request.copy()</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">Request.replace()</span></code> now also copies their <code class=\"docutils literal notranslate\"><span class=\"pre\">callback</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">errback</span></code> attributes (#231)</p>", "<p>Removed <code class=\"docutils literal notranslate\"><span class=\"pre\">UrlFilterMiddleware</span></code> from <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.contrib</span></code> (already disabled by default)</p>", "<p>Offsite middleware doesn’t filter out any request coming from a spider that doesn’t have a allowed_domains attribute (#225)</p>", "<p>Removed Spider Manager <code class=\"docutils literal notranslate\"><span class=\"pre\">load()</span></code> method. Now spiders are loaded in the <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method itself.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.core.manager.ScrapyManager</span></code> class renamed to <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.crawler.Crawler</span></code></p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.core.manager.scrapymanager</span></code> singleton moved to <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.project.crawler</span></code></p>", "<p>Moved module: <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.contrib.spidermanager</span></code> to <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.spidermanager</span></code></p>", "<p>Spider Manager singleton moved from <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.spider.spiders</span></code> to the <code class=\"docutils literal notranslate\"><span class=\"pre\">spiders`</span> <span class=\"pre\">attribute</span> <span class=\"pre\">of</span> <span class=\"pre\">``scrapy.project.crawler</span></code> singleton.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.stats.collector.StatsCollector</span></code> to <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.statscol.StatsCollector</span></code></p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.stats.collector.SimpledbStatsCollector</span></code> to <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.contrib.statscol.SimpledbStatsCollector</span></code></p>", "<p>default per-command settings are now specified in the <code class=\"docutils literal notranslate\"><span class=\"pre\">default_settings</span></code> attribute of command object class (#201)</p>", "<p>backward compatibility kept (with deprecation warning)</p>", "<p>backward compatibility kept (with deprecation warning)</p>", "<p>backward compatibility kept (with deprecation warning)</p>", "<p>added <code class=\"docutils literal notranslate\"><span class=\"pre\">handles_request()</span></code> class method to <code class=\"docutils literal notranslate\"><span class=\"pre\">BaseSpider</span></code></p>", "<p>dropped <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.log.exc()</span></code> function (use <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.log.err()</span></code> instead)</p>", "<p>dropped <code class=\"docutils literal notranslate\"><span class=\"pre\">component</span></code> argument of <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.log.msg()</span></code> function</p>", "<p>dropped <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.log.log_level</span></code> attribute</p>", "<p>Added <code class=\"docutils literal notranslate\"><span class=\"pre\">from_settings()</span></code> class methods to Spider Manager, and Item Pipeline Manager</p>", "<p>Added <code class=\"docutils literal notranslate\"><span class=\"pre\">HTTPCACHE_IGNORE_SCHEMES</span></code> setting to ignore certain schemes on !HttpCacheMiddleware (#225)</p>", "<p>Added <code class=\"docutils literal notranslate\"><span class=\"pre\">SPIDER_QUEUE_CLASS</span></code> setting which defines the spider queue to use (#220)</p>", "<p>Added <code class=\"docutils literal notranslate\"><span class=\"pre\">KEEP_ALIVE</span></code> setting (#220)</p>", "<p>Removed <code class=\"docutils literal notranslate\"><span class=\"pre\">SERVICE_QUEUE</span></code> setting (#220)</p>", "<p>Removed <code class=\"docutils literal notranslate\"><span class=\"pre\">COMMANDS_SETTINGS_MODULE</span></code> setting (#201)</p>", "<p>Renamed <code class=\"docutils literal notranslate\"><span class=\"pre\">REQUEST_HANDLERS</span></code> to <code class=\"docutils literal notranslate\"><span class=\"pre\">DOWNLOAD_HANDLERS</span></code> and make download handlers classes (instead of functions)</p>", "<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>", "<p>Added SMTP-AUTH support to scrapy.mail</p>", "<p>New settings added: <code class=\"docutils literal notranslate\"><span class=\"pre\">MAIL_USER</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">MAIL_PASS</span></code> (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2065\">r2065</a> | #149)</p>", "<p>Added new scrapy-ctl view command - To view URL in the browser, as seen by Scrapy (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2039\">r2039</a>)</p>", "<p>Added web service for controlling Scrapy process (this also deprecates the web console. (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2053\">r2053</a> | #167)</p>", "<p>Support for running Scrapy as a service, for production systems (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1988\">r1988</a>, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2054\">r2054</a>, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2055\">r2055</a>, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2056\">r2056</a>, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2057\">r2057</a> | #168)</p>", "<p>Added wrapper induction library (documentation only available in source code for now). (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2011\">r2011</a>)</p>", "<p>Simplified and improved response encoding support (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1961\">r1961</a>, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1969\">r1969</a>)</p>", "<p>Added <code class=\"docutils literal notranslate\"><span class=\"pre\">LOG_ENCODING</span></code> setting (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1956\">r1956</a>, documentation available)</p>", "<p>Added <code class=\"docutils literal notranslate\"><span class=\"pre\">RANDOMIZE_DOWNLOAD_DELAY</span></code> setting (enabled by default) (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1923\">r1923</a>, doc available)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">MailSender</span></code> is no longer IO-blocking (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1955\">r1955</a> | #146)</p>", "<p>Linkextractors and new Crawlspider now handle relative base tag urls (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1960\">r1960</a> | #148)</p>", "<p>Several improvements to Item Loaders and processors (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2022\">r2022</a>, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2023\">r2023</a>, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2024\">r2024</a>, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2025\">r2025</a>, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2026\">r2026</a>, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2027\">r2027</a>, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2028\">r2028</a>, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2029\">r2029</a>, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2030\">r2030</a>)</p>", "<p>Added support for adding variables to telnet console (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2047\">r2047</a> | #165)</p>", "<p>Support for requests without callbacks (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2050\">r2050</a> | #166)</p>", "<p>Change <code class=\"docutils literal notranslate\"><span class=\"pre\">Spider.domain_name</span></code> to <code class=\"docutils literal notranslate\"><span class=\"pre\">Spider.name</span></code> (SEP-012, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1975\">r1975</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">Response.encoding</span></code> is now the detected encoding (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1961\">r1961</a>)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">HttpErrorMiddleware</span></code> now returns None or raises an exception (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2006\">r2006</a> | #157)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.command</span></code> modules relocation (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2035\">r2035</a>, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2036\">r2036</a>, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2037\">r2037</a>)</p>", "<p>Added <code class=\"docutils literal notranslate\"><span class=\"pre\">ExecutionQueue</span></code> for feeding spiders to scrape (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2034\">r2034</a>)</p>", "<p>Removed <code class=\"docutils literal notranslate\"><span class=\"pre\">ExecutionEngine</span></code> singleton (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2039\">r2039</a>)</p>", "<p>Ported <code class=\"docutils literal notranslate\"><span class=\"pre\">S3ImagesStore</span></code> (images pipeline) to use boto and threads (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2033\">r2033</a>)</p>", "<p>Moved module: <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.management.telnet</span></code> to <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.telnet</span></code> (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/2047\">r2047</a>)</p>", "<p>Changed default <code class=\"docutils literal notranslate\"><span class=\"pre\">SCHEDULER_ORDER</span></code> to <code class=\"docutils literal notranslate\"><span class=\"pre\">DFO</span></code> (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1939\">r1939</a>)</p>", "<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>", "<p>Added DEFAULT_RESPONSE_ENCODING setting (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1809\">r1809</a>)</p>", "<p>Added <code class=\"docutils literal notranslate\"><span class=\"pre\">dont_click</span></code> argument to <code class=\"docutils literal notranslate\"><span class=\"pre\">FormRequest.from_response()</span></code> method (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1813\">r1813</a>, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1816\">r1816</a>)</p>", "<p>Added <code class=\"docutils literal notranslate\"><span class=\"pre\">clickdata</span></code> argument to <code class=\"docutils literal notranslate\"><span class=\"pre\">FormRequest.from_response()</span></code> method (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1802\">r1802</a>, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1803\">r1803</a>)</p>", "<p>Added support for HTTP proxies (<code class=\"docutils literal notranslate\"><span class=\"pre\">HttpProxyMiddleware</span></code>) (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1781\">r1781</a>, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1785\">r1785</a>)</p>", "<p>Offsite spider middleware now logs messages when filtering out requests (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1841\">r1841</a>)</p>", "<p>Changed <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.response.get_meta_refresh()</span></code> signature (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1804\">r1804</a>)</p>", "<p>Removed deprecated <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.item.ScrapedItem</span></code> class - use <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.item.Item</span> <span class=\"pre\">instead</span></code> (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1838\">r1838</a>)</p>", "<p>Removed deprecated <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.xpath</span></code> module - use <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.selector</span></code> instead. (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1836\">r1836</a>)</p>", "<p>Removed deprecated <code class=\"docutils literal notranslate\"><span class=\"pre\">core.signals.domain_open</span></code> signal - use <code class=\"docutils literal notranslate\"><span class=\"pre\">core.signals.domain_opened</span></code> instead (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1822\">r1822</a>)</p>", "<p>Old domain argument has been deprecated and will be removed in 0.9. For spiders, you should always use the <code class=\"docutils literal notranslate\"><span class=\"pre\">spider</span></code> argument and pass spider references. If you really want to pass a string, use the <code class=\"docutils literal notranslate\"><span class=\"pre\">component</span></code> argument instead.</p>", "<p>Changed core signals <code class=\"docutils literal notranslate\"><span class=\"pre\">domain_opened</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">domain_closed</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">domain_idle</span></code></p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">domain</span></code> argument of  <code class=\"docutils literal notranslate\"><span class=\"pre\">process_item()</span></code> item pipeline method was changed to  <code class=\"docutils literal notranslate\"><span class=\"pre\">spider</span></code>, the new signature is: <code class=\"docutils literal notranslate\"><span class=\"pre\">process_item(spider,</span> <span class=\"pre\">item)</span></code> (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1827\">r1827</a> | #105)</p>", "<p>To quickly port your code (to work with Scrapy 0.8) just use <code class=\"docutils literal notranslate\"><span class=\"pre\">spider.domain_name</span></code> where you previously used <code class=\"docutils literal notranslate\"><span class=\"pre\">domain</span></code>.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">StatsCollector</span></code> was changed to receive spider references (instead of domains) in its methods (<code class=\"docutils literal notranslate\"><span class=\"pre\">set_value</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">inc_value</span></code>, etc).</p>", "<p>added <code class=\"docutils literal notranslate\"><span class=\"pre\">StatsCollector.iter_spider_stats()</span></code> method</p>", "<p>removed <code class=\"docutils literal notranslate\"><span class=\"pre\">StatsCollector.list_domains()</span></code> method</p>", "<p>Also, Stats signals were renamed and now pass around spider references (instead of domains). Here’s a summary of the changes:</p>", "<p>To quickly port your code (to work with Scrapy 0.8) just use <code class=\"docutils literal notranslate\"><span class=\"pre\">spider.domain_name</span></code> where you previously used <code class=\"docutils literal notranslate\"><span class=\"pre\">domain</span></code>. <code class=\"docutils literal notranslate\"><span class=\"pre\">spider_stats</span></code> contains exactly the same data as <code class=\"docutils literal notranslate\"><span class=\"pre\">domain_stats</span></code>.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">CLOSEDOMAIN_TIMEOUT</span></code> to <code class=\"docutils literal notranslate\"><span class=\"pre\">CLOSESPIDER_TIMEOUT</span></code></p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">CLOSEDOMAIN_ITEMCOUNT</span></code> to <code class=\"docutils literal notranslate\"><span class=\"pre\">CLOSESPIDER_ITEMCOUNT</span></code></p>", "<p>Removed deprecated <code class=\"docutils literal notranslate\"><span class=\"pre\">SCRAPYSETTINGS_MODULE</span></code> environment variable - use <code class=\"docutils literal notranslate\"><span class=\"pre\">SCRAPY_SETTINGS_MODULE</span></code> instead (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1840\">r1840</a>)</p>", "<p>Renamed setting: <code class=\"docutils literal notranslate\"><span class=\"pre\">REQUESTS_PER_DOMAIN</span></code> to <code class=\"docutils literal notranslate\"><span class=\"pre\">CONCURRENT_REQUESTS_PER_SPIDER</span></code> (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1830\">r1830</a>, <a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1844\">r1844</a>)</p>", "<p>Renamed setting: <code class=\"docutils literal notranslate\"><span class=\"pre\">CONCURRENT_DOMAINS</span></code> to <code class=\"docutils literal notranslate\"><span class=\"pre\">CONCURRENT_SPIDERS</span></code> (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1830\">r1830</a>)</p>", "<p>Refactored HTTP Cache middleware</p>", "<p>HTTP Cache middleware has been heavily refactored, retaining the same functionality except for the domain sectorization which was removed. (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1843\">r1843</a> )</p>", "<p>Renamed exception: <code class=\"docutils literal notranslate\"><span class=\"pre\">DontCloseDomain</span></code> to <code class=\"docutils literal notranslate\"><span class=\"pre\">DontCloseSpider</span></code> (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1859\">r1859</a> | #120)</p>", "<p>Renamed extension: <code class=\"docutils literal notranslate\"><span class=\"pre\">DelayedCloseDomain</span></code> to <code class=\"docutils literal notranslate\"><span class=\"pre\">SpiderCloseDelay</span></code> (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1861\">r1861</a> | #121)</p>", "<p>Removed obsolete <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.markup.remove_escape_chars</span></code> function - use <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.markup.replace_escape_chars</span></code> instead (<a class=\"reference external\" href=\"http://hg.scrapy.org/scrapy/changeset/1865\">r1865</a>)</p>", "<p>First release of Scrapy.</p>"]}, "code_blocks": ["feedexport/success_count/<storage type>\nfeedexport/failed_count/<storage type>\n</pre>", "urllength/request_ignored_count\n</pre>", "httpcompression/response_bytes\nhttpcompression/response_count\n</pre>", ">>> item = MyItem()\n>>> item[\"field\"] = \"value1\"\n>>> loader = ItemLoader(item=item)\n>>> item[\"field\"]\n['value1']\n</pre>", "for href in response.css('li.page a::attr(href)').extract():\n    url = response.urljoin(href)\n    yield scrapy.Request(url, self.parse, encoding=response.encoding)\n</pre>", "for a in response.css('li.page a'):\n    yield response.follow(a, self.parse)\n</pre>", "class MyItem(scrapy.Item):\n    url = scrapy.Field()\n\nclass MySpider(scrapy.Spider):\n    def parse(self, response):\n        return MyItem(url=response.url)\n</pre>", "class MySpider(scrapy.Spider):\n    def parse(self, response):\n        return {'url': response.url}\n</pre>", "class MySpider(scrapy.Spider):\n    custom_settings = {\n        \"DOWNLOAD_DELAY\": 5.0,\n        \"RETRY_ENABLED\": False,\n    }\n</pre>", "from scrapy import log\nlog.msg('MESSAGE', log.INFO)\n</pre>", "import logging\nlogging.info('MESSAGE')\n</pre>", "class MySpider(scrapy.Spider):\n    def parse(self, response):\n        self.logger.info('Response received')\n</pre>", "from scrapy.crawler import CrawlerProcess\n\nprocess = CrawlerProcess({\n    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n})\nprocess.crawl(MySpider)\nprocess.start()\n</pre>", "69 Daniel Graña <dangra@...>\n37 Pablo Hoffman <pablo@...>\n13 Mikhail Korobov <kmike84@...>\n 9 Alex Cepoi <alex.cepoi@...>\n 9 alexanderlukanin13 <alexander.lukanin.13@...>\n 8 Rolando Espinoza La fuente <darkrho@...>\n 8 Lukasz Biedrycki <lukasz.biedrycki@...>\n 6 Nicolas Ramirez <nramirez.uy@...>\n 3 Paul Tremberth <paul.tremberth@...>\n 2 Martin Olveyra <molveyra@...>\n 2 Stefan <misc@...>\n 2 Rolando Espinoza <darkrho@...>\n 2 Loren Davie <loren@...>\n 2 irgmedeiros <irgmedeiros@...>\n 1 Stefan Koch <taikano@...>\n 1 Stefan <cct@...>\n 1 scraperdragon <dragon@...>\n 1 Kumara Tharmalingam <ktharmal@...>\n 1 Francesco Piccinno <stack.box@...>\n 1 Marcos Campal <duendex@...>\n 1 Dragon Dave <dragon@...>\n 1 Capi Etheriel <barraponto@...>\n 1 cacovsky <amarquesferraz@...>\n 1 Berend Iwema <berend@...>\n</pre>", "130 Pablo Hoffman <pablo@...>\n 97 Daniel Graña <dangra@...>\n 20 Nicolás Ramírez <nramirez.uy@...>\n 13 Mikhail Korobov <kmike84@...>\n 12 Pedro Faustino <pedrobandim@...>\n 11 Steven Almeroth <sroth77@...>\n  5 Rolando Espinoza La fuente <darkrho@...>\n  4 Michal Danilak <mimino.coder@...>\n  4 Alex Cepoi <alex.cepoi@...>\n  4 Alexandr N Zamaraev (aka tonal) <tonal@...>\n  3 paul <paul.tremberth@...>\n  3 Martin Olveyra <molveyra@...>\n  3 Jordi Llonch <llonchj@...>\n  3 arijitchakraborty <myself.arijit@...>\n  2 Shane Evans <shane.evans@...>\n  2 joehillen <joehillen@...>\n  2 Hart <HartSimha@...>\n  2 Dan <ellisd23@...>\n  1 Zuhao Wan <wanzuhao@...>\n  1 whodatninja <blake@...>\n  1 vkrest <v.krestiannykov@...>\n  1 tpeng <pengtaoo@...>\n  1 Tom Mortimer-Jones <tom@...>\n  1 Rocio Aramberri <roschegel@...>\n  1 Pedro <pedro@...>\n  1 notsobad <wangxiaohugg@...>\n  1 Natan L <kuyanatan.nlao@...>\n  1 Mark Grey <mark.grey@...>\n  1 Luan <luanpab@...>\n  1 Libor Nenadál <libor.nenadal@...>\n  1 Juan M Uys <opyate@...>\n  1 Jonas Brunsgaard <jonas.brunsgaard@...>\n  1 Ilya Baryshev <baryshev@...>\n  1 Hasnain Lakhani <m.hasnain.lakhani@...>\n  1 Emanuel Schorsch <emschorsch@...>\n  1 Chris Tilden <chris.tilden@...>\n  1 Capi Etheriel <barraponto@...>\n  1 cacovsky <amarquesferraz@...>\n  1 Berend Iwema <berend@...>\n</pre>"], "links": [{"text": "settings", "href": "topics/settings.html#topics-settings"}, {"text": "from_crawler()", "href": "topics/spiders.html#scrapy.Spider.from_crawler"}, {"text": "spider\narguments", "href": "topics/spiders.html#spiderargs"}, {"text": "scrapy.crawler.Crawler", "href": "topics/api.html#scrapy.crawler.Crawler"}, {"text": "crawl()", "href": "topics/api.html#scrapy.crawler.Crawler.crawl"}, {"text": "Crawler", "href": "topics/api.html#scrapy.crawler.Crawler"}, {"text": "crawl()", "href": "topics/api.html#scrapy.crawler.Crawler.crawl"}, {"text": "issue 6038", "href": "https://github.com/scrapy/scrapy/issues/6038"}, {"text": "scrapy.Spider.from_crawler()", "href": "topics/spiders.html#scrapy.Spider.from_crawler"}, {"text": "scrapy.Spider.from_crawler()", "href": "topics/spiders.html#scrapy.Spider.from_crawler"}, {"text": "start_requests()", "href": "topics/spiders.html#scrapy.Spider.start_requests"}, {"text": "issue 6038", "href": "https://github.com/scrapy/scrapy/issues/6038"}, {"text": "TextResponse.json", "href": "topics/request-response.html#scrapy.http.TextResponse.json"}, {"text": "issue 6016", "href": "https://github.com/scrapy/scrapy/issues/6016"}, {"text": "PythonItemExporter", "href": "topics/exporters.html#scrapy.exporters.PythonItemExporter"}, {"text": "issue 6006", "href": "https://github.com/scrapy/scrapy/issues/6006"}, {"text": "issue 6007", "href": "https://github.com/scrapy/scrapy/issues/6007"}, {"text": "issue 6010", "href": "https://github.com/scrapy/scrapy/issues/6010"}, {"text": "crawl()", "href": "topics/api.html#scrapy.crawler.Crawler.crawl"}, {"text": "scrapy.crawler.Crawler", "href": "topics/api.html#scrapy.crawler.Crawler"}, {"text": "issue 1587", "href": "https://github.com/scrapy/scrapy/issues/1587"}, {"text": "issue 6040", "href": "https://github.com/scrapy/scrapy/issues/6040"}, {"text": "from_crawler()", "href": "topics/spiders.html#scrapy.Spider.from_crawler"}, {"text": "spider\narguments", "href": "topics/spiders.html#spiderargs"}, {"text": "issue 1305", "href": "https://github.com/scrapy/scrapy/issues/1305"}, {"text": "issue 1580", "href": "https://github.com/scrapy/scrapy/issues/1580"}, {"text": "issue 2392", "href": "https://github.com/scrapy/scrapy/issues/2392"}, {"text": "issue 3663", "href": "https://github.com/scrapy/scrapy/issues/3663"}, {"text": "issue 6038", "href": "https://github.com/scrapy/scrapy/issues/6038"}, {"text": "PeriodicLog", "href": "topics/extensions.html#scrapy.extensions.periodic_log.PeriodicLog"}, {"text": "issue 5926", "href": "https://github.com/scrapy/scrapy/issues/5926"}, {"text": "TextResponse.json", "href": "topics/request-response.html#scrapy.http.TextResponse.json"}, {"text": "issue 5968", "href": "https://github.com/scrapy/scrapy/issues/5968"}, {"text": "issue 6016", "href": "https://github.com/scrapy/scrapy/issues/6016"}, {"text": "link extractors", "href": "topics/link-extractors.html#topics-link-extractors"}, {"text": "issue 6021", "href": "https://github.com/scrapy/scrapy/issues/6021"}, {"text": "issue 6036", "href": "https://github.com/scrapy/scrapy/issues/6036"}, {"text": "MailSender", "href": "topics/email.html#scrapy.mail.MailSender"}, {"text": "send()", "href": "topics/email.html#scrapy.mail.MailSender.send"}, {"text": "issue 5096", "href": "https://github.com/scrapy/scrapy/issues/5096"}, {"text": "issue 5118", "href": "https://github.com/scrapy/scrapy/issues/5118"}, {"text": "RetryMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.retry.RetryMiddleware"}, {"text": "issue 6049", "href": "https://github.com/scrapy/scrapy/issues/6049"}, {"text": "issue 6050", "href": "https://github.com/scrapy/scrapy/issues/6050"}, {"text": "scrapy.settings.BaseSettings.getdictorlist()", "href": "topics/api.html#scrapy.settings.BaseSettings.getdictorlist"}, {"text": "FEED_EXPORT_FIELDS", "href": "topics/feed-exports.html#std-setting-FEED_EXPORT_FIELDS"}, {"text": "issue 6011", "href": "https://github.com/scrapy/scrapy/issues/6011"}, {"text": "issue 6013", "href": "https://github.com/scrapy/scrapy/issues/6013"}, {"text": "issue 6014", "href": "https://github.com/scrapy/scrapy/issues/6014"}, {"text": "issue 6008", "href": "https://github.com/scrapy/scrapy/issues/6008"}, {"text": "issue 6009", "href": "https://github.com/scrapy/scrapy/issues/6009"}, {"text": "issue 6003", "href": "https://github.com/scrapy/scrapy/issues/6003"}, {"text": "issue 6005", "href": "https://github.com/scrapy/scrapy/issues/6005"}, {"text": "issue 6031", "href": "https://github.com/scrapy/scrapy/issues/6031"}, {"text": "issue 6034", "href": "https://github.com/scrapy/scrapy/issues/6034"}, {"text": "brotli", "href": "https://github.com/google/brotli"}, {"text": "issue 6044", "href": "https://github.com/scrapy/scrapy/issues/6044"}, {"text": "issue 6045", "href": "https://github.com/scrapy/scrapy/issues/6045"}, {"text": "issue 6002", "href": "https://github.com/scrapy/scrapy/issues/6002"}, {"text": "issue 6013", "href": "https://github.com/scrapy/scrapy/issues/6013"}, {"text": "issue 6046", "href": "https://github.com/scrapy/scrapy/issues/6046"}, {"text": "issue 6024", "href": "https://github.com/scrapy/scrapy/issues/6024"}, {"text": "issue 6026", "href": "https://github.com/scrapy/scrapy/issues/6026"}, {"text": "issue 5953", "href": "https://github.com/scrapy/scrapy/issues/5953"}, {"text": "issue 5984", "href": "https://github.com/scrapy/scrapy/issues/5984"}, {"text": "lxml", "href": "https://lxml.de/"}, {"text": "cryptography", "href": "https://cryptography.io/en/latest/"}, {"text": "issue 5956", "href": "https://github.com/scrapy/scrapy/issues/5956"}, {"text": "issue 5958", "href": "https://github.com/scrapy/scrapy/issues/5958"}, {"text": "boto3", "href": "https://github.com/boto/boto3"}, {"text": "botocore", "href": "https://github.com/boto/botocore"}, {"text": "issue 5833", "href": "https://github.com/scrapy/scrapy/issues/5833"}, {"text": "FEED_STORE_EMPTY", "href": "topics/feed-exports.html#std-setting-FEED_STORE_EMPTY"}, {"text": "issue 872", "href": "https://github.com/scrapy/scrapy/issues/872"}, {"text": "issue 5847", "href": "https://github.com/scrapy/scrapy/issues/5847"}, {"text": "FEED_URI_PARAMS", "href": "topics/feed-exports.html#std-setting-FEED_URI_PARAMS"}, {"text": "issue 5994", "href": "https://github.com/scrapy/scrapy/issues/5994"}, {"text": "issue 5996", "href": "https://github.com/scrapy/scrapy/issues/5996"}, {"text": "issue 5994", "href": "https://github.com/scrapy/scrapy/issues/5994"}, {"text": "issue 5996", "href": "https://github.com/scrapy/scrapy/issues/5996"}, {"text": "issue 5994", "href": "https://github.com/scrapy/scrapy/issues/5994"}, {"text": "issue 5996", "href": "https://github.com/scrapy/scrapy/issues/5996"}, {"text": "issue 5994", "href": "https://github.com/scrapy/scrapy/issues/5994"}, {"text": "issue 5998", "href": "https://github.com/scrapy/scrapy/issues/5998"}, {"text": "issue 5994", "href": "https://github.com/scrapy/scrapy/issues/5994"}, {"text": "issue 5998", "href": "https://github.com/scrapy/scrapy/issues/5998"}, {"text": "issue 5146", "href": "https://github.com/scrapy/scrapy/issues/5146"}, {"text": "scrapy.settings.BaseSettings.getwithbase()", "href": "topics/api.html#scrapy.settings.BaseSettings.getwithbase"}, {"text": "issue 5726", "href": "https://github.com/scrapy/scrapy/issues/5726"}, {"text": "issue 5923", "href": "https://github.com/scrapy/scrapy/issues/5923"}, {"text": "Scrapy add-ons", "href": "topics/addons.html#topics-addons"}, {"text": "issue 5950", "href": "https://github.com/scrapy/scrapy/issues/5950"}, {"text": "RETRY_EXCEPTIONS", "href": "topics/downloader-middleware.html#std-setting-RETRY_EXCEPTIONS"}, {"text": "RetryMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.retry.RetryMiddleware"}, {"text": "issue 2701", "href": "https://github.com/scrapy/scrapy/issues/2701"}, {"text": "issue 5929", "href": "https://github.com/scrapy/scrapy/issues/5929"}, {"text": "CLOSESPIDER_TIMEOUT_NO_ITEM", "href": "topics/extensions.html#std-setting-CLOSESPIDER_TIMEOUT_NO_ITEM"}, {"text": "issue 5979", "href": "https://github.com/scrapy/scrapy/issues/5979"}, {"text": "AWS_REGION_NAME", "href": "topics/settings.html#std-setting-AWS_REGION_NAME"}, {"text": "issue 5980", "href": "https://github.com/scrapy/scrapy/issues/5980"}, {"text": "pathlib.Path", "href": "https://docs.python.org/3/library/pathlib.html#pathlib.Path"}, {"text": "FEEDS", "href": "topics/feed-exports.html#std-setting-FEEDS"}, {"text": "issue 5939", "href": "https://github.com/scrapy/scrapy/issues/5939"}, {"text": "issue 872", "href": "https://github.com/scrapy/scrapy/issues/872"}, {"text": "issue 5847", "href": "https://github.com/scrapy/scrapy/issues/5847"}, {"text": "issue 5969", "href": "https://github.com/scrapy/scrapy/issues/5969"}, {"text": "issue 5971", "href": "https://github.com/scrapy/scrapy/issues/5971"}, {"text": "boto3", "href": "https://github.com/boto/boto3"}, {"text": "issue 960", "href": "https://github.com/scrapy/scrapy/issues/960"}, {"text": "issue 5735", "href": "https://github.com/scrapy/scrapy/issues/5735"}, {"text": "issue 5833", "href": "https://github.com/scrapy/scrapy/issues/5833"}, {"text": "issue 3090", "href": "https://github.com/scrapy/scrapy/issues/3090"}, {"text": "issue 5952", "href": "https://github.com/scrapy/scrapy/issues/5952"}, {"text": "issue 5043", "href": "https://github.com/scrapy/scrapy/issues/5043"}, {"text": "issue 5705", "href": "https://github.com/scrapy/scrapy/issues/5705"}, {"text": "NotConfigured", "href": "topics/exceptions.html#scrapy.exceptions.NotConfigured"}, {"text": "issue 5950", "href": "https://github.com/scrapy/scrapy/issues/5950"}, {"text": "issue 5992", "href": "https://github.com/scrapy/scrapy/issues/5992"}, {"text": "scrapy.settings.BaseSettings.pop()", "href": "topics/api.html#scrapy.settings.BaseSettings.pop"}, {"text": "issue 5959", "href": "https://github.com/scrapy/scrapy/issues/5959"}, {"text": "issue 5960", "href": "https://github.com/scrapy/scrapy/issues/5960"}, {"text": "issue 5963", "href": "https://github.com/scrapy/scrapy/issues/5963"}, {"text": "issue 5146", "href": "https://github.com/scrapy/scrapy/issues/5146"}, {"text": "scrapy.Spider.update_settings()", "href": "topics/spiders.html#scrapy.Spider.update_settings"}, {"text": "issue 5745", "href": "https://github.com/scrapy/scrapy/issues/5745"}, {"text": "issue 5846", "href": "https://github.com/scrapy/scrapy/issues/5846"}, {"text": "issue 5981", "href": "https://github.com/scrapy/scrapy/issues/5981"}, {"text": "issue 6000", "href": "https://github.com/scrapy/scrapy/issues/6000"}, {"text": "issue 5927", "href": "https://github.com/scrapy/scrapy/issues/5927"}, {"text": "issue 5579", "href": "https://github.com/scrapy/scrapy/issues/5579"}, {"text": "issue 5931", "href": "https://github.com/scrapy/scrapy/issues/5931"}, {"text": "issue 5707", "href": "https://github.com/scrapy/scrapy/issues/5707"}, {"text": "issue 5937", "href": "https://github.com/scrapy/scrapy/issues/5937"}, {"text": "issue 4914", "href": "https://github.com/scrapy/scrapy/issues/4914"}, {"text": "issue 5949", "href": "https://github.com/scrapy/scrapy/issues/5949"}, {"text": "issue 5925", "href": "https://github.com/scrapy/scrapy/issues/5925"}, {"text": "issue 5977", "href": "https://github.com/scrapy/scrapy/issues/5977"}, {"text": "issue 5951", "href": "https://github.com/scrapy/scrapy/issues/5951"}, {"text": "issue 5847", "href": "https://github.com/scrapy/scrapy/issues/5847"}, {"text": "issue 5999", "href": "https://github.com/scrapy/scrapy/issues/5999"}, {"text": "issue 5984", "href": "https://github.com/scrapy/scrapy/issues/5984"}, {"text": "issue 5948", "href": "https://github.com/scrapy/scrapy/issues/5948"}, {"text": "issue 5965", "href": "https://github.com/scrapy/scrapy/issues/5965"}, {"text": "issue 5986", "href": "https://github.com/scrapy/scrapy/issues/5986"}, {"text": "cryptography", "href": "https://cryptography.io/en/latest/"}, {"text": "parsel", "href": "https://github.com/scrapy/parsel"}, {"text": "parsel", "href": "https://github.com/scrapy/parsel"}, {"text": "issue 5876", "href": "https://github.com/scrapy/scrapy/issues/5876"}, {"text": "DOWNLOAD_DELAY", "href": "topics/settings.html#std-setting-DOWNLOAD_DELAY"}, {"text": "CONCURRENT_REQUESTS_PER_DOMAIN", "href": "topics/settings.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"}, {"text": "RANDOMIZE_DOWNLOAD_DELAY", "href": "topics/settings.html#std-setting-RANDOMIZE_DOWNLOAD_DELAY"}, {"text": "DOWNLOAD_SLOTS", "href": "topics/settings.html#std-setting-DOWNLOAD_SLOTS"}, {"text": "issue 5328", "href": "https://github.com/scrapy/scrapy/issues/5328"}, {"text": "parsel", "href": "https://github.com/scrapy/parsel"}, {"text": "issue 5894", "href": "https://github.com/scrapy/scrapy/issues/5894"}, {"text": "issue 5915", "href": "https://github.com/scrapy/scrapy/issues/5915"}, {"text": "feed_slot_closed", "href": "topics/signals.html#std-signal-feed_slot_closed"}, {"text": "feed_exporter_closed", "href": "topics/signals.html#std-signal-feed_exporter_closed"}, {"text": "issue 5876", "href": "https://github.com/scrapy/scrapy/issues/5876"}, {"text": "issue 5892", "href": "https://github.com/scrapy/scrapy/issues/5892"}, {"text": "FILES_STORE", "href": "topics/media-pipeline.html#std-setting-FILES_STORE"}, {"text": "IMAGES_STORE", "href": "topics/media-pipeline.html#std-setting-IMAGES_STORE"}, {"text": "pathlib.Path", "href": "https://docs.python.org/3/library/pathlib.html#pathlib.Path"}, {"text": "issue 5801", "href": "https://github.com/scrapy/scrapy/issues/5801"}, {"text": "issue 5903", "href": "https://github.com/scrapy/scrapy/issues/5903"}, {"text": "issue 5918", "href": "https://github.com/scrapy/scrapy/issues/5918"}, {"text": "issue 5500", "href": "https://github.com/scrapy/scrapy/issues/5500"}, {"text": "issue 5581", "href": "https://github.com/scrapy/scrapy/issues/5581"}, {"text": "scrapy.settings.BaseSettings.setdefault()", "href": "topics/api.html#scrapy.settings.BaseSettings.setdefault"}, {"text": "issue 5811", "href": "https://github.com/scrapy/scrapy/issues/5811"}, {"text": "issue 5821", "href": "https://github.com/scrapy/scrapy/issues/5821"}, {"text": "cryptography", "href": "https://cryptography.io/en/latest/"}, {"text": "DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING", "href": "topics/settings.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING"}, {"text": "issue 5857", "href": "https://github.com/scrapy/scrapy/issues/5857"}, {"text": "issue 5858", "href": "https://github.com/scrapy/scrapy/issues/5858"}, {"text": "FilesPipeline", "href": "topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline"}, {"text": "issue 5874", "href": "https://github.com/scrapy/scrapy/issues/5874"}, {"text": "issue 5891", "href": "https://github.com/scrapy/scrapy/issues/5891"}, {"text": "issue 5899", "href": "https://github.com/scrapy/scrapy/issues/5899"}, {"text": "issue 5901", "href": "https://github.com/scrapy/scrapy/issues/5901"}, {"text": "parse", "href": "topics/commands.html#std-command-parse"}, {"text": "issue 5819", "href": "https://github.com/scrapy/scrapy/issues/5819"}, {"text": "issue 5824", "href": "https://github.com/scrapy/scrapy/issues/5824"}, {"text": "genspider", "href": "topics/commands.html#std-command-genspider"}, {"text": "issue 3553", "href": "https://github.com/scrapy/scrapy/issues/3553"}, {"text": "issue 5808", "href": "https://github.com/scrapy/scrapy/issues/5808"}, {"text": "issue 5831", "href": "https://github.com/scrapy/scrapy/issues/5831"}, {"text": "issue 5832", "href": "https://github.com/scrapy/scrapy/issues/5832"}, {"text": "LinkExtractor", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"}, {"text": "issue 5881", "href": "https://github.com/scrapy/scrapy/issues/5881"}, {"text": "issue 5872", "href": "https://github.com/scrapy/scrapy/issues/5872"}, {"text": "issue 5885", "href": "https://github.com/scrapy/scrapy/issues/5885"}, {"text": "issue 5914", "href": "https://github.com/scrapy/scrapy/issues/5914"}, {"text": "issue 5917", "href": "https://github.com/scrapy/scrapy/issues/5917"}, {"text": "scrapy.mail.MailSender.send()", "href": "topics/email.html#scrapy.mail.MailSender.send"}, {"text": "issue 1611", "href": "https://github.com/scrapy/scrapy/issues/1611"}, {"text": "issue 5880", "href": "https://github.com/scrapy/scrapy/issues/5880"}, {"text": "issue 5109", "href": "https://github.com/scrapy/scrapy/issues/5109"}, {"text": "issue 5851", "href": "https://github.com/scrapy/scrapy/issues/5851"}, {"text": "blacken-docs", "href": "https://github.com/adamchainz/blacken-docs"}, {"text": "issue 5813", "href": "https://github.com/scrapy/scrapy/issues/5813"}, {"text": "issue 5816", "href": "https://github.com/scrapy/scrapy/issues/5816"}, {"text": "issue 5875", "href": "https://github.com/scrapy/scrapy/issues/5875"}, {"text": "issue 5877", "href": "https://github.com/scrapy/scrapy/issues/5877"}, {"text": "issue 5878", "href": "https://github.com/scrapy/scrapy/issues/5878"}, {"text": "issue 5879", "href": "https://github.com/scrapy/scrapy/issues/5879"}, {"text": "issue 5827", "href": "https://github.com/scrapy/scrapy/issues/5827"}, {"text": "issue 5839", "href": "https://github.com/scrapy/scrapy/issues/5839"}, {"text": "issue 5883", "href": "https://github.com/scrapy/scrapy/issues/5883"}, {"text": "issue 5890", "href": "https://github.com/scrapy/scrapy/issues/5890"}, {"text": "issue 5895", "href": "https://github.com/scrapy/scrapy/issues/5895"}, {"text": "issue 5904", "href": "https://github.com/scrapy/scrapy/issues/5904"}, {"text": "issue 5805", "href": "https://github.com/scrapy/scrapy/issues/5805"}, {"text": "issue 5889", "href": "https://github.com/scrapy/scrapy/issues/5889"}, {"text": "issue 5896", "href": "https://github.com/scrapy/scrapy/issues/5896"}, {"text": "issue 5816", "href": "https://github.com/scrapy/scrapy/issues/5816"}, {"text": "issue 5826", "href": "https://github.com/scrapy/scrapy/issues/5826"}, {"text": "issue 5919", "href": "https://github.com/scrapy/scrapy/issues/5919"}, {"text": "issue 5849", "href": "https://github.com/scrapy/scrapy/issues/5849"}, {"text": "issue 5820", "href": "https://github.com/scrapy/scrapy/issues/5820"}, {"text": "issue 5855", "href": "https://github.com/scrapy/scrapy/issues/5855"}, {"text": "issue 5898", "href": "https://github.com/scrapy/scrapy/issues/5898"}, {"text": "parsel", "href": "https://github.com/scrapy/parsel"}, {"text": "issue 5902", "href": "https://github.com/scrapy/scrapy/issues/5902"}, {"text": "issue 5919", "href": "https://github.com/scrapy/scrapy/issues/5919"}, {"text": "issue 5802", "href": "https://github.com/scrapy/scrapy/issues/5802"}, {"text": "issue 5823", "href": "https://github.com/scrapy/scrapy/issues/5823"}, {"text": "issue 5908", "href": "https://github.com/scrapy/scrapy/issues/5908"}, {"text": "read1()", "href": "https://docs.python.org/3/library/io.html#io.BufferedIOBase.read1"}, {"text": "GzipFile", "href": "https://docs.python.org/3/library/gzip.html#gzip.GzipFile"}, {"text": "issue 5719", "href": "https://github.com/scrapy/scrapy/issues/5719"}, {"text": "issue 5719", "href": "https://github.com/scrapy/scrapy/issues/5719"}, {"text": "issue 5719", "href": "https://github.com/scrapy/scrapy/issues/5719"}, {"text": "LinkExtractor", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"}, {"text": "issue 5720", "href": "https://github.com/scrapy/scrapy/issues/5720"}, {"text": "issue 5724", "href": "https://github.com/scrapy/scrapy/issues/5724"}, {"text": "issue 5731", "href": "https://github.com/scrapy/scrapy/issues/5731"}, {"text": "issue 5719", "href": "https://github.com/scrapy/scrapy/issues/5719"}, {"text": "issue 5719", "href": "https://github.com/scrapy/scrapy/issues/5719"}, {"text": "issue 5719", "href": "https://github.com/scrapy/scrapy/issues/5719"}, {"text": "issue 5368", "href": "https://github.com/scrapy/scrapy/issues/5368"}, {"text": "issue 5489", "href": "https://github.com/scrapy/scrapy/issues/5489"}, {"text": "issue 3055", "href": "https://github.com/scrapy/scrapy/issues/3055"}, {"text": "issue 3689", "href": "https://github.com/scrapy/scrapy/issues/3689"}, {"text": "issue 4753", "href": "https://github.com/scrapy/scrapy/issues/4753"}, {"text": "black", "href": "https://black.readthedocs.io/en/stable/"}, {"text": "genspider", "href": "topics/commands.html#std-command-genspider"}, {"text": "startproject", "href": "topics/commands.html#std-command-startproject"}, {"text": "issue 5809", "href": "https://github.com/scrapy/scrapy/issues/5809"}, {"text": "issue 5814", "href": "https://github.com/scrapy/scrapy/issues/5814"}, {"text": "FEED_EXPORT_ENCODING", "href": "topics/feed-exports.html#std-setting-FEED_EXPORT_ENCODING"}, {"text": "startproject", "href": "topics/commands.html#std-command-startproject"}, {"text": "issue 5797", "href": "https://github.com/scrapy/scrapy/issues/5797"}, {"text": "issue 5800", "href": "https://github.com/scrapy/scrapy/issues/5800"}, {"text": "MemoryUsage", "href": "topics/extensions.html#scrapy.extensions.memusage.MemoryUsage"}, {"text": "issue 5717", "href": "https://github.com/scrapy/scrapy/issues/5717"}, {"text": "issue 5722", "href": "https://github.com/scrapy/scrapy/issues/5722"}, {"text": "issue 5727", "href": "https://github.com/scrapy/scrapy/issues/5727"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "scrapy.http.request.NO_CALLBACK()", "href": "topics/request-response.html#scrapy.http.request.NO_CALLBACK"}, {"text": "parse()", "href": "topics/spiders.html#scrapy.Spider.parse"}, {"text": "issue 5798", "href": "https://github.com/scrapy/scrapy/issues/5798"}, {"text": "issue 5491", "href": "https://github.com/scrapy/scrapy/issues/5491"}, {"text": "issue 5790", "href": "https://github.com/scrapy/scrapy/issues/5790"}, {"text": "issue 5386", "href": "https://github.com/scrapy/scrapy/issues/5386"}, {"text": "issue 5406", "href": "https://github.com/scrapy/scrapy/issues/5406"}, {"text": "item exporters", "href": "topics/exporters.html#topics-exporters"}, {"text": "issue 5537", "href": "https://github.com/scrapy/scrapy/issues/5537"}, {"text": "issue 5758", "href": "https://github.com/scrapy/scrapy/issues/5758"}, {"text": "issue 5777", "href": "https://github.com/scrapy/scrapy/issues/5777"}, {"text": "shell", "href": "topics/commands.html#std-command-shell"}, {"text": "using asyncio", "href": "topics/asyncio.html#using-asyncio"}, {"text": "issue 5740", "href": "https://github.com/scrapy/scrapy/issues/5740"}, {"text": "issue 5742", "href": "https://github.com/scrapy/scrapy/issues/5742"}, {"text": "issue 5748", "href": "https://github.com/scrapy/scrapy/issues/5748"}, {"text": "issue 5759", "href": "https://github.com/scrapy/scrapy/issues/5759"}, {"text": "issue 5760", "href": "https://github.com/scrapy/scrapy/issues/5760"}, {"text": "issue 5771", "href": "https://github.com/scrapy/scrapy/issues/5771"}, {"text": "CrawlSpider", "href": "topics/spiders.html#scrapy.spiders.CrawlSpider"}, {"text": "Rule", "href": "topics/spiders.html#scrapy.spiders.Rule"}, {"text": "issue 5699", "href": "https://github.com/scrapy/scrapy/issues/5699"}, {"text": "images pipeline", "href": "topics/media-pipeline.html#images-pipeline"}, {"text": "issue 3055", "href": "https://github.com/scrapy/scrapy/issues/3055"}, {"text": "issue 3689", "href": "https://github.com/scrapy/scrapy/issues/3689"}, {"text": "issue 4753", "href": "https://github.com/scrapy/scrapy/issues/4753"}, {"text": "images pipeline", "href": "topics/media-pipeline.html#images-pipeline"}, {"text": "issue 3072", "href": "https://github.com/scrapy/scrapy/issues/3072"}, {"text": "issue 5766", "href": "https://github.com/scrapy/scrapy/issues/5766"}, {"text": "issue 5767", "href": "https://github.com/scrapy/scrapy/issues/5767"}, {"text": "issue 2918", "href": "https://github.com/scrapy/scrapy/issues/2918"}, {"text": "LinkExtractor", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"}, {"text": "issue 3798", "href": "https://github.com/scrapy/scrapy/issues/3798"}, {"text": "issue 3799", "href": "https://github.com/scrapy/scrapy/issues/3799"}, {"text": "issue 4695", "href": "https://github.com/scrapy/scrapy/issues/4695"}, {"text": "issue 5458", "href": "https://github.com/scrapy/scrapy/issues/5458"}, {"text": "RobotsTxtMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware"}, {"text": "issue 5807", "href": "https://github.com/scrapy/scrapy/issues/5807"}, {"text": "issue 5753", "href": "https://github.com/scrapy/scrapy/issues/5753"}, {"text": "issue 5754", "href": "https://github.com/scrapy/scrapy/issues/5754"}, {"text": "issue 5709", "href": "https://github.com/scrapy/scrapy/issues/5709"}, {"text": "issue 5711", "href": "https://github.com/scrapy/scrapy/issues/5711"}, {"text": "issue 5712", "href": "https://github.com/scrapy/scrapy/issues/5712"}, {"text": "commands", "href": "topics/commands.html#topics-commands"}, {"text": "issue 5715", "href": "https://github.com/scrapy/scrapy/issues/5715"}, {"text": "debug spiders from Visual Studio Code", "href": "topics/debug.html#debug-vscode"}, {"text": "issue 5721", "href": "https://github.com/scrapy/scrapy/issues/5721"}, {"text": "DOWNLOAD_DELAY", "href": "topics/settings.html#std-setting-DOWNLOAD_DELAY"}, {"text": "issue 5083", "href": "https://github.com/scrapy/scrapy/issues/5083"}, {"text": "issue 5540", "href": "https://github.com/scrapy/scrapy/issues/5540"}, {"text": "issue 5761", "href": "https://github.com/scrapy/scrapy/issues/5761"}, {"text": "issue 5714", "href": "https://github.com/scrapy/scrapy/issues/5714"}, {"text": "issue 5744", "href": "https://github.com/scrapy/scrapy/issues/5744"}, {"text": "issue 5764", "href": "https://github.com/scrapy/scrapy/issues/5764"}, {"text": "black coding style", "href": "contributing.html#coding-style"}, {"text": "pre-commit", "href": "contributing.html#scrapy-pre-commit"}, {"text": "issue 4654", "href": "https://github.com/scrapy/scrapy/issues/4654"}, {"text": "issue 4658", "href": "https://github.com/scrapy/scrapy/issues/4658"}, {"text": "issue 5734", "href": "https://github.com/scrapy/scrapy/issues/5734"}, {"text": "issue 5737", "href": "https://github.com/scrapy/scrapy/issues/5737"}, {"text": "issue 5806", "href": "https://github.com/scrapy/scrapy/issues/5806"}, {"text": "issue 5810", "href": "https://github.com/scrapy/scrapy/issues/5810"}, {"text": "os.path", "href": "https://docs.python.org/3/library/os.path.html#module-os.path"}, {"text": "pathlib", "href": "https://docs.python.org/3/library/pathlib.html#module-pathlib"}, {"text": "issue 4916", "href": "https://github.com/scrapy/scrapy/issues/4916"}, {"text": "issue 4497", "href": "https://github.com/scrapy/scrapy/issues/4497"}, {"text": "issue 5682", "href": "https://github.com/scrapy/scrapy/issues/5682"}, {"text": "issue 5677", "href": "https://github.com/scrapy/scrapy/issues/5677"}, {"text": "issue 5736", "href": "https://github.com/scrapy/scrapy/issues/5736"}, {"text": "issue 5768", "href": "https://github.com/scrapy/scrapy/issues/5768"}, {"text": "issue 5774", "href": "https://github.com/scrapy/scrapy/issues/5774"}, {"text": "issue 5776", "href": "https://github.com/scrapy/scrapy/issues/5776"}, {"text": "OrderedDict", "href": "https://docs.python.org/3/library/collections.html#collections.OrderedDict"}, {"text": "issue 5795", "href": "https://github.com/scrapy/scrapy/issues/5795"}, {"text": "issue 5150", "href": "https://github.com/scrapy/scrapy/issues/5150"}, {"text": "issue 5725", "href": "https://github.com/scrapy/scrapy/issues/5725"}, {"text": "issue 5729", "href": "https://github.com/scrapy/scrapy/issues/5729"}, {"text": "issue 5730", "href": "https://github.com/scrapy/scrapy/issues/5730"}, {"text": "issue 5732", "href": "https://github.com/scrapy/scrapy/issues/5732"}, {"text": "issue 5749", "href": "https://github.com/scrapy/scrapy/issues/5749"}, {"text": "issue 5750", "href": "https://github.com/scrapy/scrapy/issues/5750"}, {"text": "issue 5756", "href": "https://github.com/scrapy/scrapy/issues/5756"}, {"text": "issue 5762", "href": "https://github.com/scrapy/scrapy/issues/5762"}, {"text": "issue 5765", "href": "https://github.com/scrapy/scrapy/issues/5765"}, {"text": "issue 5780", "href": "https://github.com/scrapy/scrapy/issues/5780"}, {"text": "issue 5781", "href": "https://github.com/scrapy/scrapy/issues/5781"}, {"text": "issue 5782", "href": "https://github.com/scrapy/scrapy/issues/5782"}, {"text": "issue 5783", "href": "https://github.com/scrapy/scrapy/issues/5783"}, {"text": "issue 5785", "href": "https://github.com/scrapy/scrapy/issues/5785"}, {"text": "issue 5786", "href": "https://github.com/scrapy/scrapy/issues/5786"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "issue 5626", "href": "https://github.com/scrapy/scrapy/issues/5626"}, {"text": "issue 5516", "href": "https://github.com/scrapy/scrapy/issues/5516"}, {"text": "issue 5605", "href": "https://github.com/scrapy/scrapy/issues/5605"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "issue 5685", "href": "https://github.com/scrapy/scrapy/issues/5685"}, {"text": "issue 5689", "href": "https://github.com/scrapy/scrapy/issues/5689"}, {"text": "issue 5588", "href": "https://github.com/scrapy/scrapy/issues/5588"}, {"text": "issue 5589", "href": "https://github.com/scrapy/scrapy/issues/5589"}, {"text": "issue 5684", "href": "https://github.com/scrapy/scrapy/issues/5684"}, {"text": "issue 5692", "href": "https://github.com/scrapy/scrapy/issues/5692"}, {"text": "issue 5323", "href": "https://github.com/scrapy/scrapy/issues/5323"}, {"text": "issue 5592", "href": "https://github.com/scrapy/scrapy/issues/5592"}, {"text": "issue 5599", "href": "https://github.com/scrapy/scrapy/issues/5599"}, {"text": "issue 5691", "href": "https://github.com/scrapy/scrapy/issues/5691"}, {"text": "issue 5698", "href": "https://github.com/scrapy/scrapy/issues/5698"}, {"text": "issue 5681", "href": "https://github.com/scrapy/scrapy/issues/5681"}, {"text": "issue 5694", "href": "https://github.com/scrapy/scrapy/issues/5694"}, {"text": "issue 5688", "href": "https://github.com/scrapy/scrapy/issues/5688"}, {"text": "typing", "href": "https://docs.python.org/3/library/typing.html#module-typing"}, {"text": "issue 5686", "href": "https://github.com/scrapy/scrapy/issues/5686"}, {"text": "issue 5697", "href": "https://github.com/scrapy/scrapy/issues/5697"}, {"text": "issue 5695", "href": "https://github.com/scrapy/scrapy/issues/5695"}, {"text": "issue 5696", "href": "https://github.com/scrapy/scrapy/issues/5696"}, {"text": "asynchronous callbacks", "href": "topics/coroutines.html#topics-coroutines"}, {"text": "Asyncio support", "href": "topics/asyncio.html#using-asyncio"}, {"text": "request fingerprinting", "href": "topics/request-response.html#request-fingerprints"}, {"text": "lxml", "href": "https://lxml.de/"}, {"text": "Pillow", "href": "https://python-pillow.org/"}, {"text": "images pipeline", "href": "topics/media-pipeline.html#images-pipeline"}, {"text": "zope.interface", "href": "https://zopeinterface.readthedocs.io/en/latest/"}, {"text": "issue 5512", "href": "https://github.com/scrapy/scrapy/issues/5512"}, {"text": "issue 5514", "href": "https://github.com/scrapy/scrapy/issues/5514"}, {"text": "issue 5524", "href": "https://github.com/scrapy/scrapy/issues/5524"}, {"text": "issue 5563", "href": "https://github.com/scrapy/scrapy/issues/5563"}, {"text": "issue 5664", "href": "https://github.com/scrapy/scrapy/issues/5664"}, {"text": "issue 5670", "href": "https://github.com/scrapy/scrapy/issues/5670"}, {"text": "issue 5678", "href": "https://github.com/scrapy/scrapy/issues/5678"}, {"text": "ImagesPipeline.thumb_path", "href": "topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline.thumb_path"}, {"text": "issue 5504", "href": "https://github.com/scrapy/scrapy/issues/5504"}, {"text": "issue 5508", "href": "https://github.com/scrapy/scrapy/issues/5508"}, {"text": "issue 5546", "href": "https://github.com/scrapy/scrapy/issues/5546"}, {"text": "issue 5547", "href": "https://github.com/scrapy/scrapy/issues/5547"}, {"text": "process_spider_output()", "href": "topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"}, {"text": "spider middlewares", "href": "topics/spider-middleware.html#topics-spider-middleware"}, {"text": "asynchronous generator", "href": "https://docs.python.org/3/glossary.html#term-asynchronous-generator"}, {"text": "issue 4978", "href": "https://github.com/scrapy/scrapy/issues/4978"}, {"text": "coroutines", "href": "topics/coroutines.html#topics-coroutines"}, {"text": "issue 4978", "href": "https://github.com/scrapy/scrapy/issues/4978"}, {"text": "asynchronous\ncallbacks", "href": "topics/coroutines.html#topics-coroutines"}, {"text": "issue 5657", "href": "https://github.com/scrapy/scrapy/issues/5657"}, {"text": "startproject", "href": "topics/commands.html#std-command-startproject"}, {"text": "asyncio support", "href": "topics/asyncio.html#using-asyncio"}, {"text": "issue 5590", "href": "https://github.com/scrapy/scrapy/issues/5590"}, {"text": "issue 5679", "href": "https://github.com/scrapy/scrapy/issues/5679"}, {"text": "FEED_EXPORT_FIELDS", "href": "topics/feed-exports.html#std-setting-FEED_EXPORT_FIELDS"}, {"text": "issue 1008", "href": "https://github.com/scrapy/scrapy/issues/1008"}, {"text": "issue 3266", "href": "https://github.com/scrapy/scrapy/issues/3266"}, {"text": "issue 3696", "href": "https://github.com/scrapy/scrapy/issues/3696"}, {"text": "request fingerprinting", "href": "topics/request-response.html#request-fingerprints"}, {"text": "REQUEST_FINGERPRINTER_CLASS", "href": "topics/request-response.html#std-setting-REQUEST_FINGERPRINTER_CLASS"}, {"text": "issue 900", "href": "https://github.com/scrapy/scrapy/issues/900"}, {"text": "issue 3420", "href": "https://github.com/scrapy/scrapy/issues/3420"}, {"text": "issue 4113", "href": "https://github.com/scrapy/scrapy/issues/4113"}, {"text": "issue 4762", "href": "https://github.com/scrapy/scrapy/issues/4762"}, {"text": "issue 4524", "href": "https://github.com/scrapy/scrapy/issues/4524"}, {"text": "JSON\nLines", "href": "https://jsonlines.org/"}, {"text": "issue 4848", "href": "https://github.com/scrapy/scrapy/issues/4848"}, {"text": "ImagesPipeline.thumb_path", "href": "topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline.thumb_path"}, {"text": "item", "href": "topics/items.html#topics-items"}, {"text": "issue 5504", "href": "https://github.com/scrapy/scrapy/issues/5504"}, {"text": "issue 5508", "href": "https://github.com/scrapy/scrapy/issues/5508"}, {"text": "media pipeline", "href": "topics/media-pipeline.html#topics-media-pipeline"}, {"text": "FILES_EXPIRES", "href": "topics/media-pipeline.html#std-setting-FILES_EXPIRES"}, {"text": "FILES_STORE", "href": "topics/media-pipeline.html#std-setting-FILES_STORE"}, {"text": "issue 5317", "href": "https://github.com/scrapy/scrapy/issues/5317"}, {"text": "issue 5318", "href": "https://github.com/scrapy/scrapy/issues/5318"}, {"text": "parse", "href": "topics/commands.html#std-command-parse"}, {"text": "asynchronous callbacks", "href": "topics/coroutines.html#topics-coroutines"}, {"text": "issue 5424", "href": "https://github.com/scrapy/scrapy/issues/5424"}, {"text": "issue 5577", "href": "https://github.com/scrapy/scrapy/issues/5577"}, {"text": "parse", "href": "topics/commands.html#std-command-parse"}, {"text": "issue 3264", "href": "https://github.com/scrapy/scrapy/issues/3264"}, {"text": "issue 3265", "href": "https://github.com/scrapy/scrapy/issues/3265"}, {"text": "issue 5375", "href": "https://github.com/scrapy/scrapy/issues/5375"}, {"text": "issue 5376", "href": "https://github.com/scrapy/scrapy/issues/5376"}, {"text": "issue 5497", "href": "https://github.com/scrapy/scrapy/issues/5497"}, {"text": "TextResponse", "href": "topics/request-response.html#scrapy.http.TextResponse"}, {"text": "byte\norder mark", "href": "https://en.wikipedia.org/wiki/Byte_order_mark"}, {"text": "HTML living standard", "href": "https://html.spec.whatwg.org/multipage/parsing.html#determining-the-character-encoding"}, {"text": "issue 5601", "href": "https://github.com/scrapy/scrapy/issues/5601"}, {"text": "issue 5611", "href": "https://github.com/scrapy/scrapy/issues/5611"}, {"text": "issue 4873", "href": "https://github.com/scrapy/scrapy/issues/4873"}, {"text": "issue 4873", "href": "https://github.com/scrapy/scrapy/issues/4873"}, {"text": "ASYNCIO_EVENT_LOOP", "href": "topics/settings.html#std-setting-ASYNCIO_EVENT_LOOP"}, {"text": "issue 5529", "href": "https://github.com/scrapy/scrapy/issues/5529"}, {"text": "issue 5515", "href": "https://github.com/scrapy/scrapy/issues/5515"}, {"text": "issue 5526", "href": "https://github.com/scrapy/scrapy/issues/5526"}, {"text": "LinkExtractor", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"}, {"text": "issue 1837", "href": "https://github.com/scrapy/scrapy/issues/1837"}, {"text": "issue 2067", "href": "https://github.com/scrapy/scrapy/issues/2067"}, {"text": "issue 4066", "href": "https://github.com/scrapy/scrapy/issues/4066"}, {"text": "Spider.parse", "href": "topics/spiders.html#scrapy.Spider.parse"}, {"text": "issue 5602", "href": "https://github.com/scrapy/scrapy/issues/5602"}, {"text": "issue 5608", "href": "https://github.com/scrapy/scrapy/issues/5608"}, {"text": "HttpCompressionMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware"}, {"text": "brotli compression", "href": "https://www.ietf.org/rfc/rfc7932.txt"}, {"text": "brotli", "href": "https://github.com/google/brotli"}, {"text": "brotlipy", "href": "https://github.com/python-hyper/brotlipy/"}, {"text": "Signal documentation", "href": "topics/signals.html#topics-signals"}, {"text": "coroutine\nsupport", "href": "topics/coroutines.html#topics-coroutines"}, {"text": "issue 4852", "href": "https://github.com/scrapy/scrapy/issues/4852"}, {"text": "issue 5358", "href": "https://github.com/scrapy/scrapy/issues/5358"}, {"text": "Avoiding getting banned", "href": "topics/practices.html#bans"}, {"text": "Common Crawl", "href": "https://commoncrawl.org/"}, {"text": "Google cache", "href": "http://www.googleguide.com/cached_pages.html"}, {"text": "issue 3582", "href": "https://github.com/scrapy/scrapy/issues/3582"}, {"text": "issue 5432", "href": "https://github.com/scrapy/scrapy/issues/5432"}, {"text": "Components", "href": "topics/components.html#topics-components"}, {"text": "downloader middlewares", "href": "topics/downloader-middleware.html#topics-downloader-middleware"}, {"text": "extensions", "href": "topics/extensions.html#topics-extensions"}, {"text": "item pipelines", "href": "topics/item-pipeline.html#topics-item-pipeline"}, {"text": "spider middlewares", "href": "topics/spider-middleware.html#topics-spider-middleware"}, {"text": "Enforcing asyncio as a requirement", "href": "topics/asyncio.html#enforce-asyncio-requirement"}, {"text": "issue 4978", "href": "https://github.com/scrapy/scrapy/issues/4978"}, {"text": "Settings", "href": "topics/settings.html#topics-settings"}, {"text": "picklable", "href": "https://docs.python.org/3/library/pickle.html#pickle-picklable"}, {"text": "issue 5607", "href": "https://github.com/scrapy/scrapy/issues/5607"}, {"text": "issue 5629", "href": "https://github.com/scrapy/scrapy/issues/5629"}, {"text": "issue 5446", "href": "https://github.com/scrapy/scrapy/issues/5446"}, {"text": "issue 5373", "href": "https://github.com/scrapy/scrapy/issues/5373"}, {"text": "issue 5369", "href": "https://github.com/scrapy/scrapy/issues/5369"}, {"text": "issue 5370", "href": "https://github.com/scrapy/scrapy/issues/5370"}, {"text": "issue 5554", "href": "https://github.com/scrapy/scrapy/issues/5554"}, {"text": "issue 5442", "href": "https://github.com/scrapy/scrapy/issues/5442"}, {"text": "issue 5455", "href": "https://github.com/scrapy/scrapy/issues/5455"}, {"text": "issue 5457", "href": "https://github.com/scrapy/scrapy/issues/5457"}, {"text": "issue 5461", "href": "https://github.com/scrapy/scrapy/issues/5461"}, {"text": "issue 5538", "href": "https://github.com/scrapy/scrapy/issues/5538"}, {"text": "issue 5553", "href": "https://github.com/scrapy/scrapy/issues/5553"}, {"text": "issue 5558", "href": "https://github.com/scrapy/scrapy/issues/5558"}, {"text": "issue 5624", "href": "https://github.com/scrapy/scrapy/issues/5624"}, {"text": "issue 5631", "href": "https://github.com/scrapy/scrapy/issues/5631"}, {"text": "issue 5283", "href": "https://github.com/scrapy/scrapy/issues/5283"}, {"text": "issue 5284", "href": "https://github.com/scrapy/scrapy/issues/5284"}, {"text": "issue 5559", "href": "https://github.com/scrapy/scrapy/issues/5559"}, {"text": "issue 5567", "href": "https://github.com/scrapy/scrapy/issues/5567"}, {"text": "issue 5648", "href": "https://github.com/scrapy/scrapy/issues/5648"}, {"text": "issue 5659", "href": "https://github.com/scrapy/scrapy/issues/5659"}, {"text": "issue 5665", "href": "https://github.com/scrapy/scrapy/issues/5665"}, {"text": "twine check", "href": "https://twine.readthedocs.io/en/stable/#twine-check"}, {"text": "issue 5655", "href": "https://github.com/scrapy/scrapy/issues/5655"}, {"text": "issue 5656", "href": "https://github.com/scrapy/scrapy/issues/5656"}, {"text": "issue 5560", "href": "https://github.com/scrapy/scrapy/issues/5560"}, {"text": "issue 5561", "href": "https://github.com/scrapy/scrapy/issues/5561"}, {"text": "issue 5612", "href": "https://github.com/scrapy/scrapy/issues/5612"}, {"text": "issue 5617", "href": "https://github.com/scrapy/scrapy/issues/5617"}, {"text": "issue 5639", "href": "https://github.com/scrapy/scrapy/issues/5639"}, {"text": "issue 5645", "href": "https://github.com/scrapy/scrapy/issues/5645"}, {"text": "issue 5662", "href": "https://github.com/scrapy/scrapy/issues/5662"}, {"text": "issue 5671", "href": "https://github.com/scrapy/scrapy/issues/5671"}, {"text": "issue 5675", "href": "https://github.com/scrapy/scrapy/issues/5675"}, {"text": "issue 4991", "href": "https://github.com/scrapy/scrapy/issues/4991"}, {"text": "issue 4995", "href": "https://github.com/scrapy/scrapy/issues/4995"}, {"text": "issue 5451", "href": "https://github.com/scrapy/scrapy/issues/5451"}, {"text": "issue 5487", "href": "https://github.com/scrapy/scrapy/issues/5487"}, {"text": "issue 5542", "href": "https://github.com/scrapy/scrapy/issues/5542"}, {"text": "issue 5667", "href": "https://github.com/scrapy/scrapy/issues/5667"}, {"text": "issue 5668", "href": "https://github.com/scrapy/scrapy/issues/5668"}, {"text": "issue 5672", "href": "https://github.com/scrapy/scrapy/issues/5672"}, {"text": "issue 5661", "href": "https://github.com/scrapy/scrapy/issues/5661"}, {"text": "pyOpenSSL", "href": "https://www.pyopenssl.org/en/stable/"}, {"text": "issue 5634", "href": "https://github.com/scrapy/scrapy/issues/5634"}, {"text": "issue 5635", "href": "https://github.com/scrapy/scrapy/issues/5635"}, {"text": "issue 5636", "href": "https://github.com/scrapy/scrapy/issues/5636"}, {"text": "cryptography", "href": "https://cryptography.io/en/latest/"}, {"text": "pyOpenSSL", "href": "https://www.pyopenssl.org/en/stable/"}, {"text": "service_identity", "href": "https://service-identity.readthedocs.io/en/stable/"}, {"text": "Twisted", "href": "https://twistedmatrix.com/trac/"}, {"text": "zope.interface", "href": "https://zopeinterface.readthedocs.io/en/latest/"}, {"text": "issue 5621", "href": "https://github.com/scrapy/scrapy/issues/5621"}, {"text": "issue 5632", "href": "https://github.com/scrapy/scrapy/issues/5632"}, {"text": "issue 5612", "href": "https://github.com/scrapy/scrapy/issues/5612"}, {"text": "issue 5617", "href": "https://github.com/scrapy/scrapy/issues/5617"}, {"text": "issue 5631", "href": "https://github.com/scrapy/scrapy/issues/5631"}, {"text": "HttpProxyMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "HttpProxyMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "HttpProxyMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "HttpProxyMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "CrawlerProcess", "href": "topics/api.html#scrapy.crawler.CrawlerProcess"}, {"text": "issue 5435", "href": "https://github.com/scrapy/scrapy/issues/5435"}, {"text": "issue 5436", "href": "https://github.com/scrapy/scrapy/issues/5436"}, {"text": "twisted.internet.reactor", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html"}, {"text": "TWISTED_REACTOR", "href": "topics/settings.html#std-setting-TWISTED_REACTOR"}, {"text": "issue 5525", "href": "https://github.com/scrapy/scrapy/issues/5525"}, {"text": "issue 5528", "href": "https://github.com/scrapy/scrapy/issues/5528"}, {"text": "issue 5437", "href": "https://github.com/scrapy/scrapy/issues/5437"}, {"text": "issue 5440", "href": "https://github.com/scrapy/scrapy/issues/5440"}, {"text": "issue 5444", "href": "https://github.com/scrapy/scrapy/issues/5444"}, {"text": "issue 5445", "href": "https://github.com/scrapy/scrapy/issues/5445"}, {"text": "issue 5481", "href": "https://github.com/scrapy/scrapy/issues/5481"}, {"text": "issue 5482", "href": "https://github.com/scrapy/scrapy/issues/5482"}, {"text": "asyncio support", "href": "topics/asyncio.html#using-asyncio"}, {"text": "pathlib.Path", "href": "https://docs.python.org/3/library/pathlib.html#pathlib.Path"}, {"text": "item filtering", "href": "topics/feed-exports.html#item-filter"}, {"text": "post-processing", "href": "topics/feed-exports.html#post-processing"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "cjvr-mfj7-j4j8 security advisory", "href": "https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "public suffix", "href": "https://publicsuffix.org/"}, {"text": "mfjm-vh54-3f96 security\nadvisory", "href": "https://github.com/scrapy/scrapy/security/advisories/GHSA-mfjm-vh54-3f96"}, {"text": "h2", "href": "https://pypi.org/project/h2/"}, {"text": "enable HTTP/2 support", "href": "topics/settings.html#http2"}, {"text": "issue 5113", "href": "https://github.com/scrapy/scrapy/issues/5113"}, {"text": "issue 2919", "href": "https://github.com/scrapy/scrapy/issues/2919"}, {"text": "issue 3579", "href": "https://github.com/scrapy/scrapy/issues/3579"}, {"text": "FEED_URI_PARAMS", "href": "topics/feed-exports.html#std-setting-FEED_URI_PARAMS"}, {"text": "issue 4962", "href": "https://github.com/scrapy/scrapy/issues/4962"}, {"text": "issue 4966", "href": "https://github.com/scrapy/scrapy/issues/4966"}, {"text": "RuntimeError", "href": "https://docs.python.org/3/library/exceptions.html#RuntimeError"}, {"text": "issue 5090", "href": "https://github.com/scrapy/scrapy/issues/5090"}, {"text": "AttributeError", "href": "https://docs.python.org/3/library/exceptions.html#AttributeError"}, {"text": "scheduler", "href": "topics/scheduler.html#topics-scheduler"}, {"text": "TypeError", "href": "https://docs.python.org/3/library/exceptions.html#TypeError"}, {"text": "issue 3559", "href": "https://github.com/scrapy/scrapy/issues/3559"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "issue 5130", "href": "https://github.com/scrapy/scrapy/issues/5130"}, {"text": "issue 5393", "href": "https://github.com/scrapy/scrapy/issues/5393"}, {"text": "issue 5398", "href": "https://github.com/scrapy/scrapy/issues/5398"}, {"text": "issue 5398", "href": "https://github.com/scrapy/scrapy/issues/5398"}, {"text": "issue 4178", "href": "https://github.com/scrapy/scrapy/issues/4178"}, {"text": "issue 4356", "href": "https://github.com/scrapy/scrapy/issues/4356"}, {"text": "FEED_URI_PARAMS", "href": "topics/feed-exports.html#std-setting-FEED_URI_PARAMS"}, {"text": "issue 4962", "href": "https://github.com/scrapy/scrapy/issues/4962"}, {"text": "issue 4966", "href": "https://github.com/scrapy/scrapy/issues/4966"}, {"text": "issue 5130", "href": "https://github.com/scrapy/scrapy/issues/5130"}, {"text": "Request.to_dict", "href": "topics/request-response.html#scrapy.http.Request.to_dict"}, {"text": "scrapy.utils.request.request_from_dict()", "href": "topics/request-response.html#scrapy.utils.request.request_from_dict"}, {"text": "issue 5117", "href": "https://github.com/scrapy/scrapy/issues/5117"}, {"text": "Spider", "href": "topics/spiders.html#scrapy.Spider"}, {"text": "issue 5090", "href": "https://github.com/scrapy/scrapy/issues/5090"}, {"text": "Spider", "href": "topics/spiders.html#scrapy.Spider"}, {"text": "item filtering", "href": "topics/feed-exports.html#item-filter"}, {"text": "issue 4575", "href": "https://github.com/scrapy/scrapy/issues/4575"}, {"text": "issue 5178", "href": "https://github.com/scrapy/scrapy/issues/5178"}, {"text": "issue 5161", "href": "https://github.com/scrapy/scrapy/issues/5161"}, {"text": "issue 5203", "href": "https://github.com/scrapy/scrapy/issues/5203"}, {"text": "post-processing", "href": "topics/feed-exports.html#post-processing"}, {"text": "built-in post-processing plugins", "href": "topics/feed-exports.html#builtin-plugins"}, {"text": "issue 2174", "href": "https://github.com/scrapy/scrapy/issues/2174"}, {"text": "issue 5168", "href": "https://github.com/scrapy/scrapy/issues/5168"}, {"text": "issue 5190", "href": "https://github.com/scrapy/scrapy/issues/5190"}, {"text": "FEEDS", "href": "topics/feed-exports.html#std-setting-FEEDS"}, {"text": "pathlib.Path", "href": "https://docs.python.org/3/library/pathlib.html#pathlib.Path"}, {"text": "issue 5383", "href": "https://github.com/scrapy/scrapy/issues/5383"}, {"text": "issue 5384", "href": "https://github.com/scrapy/scrapy/issues/5384"}, {"text": "asyncio", "href": "topics/asyncio.html#using-asyncio"}, {"text": "Windows-specific notes", "href": "topics/asyncio.html#asyncio-windows"}, {"text": "issue 4976", "href": "https://github.com/scrapy/scrapy/issues/4976"}, {"text": "issue 5315", "href": "https://github.com/scrapy/scrapy/issues/5315"}, {"text": "genspider", "href": "topics/commands.html#std-command-genspider"}, {"text": "issue 4439", "href": "https://github.com/scrapy/scrapy/issues/4439"}, {"text": "deferred_to_future()", "href": "topics/asyncio.html#scrapy.utils.defer.deferred_to_future"}, {"text": "maybe_deferred_to_future()", "href": "topics/asyncio.html#scrapy.utils.defer.maybe_deferred_to_future"}, {"text": "await\non Deferreds when using the asyncio reactor", "href": "topics/asyncio.html#asyncio-await-dfd"}, {"text": "issue 5288", "href": "https://github.com/scrapy/scrapy/issues/5288"}, {"text": "Amazon S3 feed export storage", "href": "topics/feed-exports.html#topics-feed-storage-s3"}, {"text": "temporary security credentials", "href": "https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#temporary-access-keys"}, {"text": "AWS_SESSION_TOKEN", "href": "topics/settings.html#std-setting-AWS_SESSION_TOKEN"}, {"text": "AWS_ENDPOINT_URL", "href": "topics/settings.html#std-setting-AWS_ENDPOINT_URL"}, {"text": "issue 4998", "href": "https://github.com/scrapy/scrapy/issues/4998"}, {"text": "issue 5210", "href": "https://github.com/scrapy/scrapy/issues/5210"}, {"text": "LOG_FILE_APPEND", "href": "topics/settings.html#std-setting-LOG_FILE_APPEND"}, {"text": "issue 5279", "href": "https://github.com/scrapy/scrapy/issues/5279"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "issue 5252", "href": "https://github.com/scrapy/scrapy/issues/5252"}, {"text": "issue 5253", "href": "https://github.com/scrapy/scrapy/issues/5253"}, {"text": "CloseSpider", "href": "topics/exceptions.html#scrapy.exceptions.CloseSpider"}, {"text": "spider_idle", "href": "topics/signals.html#std-signal-spider_idle"}, {"text": "issue 5191", "href": "https://github.com/scrapy/scrapy/issues/5191"}, {"text": "HttpProxyMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"}, {"text": "issue 4505", "href": "https://github.com/scrapy/scrapy/issues/4505"}, {"text": "issue 4649", "href": "https://github.com/scrapy/scrapy/issues/4649"}, {"text": "issue 5112", "href": "https://github.com/scrapy/scrapy/issues/5112"}, {"text": "NotImplementedError", "href": "https://docs.python.org/3/library/exceptions.html#NotImplementedError"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Response", "href": "topics/request-response.html#scrapy.http.Response"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "scrapy.utils.request.request_from_dict()", "href": "topics/request-response.html#scrapy.utils.request.request_from_dict"}, {"text": "issue 1877", "href": "https://github.com/scrapy/scrapy/issues/1877"}, {"text": "issue 5130", "href": "https://github.com/scrapy/scrapy/issues/5130"}, {"text": "issue 5218", "href": "https://github.com/scrapy/scrapy/issues/5218"}, {"text": "open()", "href": "topics/scheduler.html#scrapy.core.scheduler.BaseScheduler.open"}, {"text": "close()", "href": "topics/scheduler.html#scrapy.core.scheduler.BaseScheduler.close"}, {"text": "scheduler", "href": "topics/scheduler.html#topics-scheduler"}, {"text": "issue 3559", "href": "https://github.com/scrapy/scrapy/issues/3559"}, {"text": "issue 4881", "href": "https://github.com/scrapy/scrapy/issues/4881"}, {"text": "issue 5007", "href": "https://github.com/scrapy/scrapy/issues/5007"}, {"text": "ItemLoader", "href": "topics/loaders.html#scrapy.loader.ItemLoader"}, {"text": "issue 5145", "href": "https://github.com/scrapy/scrapy/issues/5145"}, {"text": "issue 5269", "href": "https://github.com/scrapy/scrapy/issues/5269"}, {"text": "TWISTED_REACTOR", "href": "topics/settings.html#std-setting-TWISTED_REACTOR"}, {"text": "ASYNCIO_EVENT_LOOP", "href": "topics/settings.html#std-setting-ASYNCIO_EVENT_LOOP"}, {"text": "custom_settings", "href": "topics/spiders.html#scrapy.Spider.custom_settings"}, {"text": "issue 4485", "href": "https://github.com/scrapy/scrapy/issues/4485"}, {"text": "issue 5352", "href": "https://github.com/scrapy/scrapy/issues/5352"}, {"text": "using the asyncio reactor", "href": "topics/asyncio.html#using-asyncio"}, {"text": "issue 5357", "href": "https://github.com/scrapy/scrapy/issues/5357"}, {"text": "startproject", "href": "topics/commands.html#std-command-startproject"}, {"text": "issue 4665", "href": "https://github.com/scrapy/scrapy/issues/4665"}, {"text": "issue 4676", "href": "https://github.com/scrapy/scrapy/issues/4676"}, {"text": "FEED_URI_PARAMS", "href": "topics/feed-exports.html#std-setting-FEED_URI_PARAMS"}, {"text": "issue 4962", "href": "https://github.com/scrapy/scrapy/issues/4962"}, {"text": "issue 4966", "href": "https://github.com/scrapy/scrapy/issues/4966"}, {"text": "issue 5237", "href": "https://github.com/scrapy/scrapy/issues/5237"}, {"text": "issue 5251", "href": "https://github.com/scrapy/scrapy/issues/5251"}, {"text": "issue 5264", "href": "https://github.com/scrapy/scrapy/issues/5264"}, {"text": "issue 5319", "href": "https://github.com/scrapy/scrapy/issues/5319"}, {"text": "issue 5320", "href": "https://github.com/scrapy/scrapy/issues/5320"}, {"text": "CSVFeedSpider.quotechar", "href": "topics/spiders.html#scrapy.spiders.CSVFeedSpider.quotechar"}, {"text": "issue 5391", "href": "https://github.com/scrapy/scrapy/issues/5391"}, {"text": "issue 5394", "href": "https://github.com/scrapy/scrapy/issues/5394"}, {"text": "setuptools", "href": "https://pypi.org/project/setuptools/"}, {"text": "issue 5122", "href": "https://github.com/scrapy/scrapy/issues/5122"}, {"text": "LinkExtractor", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"}, {"text": "issue 5225", "href": "https://github.com/scrapy/scrapy/issues/5225"}, {"text": "TypeError", "href": "https://docs.python.org/3/library/exceptions.html#TypeError"}, {"text": "feed export", "href": "topics/feed-exports.html#topics-feed-exports"}, {"text": "issue 5359", "href": "https://github.com/scrapy/scrapy/issues/5359"}, {"text": "asyncio support", "href": "topics/asyncio.html#using-asyncio"}, {"text": "issue 5332", "href": "https://github.com/scrapy/scrapy/issues/5332"}, {"text": "Windows-specific help for asyncio usage", "href": "topics/asyncio.html#asyncio-windows"}, {"text": "issue 4976", "href": "https://github.com/scrapy/scrapy/issues/4976"}, {"text": "issue 5315", "href": "https://github.com/scrapy/scrapy/issues/5315"}, {"text": "Using a headless browser", "href": "topics/dynamic-content.html#topics-headless-browsing"}, {"text": "issue 4484", "href": "https://github.com/scrapy/scrapy/issues/4484"}, {"text": "issue 4613", "href": "https://github.com/scrapy/scrapy/issues/4613"}, {"text": "local file naming in media pipelines", "href": "topics/media-pipeline.html#topics-file-naming"}, {"text": "issue 5069", "href": "https://github.com/scrapy/scrapy/issues/5069"}, {"text": "issue 5152", "href": "https://github.com/scrapy/scrapy/issues/5152"}, {"text": "Frequently Asked Questions", "href": "faq.html#faq"}, {"text": "issue 2680", "href": "https://github.com/scrapy/scrapy/issues/2680"}, {"text": "issue 3669", "href": "https://github.com/scrapy/scrapy/issues/3669"}, {"text": "URLLENGTH_LIMIT", "href": "topics/settings.html#std-setting-URLLENGTH_LIMIT"}, {"text": "issue 5135", "href": "https://github.com/scrapy/scrapy/issues/5135"}, {"text": "issue 5250", "href": "https://github.com/scrapy/scrapy/issues/5250"}, {"text": "Reppy parser", "href": "topics/downloader-middleware.html#reppy-parser"}, {"text": "issue 5226", "href": "https://github.com/scrapy/scrapy/issues/5226"}, {"text": "issue 5231", "href": "https://github.com/scrapy/scrapy/issues/5231"}, {"text": "the scheduler component", "href": "topics/scheduler.html#topics-scheduler"}, {"text": "issue 3537", "href": "https://github.com/scrapy/scrapy/issues/3537"}, {"text": "issue 3559", "href": "https://github.com/scrapy/scrapy/issues/3559"}, {"text": "media pipelines", "href": "topics/media-pipeline.html#topics-media-pipeline"}, {"text": "determine if a file has expired", "href": "topics/media-pipeline.html#file-expiration"}, {"text": "issue 5120", "href": "https://github.com/scrapy/scrapy/issues/5120"}, {"text": "issue 5254", "href": "https://github.com/scrapy/scrapy/issues/5254"}, {"text": "Running multiple spiders in the same process", "href": "topics/practices.html#run-multiple-spiders"}, {"text": "issue 5070", "href": "https://github.com/scrapy/scrapy/issues/5070"}, {"text": "Running multiple spiders in the same process", "href": "topics/practices.html#run-multiple-spiders"}, {"text": "issue 4485", "href": "https://github.com/scrapy/scrapy/issues/4485"}, {"text": "issue 5352", "href": "https://github.com/scrapy/scrapy/issues/5352"}, {"text": "StatsMailer", "href": "topics/extensions.html#scrapy.extensions.statsmailer.StatsMailer"}, {"text": "issue 5199", "href": "https://github.com/scrapy/scrapy/issues/5199"}, {"text": "issue 5217", "href": "https://github.com/scrapy/scrapy/issues/5217"}, {"text": "JOBDIR", "href": "topics/settings.html#std-setting-JOBDIR"}, {"text": "Settings", "href": "topics/settings.html#topics-settings"}, {"text": "issue 5173", "href": "https://github.com/scrapy/scrapy/issues/5173"}, {"text": "issue 5224", "href": "https://github.com/scrapy/scrapy/issues/5224"}, {"text": "issue 5174", "href": "https://github.com/scrapy/scrapy/issues/5174"}, {"text": "issue 5244", "href": "https://github.com/scrapy/scrapy/issues/5244"}, {"text": "TextResponse.urljoin", "href": "topics/request-response.html#scrapy.http.TextResponse.urljoin"}, {"text": "issue 1582", "href": "https://github.com/scrapy/scrapy/issues/1582"}, {"text": "headers_received", "href": "topics/signals.html#std-signal-headers_received"}, {"text": "issue 5270", "href": "https://github.com/scrapy/scrapy/issues/5270"}, {"text": "SelectorList.get", "href": "topics/selectors.html#scrapy.selector.SelectorList.get"}, {"text": "tutorial", "href": "intro/tutorial.html#intro-tutorial"}, {"text": "issue 5256", "href": "https://github.com/scrapy/scrapy/issues/5256"}, {"text": "issue 2733", "href": "https://github.com/scrapy/scrapy/issues/2733"}, {"text": "issue 5099", "href": "https://github.com/scrapy/scrapy/issues/5099"}, {"text": "issue 5395", "href": "https://github.com/scrapy/scrapy/issues/5395"}, {"text": "issue 5396", "href": "https://github.com/scrapy/scrapy/issues/5396"}, {"text": "our Discord server", "href": "https://discord.gg/mv3yErfpvq"}, {"text": "Getting help", "href": "index.html#getting-help"}, {"text": "issue 5421", "href": "https://github.com/scrapy/scrapy/issues/5421"}, {"text": "issue 5422", "href": "https://github.com/scrapy/scrapy/issues/5422"}, {"text": "officially", "href": "intro/overview.html#intro-overview"}, {"text": "issue 5280", "href": "https://github.com/scrapy/scrapy/issues/5280"}, {"text": "issue 5281", "href": "https://github.com/scrapy/scrapy/issues/5281"}, {"text": "issue 5255", "href": "https://github.com/scrapy/scrapy/issues/5255"}, {"text": "issue 5258", "href": "https://github.com/scrapy/scrapy/issues/5258"}, {"text": "issue 3155", "href": "https://github.com/scrapy/scrapy/issues/3155"}, {"text": "issue 4335", "href": "https://github.com/scrapy/scrapy/issues/4335"}, {"text": "issue 5074", "href": "https://github.com/scrapy/scrapy/issues/5074"}, {"text": "issue 5098", "href": "https://github.com/scrapy/scrapy/issues/5098"}, {"text": "issue 5134", "href": "https://github.com/scrapy/scrapy/issues/5134"}, {"text": "issue 5180", "href": "https://github.com/scrapy/scrapy/issues/5180"}, {"text": "issue 5194", "href": "https://github.com/scrapy/scrapy/issues/5194"}, {"text": "issue 5239", "href": "https://github.com/scrapy/scrapy/issues/5239"}, {"text": "issue 5266", "href": "https://github.com/scrapy/scrapy/issues/5266"}, {"text": "issue 5271", "href": "https://github.com/scrapy/scrapy/issues/5271"}, {"text": "issue 5273", "href": "https://github.com/scrapy/scrapy/issues/5273"}, {"text": "issue 5274", "href": "https://github.com/scrapy/scrapy/issues/5274"}, {"text": "issue 5276", "href": "https://github.com/scrapy/scrapy/issues/5276"}, {"text": "issue 5347", "href": "https://github.com/scrapy/scrapy/issues/5347"}, {"text": "issue 5356", "href": "https://github.com/scrapy/scrapy/issues/5356"}, {"text": "issue 5414", "href": "https://github.com/scrapy/scrapy/issues/5414"}, {"text": "issue 5415", "href": "https://github.com/scrapy/scrapy/issues/5415"}, {"text": "issue 5416", "href": "https://github.com/scrapy/scrapy/issues/5416"}, {"text": "issue 5419", "href": "https://github.com/scrapy/scrapy/issues/5419"}, {"text": "issue 5420", "href": "https://github.com/scrapy/scrapy/issues/5420"}, {"text": "issue 5212", "href": "https://github.com/scrapy/scrapy/issues/5212"}, {"text": "issue 5221", "href": "https://github.com/scrapy/scrapy/issues/5221"}, {"text": "issue 5265", "href": "https://github.com/scrapy/scrapy/issues/5265"}, {"text": "DownloaderStats", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.stats.DownloaderStats"}, {"text": "issue 4964", "href": "https://github.com/scrapy/scrapy/issues/4964"}, {"text": "issue 4972", "href": "https://github.com/scrapy/scrapy/issues/4972"}, {"text": "optparse", "href": "https://docs.python.org/3/library/optparse.html#module-optparse"}, {"text": "issue 5366", "href": "https://github.com/scrapy/scrapy/issues/5366"}, {"text": "issue 5374", "href": "https://github.com/scrapy/scrapy/issues/5374"}, {"text": "issue 5077", "href": "https://github.com/scrapy/scrapy/issues/5077"}, {"text": "issue 5090", "href": "https://github.com/scrapy/scrapy/issues/5090"}, {"text": "issue 5100", "href": "https://github.com/scrapy/scrapy/issues/5100"}, {"text": "issue 5108", "href": "https://github.com/scrapy/scrapy/issues/5108"}, {"text": "issue 5171", "href": "https://github.com/scrapy/scrapy/issues/5171"}, {"text": "issue 5215", "href": "https://github.com/scrapy/scrapy/issues/5215"}, {"text": "issue 5334", "href": "https://github.com/scrapy/scrapy/issues/5334"}, {"text": "issue 5094", "href": "https://github.com/scrapy/scrapy/issues/5094"}, {"text": "issue 5157", "href": "https://github.com/scrapy/scrapy/issues/5157"}, {"text": "issue 5162", "href": "https://github.com/scrapy/scrapy/issues/5162"}, {"text": "issue 5198", "href": "https://github.com/scrapy/scrapy/issues/5198"}, {"text": "issue 5207", "href": "https://github.com/scrapy/scrapy/issues/5207"}, {"text": "issue 5208", "href": "https://github.com/scrapy/scrapy/issues/5208"}, {"text": "issue 5229", "href": "https://github.com/scrapy/scrapy/issues/5229"}, {"text": "issue 5298", "href": "https://github.com/scrapy/scrapy/issues/5298"}, {"text": "issue 5299", "href": "https://github.com/scrapy/scrapy/issues/5299"}, {"text": "issue 5310", "href": "https://github.com/scrapy/scrapy/issues/5310"}, {"text": "issue 5316", "href": "https://github.com/scrapy/scrapy/issues/5316"}, {"text": "issue 5333", "href": "https://github.com/scrapy/scrapy/issues/5333"}, {"text": "issue 5388", "href": "https://github.com/scrapy/scrapy/issues/5388"}, {"text": "issue 5389", "href": "https://github.com/scrapy/scrapy/issues/5389"}, {"text": "issue 5400", "href": "https://github.com/scrapy/scrapy/issues/5400"}, {"text": "issue 5401", "href": "https://github.com/scrapy/scrapy/issues/5401"}, {"text": "issue 5404", "href": "https://github.com/scrapy/scrapy/issues/5404"}, {"text": "issue 5405", "href": "https://github.com/scrapy/scrapy/issues/5405"}, {"text": "issue 5407", "href": "https://github.com/scrapy/scrapy/issues/5407"}, {"text": "issue 5410", "href": "https://github.com/scrapy/scrapy/issues/5410"}, {"text": "issue 5412", "href": "https://github.com/scrapy/scrapy/issues/5412"}, {"text": "issue 5425", "href": "https://github.com/scrapy/scrapy/issues/5425"}, {"text": "issue 5427", "href": "https://github.com/scrapy/scrapy/issues/5427"}, {"text": "issue 5080", "href": "https://github.com/scrapy/scrapy/issues/5080"}, {"text": "issue 5082", "href": "https://github.com/scrapy/scrapy/issues/5082"}, {"text": "issue 5177", "href": "https://github.com/scrapy/scrapy/issues/5177"}, {"text": "issue 5200", "href": "https://github.com/scrapy/scrapy/issues/5200"}, {"text": "issue 5095", "href": "https://github.com/scrapy/scrapy/issues/5095"}, {"text": "issue 5106", "href": "https://github.com/scrapy/scrapy/issues/5106"}, {"text": "issue 5209", "href": "https://github.com/scrapy/scrapy/issues/5209"}, {"text": "issue 5228", "href": "https://github.com/scrapy/scrapy/issues/5228"}, {"text": "issue 5235", "href": "https://github.com/scrapy/scrapy/issues/5235"}, {"text": "issue 5245", "href": "https://github.com/scrapy/scrapy/issues/5245"}, {"text": "issue 5246", "href": "https://github.com/scrapy/scrapy/issues/5246"}, {"text": "issue 5292", "href": "https://github.com/scrapy/scrapy/issues/5292"}, {"text": "issue 5314", "href": "https://github.com/scrapy/scrapy/issues/5314"}, {"text": "issue 5322", "href": "https://github.com/scrapy/scrapy/issues/5322"}, {"text": "HttpAuthMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware"}, {"text": "w3lib.http.basic_auth_header()", "href": "https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header"}, {"text": "scrapy-splash", "href": "https://github.com/scrapy-plugins/scrapy-splash"}, {"text": "HTTP/2 support", "href": "topics/settings.html#http2"}, {"text": "get_retry_request()", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.retry.get_retry_request"}, {"text": "headers_received", "href": "topics/signals.html#scrapy.signals.headers_received"}, {"text": "Response.protocol", "href": "topics/request-response.html#scrapy.http.Response.protocol"}, {"text": "issue 4901", "href": "https://github.com/scrapy/scrapy/issues/4901"}, {"text": "issue 4912", "href": "https://github.com/scrapy/scrapy/issues/4912"}, {"text": "issue 4900", "href": "https://github.com/scrapy/scrapy/issues/4900"}, {"text": "HTTP/2 support", "href": "topics/settings.html#http2"}, {"text": "DOWNLOAD_HANDLERS", "href": "topics/settings.html#std-setting-DOWNLOAD_HANDLERS"}, {"text": "issue 1854", "href": "https://github.com/scrapy/scrapy/issues/1854"}, {"text": "issue 4769", "href": "https://github.com/scrapy/scrapy/issues/4769"}, {"text": "issue 5058", "href": "https://github.com/scrapy/scrapy/issues/5058"}, {"text": "issue 5059", "href": "https://github.com/scrapy/scrapy/issues/5059"}, {"text": "issue 5066", "href": "https://github.com/scrapy/scrapy/issues/5066"}, {"text": "scrapy.downloadermiddlewares.retry.get_retry_request()", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.retry.get_retry_request"}, {"text": "RetryMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.retry.RetryMiddleware"}, {"text": "issue 3590", "href": "https://github.com/scrapy/scrapy/issues/3590"}, {"text": "issue 3685", "href": "https://github.com/scrapy/scrapy/issues/3685"}, {"text": "issue 4902", "href": "https://github.com/scrapy/scrapy/issues/4902"}, {"text": "headers_received", "href": "topics/signals.html#scrapy.signals.headers_received"}, {"text": "stopping downloads", "href": "topics/request-response.html#topics-stop-response-download"}, {"text": "issue 1772", "href": "https://github.com/scrapy/scrapy/issues/1772"}, {"text": "issue 4897", "href": "https://github.com/scrapy/scrapy/issues/4897"}, {"text": "Response.protocol", "href": "topics/request-response.html#scrapy.http.Response.protocol"}, {"text": "issue 4878", "href": "https://github.com/scrapy/scrapy/issues/4878"}, {"text": "Stats", "href": "topics/stats.html#topics-stats"}, {"text": "feeds", "href": "topics/feed-exports.html#topics-feed-exports"}, {"text": "issue 3947", "href": "https://github.com/scrapy/scrapy/issues/3947"}, {"text": "issue 4850", "href": "https://github.com/scrapy/scrapy/issues/4850"}, {"text": "UrlLengthMiddleware", "href": "topics/spider-middleware.html#scrapy.spidermiddlewares.urllength.UrlLengthMiddleware"}, {"text": "logging level", "href": "https://docs.python.org/3/library/logging.html#levels"}, {"text": "stats", "href": "topics/stats.html#topics-stats"}, {"text": "issue 5036", "href": "https://github.com/scrapy/scrapy/issues/5036"}, {"text": "HttpCompressionMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware"}, {"text": "issue 4797", "href": "https://github.com/scrapy/scrapy/issues/4797"}, {"text": "issue 4799", "href": "https://github.com/scrapy/scrapy/issues/4799"}, {"text": "issue 4710", "href": "https://github.com/scrapy/scrapy/issues/4710"}, {"text": "issue 4814", "href": "https://github.com/scrapy/scrapy/issues/4814"}, {"text": "issue 4477", "href": "https://github.com/scrapy/scrapy/issues/4477"}, {"text": "issue 4935", "href": "https://github.com/scrapy/scrapy/issues/4935"}, {"text": "Content-Length", "href": "https://tools.ietf.org/html/rfc2616#section-14.13"}, {"text": "DOWNLOAD_HANDLERS", "href": "topics/settings.html#std-setting-DOWNLOAD_HANDLERS"}, {"text": "issue 5009", "href": "https://github.com/scrapy/scrapy/issues/5009"}, {"text": "issue 5034", "href": "https://github.com/scrapy/scrapy/issues/5034"}, {"text": "issue 5045", "href": "https://github.com/scrapy/scrapy/issues/5045"}, {"text": "issue 5057", "href": "https://github.com/scrapy/scrapy/issues/5057"}, {"text": "issue 5062", "href": "https://github.com/scrapy/scrapy/issues/5062"}, {"text": "handle_httpstatus_all", "href": "topics/spider-middleware.html#std-reqmeta-handle_httpstatus_all"}, {"text": "issue 3851", "href": "https://github.com/scrapy/scrapy/issues/3851"}, {"text": "issue 4694", "href": "https://github.com/scrapy/scrapy/issues/4694"}, {"text": "install Scrapy in Windows using pip", "href": "intro/install.html#intro-install-windows"}, {"text": "issue 4715", "href": "https://github.com/scrapy/scrapy/issues/4715"}, {"text": "issue 4736", "href": "https://github.com/scrapy/scrapy/issues/4736"}, {"text": "additional ways to filter logs", "href": "topics/logging.html#topics-logging-advanced-customization"}, {"text": "issue 4216", "href": "https://github.com/scrapy/scrapy/issues/4216"}, {"text": "issue 4257", "href": "https://github.com/scrapy/scrapy/issues/4257"}, {"text": "issue 4965", "href": "https://github.com/scrapy/scrapy/issues/4965"}, {"text": "FAQ", "href": "faq.html#faq"}, {"text": "issue 2263", "href": "https://github.com/scrapy/scrapy/issues/2263"}, {"text": "issue 3667", "href": "https://github.com/scrapy/scrapy/issues/3667"}, {"text": "scrapy-bench", "href": "https://github.com/scrapy/scrapy-bench"}, {"text": "Benchmarking", "href": "topics/benchmarking.html#benchmarking"}, {"text": "issue 4996", "href": "https://github.com/scrapy/scrapy/issues/4996"}, {"text": "issue 5016", "href": "https://github.com/scrapy/scrapy/issues/5016"}, {"text": "extension", "href": "topics/extensions.html#topics-extensions"}, {"text": "issue 5014", "href": "https://github.com/scrapy/scrapy/issues/5014"}, {"text": "issue 4829", "href": "https://github.com/scrapy/scrapy/issues/4829"}, {"text": "issue 4830", "href": "https://github.com/scrapy/scrapy/issues/4830"}, {"text": "issue 4907", "href": "https://github.com/scrapy/scrapy/issues/4907"}, {"text": "issue 4909", "href": "https://github.com/scrapy/scrapy/issues/4909"}, {"text": "issue 5008", "href": "https://github.com/scrapy/scrapy/issues/5008"}, {"text": "issue 4892", "href": "https://github.com/scrapy/scrapy/issues/4892"}, {"text": "issue 4899", "href": "https://github.com/scrapy/scrapy/issues/4899"}, {"text": "issue 4936", "href": "https://github.com/scrapy/scrapy/issues/4936"}, {"text": "issue 4942", "href": "https://github.com/scrapy/scrapy/issues/4942"}, {"text": "issue 5005", "href": "https://github.com/scrapy/scrapy/issues/5005"}, {"text": "issue 5063", "href": "https://github.com/scrapy/scrapy/issues/5063"}, {"text": "list of Request.meta keys", "href": "topics/request-response.html#topics-request-meta"}, {"text": "issue 5061", "href": "https://github.com/scrapy/scrapy/issues/5061"}, {"text": "issue 5065", "href": "https://github.com/scrapy/scrapy/issues/5065"}, {"text": "issue 4973", "href": "https://github.com/scrapy/scrapy/issues/4973"}, {"text": "issue 5072", "href": "https://github.com/scrapy/scrapy/issues/5072"}, {"text": "issue 4956", "href": "https://github.com/scrapy/scrapy/issues/4956"}, {"text": "issue 4974", "href": "https://github.com/scrapy/scrapy/issues/4974"}, {"text": "issue 4757", "href": "https://github.com/scrapy/scrapy/issues/4757"}, {"text": "issue 4759", "href": "https://github.com/scrapy/scrapy/issues/4759"}, {"text": "issue 4895", "href": "https://github.com/scrapy/scrapy/issues/4895"}, {"text": "issue 4940", "href": "https://github.com/scrapy/scrapy/issues/4940"}, {"text": "issue 4950", "href": "https://github.com/scrapy/scrapy/issues/4950"}, {"text": "issue 5073", "href": "https://github.com/scrapy/scrapy/issues/5073"}, {"text": "issue 4710", "href": "https://github.com/scrapy/scrapy/issues/4710"}, {"text": "issue 4814", "href": "https://github.com/scrapy/scrapy/issues/4814"}, {"text": "coroutine support", "href": "topics/coroutines.html#coroutine-support"}, {"text": "issue 4987", "href": "https://github.com/scrapy/scrapy/issues/4987"}, {"text": "issue 4924", "href": "https://github.com/scrapy/scrapy/issues/4924"}, {"text": "issue 4986", "href": "https://github.com/scrapy/scrapy/issues/4986"}, {"text": "issue 5020", "href": "https://github.com/scrapy/scrapy/issues/5020"}, {"text": "issue 5022", "href": "https://github.com/scrapy/scrapy/issues/5022"}, {"text": "issue 5027", "href": "https://github.com/scrapy/scrapy/issues/5027"}, {"text": "issue 5052", "href": "https://github.com/scrapy/scrapy/issues/5052"}, {"text": "issue 5053", "href": "https://github.com/scrapy/scrapy/issues/5053"}, {"text": "issue 4911", "href": "https://github.com/scrapy/scrapy/issues/4911"}, {"text": "issue 4982", "href": "https://github.com/scrapy/scrapy/issues/4982"}, {"text": "issue 5001", "href": "https://github.com/scrapy/scrapy/issues/5001"}, {"text": "issue 5002", "href": "https://github.com/scrapy/scrapy/issues/5002"}, {"text": "issue 5076", "href": "https://github.com/scrapy/scrapy/issues/5076"}, {"text": "feed exports", "href": "topics/feed-exports.html#topics-feed-exports"}, {"text": "issue 4845", "href": "https://github.com/scrapy/scrapy/issues/4845"}, {"text": "issue 4857", "href": "https://github.com/scrapy/scrapy/issues/4857"}, {"text": "issue 4859", "href": "https://github.com/scrapy/scrapy/issues/4859"}, {"text": "issue 4855", "href": "https://github.com/scrapy/scrapy/issues/4855"}, {"text": "issue 4872", "href": "https://github.com/scrapy/scrapy/issues/4872"}, {"text": "reactor.resolve", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.interfaces.IReactorCore.html#resolve"}, {"text": "issue 4802", "href": "https://github.com/scrapy/scrapy/issues/4802"}, {"text": "issue 4803", "href": "https://github.com/scrapy/scrapy/issues/4803"}, {"text": "genspider", "href": "topics/commands.html#std-command-genspider"}, {"text": "issue 4874", "href": "https://github.com/scrapy/scrapy/issues/4874"}, {"text": "issue 4869", "href": "https://github.com/scrapy/scrapy/issues/4869"}, {"text": "issue 4876", "href": "https://github.com/scrapy/scrapy/issues/4876"}, {"text": "media pipelines", "href": "topics/media-pipeline.html#topics-media-pipeline"}, {"text": "item", "href": "topics/items.html#topics-items"}, {"text": "FEEDS", "href": "topics/feed-exports.html#std-setting-FEEDS"}, {"text": "item exporter classes", "href": "topics/exporters.html#topics-exporters"}, {"text": "feed exports", "href": "topics/feed-exports.html#topics-feed-exports"}, {"text": "crawl", "href": "topics/commands.html#std-command-crawl"}, {"text": "runspider", "href": "topics/commands.html#std-command-runspider"}, {"text": "zstandard", "href": "https://pypi.org/project/zstandard/"}, {"text": "is now required", "href": "intro/install.html#faq-python-versions"}, {"text": "feed exports", "href": "topics/feed-exports.html#topics-feed-storage-s3"}, {"text": "media pipelines", "href": "topics/media-pipeline.html#media-pipelines-s3"}, {"text": "botocore", "href": "https://github.com/boto/botocore"}, {"text": "images pipeline", "href": "topics/media-pipeline.html#images-pipeline"}, {"text": "Pillow", "href": "https://python-pillow.org/"}, {"text": "issue 4718", "href": "https://github.com/scrapy/scrapy/issues/4718"}, {"text": "issue 4732", "href": "https://github.com/scrapy/scrapy/issues/4732"}, {"text": "issue 4733", "href": "https://github.com/scrapy/scrapy/issues/4733"}, {"text": "issue 4742", "href": "https://github.com/scrapy/scrapy/issues/4742"}, {"text": "issue 4743", "href": "https://github.com/scrapy/scrapy/issues/4743"}, {"text": "issue 4764", "href": "https://github.com/scrapy/scrapy/issues/4764"}, {"text": "CookiesMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware"}, {"text": "Request.headers", "href": "topics/request-response.html#scrapy.http.Request.headers"}, {"text": "Request.cookies", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "issue 4717", "href": "https://github.com/scrapy/scrapy/issues/4717"}, {"text": "issue 4823", "href": "https://github.com/scrapy/scrapy/issues/4823"}, {"text": "issue 4356", "href": "https://github.com/scrapy/scrapy/issues/4356"}, {"text": "issue 4411", "href": "https://github.com/scrapy/scrapy/issues/4411"}, {"text": "issue 4688", "href": "https://github.com/scrapy/scrapy/issues/4688"}, {"text": "issue 4818", "href": "https://github.com/scrapy/scrapy/issues/4818"}, {"text": "media pipelines", "href": "topics/media-pipeline.html#topics-media-pipeline"}, {"text": "issue 4628", "href": "https://github.com/scrapy/scrapy/issues/4628"}, {"text": "issue 4686", "href": "https://github.com/scrapy/scrapy/issues/4686"}, {"text": "feed storage backend classes", "href": "topics/feed-exports.html#topics-feed-storage"}, {"text": "issue 547", "href": "https://github.com/scrapy/scrapy/issues/547"}, {"text": "issue 716", "href": "https://github.com/scrapy/scrapy/issues/716"}, {"text": "issue 4512", "href": "https://github.com/scrapy/scrapy/issues/4512"}, {"text": "issue 4684", "href": "https://github.com/scrapy/scrapy/issues/4684"}, {"text": "issue 4701", "href": "https://github.com/scrapy/scrapy/issues/4701"}, {"text": "issue 4734", "href": "https://github.com/scrapy/scrapy/issues/4734"}, {"text": "issue 4776", "href": "https://github.com/scrapy/scrapy/issues/4776"}, {"text": "media pipelines", "href": "topics/media-pipeline.html#topics-media-pipeline"}, {"text": "item", "href": "topics/items.html#topics-items"}, {"text": "scrapy.pipelines.files.FilesPipeline", "href": "topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline"}, {"text": "file_path()", "href": "topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.file_path"}, {"text": "scrapy.pipelines.images.ImagesPipeline", "href": "topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline"}, {"text": "file_path()", "href": "topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline.file_path"}, {"text": "issue 4628", "href": "https://github.com/scrapy/scrapy/issues/4628"}, {"text": "issue 4686", "href": "https://github.com/scrapy/scrapy/issues/4686"}, {"text": "FEEDS", "href": "topics/feed-exports.html#std-setting-FEEDS"}, {"text": "item exporter classes", "href": "topics/exporters.html#topics-exporters"}, {"text": "issue 4606", "href": "https://github.com/scrapy/scrapy/issues/4606"}, {"text": "issue 4768", "href": "https://github.com/scrapy/scrapy/issues/4768"}, {"text": "Feed exports", "href": "topics/feed-exports.html#topics-feed-exports"}, {"text": "crawl", "href": "topics/commands.html#std-command-crawl"}, {"text": "runspider", "href": "topics/commands.html#std-command-runspider"}, {"text": "FEEDS", "href": "topics/feed-exports.html#std-setting-FEEDS"}, {"text": "feed storage\nbackend classes", "href": "topics/feed-exports.html#topics-feed-storage"}, {"text": "feed\noptions", "href": "topics/feed-exports.html#feed-options"}, {"text": "issue 547", "href": "https://github.com/scrapy/scrapy/issues/547"}, {"text": "issue 716", "href": "https://github.com/scrapy/scrapy/issues/716"}, {"text": "issue 4512", "href": "https://github.com/scrapy/scrapy/issues/4512"}, {"text": "zstandard", "href": "https://pypi.org/project/zstandard/"}, {"text": "issue 4831", "href": "https://github.com/scrapy/scrapy/issues/4831"}, {"text": "issue 3870", "href": "https://github.com/scrapy/scrapy/issues/3870"}, {"text": "issue 3873", "href": "https://github.com/scrapy/scrapy/issues/3873"}, {"text": "DOWNLOADER_MIDDLEWARES", "href": "topics/settings.html#std-setting-DOWNLOADER_MIDDLEWARES"}, {"text": "DOWNLOAD_HANDLERS", "href": "topics/settings.html#std-setting-DOWNLOAD_HANDLERS"}, {"text": "Downloader middlewares", "href": "topics/downloader-middleware.html#topics-downloader-middleware"}, {"text": "response.request", "href": "topics/request-response.html#scrapy.http.Response.request"}, {"text": "downloader middleware", "href": "topics/downloader-middleware.html#topics-downloader-middleware"}, {"text": "Response", "href": "topics/request-response.html#scrapy.http.Response"}, {"text": "process_response()", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"}, {"text": "process_exception()", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "response.request", "href": "topics/request-response.html#scrapy.http.Response.request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "response_received", "href": "topics/signals.html#std-signal-response_received"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "issue 4529", "href": "https://github.com/scrapy/scrapy/issues/4529"}, {"text": "issue 4632", "href": "https://github.com/scrapy/scrapy/issues/4632"}, {"text": "FTP feed storage backend", "href": "topics/feed-exports.html#topics-feed-storage-ftp"}, {"text": "feed option", "href": "topics/feed-exports.html#feed-options"}, {"text": "issue 547", "href": "https://github.com/scrapy/scrapy/issues/547"}, {"text": "issue 716", "href": "https://github.com/scrapy/scrapy/issues/716"}, {"text": "issue 4512", "href": "https://github.com/scrapy/scrapy/issues/4512"}, {"text": "CsvItemExporter", "href": "topics/exporters.html#scrapy.exporters.CsvItemExporter"}, {"text": "issue 4755", "href": "https://github.com/scrapy/scrapy/issues/4755"}, {"text": "using asyncio", "href": "topics/asyncio.html#using-asyncio"}, {"text": "set a custom asyncio loop", "href": "topics/asyncio.html#using-custom-loops"}, {"text": "issue 4306", "href": "https://github.com/scrapy/scrapy/issues/4306"}, {"text": "issue 4414", "href": "https://github.com/scrapy/scrapy/issues/4414"}, {"text": "Jobs: pausing and resuming crawls", "href": "topics/jobs.html#topics-jobs"}, {"text": "issue 4756", "href": "https://github.com/scrapy/scrapy/issues/4756"}, {"text": "DOWNLOAD_MAXSIZE", "href": "topics/settings.html#std-setting-DOWNLOAD_MAXSIZE"}, {"text": "issue 3874", "href": "https://github.com/scrapy/scrapy/issues/3874"}, {"text": "issue 3886", "href": "https://github.com/scrapy/scrapy/issues/3886"}, {"text": "issue 4752", "href": "https://github.com/scrapy/scrapy/issues/4752"}, {"text": "genspider", "href": "topics/commands.html#std-command-genspider"}, {"text": "issue 4561", "href": "https://github.com/scrapy/scrapy/issues/4561"}, {"text": "issue 4616", "href": "https://github.com/scrapy/scrapy/issues/4616"}, {"text": "issue 4623", "href": "https://github.com/scrapy/scrapy/issues/4623"}, {"text": "issue 4772", "href": "https://github.com/scrapy/scrapy/issues/4772"}, {"text": "runspider", "href": "topics/commands.html#std-command-runspider"}, {"text": "issue 4643", "href": "https://github.com/scrapy/scrapy/issues/4643"}, {"text": "issue 4646", "href": "https://github.com/scrapy/scrapy/issues/4646"}, {"text": "HttpProxyMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"}, {"text": "issue 3331", "href": "https://github.com/scrapy/scrapy/issues/3331"}, {"text": "issue 4778", "href": "https://github.com/scrapy/scrapy/issues/4778"}, {"text": "issue 4720", "href": "https://github.com/scrapy/scrapy/issues/4720"}, {"text": "issue 4721", "href": "https://github.com/scrapy/scrapy/issues/4721"}, {"text": "startproject", "href": "topics/commands.html#std-command-startproject"}, {"text": "issue 4722", "href": "https://github.com/scrapy/scrapy/issues/4722"}, {"text": "issue 861", "href": "https://github.com/scrapy/scrapy/issues/861"}, {"text": "issue 4746", "href": "https://github.com/scrapy/scrapy/issues/4746"}, {"text": "issue 4835", "href": "https://github.com/scrapy/scrapy/issues/4835"}, {"text": "FEED_URI_PARAMS", "href": "topics/feed-exports.html#std-setting-FEED_URI_PARAMS"}, {"text": "issue 4671", "href": "https://github.com/scrapy/scrapy/issues/4671"}, {"text": "issue 4724", "href": "https://github.com/scrapy/scrapy/issues/4724"}, {"text": "link extractors", "href": "topics/link-extractors.html#topics-link-extractors"}, {"text": "Link", "href": "topics/link-extractors.html#scrapy.link.Link"}, {"text": "issue 4751", "href": "https://github.com/scrapy/scrapy/issues/4751"}, {"text": "issue 4775", "href": "https://github.com/scrapy/scrapy/issues/4775"}, {"text": "CONCURRENT_REQUESTS", "href": "topics/settings.html#std-setting-CONCURRENT_REQUESTS"}, {"text": "CloseSpider", "href": "topics/extensions.html#scrapy.extensions.closespider.CloseSpider"}, {"text": "issue 4836", "href": "https://github.com/scrapy/scrapy/issues/4836"}, {"text": "issue 4547", "href": "https://github.com/scrapy/scrapy/issues/4547"}, {"text": "issue 4703", "href": "https://github.com/scrapy/scrapy/issues/4703"}, {"text": "official deprecation policy", "href": "versioning.html#deprecation-policy"}, {"text": "issue 4705", "href": "https://github.com/scrapy/scrapy/issues/4705"}, {"text": "documentation policies", "href": "contributing.html#documentation-policies"}, {"text": "versionadded", "href": "https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionadded"}, {"text": "versionchanged", "href": "https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionchanged"}, {"text": "issue 3971", "href": "https://github.com/scrapy/scrapy/issues/3971"}, {"text": "issue 4310", "href": "https://github.com/scrapy/scrapy/issues/4310"}, {"text": "issue 4090", "href": "https://github.com/scrapy/scrapy/issues/4090"}, {"text": "issue 4782", "href": "https://github.com/scrapy/scrapy/issues/4782"}, {"text": "issue 4800", "href": "https://github.com/scrapy/scrapy/issues/4800"}, {"text": "issue 4801", "href": "https://github.com/scrapy/scrapy/issues/4801"}, {"text": "issue 4809", "href": "https://github.com/scrapy/scrapy/issues/4809"}, {"text": "issue 4816", "href": "https://github.com/scrapy/scrapy/issues/4816"}, {"text": "issue 4825", "href": "https://github.com/scrapy/scrapy/issues/4825"}, {"text": "issue 4243", "href": "https://github.com/scrapy/scrapy/issues/4243"}, {"text": "issue 4691", "href": "https://github.com/scrapy/scrapy/issues/4691"}, {"text": "check", "href": "topics/commands.html#std-command-check"}, {"text": "issue 4663", "href": "https://github.com/scrapy/scrapy/issues/4663"}, {"text": "issue 4726", "href": "https://github.com/scrapy/scrapy/issues/4726"}, {"text": "issue 4727", "href": "https://github.com/scrapy/scrapy/issues/4727"}, {"text": "issue 4735", "href": "https://github.com/scrapy/scrapy/issues/4735"}, {"text": "issue 4723", "href": "https://github.com/scrapy/scrapy/issues/4723"}, {"text": "formatted string literals", "href": "https://docs.python.org/3/reference/lexical_analysis.html#f-strings"}, {"text": "issue 4307", "href": "https://github.com/scrapy/scrapy/issues/4307"}, {"text": "issue 4324", "href": "https://github.com/scrapy/scrapy/issues/4324"}, {"text": "issue 4672", "href": "https://github.com/scrapy/scrapy/issues/4672"}, {"text": "issue 4707", "href": "https://github.com/scrapy/scrapy/issues/4707"}, {"text": "issue 1790", "href": "https://github.com/scrapy/scrapy/issues/1790"}, {"text": "issue 3288", "href": "https://github.com/scrapy/scrapy/issues/3288"}, {"text": "issue 4165", "href": "https://github.com/scrapy/scrapy/issues/4165"}, {"text": "issue 4564", "href": "https://github.com/scrapy/scrapy/issues/4564"}, {"text": "issue 4651", "href": "https://github.com/scrapy/scrapy/issues/4651"}, {"text": "issue 4714", "href": "https://github.com/scrapy/scrapy/issues/4714"}, {"text": "issue 4738", "href": "https://github.com/scrapy/scrapy/issues/4738"}, {"text": "issue 4745", "href": "https://github.com/scrapy/scrapy/issues/4745"}, {"text": "issue 4747", "href": "https://github.com/scrapy/scrapy/issues/4747"}, {"text": "issue 4761", "href": "https://github.com/scrapy/scrapy/issues/4761"}, {"text": "issue 4765", "href": "https://github.com/scrapy/scrapy/issues/4765"}, {"text": "issue 4804", "href": "https://github.com/scrapy/scrapy/issues/4804"}, {"text": "issue 4817", "href": "https://github.com/scrapy/scrapy/issues/4817"}, {"text": "issue 4820", "href": "https://github.com/scrapy/scrapy/issues/4820"}, {"text": "issue 4822", "href": "https://github.com/scrapy/scrapy/issues/4822"}, {"text": "issue 4839", "href": "https://github.com/scrapy/scrapy/issues/4839"}, {"text": "Feed exports", "href": "topics/feed-exports.html#topics-feed-exports"}, {"text": "Google Cloud\nStorage", "href": "topics/feed-exports.html#topics-feed-storage-gcs"}, {"text": "FEED_EXPORT_BATCH_ITEM_COUNT", "href": "topics/feed-exports.html#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT"}, {"text": "delayed file delivery", "href": "topics/feed-exports.html#delayed-file-delivery"}, {"text": "S3", "href": "topics/feed-exports.html#topics-feed-storage-s3"}, {"text": "FTP", "href": "topics/feed-exports.html#topics-feed-storage-ftp"}, {"text": "GCS", "href": "topics/feed-exports.html#topics-feed-storage-gcs"}, {"text": "item loaders", "href": "topics/loaders.html#topics-loaders"}, {"text": "itemloaders", "href": "https://itemloaders.readthedocs.io/en/latest/index.html"}, {"text": "LinkExtractor", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"}, {"text": "issue 4356", "href": "https://github.com/scrapy/scrapy/issues/4356"}, {"text": "issue 4679", "href": "https://github.com/scrapy/scrapy/issues/4679"}, {"text": "issue 4683", "href": "https://github.com/scrapy/scrapy/issues/4683"}, {"text": "Feed exports", "href": "topics/feed-exports.html#topics-feed-exports"}, {"text": "Google Cloud\nStorage", "href": "topics/feed-exports.html#topics-feed-storage-gcs"}, {"text": "issue 685", "href": "https://github.com/scrapy/scrapy/issues/685"}, {"text": "issue 3608", "href": "https://github.com/scrapy/scrapy/issues/3608"}, {"text": "FEED_EXPORT_BATCH_ITEM_COUNT", "href": "topics/feed-exports.html#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT"}, {"text": "issue 4250", "href": "https://github.com/scrapy/scrapy/issues/4250"}, {"text": "issue 4434", "href": "https://github.com/scrapy/scrapy/issues/4434"}, {"text": "parse", "href": "topics/commands.html#std-command-parse"}, {"text": "issue 4317", "href": "https://github.com/scrapy/scrapy/issues/4317"}, {"text": "issue 4377", "href": "https://github.com/scrapy/scrapy/issues/4377"}, {"text": "Request.from_curl", "href": "topics/request-response.html#scrapy.http.Request.from_curl"}, {"text": "curl_to_request_kwargs()", "href": "topics/developer-tools.html#scrapy.utils.curl.curl_to_request_kwargs"}, {"text": "issue 4612", "href": "https://github.com/scrapy/scrapy/issues/4612"}, {"text": "CrawlSpider", "href": "topics/spiders.html#scrapy.spiders.CrawlSpider"}, {"text": "issue 712", "href": "https://github.com/scrapy/scrapy/issues/712"}, {"text": "issue 732", "href": "https://github.com/scrapy/scrapy/issues/732"}, {"text": "issue 781", "href": "https://github.com/scrapy/scrapy/issues/781"}, {"text": "issue 4254", "href": "https://github.com/scrapy/scrapy/issues/4254"}, {"text": "CSV exporting", "href": "topics/feed-exports.html#topics-feed-format-csv"}, {"text": "dataclass items", "href": "topics/items.html#dataclass-items"}, {"text": "attr.s items", "href": "topics/items.html#attrs-items"}, {"text": "issue 4667", "href": "https://github.com/scrapy/scrapy/issues/4667"}, {"text": "issue 4668", "href": "https://github.com/scrapy/scrapy/issues/4668"}, {"text": "Request.from_curl", "href": "topics/request-response.html#scrapy.http.Request.from_curl"}, {"text": "curl_to_request_kwargs()", "href": "topics/developer-tools.html#scrapy.utils.curl.curl_to_request_kwargs"}, {"text": "issue 4612", "href": "https://github.com/scrapy/scrapy/issues/4612"}, {"text": "issue 4393", "href": "https://github.com/scrapy/scrapy/issues/4393"}, {"text": "issue 4403", "href": "https://github.com/scrapy/scrapy/issues/4403"}, {"text": "OpenSSL cipher list format", "href": "https://www.openssl.org/docs/manmaster/man1/openssl-ciphers.html#CIPHER-LIST-FORMAT"}, {"text": "DOWNLOADER_CLIENT_TLS_CIPHERS", "href": "topics/settings.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS"}, {"text": "issue 4653", "href": "https://github.com/scrapy/scrapy/issues/4653"}, {"text": "Working with dataclass items", "href": "topics/loaders.html#topics-loaders-dataclass"}, {"text": "issue 4652", "href": "https://github.com/scrapy/scrapy/issues/4652"}, {"text": "item loaders", "href": "topics/loaders.html#topics-loaders"}, {"text": "itemloaders", "href": "https://itemloaders.readthedocs.io/en/latest/index.html"}, {"text": "issue 4005", "href": "https://github.com/scrapy/scrapy/issues/4005"}, {"text": "issue 4516", "href": "https://github.com/scrapy/scrapy/issues/4516"}, {"text": "issue 4644", "href": "https://github.com/scrapy/scrapy/issues/4644"}, {"text": "issue 4645", "href": "https://github.com/scrapy/scrapy/issues/4645"}, {"text": "issue 4650", "href": "https://github.com/scrapy/scrapy/issues/4650"}, {"text": "issue 4682", "href": "https://github.com/scrapy/scrapy/issues/4682"}, {"text": "issue 4704", "href": "https://github.com/scrapy/scrapy/issues/4704"}, {"text": "issue 4673", "href": "https://github.com/scrapy/scrapy/issues/4673"}, {"text": "issue 4690", "href": "https://github.com/scrapy/scrapy/issues/4690"}, {"text": "issue 4458", "href": "https://github.com/scrapy/scrapy/issues/4458"}, {"text": "issue 4504", "href": "https://github.com/scrapy/scrapy/issues/4504"}, {"text": "startproject", "href": "topics/commands.html#std-command-startproject"}, {"text": "issue 4662", "href": "https://github.com/scrapy/scrapy/issues/4662"}, {"text": "issue 4666", "href": "https://github.com/scrapy/scrapy/issues/4666"}, {"text": "dataclass objects", "href": "topics/items.html#dataclass-items"}, {"text": "attrs objects", "href": "topics/items.html#attrs-items"}, {"text": "item types", "href": "topics/items.html#item-types"}, {"text": "TextResponse.json", "href": "topics/request-response.html#scrapy.http.TextResponse.json"}, {"text": "bytes_received", "href": "topics/signals.html#std-signal-bytes_received"}, {"text": "CookiesMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware"}, {"text": "typing.Type", "href": "https://docs.python.org/3/library/typing.html#typing.Type"}, {"text": "issue 4615", "href": "https://github.com/scrapy/scrapy/issues/4615"}, {"text": "TextResponse.text", "href": "topics/request-response.html#scrapy.http.TextResponse.text"}, {"text": "issue 4546", "href": "https://github.com/scrapy/scrapy/issues/4546"}, {"text": "issue 4555", "href": "https://github.com/scrapy/scrapy/issues/4555"}, {"text": "issue 4579", "href": "https://github.com/scrapy/scrapy/issues/4579"}, {"text": "issue 4534", "href": "https://github.com/scrapy/scrapy/issues/4534"}, {"text": "dataclass objects", "href": "topics/items.html#dataclass-items"}, {"text": "attrs objects", "href": "topics/items.html#attrs-items"}, {"text": "item types", "href": "topics/items.html#item-types"}, {"text": "itemadapter", "href": "https://github.com/scrapy/itemadapter"}, {"text": "supports any item type", "href": "topics/items.html#supporting-item-types"}, {"text": "issue 2749", "href": "https://github.com/scrapy/scrapy/issues/2749"}, {"text": "issue 2807", "href": "https://github.com/scrapy/scrapy/issues/2807"}, {"text": "issue 3761", "href": "https://github.com/scrapy/scrapy/issues/3761"}, {"text": "issue 3881", "href": "https://github.com/scrapy/scrapy/issues/3881"}, {"text": "issue 4642", "href": "https://github.com/scrapy/scrapy/issues/4642"}, {"text": "TextResponse.json", "href": "topics/request-response.html#scrapy.http.TextResponse.json"}, {"text": "issue 2444", "href": "https://github.com/scrapy/scrapy/issues/2444"}, {"text": "issue 4460", "href": "https://github.com/scrapy/scrapy/issues/4460"}, {"text": "issue 4574", "href": "https://github.com/scrapy/scrapy/issues/4574"}, {"text": "bytes_received", "href": "topics/signals.html#std-signal-bytes_received"}, {"text": "stopping downloads", "href": "topics/request-response.html#topics-stop-response-download"}, {"text": "issue 4205", "href": "https://github.com/scrapy/scrapy/issues/4205"}, {"text": "issue 4559", "href": "https://github.com/scrapy/scrapy/issues/4559"}, {"text": "media pipeline", "href": "topics/media-pipeline.html#topics-media-pipeline"}, {"text": "FilesPipeline.get_media_requests", "href": "topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.get_media_requests"}, {"text": "issue 2893", "href": "https://github.com/scrapy/scrapy/issues/2893"}, {"text": "issue 4486", "href": "https://github.com/scrapy/scrapy/issues/4486"}, {"text": "Google Cloud Storage", "href": "topics/media-pipeline.html#media-pipeline-gcs"}, {"text": "media pipeline", "href": "topics/media-pipeline.html#topics-media-pipeline"}, {"text": "issue 4346", "href": "https://github.com/scrapy/scrapy/issues/4346"}, {"text": "issue 4508", "href": "https://github.com/scrapy/scrapy/issues/4508"}, {"text": "Link extractors", "href": "topics/link-extractors.html#topics-link-extractors"}, {"text": "lambdas", "href": "https://docs.python.org/3/reference/expressions.html#lambda"}, {"text": "Request.cb_kwargs", "href": "topics/request-response.html#scrapy.http.Request.cb_kwargs"}, {"text": "Request.meta", "href": "topics/request-response.html#scrapy.http.Request.meta"}, {"text": "persisting\nscheduled requests", "href": "topics/jobs.html#topics-jobs"}, {"text": "issue 4554", "href": "https://github.com/scrapy/scrapy/issues/4554"}, {"text": "pickle protocol", "href": "https://docs.python.org/3/library/pickle.html#pickle-protocols"}, {"text": "issue 4135", "href": "https://github.com/scrapy/scrapy/issues/4135"}, {"text": "issue 4541", "href": "https://github.com/scrapy/scrapy/issues/4541"}, {"text": "TypeError", "href": "https://docs.python.org/3/library/exceptions.html#TypeError"}, {"text": "issue 4528", "href": "https://github.com/scrapy/scrapy/issues/4528"}, {"text": "issue 4532", "href": "https://github.com/scrapy/scrapy/issues/4532"}, {"text": "CookiesMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware"}, {"text": "Request.headers", "href": "topics/request-response.html#scrapy.http.Request.headers"}, {"text": "issue 1992", "href": "https://github.com/scrapy/scrapy/issues/1992"}, {"text": "issue 2400", "href": "https://github.com/scrapy/scrapy/issues/2400"}, {"text": "CookiesMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware"}, {"text": "bytes", "href": "https://docs.python.org/3/library/stdtypes.html#bytes"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "issue 2400", "href": "https://github.com/scrapy/scrapy/issues/2400"}, {"text": "issue 3575", "href": "https://github.com/scrapy/scrapy/issues/3575"}, {"text": "FEEDS", "href": "topics/feed-exports.html#std-setting-FEEDS"}, {"text": "FEED_STORE_EMPTY", "href": "topics/feed-exports.html#std-setting-FEED_STORE_EMPTY"}, {"text": "issue 4621", "href": "https://github.com/scrapy/scrapy/issues/4621"}, {"text": "issue 4626", "href": "https://github.com/scrapy/scrapy/issues/4626"}, {"text": "Spider", "href": "topics/spiders.html#scrapy.spiders.Spider"}, {"text": "coroutine\nsyntax", "href": "topics/coroutines.html"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "item", "href": "topics/items.html#topics-items"}, {"text": "issue 4609", "href": "https://github.com/scrapy/scrapy/issues/4609"}, {"text": "startproject", "href": "topics/commands.html#std-command-startproject"}, {"text": "issue 4604", "href": "https://github.com/scrapy/scrapy/issues/4604"}, {"text": "KeyError", "href": "https://docs.python.org/3/library/exceptions.html#KeyError"}, {"text": "issue 4597", "href": "https://github.com/scrapy/scrapy/issues/4597"}, {"text": "issue 4599", "href": "https://github.com/scrapy/scrapy/issues/4599"}, {"text": "FEEDS", "href": "topics/feed-exports.html#std-setting-FEEDS"}, {"text": "issue 4619", "href": "https://github.com/scrapy/scrapy/issues/4619"}, {"text": "issue 4629", "href": "https://github.com/scrapy/scrapy/issues/4629"}, {"text": "accessing cb_kwargs from errbacks", "href": "topics/request-response.html#errback-cb-kwargs"}, {"text": "issue 4598", "href": "https://github.com/scrapy/scrapy/issues/4598"}, {"text": "issue 4634", "href": "https://github.com/scrapy/scrapy/issues/4634"}, {"text": "chompjs", "href": "https://github.com/Nykakin/chompjs"}, {"text": "Parsing JavaScript code", "href": "topics/dynamic-content.html#topics-parsing-javascript"}, {"text": "issue 4556", "href": "https://github.com/scrapy/scrapy/issues/4556"}, {"text": "issue 4562", "href": "https://github.com/scrapy/scrapy/issues/4562"}, {"text": "Coroutines", "href": "topics/coroutines.html"}, {"text": "issue 4511", "href": "https://github.com/scrapy/scrapy/issues/4511"}, {"text": "issue 4513", "href": "https://github.com/scrapy/scrapy/issues/4513"}, {"text": "Twisted", "href": "https://docs.twisted.org/en/stable/index.html"}, {"text": "issue 4533", "href": "https://github.com/scrapy/scrapy/issues/4533"}, {"text": "screenshot pipeline example", "href": "topics/item-pipeline.html#screenshotpipeline"}, {"text": "coroutine syntax", "href": "topics/coroutines.html"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "issue 4514", "href": "https://github.com/scrapy/scrapy/issues/4514"}, {"text": "issue 4593", "href": "https://github.com/scrapy/scrapy/issues/4593"}, {"text": "scrapy.utils.log.configure_logging()", "href": "topics/logging.html#scrapy.utils.log.configure_logging"}, {"text": "issue 4510", "href": "https://github.com/scrapy/scrapy/issues/4510"}, {"text": "issue 4587", "href": "https://github.com/scrapy/scrapy/issues/4587"}, {"text": "commands", "href": "topics/commands.html#topics-commands"}, {"text": "Request.meta", "href": "topics/request-response.html#scrapy.http.Request.meta"}, {"text": "settings", "href": "topics/settings.html#topics-settings"}, {"text": "signals", "href": "topics/signals.html#topics-signals"}, {"text": "issue 4495", "href": "https://github.com/scrapy/scrapy/issues/4495"}, {"text": "issue 4563", "href": "https://github.com/scrapy/scrapy/issues/4563"}, {"text": "issue 4578", "href": "https://github.com/scrapy/scrapy/issues/4578"}, {"text": "issue 4585", "href": "https://github.com/scrapy/scrapy/issues/4585"}, {"text": "issue 4592", "href": "https://github.com/scrapy/scrapy/issues/4592"}, {"text": "issue 4596", "href": "https://github.com/scrapy/scrapy/issues/4596"}, {"text": "style guidelines", "href": "contributing.html#coding-style"}, {"text": "issue 4237", "href": "https://github.com/scrapy/scrapy/issues/4237"}, {"text": "issue 4525", "href": "https://github.com/scrapy/scrapy/issues/4525"}, {"text": "issue 4538", "href": "https://github.com/scrapy/scrapy/issues/4538"}, {"text": "issue 4539", "href": "https://github.com/scrapy/scrapy/issues/4539"}, {"text": "issue 4540", "href": "https://github.com/scrapy/scrapy/issues/4540"}, {"text": "issue 4542", "href": "https://github.com/scrapy/scrapy/issues/4542"}, {"text": "issue 4543", "href": "https://github.com/scrapy/scrapy/issues/4543"}, {"text": "issue 4544", "href": "https://github.com/scrapy/scrapy/issues/4544"}, {"text": "issue 4545", "href": "https://github.com/scrapy/scrapy/issues/4545"}, {"text": "issue 4557", "href": "https://github.com/scrapy/scrapy/issues/4557"}, {"text": "issue 4558", "href": "https://github.com/scrapy/scrapy/issues/4558"}, {"text": "issue 4566", "href": "https://github.com/scrapy/scrapy/issues/4566"}, {"text": "issue 4568", "href": "https://github.com/scrapy/scrapy/issues/4568"}, {"text": "issue 4572", "href": "https://github.com/scrapy/scrapy/issues/4572"}, {"text": "issue 4550", "href": "https://github.com/scrapy/scrapy/issues/4550"}, {"text": "issue 4553", "href": "https://github.com/scrapy/scrapy/issues/4553"}, {"text": "issue 4568", "href": "https://github.com/scrapy/scrapy/issues/4568"}, {"text": "crawl", "href": "topics/commands.html#std-command-crawl"}, {"text": "runspider", "href": "topics/commands.html#std-command-runspider"}, {"text": "issue 4548", "href": "https://github.com/scrapy/scrapy/issues/4548"}, {"text": "issue 4552", "href": "https://github.com/scrapy/scrapy/issues/4552"}, {"text": "issue 4635", "href": "https://github.com/scrapy/scrapy/issues/4635"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "issue 4521", "href": "https://github.com/scrapy/scrapy/issues/4521"}, {"text": "issue 4588", "href": "https://github.com/scrapy/scrapy/issues/4588"}, {"text": "SpiderLoader", "href": "topics/api.html#scrapy.spiderloader.SpiderLoader"}, {"text": "issue 4549", "href": "https://github.com/scrapy/scrapy/issues/4549"}, {"text": "issue 4560", "href": "https://github.com/scrapy/scrapy/issues/4560"}, {"text": "issue 4518", "href": "https://github.com/scrapy/scrapy/issues/4518"}, {"text": "issue 4615", "href": "https://github.com/scrapy/scrapy/issues/4615"}, {"text": "Pylint", "href": "https://www.pylint.org/"}, {"text": "issue 3727", "href": "https://github.com/scrapy/scrapy/issues/3727"}, {"text": "Mypy", "href": "http://mypy-lang.org/"}, {"text": "issue 4637", "href": "https://github.com/scrapy/scrapy/issues/4637"}, {"text": "issue 4573", "href": "https://github.com/scrapy/scrapy/issues/4573"}, {"text": "issue 4517", "href": "https://github.com/scrapy/scrapy/issues/4517"}, {"text": "issue 4519", "href": "https://github.com/scrapy/scrapy/issues/4519"}, {"text": "issue 4522", "href": "https://github.com/scrapy/scrapy/issues/4522"}, {"text": "issue 4537", "href": "https://github.com/scrapy/scrapy/issues/4537"}, {"text": "FEEDS", "href": "topics/feed-exports.html#std-setting-FEEDS"}, {"text": "Response.ip_address", "href": "topics/request-response.html#scrapy.http.Response.ip_address"}, {"text": "AssertionError", "href": "https://docs.python.org/3/library/exceptions.html#AssertionError"}, {"text": "assert", "href": "https://docs.python.org/3/reference/simple_stmts.html#assert"}, {"text": "-O", "href": "https://docs.python.org/3/using/cmdline.html#cmdoption-O"}, {"text": "AssertionError", "href": "https://docs.python.org/3/library/exceptions.html#AssertionError"}, {"text": "issue 4440", "href": "https://github.com/scrapy/scrapy/issues/4440"}, {"text": "SCHEDULER_DEBUG", "href": "topics/settings.html#std-setting-SCHEDULER_DEBUG"}, {"text": "issue 4385", "href": "https://github.com/scrapy/scrapy/issues/4385"}, {"text": "METAREFRESH_MAXDELAY", "href": "topics/downloader-middleware.html#std-setting-METAREFRESH_MAXDELAY"}, {"text": "issue 4385", "href": "https://github.com/scrapy/scrapy/issues/4385"}, {"text": "issue 4431", "href": "https://github.com/scrapy/scrapy/issues/4431"}, {"text": "Crawler", "href": "topics/api.html#scrapy.crawler.Crawler"}, {"text": "SPIDER_LOADER_CLASS", "href": "topics/settings.html#std-setting-SPIDER_LOADER_CLASS"}, {"text": "issue 4398", "href": "https://github.com/scrapy/scrapy/issues/4398"}, {"text": "issue 4400", "href": "https://github.com/scrapy/scrapy/issues/4400"}, {"text": "FEEDS", "href": "topics/feed-exports.html#std-setting-FEEDS"}, {"text": "issue 1336", "href": "https://github.com/scrapy/scrapy/issues/1336"}, {"text": "issue 3858", "href": "https://github.com/scrapy/scrapy/issues/3858"}, {"text": "issue 4507", "href": "https://github.com/scrapy/scrapy/issues/4507"}, {"text": "FEEDS", "href": "topics/feed-exports.html#std-setting-FEEDS"}, {"text": "issue 1336", "href": "https://github.com/scrapy/scrapy/issues/1336"}, {"text": "issue 3858", "href": "https://github.com/scrapy/scrapy/issues/3858"}, {"text": "issue 4507", "href": "https://github.com/scrapy/scrapy/issues/4507"}, {"text": "crawl", "href": "topics/commands.html#std-command-crawl"}, {"text": "runspider", "href": "topics/commands.html#std-command-runspider"}, {"text": "issue 1336", "href": "https://github.com/scrapy/scrapy/issues/1336"}, {"text": "issue 3858", "href": "https://github.com/scrapy/scrapy/issues/3858"}, {"text": "issue 4507", "href": "https://github.com/scrapy/scrapy/issues/4507"}, {"text": "crawl", "href": "topics/commands.html#std-command-crawl"}, {"text": "runspider", "href": "topics/commands.html#std-command-runspider"}, {"text": "issue 1336", "href": "https://github.com/scrapy/scrapy/issues/1336"}, {"text": "issue 3858", "href": "https://github.com/scrapy/scrapy/issues/3858"}, {"text": "issue 4507", "href": "https://github.com/scrapy/scrapy/issues/4507"}, {"text": "Response.ip_address", "href": "topics/request-response.html#scrapy.http.Response.ip_address"}, {"text": "issue 3903", "href": "https://github.com/scrapy/scrapy/issues/3903"}, {"text": "issue 3940", "href": "https://github.com/scrapy/scrapy/issues/3940"}, {"text": "issue 50", "href": "https://github.com/scrapy/scrapy/issues/50"}, {"text": "issue 3198", "href": "https://github.com/scrapy/scrapy/issues/3198"}, {"text": "issue 4413", "href": "https://github.com/scrapy/scrapy/issues/4413"}, {"text": "issue 4438", "href": "https://github.com/scrapy/scrapy/issues/4438"}, {"text": "Request serialization", "href": "topics/jobs.html#request-serialization"}, {"text": "issue 4500", "href": "https://github.com/scrapy/scrapy/issues/4500"}, {"text": "TypeError", "href": "https://docs.python.org/3/library/exceptions.html#TypeError"}, {"text": "issue 4410", "href": "https://github.com/scrapy/scrapy/issues/4410"}, {"text": "issue 4438", "href": "https://github.com/scrapy/scrapy/issues/4438"}, {"text": "issue 4447", "href": "https://github.com/scrapy/scrapy/issues/4447"}, {"text": "issue 4448", "href": "https://github.com/scrapy/scrapy/issues/4448"}, {"text": "issue 4412", "href": "https://github.com/scrapy/scrapy/issues/4412"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "issue 4295", "href": "https://github.com/scrapy/scrapy/issues/4295"}, {"text": "issue 4390", "href": "https://github.com/scrapy/scrapy/issues/4390"}, {"text": "issue 4456", "href": "https://github.com/scrapy/scrapy/issues/4456"}, {"text": "curl2scrapy", "href": "https://michael-shub.github.io/curl2scrapy/"}, {"text": "issue 4206", "href": "https://github.com/scrapy/scrapy/issues/4206"}, {"text": "issue 4455", "href": "https://github.com/scrapy/scrapy/issues/4455"}, {"text": "issue 4285", "href": "https://github.com/scrapy/scrapy/issues/4285"}, {"text": "issue 4343", "href": "https://github.com/scrapy/scrapy/issues/4343"}, {"text": "issue 4444", "href": "https://github.com/scrapy/scrapy/issues/4444"}, {"text": "issue 4445", "href": "https://github.com/scrapy/scrapy/issues/4445"}, {"text": "issue 4475", "href": "https://github.com/scrapy/scrapy/issues/4475"}, {"text": "issue 4480", "href": "https://github.com/scrapy/scrapy/issues/4480"}, {"text": "issue 4496", "href": "https://github.com/scrapy/scrapy/issues/4496"}, {"text": "issue 4503", "href": "https://github.com/scrapy/scrapy/issues/4503"}, {"text": "issue 4404", "href": "https://github.com/scrapy/scrapy/issues/4404"}, {"text": "StringTransport", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.testing.StringTransport.html"}, {"text": "issue 4409", "href": "https://github.com/scrapy/scrapy/issues/4409"}, {"text": "issue 4384", "href": "https://github.com/scrapy/scrapy/issues/4384"}, {"text": "object", "href": "https://docs.python.org/3/library/functions.html#object"}, {"text": "issue 4430", "href": "https://github.com/scrapy/scrapy/issues/4430"}, {"text": "issue 4472", "href": "https://github.com/scrapy/scrapy/issues/4472"}, {"text": "issue 4468", "href": "https://github.com/scrapy/scrapy/issues/4468"}, {"text": "issue 4469", "href": "https://github.com/scrapy/scrapy/issues/4469"}, {"text": "issue 4471", "href": "https://github.com/scrapy/scrapy/issues/4471"}, {"text": "issue 4481", "href": "https://github.com/scrapy/scrapy/issues/4481"}, {"text": "twisted.internet.defer.returnValue()", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.html#returnValue"}, {"text": "issue 4443", "href": "https://github.com/scrapy/scrapy/issues/4443"}, {"text": "issue 4446", "href": "https://github.com/scrapy/scrapy/issues/4446"}, {"text": "issue 4489", "href": "https://github.com/scrapy/scrapy/issues/4489"}, {"text": "Response.follow_all", "href": "topics/request-response.html#scrapy.http.Response.follow_all"}, {"text": "issue 4408", "href": "https://github.com/scrapy/scrapy/issues/4408"}, {"text": "issue 4420", "href": "https://github.com/scrapy/scrapy/issues/4420"}, {"text": "reactor", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html"}, {"text": "TWISTED_REACTOR", "href": "topics/settings.html#std-setting-TWISTED_REACTOR"}, {"text": "issue 4401", "href": "https://github.com/scrapy/scrapy/issues/4401"}, {"text": "issue 4406", "href": "https://github.com/scrapy/scrapy/issues/4406"}, {"text": "issue 4422", "href": "https://github.com/scrapy/scrapy/issues/4422"}, {"text": "Partial", "href": "topics/coroutines.html"}, {"text": "coroutine syntax", "href": "https://docs.python.org/3/reference/compound_stmts.html#async"}, {"text": "experimental", "href": "topics/asyncio.html"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "Response.follow_all", "href": "topics/request-response.html#scrapy.http.Response.follow_all"}, {"text": "FTP support", "href": "topics/media-pipeline.html#media-pipeline-ftp"}, {"text": "Response.certificate", "href": "topics/request-response.html#scrapy.http.Response.certificate"}, {"text": "DNS_RESOLVER", "href": "topics/settings.html#std-setting-DNS_RESOLVER"}, {"text": "Python 2 end-of-life on\nJanuary 1, 2020", "href": "https://www.python.org/doc/sunset-python-2/"}, {"text": "issue 4091", "href": "https://github.com/scrapy/scrapy/issues/4091"}, {"text": "issue 4114", "href": "https://github.com/scrapy/scrapy/issues/4114"}, {"text": "issue 4115", "href": "https://github.com/scrapy/scrapy/issues/4115"}, {"text": "issue 4121", "href": "https://github.com/scrapy/scrapy/issues/4121"}, {"text": "issue 4138", "href": "https://github.com/scrapy/scrapy/issues/4138"}, {"text": "issue 4231", "href": "https://github.com/scrapy/scrapy/issues/4231"}, {"text": "issue 4242", "href": "https://github.com/scrapy/scrapy/issues/4242"}, {"text": "issue 4304", "href": "https://github.com/scrapy/scrapy/issues/4304"}, {"text": "issue 4309", "href": "https://github.com/scrapy/scrapy/issues/4309"}, {"text": "issue 4373", "href": "https://github.com/scrapy/scrapy/issues/4373"}, {"text": "RETRY_TIMES", "href": "topics/downloader-middleware.html#std-setting-RETRY_TIMES"}, {"text": "issue 3171", "href": "https://github.com/scrapy/scrapy/issues/3171"}, {"text": "issue 3566", "href": "https://github.com/scrapy/scrapy/issues/3566"}, {"text": "LinkExtractor", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"}, {"text": "issue 1837", "href": "https://github.com/scrapy/scrapy/issues/1837"}, {"text": "issue 2067", "href": "https://github.com/scrapy/scrapy/issues/2067"}, {"text": "issue 4066", "href": "https://github.com/scrapy/scrapy/issues/4066"}, {"text": "METAREFRESH_IGNORE_TAGS", "href": "topics/downloader-middleware.html#std-setting-METAREFRESH_IGNORE_TAGS"}, {"text": "issue 3844", "href": "https://github.com/scrapy/scrapy/issues/3844"}, {"text": "issue 4311", "href": "https://github.com/scrapy/scrapy/issues/4311"}, {"text": "HttpCompressionMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware"}, {"text": "issue 4293", "href": "https://github.com/scrapy/scrapy/issues/4293"}, {"text": "DOWNLOAD_HANDLERS", "href": "topics/settings.html#std-setting-DOWNLOAD_HANDLERS"}, {"text": "issue 4126", "href": "https://github.com/scrapy/scrapy/issues/4126"}, {"text": "scrapy.core.scheduler.Scheduler", "href": "topics/scheduler.html#scrapy.core.scheduler.Scheduler"}, {"text": "SCHEDULER_PRIORITY_QUEUE", "href": "topics/settings.html#std-setting-SCHEDULER_PRIORITY_QUEUE"}, {"text": "SCHEDULER_DISK_QUEUE", "href": "topics/settings.html#std-setting-SCHEDULER_DISK_QUEUE"}, {"text": "SCHEDULER_MEMORY_QUEUE", "href": "topics/settings.html#std-setting-SCHEDULER_MEMORY_QUEUE"}, {"text": "issue 4199", "href": "https://github.com/scrapy/scrapy/issues/4199"}, {"text": "Scrapy shell", "href": "topics/shell.html#topics-shell"}, {"text": "issue 4347", "href": "https://github.com/scrapy/scrapy/issues/4347"}, {"text": "issue 4112", "href": "https://github.com/scrapy/scrapy/issues/4112"}, {"text": "issue 4362", "href": "https://github.com/scrapy/scrapy/issues/4362"}, {"text": "issue 4300", "href": "https://github.com/scrapy/scrapy/issues/4300"}, {"text": "issue 4374", "href": "https://github.com/scrapy/scrapy/issues/4374"}, {"text": "issue 4375", "href": "https://github.com/scrapy/scrapy/issues/4375"}, {"text": "scrapy.linkextractors.LinkExtractor", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"}, {"text": "issue 4045", "href": "https://github.com/scrapy/scrapy/issues/4045"}, {"text": "issue 4198", "href": "https://github.com/scrapy/scrapy/issues/4198"}, {"text": "next()", "href": "https://docs.python.org/3/library/functions.html#next"}, {"text": "issue 4153", "href": "https://github.com/scrapy/scrapy/issues/4153"}, {"text": "partial support", "href": "topics/coroutines.html"}, {"text": "coroutine syntax", "href": "https://docs.python.org/3/reference/compound_stmts.html#async"}, {"text": "experimental support", "href": "topics/asyncio.html"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "issue 4010", "href": "https://github.com/scrapy/scrapy/issues/4010"}, {"text": "issue 4259", "href": "https://github.com/scrapy/scrapy/issues/4259"}, {"text": "issue 4269", "href": "https://github.com/scrapy/scrapy/issues/4269"}, {"text": "issue 4270", "href": "https://github.com/scrapy/scrapy/issues/4270"}, {"text": "issue 4271", "href": "https://github.com/scrapy/scrapy/issues/4271"}, {"text": "issue 4316", "href": "https://github.com/scrapy/scrapy/issues/4316"}, {"text": "issue 4318", "href": "https://github.com/scrapy/scrapy/issues/4318"}, {"text": "Response.follow_all", "href": "topics/request-response.html#scrapy.http.Response.follow_all"}, {"text": "Response.follow", "href": "topics/request-response.html#scrapy.http.Response.follow"}, {"text": "issue 2582", "href": "https://github.com/scrapy/scrapy/issues/2582"}, {"text": "issue 4057", "href": "https://github.com/scrapy/scrapy/issues/4057"}, {"text": "issue 4286", "href": "https://github.com/scrapy/scrapy/issues/4286"}, {"text": "Media pipelines", "href": "topics/media-pipeline.html#topics-media-pipeline"}, {"text": "FTP\nstorage", "href": "topics/media-pipeline.html#media-pipeline-ftp"}, {"text": "issue 3928", "href": "https://github.com/scrapy/scrapy/issues/3928"}, {"text": "issue 3961", "href": "https://github.com/scrapy/scrapy/issues/3961"}, {"text": "Response.certificate", "href": "topics/request-response.html#scrapy.http.Response.certificate"}, {"text": "twisted.internet.ssl.Certificate", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.ssl.Certificate.html"}, {"text": "issue 2726", "href": "https://github.com/scrapy/scrapy/issues/2726"}, {"text": "issue 4054", "href": "https://github.com/scrapy/scrapy/issues/4054"}, {"text": "DNS_RESOLVER", "href": "topics/settings.html#std-setting-DNS_RESOLVER"}, {"text": "issue 1031", "href": "https://github.com/scrapy/scrapy/issues/1031"}, {"text": "issue 4227", "href": "https://github.com/scrapy/scrapy/issues/4227"}, {"text": "SCRAPER_SLOT_MAX_ACTIVE_SIZE", "href": "topics/settings.html#std-setting-SCRAPER_SLOT_MAX_ACTIVE_SIZE"}, {"text": "issue 1410", "href": "https://github.com/scrapy/scrapy/issues/1410"}, {"text": "issue 3551", "href": "https://github.com/scrapy/scrapy/issues/3551"}, {"text": "TWISTED_REACTOR", "href": "topics/settings.html#std-setting-TWISTED_REACTOR"}, {"text": "reactor", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html"}, {"text": "enable asyncio support", "href": "topics/asyncio.html"}, {"text": "common macOS issue", "href": "faq.html#faq-specific-reactor"}, {"text": "issue 2905", "href": "https://github.com/scrapy/scrapy/issues/2905"}, {"text": "issue 4294", "href": "https://github.com/scrapy/scrapy/issues/4294"}, {"text": "issue 3884", "href": "https://github.com/scrapy/scrapy/issues/3884"}, {"text": "Response.cb_kwargs", "href": "topics/request-response.html#scrapy.http.Response.cb_kwargs"}, {"text": "Response.request.cb_kwargs", "href": "topics/request-response.html#scrapy.http.Request.cb_kwargs"}, {"text": "issue 4331", "href": "https://github.com/scrapy/scrapy/issues/4331"}, {"text": "Response.follow", "href": "topics/request-response.html#scrapy.http.Response.follow"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "issue 4277", "href": "https://github.com/scrapy/scrapy/issues/4277"}, {"text": "issue 4279", "href": "https://github.com/scrapy/scrapy/issues/4279"}, {"text": "Item loader processors", "href": "topics/loaders.html#topics-loaders-processors"}, {"text": "issue 3899", "href": "https://github.com/scrapy/scrapy/issues/3899"}, {"text": "Rule", "href": "topics/spiders.html#scrapy.spiders.Rule"}, {"text": "issue 4000", "href": "https://github.com/scrapy/scrapy/issues/4000"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "issue 3586", "href": "https://github.com/scrapy/scrapy/issues/3586"}, {"text": "issue 4008", "href": "https://github.com/scrapy/scrapy/issues/4008"}, {"text": "LogFormatter", "href": "topics/logging.html#scrapy.logformatter.LogFormatter"}, {"text": "download_error", "href": "topics/logging.html#scrapy.logformatter.LogFormatter.download_error"}, {"text": "item_error", "href": "topics/logging.html#scrapy.logformatter.LogFormatter.item_error"}, {"text": "item pipelines", "href": "topics/item-pipeline.html#topics-item-pipeline"}, {"text": "spider_error", "href": "topics/logging.html#scrapy.logformatter.LogFormatter.spider_error"}, {"text": "spider callbacks", "href": "topics/spiders.html#topics-spiders"}, {"text": "issue 374", "href": "https://github.com/scrapy/scrapy/issues/374"}, {"text": "issue 3986", "href": "https://github.com/scrapy/scrapy/issues/3986"}, {"text": "issue 3989", "href": "https://github.com/scrapy/scrapy/issues/3989"}, {"text": "issue 4176", "href": "https://github.com/scrapy/scrapy/issues/4176"}, {"text": "issue 4188", "href": "https://github.com/scrapy/scrapy/issues/4188"}, {"text": "pathlib.Path", "href": "https://docs.python.org/3/library/pathlib.html#pathlib.Path"}, {"text": "issue 3731", "href": "https://github.com/scrapy/scrapy/issues/3731"}, {"text": "issue 4074", "href": "https://github.com/scrapy/scrapy/issues/4074"}, {"text": "request_left_downloader", "href": "topics/signals.html#std-signal-request_left_downloader"}, {"text": "issue 4303", "href": "https://github.com/scrapy/scrapy/issues/4303"}, {"text": "issue 3484", "href": "https://github.com/scrapy/scrapy/issues/3484"}, {"text": "issue 3869", "href": "https://github.com/scrapy/scrapy/issues/3869"}, {"text": "Spider", "href": "topics/spiders.html#scrapy.spiders.Spider"}, {"text": "AttributeError", "href": "https://docs.python.org/3/library/exceptions.html#AttributeError"}, {"text": "issue 4133", "href": "https://github.com/scrapy/scrapy/issues/4133"}, {"text": "issue 4170", "href": "https://github.com/scrapy/scrapy/issues/4170"}, {"text": "BaseItemExporter", "href": "topics/exporters.html#scrapy.exporters.BaseItemExporter"}, {"text": "issue 4193", "href": "https://github.com/scrapy/scrapy/issues/4193"}, {"text": "issue 4370", "href": "https://github.com/scrapy/scrapy/issues/4370"}, {"text": "issue 4104", "href": "https://github.com/scrapy/scrapy/issues/4104"}, {"text": "DOWNLOAD_HANDLERS", "href": "topics/settings.html#std-setting-DOWNLOAD_HANDLERS"}, {"text": "issue 4126", "href": "https://github.com/scrapy/scrapy/issues/4126"}, {"text": "allowing it to be used as a sequence", "href": "https://lgtm.com/rules/4850080/"}, {"text": "issue 4153", "href": "https://github.com/scrapy/scrapy/issues/4153"}, {"text": "crawl", "href": "topics/commands.html#std-command-crawl"}, {"text": "issue 4175", "href": "https://github.com/scrapy/scrapy/issues/4175"}, {"text": "issue 4207", "href": "https://github.com/scrapy/scrapy/issues/4207"}, {"text": "LinkExtractor.extract_links", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links"}, {"text": "issue 998", "href": "https://github.com/scrapy/scrapy/issues/998"}, {"text": "issue 1403", "href": "https://github.com/scrapy/scrapy/issues/1403"}, {"text": "issue 1949", "href": "https://github.com/scrapy/scrapy/issues/1949"}, {"text": "issue 4321", "href": "https://github.com/scrapy/scrapy/issues/4321"}, {"text": "SPIDER_MIDDLEWARES", "href": "topics/settings.html#std-setting-SPIDER_MIDDLEWARES"}, {"text": "issue 4260", "href": "https://github.com/scrapy/scrapy/issues/4260"}, {"text": "issue 4272", "href": "https://github.com/scrapy/scrapy/issues/4272"}, {"text": "issue 4032", "href": "https://github.com/scrapy/scrapy/issues/4032"}, {"text": "issue 4042", "href": "https://github.com/scrapy/scrapy/issues/4042"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "issue 2552", "href": "https://github.com/scrapy/scrapy/issues/2552"}, {"text": "issue 4094", "href": "https://github.com/scrapy/scrapy/issues/4094"}, {"text": "MailSender", "href": "topics/email.html#scrapy.mail.MailSender"}, {"text": "issue 4229", "href": "https://github.com/scrapy/scrapy/issues/4229"}, {"text": "issue 4239", "href": "https://github.com/scrapy/scrapy/issues/4239"}, {"text": "DUPEFILTER_CLASS", "href": "topics/settings.html#std-setting-DUPEFILTER_CLASS"}, {"text": "issue 4283", "href": "https://github.com/scrapy/scrapy/issues/4283"}, {"text": "issue 4122", "href": "https://github.com/scrapy/scrapy/issues/4122"}, {"text": "issue 4291", "href": "https://github.com/scrapy/scrapy/issues/4291"}, {"text": "TypeError", "href": "https://docs.python.org/3/library/exceptions.html#TypeError"}, {"text": "issue 4123", "href": "https://github.com/scrapy/scrapy/issues/4123"}, {"text": "ValueError", "href": "https://docs.python.org/3/library/exceptions.html#ValueError"}, {"text": "issue 4128", "href": "https://github.com/scrapy/scrapy/issues/4128"}, {"text": "issue 4148", "href": "https://github.com/scrapy/scrapy/issues/4148"}, {"text": "issue 4152", "href": "https://github.com/scrapy/scrapy/issues/4152"}, {"text": "issue 4169", "href": "https://github.com/scrapy/scrapy/issues/4169"}, {"text": "issue 4173", "href": "https://github.com/scrapy/scrapy/issues/4173"}, {"text": "issue 4183", "href": "https://github.com/scrapy/scrapy/issues/4183"}, {"text": "LinkExtractor.extract_links", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links"}, {"text": "Link Extractors", "href": "topics/link-extractors.html#topics-link-extractors"}, {"text": "issue 4045", "href": "https://github.com/scrapy/scrapy/issues/4045"}, {"text": "ItemLoader.item", "href": "topics/loaders.html#scrapy.loader.ItemLoader.item"}, {"text": "issue 3574", "href": "https://github.com/scrapy/scrapy/issues/3574"}, {"text": "issue 4099", "href": "https://github.com/scrapy/scrapy/issues/4099"}, {"text": "logging.basicConfig()", "href": "https://docs.python.org/3/library/logging.html#logging.basicConfig"}, {"text": "CrawlerProcess", "href": "topics/api.html#scrapy.crawler.CrawlerProcess"}, {"text": "issue 2149", "href": "https://github.com/scrapy/scrapy/issues/2149"}, {"text": "issue 2352", "href": "https://github.com/scrapy/scrapy/issues/2352"}, {"text": "issue 3146", "href": "https://github.com/scrapy/scrapy/issues/3146"}, {"text": "issue 3960", "href": "https://github.com/scrapy/scrapy/issues/3960"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "when using persistence", "href": "topics/jobs.html#request-serialization"}, {"text": "issue 4124", "href": "https://github.com/scrapy/scrapy/issues/4124"}, {"text": "issue 4139", "href": "https://github.com/scrapy/scrapy/issues/4139"}, {"text": "custom image pipeline", "href": "topics/media-pipeline.html#media-pipeline-example"}, {"text": "issue 4034", "href": "https://github.com/scrapy/scrapy/issues/4034"}, {"text": "issue 4252", "href": "https://github.com/scrapy/scrapy/issues/4252"}, {"text": "media pipeline", "href": "topics/media-pipeline.html#topics-media-pipeline"}, {"text": "issue 4290", "href": "https://github.com/scrapy/scrapy/issues/4290"}, {"text": "scrapy.core.scheduler.Scheduler", "href": "topics/scheduler.html#scrapy.core.scheduler.Scheduler"}, {"text": "issue 4274", "href": "https://github.com/scrapy/scrapy/issues/4274"}, {"text": "issue 4059", "href": "https://github.com/scrapy/scrapy/issues/4059"}, {"text": "issue 4142", "href": "https://github.com/scrapy/scrapy/issues/4142"}, {"text": "issue 4146", "href": "https://github.com/scrapy/scrapy/issues/4146"}, {"text": "issue 4171", "href": "https://github.com/scrapy/scrapy/issues/4171"}, {"text": "issue 4184", "href": "https://github.com/scrapy/scrapy/issues/4184"}, {"text": "issue 4190", "href": "https://github.com/scrapy/scrapy/issues/4190"}, {"text": "issue 4247", "href": "https://github.com/scrapy/scrapy/issues/4247"}, {"text": "issue 4258", "href": "https://github.com/scrapy/scrapy/issues/4258"}, {"text": "issue 4282", "href": "https://github.com/scrapy/scrapy/issues/4282"}, {"text": "issue 4288", "href": "https://github.com/scrapy/scrapy/issues/4288"}, {"text": "issue 4305", "href": "https://github.com/scrapy/scrapy/issues/4305"}, {"text": "issue 4308", "href": "https://github.com/scrapy/scrapy/issues/4308"}, {"text": "issue 4323", "href": "https://github.com/scrapy/scrapy/issues/4323"}, {"text": "issue 4338", "href": "https://github.com/scrapy/scrapy/issues/4338"}, {"text": "issue 4359", "href": "https://github.com/scrapy/scrapy/issues/4359"}, {"text": "issue 4361", "href": "https://github.com/scrapy/scrapy/issues/4361"}, {"text": "issue 4086", "href": "https://github.com/scrapy/scrapy/issues/4086"}, {"text": "issue 4088", "href": "https://github.com/scrapy/scrapy/issues/4088"}, {"text": "Scrapy at a glance", "href": "intro/overview.html#intro-overview"}, {"text": "issue 4213", "href": "https://github.com/scrapy/scrapy/issues/4213"}, {"text": "intersphinx", "href": "https://www.sphinx-doc.org/en/master/usage/extensions/intersphinx.html#module-sphinx.ext.intersphinx"}, {"text": "issue 4147", "href": "https://github.com/scrapy/scrapy/issues/4147"}, {"text": "issue 4172", "href": "https://github.com/scrapy/scrapy/issues/4172"}, {"text": "issue 4185", "href": "https://github.com/scrapy/scrapy/issues/4185"}, {"text": "issue 4194", "href": "https://github.com/scrapy/scrapy/issues/4194"}, {"text": "issue 4197", "href": "https://github.com/scrapy/scrapy/issues/4197"}, {"text": "issue 4140", "href": "https://github.com/scrapy/scrapy/issues/4140"}, {"text": "issue 4249", "href": "https://github.com/scrapy/scrapy/issues/4249"}, {"text": "issue 4143", "href": "https://github.com/scrapy/scrapy/issues/4143"}, {"text": "issue 4275", "href": "https://github.com/scrapy/scrapy/issues/4275"}, {"text": "issue 2545", "href": "https://github.com/scrapy/scrapy/issues/2545"}, {"text": "issue 4114", "href": "https://github.com/scrapy/scrapy/issues/4114"}, {"text": "Bandit", "href": "https://bandit.readthedocs.io/"}, {"text": "issue 4162", "href": "https://github.com/scrapy/scrapy/issues/4162"}, {"text": "issue 4181", "href": "https://github.com/scrapy/scrapy/issues/4181"}, {"text": "Flake8", "href": "https://flake8.pycqa.org/en/latest/"}, {"text": "issue 3944", "href": "https://github.com/scrapy/scrapy/issues/3944"}, {"text": "issue 3945", "href": "https://github.com/scrapy/scrapy/issues/3945"}, {"text": "issue 4137", "href": "https://github.com/scrapy/scrapy/issues/4137"}, {"text": "issue 4157", "href": "https://github.com/scrapy/scrapy/issues/4157"}, {"text": "issue 4167", "href": "https://github.com/scrapy/scrapy/issues/4167"}, {"text": "issue 4174", "href": "https://github.com/scrapy/scrapy/issues/4174"}, {"text": "issue 4186", "href": "https://github.com/scrapy/scrapy/issues/4186"}, {"text": "issue 4195", "href": "https://github.com/scrapy/scrapy/issues/4195"}, {"text": "issue 4238", "href": "https://github.com/scrapy/scrapy/issues/4238"}, {"text": "issue 4246", "href": "https://github.com/scrapy/scrapy/issues/4246"}, {"text": "issue 4355", "href": "https://github.com/scrapy/scrapy/issues/4355"}, {"text": "issue 4360", "href": "https://github.com/scrapy/scrapy/issues/4360"}, {"text": "issue 4365", "href": "https://github.com/scrapy/scrapy/issues/4365"}, {"text": "issue 4097", "href": "https://github.com/scrapy/scrapy/issues/4097"}, {"text": "issue 4218", "href": "https://github.com/scrapy/scrapy/issues/4218"}, {"text": "issue 4236", "href": "https://github.com/scrapy/scrapy/issues/4236"}, {"text": "issue 4163", "href": "https://github.com/scrapy/scrapy/issues/4163"}, {"text": "issue 4164", "href": "https://github.com/scrapy/scrapy/issues/4164"}, {"text": "issue 4014", "href": "https://github.com/scrapy/scrapy/issues/4014"}, {"text": "issue 4095", "href": "https://github.com/scrapy/scrapy/issues/4095"}, {"text": "issue 4244", "href": "https://github.com/scrapy/scrapy/issues/4244"}, {"text": "issue 4268", "href": "https://github.com/scrapy/scrapy/issues/4268"}, {"text": "issue 4372", "href": "https://github.com/scrapy/scrapy/issues/4372"}, {"text": "tox", "href": "https://tox.wiki/en/latest/index.html"}, {"text": "Bandit", "href": "https://bandit.readthedocs.io/"}, {"text": "Flake8", "href": "https://flake8.pycqa.org/en/latest/"}, {"text": "issue 4179", "href": "https://github.com/scrapy/scrapy/issues/4179"}, {"text": "issue 3937", "href": "https://github.com/scrapy/scrapy/issues/3937"}, {"text": "issue 4208", "href": "https://github.com/scrapy/scrapy/issues/4208"}, {"text": "issue 4209", "href": "https://github.com/scrapy/scrapy/issues/4209"}, {"text": "issue 4210", "href": "https://github.com/scrapy/scrapy/issues/4210"}, {"text": "issue 4212", "href": "https://github.com/scrapy/scrapy/issues/4212"}, {"text": "issue 4369", "href": "https://github.com/scrapy/scrapy/issues/4369"}, {"text": "issue 4376", "href": "https://github.com/scrapy/scrapy/issues/4376"}, {"text": "issue 4378", "href": "https://github.com/scrapy/scrapy/issues/4378"}, {"text": "scrapy.core.scheduler", "href": "topics/scheduler.html#module-scrapy.core.scheduler"}, {"text": "issue 3884", "href": "https://github.com/scrapy/scrapy/issues/3884"}, {"text": "HttpProxyMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "HttpProxyMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "HttpProxyMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "HttpProxyMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "cjvr-mfj7-j4j8 security advisory", "href": "https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "public suffix", "href": "https://publicsuffix.org/"}, {"text": "mfjm-vh54-3f96\nsecurity advisory", "href": "https://github.com/scrapy/scrapy/security/advisories/GHSA-mfjm-vh54-3f96"}, {"text": "HttpAuthMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware"}, {"text": "w3lib.http.basic_auth_header()", "href": "https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header"}, {"text": "scrapy-splash", "href": "https://github.com/scrapy-plugins/scrapy-splash"}, {"text": "Request.from_curl", "href": "topics/request-response.html#scrapy.http.Request.from_curl"}, {"text": "ROBOTSTXT_PARSER", "href": "topics/settings.html#std-setting-ROBOTSTXT_PARSER"}, {"text": "ROBOTSTXT_USER_AGENT", "href": "topics/settings.html#std-setting-ROBOTSTXT_USER_AGENT"}, {"text": "DOWNLOADER_CLIENT_TLS_CIPHERS", "href": "topics/settings.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS"}, {"text": "DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING", "href": "topics/settings.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING"}, {"text": "cssselect", "href": "https://cssselect.readthedocs.io/en/latest/index.html"}, {"text": "cryptography", "href": "https://cryptography.io/en/latest/"}, {"text": "lxml", "href": "https://lxml.de/"}, {"text": "pyOpenSSL", "href": "https://www.pyopenssl.org/en/stable/"}, {"text": "queuelib", "href": "https://github.com/scrapy/queuelib"}, {"text": "service_identity", "href": "https://service-identity.readthedocs.io/en/stable/"}, {"text": "six", "href": "https://six.readthedocs.io/"}, {"text": "Twisted", "href": "https://twistedmatrix.com/trac/"}, {"text": "zope.interface", "href": "https://zopeinterface.readthedocs.io/en/latest/"}, {"text": "issue 3892", "href": "https://github.com/scrapy/scrapy/issues/3892"}, {"text": "JsonRequest", "href": "topics/request-response.html#scrapy.http.JsonRequest"}, {"text": "issue 3929", "href": "https://github.com/scrapy/scrapy/issues/3929"}, {"text": "issue 3982", "href": "https://github.com/scrapy/scrapy/issues/3982"}, {"text": "DOWNLOADER_CLIENTCONTEXTFACTORY", "href": "topics/settings.html#std-setting-DOWNLOADER_CLIENTCONTEXTFACTORY"}, {"text": "issue 2111", "href": "https://github.com/scrapy/scrapy/issues/2111"}, {"text": "issue 3392", "href": "https://github.com/scrapy/scrapy/issues/3392"}, {"text": "issue 3442", "href": "https://github.com/scrapy/scrapy/issues/3442"}, {"text": "issue 3450", "href": "https://github.com/scrapy/scrapy/issues/3450"}, {"text": "ItemLoader", "href": "topics/loaders.html#scrapy.loader.ItemLoader"}, {"text": "issue 3804", "href": "https://github.com/scrapy/scrapy/issues/3804"}, {"text": "issue 3819", "href": "https://github.com/scrapy/scrapy/issues/3819"}, {"text": "issue 3897", "href": "https://github.com/scrapy/scrapy/issues/3897"}, {"text": "issue 3976", "href": "https://github.com/scrapy/scrapy/issues/3976"}, {"text": "issue 3998", "href": "https://github.com/scrapy/scrapy/issues/3998"}, {"text": "issue 4036", "href": "https://github.com/scrapy/scrapy/issues/4036"}, {"text": "Request.from_curl", "href": "topics/request-response.html#scrapy.http.Request.from_curl"}, {"text": "creating a request from a cURL command", "href": "topics/developer-tools.html#requests-from-curl"}, {"text": "issue 2985", "href": "https://github.com/scrapy/scrapy/issues/2985"}, {"text": "issue 3862", "href": "https://github.com/scrapy/scrapy/issues/3862"}, {"text": "ROBOTSTXT_PARSER", "href": "topics/settings.html#std-setting-ROBOTSTXT_PARSER"}, {"text": "robots.txt", "href": "https://www.robotstxt.org/"}, {"text": "RobotFileParser", "href": "topics/downloader-middleware.html#python-robotfileparser"}, {"text": "Protego", "href": "topics/downloader-middleware.html#protego-parser"}, {"text": "Reppy", "href": "topics/downloader-middleware.html#reppy-parser"}, {"text": "Robotexclusionrulesparser", "href": "topics/downloader-middleware.html#rerp-parser"}, {"text": "implement support for additional parsers", "href": "topics/downloader-middleware.html#support-for-new-robots-parser"}, {"text": "issue 754", "href": "https://github.com/scrapy/scrapy/issues/754"}, {"text": "issue 2669", "href": "https://github.com/scrapy/scrapy/issues/2669"}, {"text": "issue 3796", "href": "https://github.com/scrapy/scrapy/issues/3796"}, {"text": "issue 3935", "href": "https://github.com/scrapy/scrapy/issues/3935"}, {"text": "issue 3969", "href": "https://github.com/scrapy/scrapy/issues/3969"}, {"text": "issue 4006", "href": "https://github.com/scrapy/scrapy/issues/4006"}, {"text": "ROBOTSTXT_USER_AGENT", "href": "topics/settings.html#std-setting-ROBOTSTXT_USER_AGENT"}, {"text": "robots.txt", "href": "https://www.robotstxt.org/"}, {"text": "issue 3931", "href": "https://github.com/scrapy/scrapy/issues/3931"}, {"text": "issue 3966", "href": "https://github.com/scrapy/scrapy/issues/3966"}, {"text": "Rule", "href": "topics/spiders.html#scrapy.spiders.Rule"}, {"text": "LinkExtractor", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"}, {"text": "issue 781", "href": "https://github.com/scrapy/scrapy/issues/781"}, {"text": "issue 4016", "href": "https://github.com/scrapy/scrapy/issues/4016"}, {"text": "DOWNLOADER_CLIENT_TLS_CIPHERS", "href": "topics/settings.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS"}, {"text": "issue 3392", "href": "https://github.com/scrapy/scrapy/issues/3392"}, {"text": "issue 3442", "href": "https://github.com/scrapy/scrapy/issues/3442"}, {"text": "DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING", "href": "topics/settings.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING"}, {"text": "issue 2111", "href": "https://github.com/scrapy/scrapy/issues/2111"}, {"text": "issue 3450", "href": "https://github.com/scrapy/scrapy/issues/3450"}, {"text": "Request.cb_kwargs", "href": "topics/request-response.html#scrapy.http.Request.cb_kwargs"}, {"text": "@cb_kwargs", "href": "topics/contracts.html#scrapy.contracts.default.CallbackKeywordArgumentsContract"}, {"text": "spider contract", "href": "topics/contracts.html#topics-contracts"}, {"text": "issue 3985", "href": "https://github.com/scrapy/scrapy/issues/3985"}, {"text": "issue 3988", "href": "https://github.com/scrapy/scrapy/issues/3988"}, {"text": "@scrapes", "href": "topics/contracts.html#scrapy.contracts.default.ScrapesContract"}, {"text": "issue 766", "href": "https://github.com/scrapy/scrapy/issues/766"}, {"text": "issue 3939", "href": "https://github.com/scrapy/scrapy/issues/3939"}, {"text": "Custom log formats", "href": "topics/logging.html#custom-log-formats"}, {"text": "LOG_FORMATTER", "href": "topics/settings.html#std-setting-LOG_FORMATTER"}, {"text": "issue 3984", "href": "https://github.com/scrapy/scrapy/issues/3984"}, {"text": "issue 3987", "href": "https://github.com/scrapy/scrapy/issues/3987"}, {"text": "Zsh", "href": "https://www.zsh.org/"}, {"text": "issue 4069", "href": "https://github.com/scrapy/scrapy/issues/4069"}, {"text": "ItemLoader.load_item()", "href": "topics/loaders.html#scrapy.loader.ItemLoader.load_item"}, {"text": "ItemLoader.get_output_value()", "href": "topics/loaders.html#scrapy.loader.ItemLoader.get_output_value"}, {"text": "ItemLoader.load_item()", "href": "topics/loaders.html#scrapy.loader.ItemLoader.load_item"}, {"text": "issue 3804", "href": "https://github.com/scrapy/scrapy/issues/3804"}, {"text": "issue 3819", "href": "https://github.com/scrapy/scrapy/issues/3819"}, {"text": "issue 3897", "href": "https://github.com/scrapy/scrapy/issues/3897"}, {"text": "issue 3976", "href": "https://github.com/scrapy/scrapy/issues/3976"}, {"text": "issue 3998", "href": "https://github.com/scrapy/scrapy/issues/3998"}, {"text": "issue 4036", "href": "https://github.com/scrapy/scrapy/issues/4036"}, {"text": "DummyStatsCollector", "href": "topics/stats.html#scrapy.statscollectors.DummyStatsCollector"}, {"text": "TypeError", "href": "https://docs.python.org/3/library/exceptions.html#TypeError"}, {"text": "issue 4007", "href": "https://github.com/scrapy/scrapy/issues/4007"}, {"text": "issue 4052", "href": "https://github.com/scrapy/scrapy/issues/4052"}, {"text": "FilesPipeline.file_path", "href": "topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.file_path"}, {"text": "ImagesPipeline.file_path", "href": "topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline.file_path"}, {"text": "registered with IANA", "href": "https://www.iana.org/assignments/media-types/media-types.xhtml"}, {"text": "issue 1287", "href": "https://github.com/scrapy/scrapy/issues/1287"}, {"text": "issue 3953", "href": "https://github.com/scrapy/scrapy/issues/3953"}, {"text": "issue 3954", "href": "https://github.com/scrapy/scrapy/issues/3954"}, {"text": "botocore", "href": "https://github.com/boto/botocore"}, {"text": "issue 3904", "href": "https://github.com/scrapy/scrapy/issues/3904"}, {"text": "issue 3905", "href": "https://github.com/scrapy/scrapy/issues/3905"}, {"text": "issue 3941", "href": "https://github.com/scrapy/scrapy/issues/3941"}, {"text": "issue 3920", "href": "https://github.com/scrapy/scrapy/issues/3920"}, {"text": "custom log\nformat", "href": "topics/logging.html#custom-log-formats"}, {"text": "issue 3616", "href": "https://github.com/scrapy/scrapy/issues/3616"}, {"text": "issue 3660", "href": "https://github.com/scrapy/scrapy/issues/3660"}, {"text": "MarshalItemExporter", "href": "topics/exporters.html#scrapy.exporters.MarshalItemExporter"}, {"text": "PythonItemExporter", "href": "topics/exporters.html#scrapy.exporters.PythonItemExporter"}, {"text": "issue 3973", "href": "https://github.com/scrapy/scrapy/issues/3973"}, {"text": "ItemMeta", "href": "topics/items.html#scrapy.item.ItemMeta"}, {"text": "issue 3999", "href": "https://github.com/scrapy/scrapy/issues/3999"}, {"text": "issue 2998", "href": "https://github.com/scrapy/scrapy/issues/2998"}, {"text": "issue 3398", "href": "https://github.com/scrapy/scrapy/issues/3398"}, {"text": "issue 3597", "href": "https://github.com/scrapy/scrapy/issues/3597"}, {"text": "issue 3894", "href": "https://github.com/scrapy/scrapy/issues/3894"}, {"text": "issue 3934", "href": "https://github.com/scrapy/scrapy/issues/3934"}, {"text": "issue 3978", "href": "https://github.com/scrapy/scrapy/issues/3978"}, {"text": "issue 3993", "href": "https://github.com/scrapy/scrapy/issues/3993"}, {"text": "issue 4022", "href": "https://github.com/scrapy/scrapy/issues/4022"}, {"text": "issue 4028", "href": "https://github.com/scrapy/scrapy/issues/4028"}, {"text": "issue 4033", "href": "https://github.com/scrapy/scrapy/issues/4033"}, {"text": "issue 4046", "href": "https://github.com/scrapy/scrapy/issues/4046"}, {"text": "issue 4050", "href": "https://github.com/scrapy/scrapy/issues/4050"}, {"text": "issue 4055", "href": "https://github.com/scrapy/scrapy/issues/4055"}, {"text": "issue 4056", "href": "https://github.com/scrapy/scrapy/issues/4056"}, {"text": "issue 4061", "href": "https://github.com/scrapy/scrapy/issues/4061"}, {"text": "issue 4072", "href": "https://github.com/scrapy/scrapy/issues/4072"}, {"text": "issue 4071", "href": "https://github.com/scrapy/scrapy/issues/4071"}, {"text": "issue 4079", "href": "https://github.com/scrapy/scrapy/issues/4079"}, {"text": "issue 4081", "href": "https://github.com/scrapy/scrapy/issues/4081"}, {"text": "issue 4089", "href": "https://github.com/scrapy/scrapy/issues/4089"}, {"text": "issue 4093", "href": "https://github.com/scrapy/scrapy/issues/4093"}, {"text": "issue 4015", "href": "https://github.com/scrapy/scrapy/issues/4015"}, {"text": "LevelDB", "href": "https://github.com/google/leveldb"}, {"text": "HttpCacheMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware"}, {"text": "issue 4085", "href": "https://github.com/scrapy/scrapy/issues/4085"}, {"text": "issue 4092", "href": "https://github.com/scrapy/scrapy/issues/4092"}, {"text": "issue 3910", "href": "https://github.com/scrapy/scrapy/issues/3910"}, {"text": "issue 3999", "href": "https://github.com/scrapy/scrapy/issues/3999"}, {"text": "botocore", "href": "https://github.com/boto/botocore"}, {"text": "Pillow", "href": "https://python-pillow.org/"}, {"text": "issue 3892", "href": "https://github.com/scrapy/scrapy/issues/3892"}, {"text": "issue 3126", "href": "https://github.com/scrapy/scrapy/issues/3126"}, {"text": "issue 3471", "href": "https://github.com/scrapy/scrapy/issues/3471"}, {"text": "issue 3749", "href": "https://github.com/scrapy/scrapy/issues/3749"}, {"text": "issue 3754", "href": "https://github.com/scrapy/scrapy/issues/3754"}, {"text": "issue 3923", "href": "https://github.com/scrapy/scrapy/issues/3923"}, {"text": "issue 3391", "href": "https://github.com/scrapy/scrapy/issues/3391"}, {"text": "issue 3907", "href": "https://github.com/scrapy/scrapy/issues/3907"}, {"text": "issue 3946", "href": "https://github.com/scrapy/scrapy/issues/3946"}, {"text": "issue 3950", "href": "https://github.com/scrapy/scrapy/issues/3950"}, {"text": "issue 4023", "href": "https://github.com/scrapy/scrapy/issues/4023"}, {"text": "issue 4031", "href": "https://github.com/scrapy/scrapy/issues/4031"}, {"text": "issue 3804", "href": "https://github.com/scrapy/scrapy/issues/3804"}, {"text": "issue 3819", "href": "https://github.com/scrapy/scrapy/issues/3819"}, {"text": "issue 3897", "href": "https://github.com/scrapy/scrapy/issues/3897"}, {"text": "issue 3976", "href": "https://github.com/scrapy/scrapy/issues/3976"}, {"text": "ItemLoader.load_item()", "href": "topics/loaders.html#scrapy.loader.ItemLoader.load_item"}, {"text": "ItemLoader.get_output_value()", "href": "topics/loaders.html#scrapy.loader.ItemLoader.get_output_value"}, {"text": "ItemLoader.load_item()", "href": "topics/loaders.html#scrapy.loader.ItemLoader.load_item"}, {"text": "issue 3912", "href": "https://github.com/scrapy/scrapy/issues/3912"}, {"text": "issue 3918", "href": "https://github.com/scrapy/scrapy/issues/3918"}, {"text": "issue 3889", "href": "https://github.com/scrapy/scrapy/issues/3889"}, {"text": "issue 3893", "href": "https://github.com/scrapy/scrapy/issues/3893"}, {"text": "issue 3896", "href": "https://github.com/scrapy/scrapy/issues/3896"}, {"text": "RETRY_HTTP_CODES", "href": "topics/downloader-middleware.html#std-setting-RETRY_HTTP_CODES"}, {"text": "RETRY_HTTP_CODES", "href": "topics/downloader-middleware.html#std-setting-RETRY_HTTP_CODES"}, {"text": "Crawler", "href": "topics/api.html#scrapy.crawler.Crawler"}, {"text": "CrawlerRunner.crawl", "href": "topics/api.html#scrapy.crawler.CrawlerRunner.crawl"}, {"text": "CrawlerRunner.create_crawler", "href": "topics/api.html#scrapy.crawler.CrawlerRunner.create_crawler"}, {"text": "Spider", "href": "topics/spiders.html#scrapy.spiders.Spider"}, {"text": "Spider", "href": "topics/spiders.html#scrapy.spiders.Spider"}, {"text": "Spider", "href": "topics/spiders.html#scrapy.spiders.Spider"}, {"text": "Spider", "href": "topics/spiders.html#scrapy.spiders.Spider"}, {"text": "SCHEDULER_PRIORITY_QUEUE", "href": "topics/settings.html#std-setting-SCHEDULER_PRIORITY_QUEUE"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "Scheduler", "href": "topics/scheduler.html#scrapy.core.scheduler.Scheduler"}, {"text": "SCHEDULER", "href": "topics/settings.html#std-setting-SCHEDULER"}, {"text": "enabled", "href": "topics/broad-crawls.html#broad-crawls-scheduler-priority-queue"}, {"text": "CONCURRENT_REQUESTS_PER_IP", "href": "topics/settings.html#std-setting-CONCURRENT_REQUESTS_PER_IP"}, {"text": "issue 3520", "href": "https://github.com/scrapy/scrapy/issues/3520"}, {"text": "Request.cb_kwargs", "href": "topics/request-response.html#scrapy.http.Request.cb_kwargs"}, {"text": "issue 1138", "href": "https://github.com/scrapy/scrapy/issues/1138"}, {"text": "issue 3563", "href": "https://github.com/scrapy/scrapy/issues/3563"}, {"text": "JSONRequest", "href": "topics/request-response.html#scrapy.http.JsonRequest"}, {"text": "issue 3504", "href": "https://github.com/scrapy/scrapy/issues/3504"}, {"text": "issue 3505", "href": "https://github.com/scrapy/scrapy/issues/3505"}, {"text": "Rule", "href": "topics/spiders.html#scrapy.spiders.Rule"}, {"text": "Response", "href": "topics/request-response.html#scrapy.http.Response"}, {"text": "issue 3682", "href": "https://github.com/scrapy/scrapy/issues/3682"}, {"text": "LinkExtractor", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"}, {"text": "issue 3622", "href": "https://github.com/scrapy/scrapy/issues/3622"}, {"text": "issue 3635", "href": "https://github.com/scrapy/scrapy/issues/3635"}, {"text": "FEED_STORAGE_S3_ACL", "href": "topics/feed-exports.html#std-setting-FEED_STORAGE_S3_ACL"}, {"text": "issue 3607", "href": "https://github.com/scrapy/scrapy/issues/3607"}, {"text": "FEED_STORAGE_FTP_ACTIVE", "href": "topics/feed-exports.html#std-setting-FEED_STORAGE_FTP_ACTIVE"}, {"text": "issue 3829", "href": "https://github.com/scrapy/scrapy/issues/3829"}, {"text": "METAREFRESH_IGNORE_TAGS", "href": "topics/downloader-middleware.html#std-setting-METAREFRESH_IGNORE_TAGS"}, {"text": "issue 1422", "href": "https://github.com/scrapy/scrapy/issues/1422"}, {"text": "issue 3768", "href": "https://github.com/scrapy/scrapy/issues/3768"}, {"text": "redirect_reasons", "href": "topics/downloader-middleware.html#std-reqmeta-redirect_reasons"}, {"text": "issue 3581", "href": "https://github.com/scrapy/scrapy/issues/3581"}, {"text": "issue 3687", "href": "https://github.com/scrapy/scrapy/issues/3687"}, {"text": "check", "href": "topics/commands.html#std-command-check"}, {"text": "detecting contract\ncheck runs from code", "href": "topics/contracts.html#detecting-contract-check-runs"}, {"text": "issue 3704", "href": "https://github.com/scrapy/scrapy/issues/3704"}, {"text": "issue 3739", "href": "https://github.com/scrapy/scrapy/issues/3739"}, {"text": "deep-copy items", "href": "topics/items.html#copying-items"}, {"text": "issue 1493", "href": "https://github.com/scrapy/scrapy/issues/1493"}, {"text": "issue 3671", "href": "https://github.com/scrapy/scrapy/issues/3671"}, {"text": "CoreStats", "href": "topics/extensions.html#scrapy.extensions.corestats.CoreStats"}, {"text": "issue 3638", "href": "https://github.com/scrapy/scrapy/issues/3638"}, {"text": "ItemLoader", "href": "topics/loaders.html#scrapy.loader.ItemLoader"}, {"text": "input and output\nprocessors", "href": "topics/loaders.html#topics-loaders-processors"}, {"text": "issue 3836", "href": "https://github.com/scrapy/scrapy/issues/3836"}, {"text": "issue 3840", "href": "https://github.com/scrapy/scrapy/issues/3840"}, {"text": "Crawler", "href": "topics/api.html#scrapy.crawler.Crawler"}, {"text": "CrawlerRunner.crawl", "href": "topics/api.html#scrapy.crawler.CrawlerRunner.crawl"}, {"text": "CrawlerRunner.create_crawler", "href": "topics/api.html#scrapy.crawler.CrawlerRunner.create_crawler"}, {"text": "Spider", "href": "topics/spiders.html#scrapy.spiders.Spider"}, {"text": "issue 2283", "href": "https://github.com/scrapy/scrapy/issues/2283"}, {"text": "issue 3610", "href": "https://github.com/scrapy/scrapy/issues/3610"}, {"text": "issue 3872", "href": "https://github.com/scrapy/scrapy/issues/3872"}, {"text": "process_spider_exception()", "href": "topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"}, {"text": "issue 220", "href": "https://github.com/scrapy/scrapy/issues/220"}, {"text": "issue 2061", "href": "https://github.com/scrapy/scrapy/issues/2061"}, {"text": "KeyboardInterrupt", "href": "https://docs.python.org/3/library/exceptions.html#KeyboardInterrupt"}, {"text": "issue 3726", "href": "https://github.com/scrapy/scrapy/issues/3726"}, {"text": "ItemLoader.load_item()", "href": "topics/loaders.html#scrapy.loader.ItemLoader.load_item"}, {"text": "ItemLoader.get_output_value()", "href": "topics/loaders.html#scrapy.loader.ItemLoader.get_output_value"}, {"text": "ItemLoader.load_item()", "href": "topics/loaders.html#scrapy.loader.ItemLoader.load_item"}, {"text": "issue 3804", "href": "https://github.com/scrapy/scrapy/issues/3804"}, {"text": "issue 3819", "href": "https://github.com/scrapy/scrapy/issues/3819"}, {"text": "ImagesPipeline", "href": "topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline"}, {"text": "AWS_ENDPOINT_URL", "href": "topics/settings.html#std-setting-AWS_ENDPOINT_URL"}, {"text": "AWS_REGION_NAME", "href": "topics/settings.html#std-setting-AWS_REGION_NAME"}, {"text": "AWS_USE_SSL", "href": "topics/settings.html#std-setting-AWS_USE_SSL"}, {"text": "AWS_VERIFY", "href": "topics/settings.html#std-setting-AWS_VERIFY"}, {"text": "issue 3625", "href": "https://github.com/scrapy/scrapy/issues/3625"}, {"text": "issue 3813", "href": "https://github.com/scrapy/scrapy/issues/3813"}, {"text": "issue 3790", "href": "https://github.com/scrapy/scrapy/issues/3790"}, {"text": "issue 3777", "href": "https://github.com/scrapy/scrapy/issues/3777"}, {"text": "issue 3794", "href": "https://github.com/scrapy/scrapy/issues/3794"}, {"text": "Selecting dynamically-loaded content", "href": "topics/dynamic-content.html#topics-dynamic-content"}, {"text": "issue 3703", "href": "https://github.com/scrapy/scrapy/issues/3703"}, {"text": "Broad Crawls", "href": "topics/broad-crawls.html#topics-broad-crawls"}, {"text": "issue 1264", "href": "https://github.com/scrapy/scrapy/issues/1264"}, {"text": "issue 3866", "href": "https://github.com/scrapy/scrapy/issues/3866"}, {"text": "Rule", "href": "topics/spiders.html#scrapy.spiders.Rule"}, {"text": "CrawlSpider", "href": "topics/spiders.html#scrapy.spiders.CrawlSpider"}, {"text": "issue 3711", "href": "https://github.com/scrapy/scrapy/issues/3711"}, {"text": "issue 3712", "href": "https://github.com/scrapy/scrapy/issues/3712"}, {"text": "Writing your own storage backend", "href": "topics/downloader-middleware.html#httpcache-storage-custom"}, {"text": "HttpCacheMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware"}, {"text": "issue 3683", "href": "https://github.com/scrapy/scrapy/issues/3683"}, {"text": "issue 3692", "href": "https://github.com/scrapy/scrapy/issues/3692"}, {"text": "FAQ", "href": "faq.html#faq"}, {"text": "How to split an item into multiple items in an item pipeline?", "href": "faq.html#faq-split-item"}, {"text": "issue 2240", "href": "https://github.com/scrapy/scrapy/issues/2240"}, {"text": "issue 3672", "href": "https://github.com/scrapy/scrapy/issues/3672"}, {"text": "FAQ entry about crawl order", "href": "faq.html#faq-bfo-dfo"}, {"text": "issue 1739", "href": "https://github.com/scrapy/scrapy/issues/1739"}, {"text": "issue 3621", "href": "https://github.com/scrapy/scrapy/issues/3621"}, {"text": "LOGSTATS_INTERVAL", "href": "topics/settings.html#std-setting-LOGSTATS_INTERVAL"}, {"text": "issue 3730", "href": "https://github.com/scrapy/scrapy/issues/3730"}, {"text": "FilesPipeline.file_path", "href": "topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline.file_path"}, {"text": "ImagesPipeline.file_path", "href": "topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline.file_path"}, {"text": "issue 2253", "href": "https://github.com/scrapy/scrapy/issues/2253"}, {"text": "issue 3609", "href": "https://github.com/scrapy/scrapy/issues/3609"}, {"text": "Crawler.stop()", "href": "topics/api.html#scrapy.crawler.Crawler.stop"}, {"text": "issue 3842", "href": "https://github.com/scrapy/scrapy/issues/3842"}, {"text": "issue 1347", "href": "https://github.com/scrapy/scrapy/issues/1347"}, {"text": "issue 1789", "href": "https://github.com/scrapy/scrapy/issues/1789"}, {"text": "issue 2289", "href": "https://github.com/scrapy/scrapy/issues/2289"}, {"text": "issue 3069", "href": "https://github.com/scrapy/scrapy/issues/3069"}, {"text": "issue 3615", "href": "https://github.com/scrapy/scrapy/issues/3615"}, {"text": "issue 3626", "href": "https://github.com/scrapy/scrapy/issues/3626"}, {"text": "issue 3668", "href": "https://github.com/scrapy/scrapy/issues/3668"}, {"text": "issue 3670", "href": "https://github.com/scrapy/scrapy/issues/3670"}, {"text": "issue 3673", "href": "https://github.com/scrapy/scrapy/issues/3673"}, {"text": "issue 3728", "href": "https://github.com/scrapy/scrapy/issues/3728"}, {"text": "issue 3762", "href": "https://github.com/scrapy/scrapy/issues/3762"}, {"text": "issue 3861", "href": "https://github.com/scrapy/scrapy/issues/3861"}, {"text": "issue 3882", "href": "https://github.com/scrapy/scrapy/issues/3882"}, {"text": "issue 3648", "href": "https://github.com/scrapy/scrapy/issues/3648"}, {"text": "issue 3649", "href": "https://github.com/scrapy/scrapy/issues/3649"}, {"text": "issue 3662", "href": "https://github.com/scrapy/scrapy/issues/3662"}, {"text": "issue 3674", "href": "https://github.com/scrapy/scrapy/issues/3674"}, {"text": "issue 3676", "href": "https://github.com/scrapy/scrapy/issues/3676"}, {"text": "issue 3694", "href": "https://github.com/scrapy/scrapy/issues/3694"}, {"text": "issue 3724", "href": "https://github.com/scrapy/scrapy/issues/3724"}, {"text": "issue 3764", "href": "https://github.com/scrapy/scrapy/issues/3764"}, {"text": "issue 3767", "href": "https://github.com/scrapy/scrapy/issues/3767"}, {"text": "issue 3791", "href": "https://github.com/scrapy/scrapy/issues/3791"}, {"text": "issue 3797", "href": "https://github.com/scrapy/scrapy/issues/3797"}, {"text": "issue 3806", "href": "https://github.com/scrapy/scrapy/issues/3806"}, {"text": "issue 3812", "href": "https://github.com/scrapy/scrapy/issues/3812"}, {"text": "issue 3578", "href": "https://github.com/scrapy/scrapy/issues/3578"}, {"text": "Crawler.settings", "href": "topics/api.html#scrapy.crawler.Crawler.settings"}, {"text": "ItemLoader", "href": "topics/loaders.html#scrapy.loader.ItemLoader"}, {"text": "Logging", "href": "topics/logging.html#topics-logging"}, {"text": "Selector", "href": "topics/selectors.html#scrapy.selector.Selector"}, {"text": "Selector", "href": "topics/selectors.html#scrapy.selector.Selector"}, {"text": "Selector", "href": "topics/selectors.html#scrapy.selector.Selector"}, {"text": "Selector", "href": "topics/selectors.html#scrapy.selector.Selector"}, {"text": "parsel.csstranslator.GenericTranslator", "href": "https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.GenericTranslator"}, {"text": "parsel.csstranslator.HTMLTranslator", "href": "https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.HTMLTranslator"}, {"text": "parsel.csstranslator.XPathExpr", "href": "https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.XPathExpr"}, {"text": "Selector", "href": "topics/selectors.html#scrapy.selector.Selector"}, {"text": "SelectorList", "href": "topics/selectors.html#scrapy.selector.SelectorList"}, {"text": "Spider", "href": "topics/spiders.html#scrapy.spiders.Spider"}, {"text": "Spider", "href": "topics/spiders.html#scrapy.spiders.Spider"}, {"text": "download_delay", "href": "topics/settings.html#spider-download-delay-attribute"}, {"text": "SpiderLoader", "href": "topics/api.html#scrapy.spiderloader.SpiderLoader"}, {"text": "scrapy.extensions.telnet", "href": "topics/extensions.html#module-scrapy.extensions.telnet"}, {"text": "issue 3578", "href": "https://github.com/scrapy/scrapy/issues/3578"}, {"text": "SPIDER_LOADER_CLASS", "href": "topics/settings.html#std-setting-SPIDER_LOADER_CLASS"}, {"text": "SCHEDULER_PRIORITY_QUEUE", "href": "topics/settings.html#std-setting-SCHEDULER_PRIORITY_QUEUE"}, {"text": "Rule", "href": "topics/spiders.html#scrapy.spiders.Rule"}, {"text": "w3lib.http", "href": "https://w3lib.readthedocs.io/en/latest/w3lib.html#module-w3lib.http"}, {"text": "w3lib.html", "href": "https://w3lib.readthedocs.io/en/latest/w3lib.html#module-w3lib.html"}, {"text": "urllib3", "href": "https://urllib3.readthedocs.io/en/latest/index.html"}, {"text": "ChainMap", "href": "https://docs.python.org/3/library/collections.html#collections.ChainMap"}, {"text": "issue 3878", "href": "https://github.com/scrapy/scrapy/issues/3878"}, {"text": "tox", "href": "https://pypi.org/project/tox/"}, {"text": "this and other ways to run\ntests", "href": "contributing.html#running-tests"}, {"text": "issue 3707", "href": "https://github.com/scrapy/scrapy/issues/3707"}, {"text": "issue 3806", "href": "https://github.com/scrapy/scrapy/issues/3806"}, {"text": "issue 3810", "href": "https://github.com/scrapy/scrapy/issues/3810"}, {"text": "issue 3860", "href": "https://github.com/scrapy/scrapy/issues/3860"}, {"text": "documentation policies", "href": "contributing.html#documentation-policies"}, {"text": "docstrings", "href": "https://docs.python.org/3/glossary.html#term-docstring"}, {"text": "issue 3701", "href": "https://github.com/scrapy/scrapy/issues/3701"}, {"text": "PEP 257", "href": "https://www.python.org/dev/peps/pep-0257/"}, {"text": "issue 3748", "href": "https://github.com/scrapy/scrapy/issues/3748"}, {"text": "issue 3629", "href": "https://github.com/scrapy/scrapy/issues/3629"}, {"text": "issue 3643", "href": "https://github.com/scrapy/scrapy/issues/3643"}, {"text": "issue 3684", "href": "https://github.com/scrapy/scrapy/issues/3684"}, {"text": "issue 3698", "href": "https://github.com/scrapy/scrapy/issues/3698"}, {"text": "issue 3734", "href": "https://github.com/scrapy/scrapy/issues/3734"}, {"text": "issue 3735", "href": "https://github.com/scrapy/scrapy/issues/3735"}, {"text": "issue 3736", "href": "https://github.com/scrapy/scrapy/issues/3736"}, {"text": "issue 3737", "href": "https://github.com/scrapy/scrapy/issues/3737"}, {"text": "issue 3809", "href": "https://github.com/scrapy/scrapy/issues/3809"}, {"text": "issue 3821", "href": "https://github.com/scrapy/scrapy/issues/3821"}, {"text": "issue 3825", "href": "https://github.com/scrapy/scrapy/issues/3825"}, {"text": "issue 3827", "href": "https://github.com/scrapy/scrapy/issues/3827"}, {"text": "issue 3833", "href": "https://github.com/scrapy/scrapy/issues/3833"}, {"text": "issue 3857", "href": "https://github.com/scrapy/scrapy/issues/3857"}, {"text": "issue 3877", "href": "https://github.com/scrapy/scrapy/issues/3877"}, {"text": "item_error", "href": "topics/signals.html#std-signal-item_error"}, {"text": "request_reached_downloader", "href": "topics/signals.html#std-signal-request_reached_downloader"}, {"text": "parsel", "href": "https://github.com/scrapy/parsel"}, {"text": "extract() and extract_first()", "href": "topics/selectors.html#old-extraction-api"}, {"text": "Selecting element attributes", "href": "topics/selectors.html#selecting-attributes"}, {"text": "parsel changelog", "href": "https://parsel.readthedocs.io/en/latest/history.html"}, {"text": "Telnet Console", "href": "topics/telnetconsole.html#topics-telnetconsole"}, {"text": "issue 1605", "href": "https://github.com/scrapy/scrapy/issues/1605"}, {"text": "issue 3348", "href": "https://github.com/scrapy/scrapy/issues/3348"}, {"text": "issue 2956", "href": "https://github.com/scrapy/scrapy/issues/2956"}, {"text": "item_error", "href": "topics/signals.html#std-signal-item_error"}, {"text": "issue 3256", "href": "https://github.com/scrapy/scrapy/issues/3256"}, {"text": "request_reached_downloader", "href": "topics/signals.html#std-signal-request_reached_downloader"}, {"text": "issue 3393", "href": "https://github.com/scrapy/scrapy/issues/3393"}, {"text": "sitemap_filter()", "href": "topics/spiders.html#scrapy.spiders.SitemapSpider.sitemap_filter"}, {"text": "issue 3512", "href": "https://github.com/scrapy/scrapy/issues/3512"}, {"text": "issue 3394", "href": "https://github.com/scrapy/scrapy/issues/3394"}, {"text": "AWS_ENDPOINT_URL", "href": "topics/settings.html#std-setting-AWS_ENDPOINT_URL"}, {"text": "AWS_USE_SSL", "href": "topics/settings.html#std-setting-AWS_USE_SSL"}, {"text": "AWS_VERIFY", "href": "topics/settings.html#std-setting-AWS_VERIFY"}, {"text": "AWS_REGION_NAME", "href": "topics/settings.html#std-setting-AWS_REGION_NAME"}, {"text": "issue 2609", "href": "https://github.com/scrapy/scrapy/issues/2609"}, {"text": "issue 3548", "href": "https://github.com/scrapy/scrapy/issues/3548"}, {"text": "FILES_STORE_GCS_ACL", "href": "topics/media-pipeline.html#std-setting-FILES_STORE_GCS_ACL"}, {"text": "IMAGES_STORE_GCS_ACL", "href": "topics/media-pipeline.html#std-setting-IMAGES_STORE_GCS_ACL"}, {"text": "issue 3199", "href": "https://github.com/scrapy/scrapy/issues/3199"}, {"text": "issue 3377", "href": "https://github.com/scrapy/scrapy/issues/3377"}, {"text": "issue 3381", "href": "https://github.com/scrapy/scrapy/issues/3381"}, {"text": "issue 3383", "href": "https://github.com/scrapy/scrapy/issues/3383"}, {"text": "issue 3371", "href": "https://github.com/scrapy/scrapy/issues/3371"}, {"text": "issue 3100", "href": "https://github.com/scrapy/scrapy/issues/3100"}, {"text": "issue 3115", "href": "https://github.com/scrapy/scrapy/issues/3115"}, {"text": "issue 3113", "href": "https://github.com/scrapy/scrapy/issues/3113"}, {"text": "issue 3131", "href": "https://github.com/scrapy/scrapy/issues/3131"}, {"text": "issue 3226", "href": "https://github.com/scrapy/scrapy/issues/3226"}, {"text": "issue 3152", "href": "https://github.com/scrapy/scrapy/issues/3152"}, {"text": "issue 3165", "href": "https://github.com/scrapy/scrapy/issues/3165"}, {"text": "issue 3358", "href": "https://github.com/scrapy/scrapy/issues/3358"}, {"text": "issue 3496", "href": "https://github.com/scrapy/scrapy/issues/3496"}, {"text": "issue 3588", "href": "https://github.com/scrapy/scrapy/issues/3588"}, {"text": "issue 3039", "href": "https://github.com/scrapy/scrapy/issues/3039"}, {"text": "issue 3082", "href": "https://github.com/scrapy/scrapy/issues/3082"}, {"text": "issue 3342", "href": "https://github.com/scrapy/scrapy/issues/3342"}, {"text": "issue 3153", "href": "https://github.com/scrapy/scrapy/issues/3153"}, {"text": "issue 3247", "href": "https://github.com/scrapy/scrapy/issues/3247"}, {"text": "Selectors", "href": "topics/selectors.html#topics-selectors"}, {"text": "Selecting element attributes", "href": "topics/selectors.html#selecting-attributes"}, {"text": "Extensions to CSS Selectors", "href": "topics/selectors.html#topics-selectors-css-extensions"}, {"text": "issue 3390", "href": "https://github.com/scrapy/scrapy/issues/3390"}, {"text": "Using your browser’s Developer Tools for scraping", "href": "topics/developer-tools.html#topics-developer-tools"}, {"text": "issue 3400", "href": "https://github.com/scrapy/scrapy/issues/3400"}, {"text": "issue 3518", "href": "https://github.com/scrapy/scrapy/issues/3518"}, {"text": "issue 3517", "href": "https://github.com/scrapy/scrapy/issues/3517"}, {"text": "issue 3367", "href": "https://github.com/scrapy/scrapy/issues/3367"}, {"text": "issue 3468", "href": "https://github.com/scrapy/scrapy/issues/3468"}, {"text": "RETRY_HTTP_CODES", "href": "topics/downloader-middleware.html#std-setting-RETRY_HTTP_CODES"}, {"text": "issue 3335", "href": "https://github.com/scrapy/scrapy/issues/3335"}, {"text": "issue 3245", "href": "https://github.com/scrapy/scrapy/issues/3245"}, {"text": "issue 3347", "href": "https://github.com/scrapy/scrapy/issues/3347"}, {"text": "issue 3350", "href": "https://github.com/scrapy/scrapy/issues/3350"}, {"text": "issue 3445", "href": "https://github.com/scrapy/scrapy/issues/3445"}, {"text": "issue 3544", "href": "https://github.com/scrapy/scrapy/issues/3544"}, {"text": "issue 3605", "href": "https://github.com/scrapy/scrapy/issues/3605"}, {"text": "issue 3318", "href": "https://github.com/scrapy/scrapy/issues/3318"}, {"text": "issue 3327", "href": "https://github.com/scrapy/scrapy/issues/3327"}, {"text": "issue 3327", "href": "https://github.com/scrapy/scrapy/issues/3327"}, {"text": "issue 3359", "href": "https://github.com/scrapy/scrapy/issues/3359"}, {"text": "issue 3315", "href": "https://github.com/scrapy/scrapy/issues/3315"}, {"text": "issue 3326", "href": "https://github.com/scrapy/scrapy/issues/3326"}, {"text": "issue 3150", "href": "https://github.com/scrapy/scrapy/issues/3150"}, {"text": "issue 3547", "href": "https://github.com/scrapy/scrapy/issues/3547"}, {"text": "issue 3526", "href": "https://github.com/scrapy/scrapy/issues/3526"}, {"text": "issue 3538", "href": "https://github.com/scrapy/scrapy/issues/3538"}, {"text": "issue 3308", "href": "https://github.com/scrapy/scrapy/issues/3308"}, {"text": "issue 3311", "href": "https://github.com/scrapy/scrapy/issues/3311"}, {"text": "issue 3309", "href": "https://github.com/scrapy/scrapy/issues/3309"}, {"text": "issue 3305", "href": "https://github.com/scrapy/scrapy/issues/3305"}, {"text": "issue 3210", "href": "https://github.com/scrapy/scrapy/issues/3210"}, {"text": "issue 3299", "href": "https://github.com/scrapy/scrapy/issues/3299"}, {"text": "issue 3231", "href": "https://github.com/scrapy/scrapy/issues/3231"}, {"text": "issue 3495", "href": "https://github.com/scrapy/scrapy/issues/3495"}, {"text": "issue 3405", "href": "https://github.com/scrapy/scrapy/issues/3405"}, {"text": "issue 3304", "href": "https://github.com/scrapy/scrapy/issues/3304"}, {"text": "issue 3519", "href": "https://github.com/scrapy/scrapy/issues/3519"}, {"text": "issue 3476", "href": "https://github.com/scrapy/scrapy/issues/3476"}, {"text": "http://localhost:6023", "href": "http://localhost:6023"}, {"text": "TELNETCONSOLE_PORT", "href": "topics/telnetconsole.html#std-setting-TELNETCONSOLE_PORT"}, {"text": "telnet console", "href": "topics/telnetconsole.html#topics-telnetconsole"}, {"text": "issue 3281", "href": "https://github.com/scrapy/scrapy/issues/3281"}, {"text": "issue 3166", "href": "https://github.com/scrapy/scrapy/issues/3166"}, {"text": "issue 3096", "href": "https://github.com/scrapy/scrapy/issues/3096"}, {"text": "issue 3092", "href": "https://github.com/scrapy/scrapy/issues/3092"}, {"text": "issue 3263", "href": "https://github.com/scrapy/scrapy/issues/3263"}, {"text": "issue 3058", "href": "https://github.com/scrapy/scrapy/issues/3058"}, {"text": "issue 3059", "href": "https://github.com/scrapy/scrapy/issues/3059"}, {"text": "issue 3089", "href": "https://github.com/scrapy/scrapy/issues/3089"}, {"text": "issue 3123", "href": "https://github.com/scrapy/scrapy/issues/3123"}, {"text": "issue 3127", "href": "https://github.com/scrapy/scrapy/issues/3127"}, {"text": "issue 3189", "href": "https://github.com/scrapy/scrapy/issues/3189"}, {"text": "issue 3224", "href": "https://github.com/scrapy/scrapy/issues/3224"}, {"text": "issue 3280", "href": "https://github.com/scrapy/scrapy/issues/3280"}, {"text": "issue 3279", "href": "https://github.com/scrapy/scrapy/issues/3279"}, {"text": "issue 3201", "href": "https://github.com/scrapy/scrapy/issues/3201"}, {"text": "issue 3260", "href": "https://github.com/scrapy/scrapy/issues/3260"}, {"text": "issue 3284", "href": "https://github.com/scrapy/scrapy/issues/3284"}, {"text": "issue 3298", "href": "https://github.com/scrapy/scrapy/issues/3298"}, {"text": "issue 3294", "href": "https://github.com/scrapy/scrapy/issues/3294"}, {"text": "issue 2983", "href": "https://github.com/scrapy/scrapy/issues/2983"}, {"text": "USER_AGENT", "href": "topics/settings.html#std-setting-USER_AGENT"}, {"text": "issue 1343", "href": "https://github.com/scrapy/scrapy/issues/1343"}, {"text": "issue 2851", "href": "https://github.com/scrapy/scrapy/issues/2851"}, {"text": "issue 2785", "href": "https://github.com/scrapy/scrapy/issues/2785"}, {"text": "issue 2654", "href": "https://github.com/scrapy/scrapy/issues/2654"}, {"text": "issue 2923", "href": "https://github.com/scrapy/scrapy/issues/2923"}, {"text": "issue 2883", "href": "https://github.com/scrapy/scrapy/issues/2883"}, {"text": "issue 2812", "href": "https://github.com/scrapy/scrapy/issues/2812"}, {"text": "issue 2844", "href": "https://github.com/scrapy/scrapy/issues/2844"}, {"text": "issue 2851", "href": "https://github.com/scrapy/scrapy/issues/2851"}, {"text": "issue 2857", "href": "https://github.com/scrapy/scrapy/issues/2857"}, {"text": "issue 2743", "href": "https://github.com/scrapy/scrapy/issues/2743"}, {"text": "issue 2755", "href": "https://github.com/scrapy/scrapy/issues/2755"}, {"text": "issue 2831", "href": "https://github.com/scrapy/scrapy/issues/2831"}, {"text": "issue 2921", "href": "https://github.com/scrapy/scrapy/issues/2921"}, {"text": "DOWNLOAD_WARNSIZE", "href": "topics/settings.html#std-setting-DOWNLOAD_WARNSIZE"}, {"text": "DOWNLOAD_MAXSIZE", "href": "topics/settings.html#std-setting-DOWNLOAD_MAXSIZE"}, {"text": "issue 2927", "href": "https://github.com/scrapy/scrapy/issues/2927"}, {"text": "issue 2250", "href": "https://github.com/scrapy/scrapy/issues/2250"}, {"text": "issue 1343", "href": "https://github.com/scrapy/scrapy/issues/1343"}, {"text": "issue 2983", "href": "https://github.com/scrapy/scrapy/issues/2983"}, {"text": "USER_AGENT", "href": "topics/settings.html#std-setting-USER_AGENT"}, {"text": "issue 2793", "href": "https://github.com/scrapy/scrapy/issues/2793"}, {"text": "issue 2935", "href": "https://github.com/scrapy/scrapy/issues/2935"}, {"text": "issue 2990", "href": "https://github.com/scrapy/scrapy/issues/2990"}, {"text": "issue 3050", "href": "https://github.com/scrapy/scrapy/issues/3050"}, {"text": "issue 2213", "href": "https://github.com/scrapy/scrapy/issues/2213"}, {"text": "issue 3048", "href": "https://github.com/scrapy/scrapy/issues/3048"}, {"text": "issue 2811", "href": "https://github.com/scrapy/scrapy/issues/2811"}, {"text": "issue 2848", "href": "https://github.com/scrapy/scrapy/issues/2848"}, {"text": "issue 2766", "href": "https://github.com/scrapy/scrapy/issues/2766"}, {"text": "issue 2849", "href": "https://github.com/scrapy/scrapy/issues/2849"}, {"text": "issue 2862", "href": "https://github.com/scrapy/scrapy/issues/2862"}, {"text": "issue 2876", "href": "https://github.com/scrapy/scrapy/issues/2876"}, {"text": "issue 2853", "href": "https://github.com/scrapy/scrapy/issues/2853"}, {"text": "issue 2756", "href": "https://github.com/scrapy/scrapy/issues/2756"}, {"text": "issue 2762", "href": "https://github.com/scrapy/scrapy/issues/2762"}, {"text": "https://", "href": "https://"}, {"text": "issue 2978", "href": "https://github.com/scrapy/scrapy/issues/2978"}, {"text": "issue 2982", "href": "https://github.com/scrapy/scrapy/issues/2982"}, {"text": "issue 2958", "href": "https://github.com/scrapy/scrapy/issues/2958"}, {"text": "issue 2759", "href": "https://github.com/scrapy/scrapy/issues/2759"}, {"text": "issue 2781", "href": "https://github.com/scrapy/scrapy/issues/2781"}, {"text": "issue 2828", "href": "https://github.com/scrapy/scrapy/issues/2828"}, {"text": "issue 2837", "href": "https://github.com/scrapy/scrapy/issues/2837"}, {"text": "issue 2884", "href": "https://github.com/scrapy/scrapy/issues/2884"}, {"text": "issue 2924", "href": "https://github.com/scrapy/scrapy/issues/2924"}, {"text": "issue 2826", "href": "https://github.com/scrapy/scrapy/issues/2826"}, {"text": "issue 2791", "href": "https://github.com/scrapy/scrapy/issues/2791"}, {"text": "issue 2764", "href": "https://github.com/scrapy/scrapy/issues/2764"}, {"text": "issue 2763", "href": "https://github.com/scrapy/scrapy/issues/2763"}, {"text": "issue 2866", "href": "https://github.com/scrapy/scrapy/issues/2866"}, {"text": "issue 2922", "href": "https://github.com/scrapy/scrapy/issues/2922"}, {"text": "issue 2374", "href": "https://github.com/scrapy/scrapy/issues/2374"}, {"text": "issue 2999", "href": "https://github.com/scrapy/scrapy/issues/2999"}, {"text": "issue 2964", "href": "https://github.com/scrapy/scrapy/issues/2964"}, {"text": "issue 2976", "href": "https://github.com/scrapy/scrapy/issues/2976"}, {"text": "issue 2989", "href": "https://github.com/scrapy/scrapy/issues/2989"}, {"text": "issue 3019", "href": "https://github.com/scrapy/scrapy/issues/3019"}, {"text": "FTP_USER", "href": "topics/settings.html#std-setting-FTP_USER"}, {"text": "FTP_PASSWORD", "href": "topics/settings.html#std-setting-FTP_PASSWORD"}, {"text": "response.follow", "href": "topics/request-response.html#scrapy.http.TextResponse.follow"}, {"text": "REFERRER_POLICY", "href": "topics/spider-middleware.html#std-setting-REFERRER_POLICY"}, {"text": "FEED_EXPORT_INDENT", "href": "topics/feed-exports.html#std-setting-FEED_EXPORT_INDENT"}, {"text": "scrapy.linkextractors.LinkExtractor", "href": "topics/link-extractors.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"}, {"text": "issue 2537", "href": "https://github.com/scrapy/scrapy/issues/2537"}, {"text": "issue 1941", "href": "https://github.com/scrapy/scrapy/issues/1941"}, {"text": "issue 1982", "href": "https://github.com/scrapy/scrapy/issues/1982"}, {"text": "issue 2539", "href": "https://github.com/scrapy/scrapy/issues/2539"}, {"text": "issue 2187", "href": "https://github.com/scrapy/scrapy/issues/2187"}, {"text": "issue 1829", "href": "https://github.com/scrapy/scrapy/issues/1829"}, {"text": "issue 1728", "href": "https://github.com/scrapy/scrapy/issues/1728"}, {"text": "issue 1495", "href": "https://github.com/scrapy/scrapy/issues/1495"}, {"text": "proxy", "href": "topics/downloader-middleware.html#std-reqmeta-proxy"}, {"text": "issue 2526", "href": "https://github.com/scrapy/scrapy/issues/2526"}, {"text": "brotli-compressed", "href": "https://www.ietf.org/rfc/rfc7932.txt"}, {"text": "brotlipy", "href": "https://github.com/python-hyper/brotlipy/"}, {"text": "issue 2535", "href": "https://github.com/scrapy/scrapy/issues/2535"}, {"text": "response.follow", "href": "intro/tutorial.html#response-follow-example"}, {"text": "issue 1940", "href": "https://github.com/scrapy/scrapy/issues/1940"}, {"text": "Request", "href": "topics/request-response.html#scrapy.http.Request"}, {"text": "issue 2047", "href": "https://github.com/scrapy/scrapy/issues/2047"}, {"text": "issue 2342", "href": "https://github.com/scrapy/scrapy/issues/2342"}, {"text": "RetryMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.retry.RetryMiddleware"}, {"text": "issue 2543", "href": "https://github.com/scrapy/scrapy/issues/2543"}, {"text": "HttpErrorMiddleware", "href": "topics/spider-middleware.html#scrapy.spidermiddlewares.httperror.HttpErrorMiddleware"}, {"text": "issue 2566", "href": "https://github.com/scrapy/scrapy/issues/2566"}, {"text": "Referrer policy", "href": "topics/spider-middleware.html#std-setting-REFERRER_POLICY"}, {"text": "RefererMiddleware", "href": "topics/spider-middleware.html#scrapy.spidermiddlewares.referer.RefererMiddleware"}, {"text": "issue 2306", "href": "https://github.com/scrapy/scrapy/issues/2306"}, {"text": "issue 2334", "href": "https://github.com/scrapy/scrapy/issues/2334"}, {"text": "issue 2156", "href": "https://github.com/scrapy/scrapy/issues/2156"}, {"text": "issue 2611", "href": "https://github.com/scrapy/scrapy/issues/2611"}, {"text": "issue 2604", "href": "https://github.com/scrapy/scrapy/issues/2604"}, {"text": "issue 2181", "href": "https://github.com/scrapy/scrapy/issues/2181"}, {"text": "issue 2646", "href": "https://github.com/scrapy/scrapy/issues/2646"}, {"text": "Media downloads", "href": "topics/media-pipeline.html#topics-media-pipeline"}, {"text": "FilesPipeline", "href": "topics/media-pipeline.html#scrapy.pipelines.files.FilesPipeline"}, {"text": "ImagesPipeline", "href": "topics/media-pipeline.html#scrapy.pipelines.images.ImagesPipeline"}, {"text": "MEDIA_ALLOW_REDIRECTS", "href": "topics/media-pipeline.html#std-setting-MEDIA_ALLOW_REDIRECTS"}, {"text": "issue 2616", "href": "https://github.com/scrapy/scrapy/issues/2616"}, {"text": "issue 2004", "href": "https://github.com/scrapy/scrapy/issues/2004"}, {"text": "DOWNLOAD_FAIL_ON_DATALOSS", "href": "topics/settings.html#std-setting-DOWNLOAD_FAIL_ON_DATALOSS"}, {"text": "issue 2590", "href": "https://github.com/scrapy/scrapy/issues/2590"}, {"text": "issue 2586", "href": "https://github.com/scrapy/scrapy/issues/2586"}, {"text": "FEED_EXPORT_INDENT", "href": "topics/feed-exports.html#std-setting-FEED_EXPORT_INDENT"}, {"text": "issue 2456", "href": "https://github.com/scrapy/scrapy/issues/2456"}, {"text": "issue 1327", "href": "https://github.com/scrapy/scrapy/issues/1327"}, {"text": "issue 667", "href": "https://github.com/scrapy/scrapy/issues/667"}, {"text": "max_retry_times", "href": "topics/request-response.html#std-reqmeta-max_retry_times"}, {"text": "issue 2642", "href": "https://github.com/scrapy/scrapy/issues/2642"}, {"text": "issue 2740", "href": "https://github.com/scrapy/scrapy/issues/2740"}, {"text": "issue 2547", "href": "https://github.com/scrapy/scrapy/issues/2547"}, {"text": "issue 1614", "href": "https://github.com/scrapy/scrapy/issues/1614"}, {"text": "issue 2548", "href": "https://github.com/scrapy/scrapy/issues/2548"}, {"text": "issue 2495", "href": "https://github.com/scrapy/scrapy/issues/2495"}, {"text": "issue 2491", "href": "https://github.com/scrapy/scrapy/issues/2491"}, {"text": "issue 2599", "href": "https://github.com/scrapy/scrapy/issues/2599"}, {"text": "issue 2393", "href": "https://github.com/scrapy/scrapy/issues/2393"}, {"text": "issue 2145", "href": "https://github.com/scrapy/scrapy/issues/2145"}, {"text": "HttpCompressionMiddleware", "href": "topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware"}, {"text": "issue 2391", "href": "https://github.com/scrapy/scrapy/issues/2391"}, {"text": "issue 2581", "href": "https://github.com/scrapy/scrapy/issues/2581"}, {"text": "issue 1612", "href": "https://github.com/scrapy/scrapy/issues/1612"}, {"text": "issue 2661", "href": "https://github.com/scrapy/scrapy/issues/2661"}, {"text": "issue 2695", "href": "https://github.com/scrapy/scrapy/issues/2695"}, {"text": "issue 2677", "href": "https://github.com/scrapy/scrapy/issues/2677"}, {"text": "DOWNLOAD_MAXSIZE", "href": "topics/settings.html#std-setting-DOWNLOAD_MAXSIZE"}, {"text": "issue 1616", "href": "https://github.com/scrapy/scrapy/issues/1616"}, {"text": "issue 2675", "href": "https://github.com/scrapy/scrapy/issues/2675"}, {"text": "issue 2570", "href": "https://github.com/scrapy/scrapy/issues/2570"}, {"text": "issue 2569", "href": "https://github.com/scrapy/scrapy/issues/2569"}, {"text": "issue 2710", "href": "https://github.com/scrapy/scrapy/issues/2710"}, {"text": "issue 2562", "href": "https://github.com/scrapy/scrapy/issues/2562"}, {"text": "issue 2567", "href": "https://github.com/scrapy/scrapy/issues/2567"}, {"text": "issue 2557", "href": "https://github.com/scrapy/scrapy/issues/2557"}, {"text": "issue 2159", "href": "https://github.com/scrapy/scrapy/issues/2159"}, {"text": "issue 2750", "href": "https://github.com/scrapy/scrapy/issues/2750"}, {"text": "issue 2577", "href": "https://github.com/scrapy/scrapy/issues/2577"}, {"text": "issue 2560", "href": "https://github.com/scrapy/scrapy/issues/2560"}, {"text": "issue 2595", "href": "https://github.com/scrapy/scrapy/issues/2595"}, {"text": "issue 2617", "href": "https://github.com/scrapy/scrapy/issues/2617"}, {"text": "issue 2644", "href": "https://github.com/scrapy/scrapy/issues/2644"}, {"text": "issue 2720", "href": "https://github.com/scrapy/scrapy/issues/2720"}, {"text": "issue 2576", "href": "https://github.com/scrapy/scrapy/issues/2576"}, {"text": "issue 2564", "href": "https://github.com/scrapy/scrapy/issues/2564"}, {"text": "issue 2553", "href": "https://github.com/scrapy/scrapy/issues/2553"}, {"text": "issue 2572", "href": "https://github.com/scrapy/scrapy/issues/2572"}, {"text": "issue 2596", "href": "https://github.com/scrapy/scrapy/issues/2596"}, {"text": "ftp_user", "href": "topics/settings.html#std-reqmeta-ftp_user"}, {"text": "ftp_password", "href": "topics/settings.html#std-reqmeta-ftp_password"}, {"text": "issue 2587", "href": "https://github.com/scrapy/scrapy/issues/2587"}, {"text": "issue 2636", "href": "https://github.com/scrapy/scrapy/issues/2636"}, {"text": "issue 2477", "href": "https://github.com/scrapy/scrapy/issues/2477"}, {"text": "issue 2475", "href": "https://github.com/scrapy/scrapy/issues/2475"}, {"text": "issue 2690", "href": "https://github.com/scrapy/scrapy/issues/2690"}, {"text": "issue 2705", "href": "https://github.com/scrapy/scrapy/issues/2705"}, {"text": "SelectorList", "href": "topics/selectors.html#scrapy.selector.SelectorList"}, {"text": "issue 2683", "href": "https://github.com/scrapy/scrapy/issues/2683"}, {"text": "DUPEFILTER_CLASS", "href": "topics/settings.html#std-setting-DUPEFILTER_CLASS"}, {"text": "issue 2714", "href": "https://github.com/scrapy/scrapy/issues/2714"}, {"text": "issue 2668", "href": "https://github.com/scrapy/scrapy/issues/2668"}, {"text": "issue 2729", "href": "https://github.com/scrapy/scrapy/issues/2729"}, {"text": "issue 2670", "href": "https://github.com/scrapy/scrapy/issues/2670"}, {"text": "SPIDER_MODULES", "href": "topics/settings.html#std-setting-SPIDER_MODULES"}, {"text": "SPIDER_LOADER_WARN_ONLY", "href": "topics/settings.html#std-setting-SPIDER_LOADER_WARN_ONLY"}, {"text": "issue 2510", "href": "https://github.com/scrapy/scrapy/issues/2510"}, {"text": "issue 2551", "href": "https://github.com/scrapy/scrapy/issues/2551"}, {"text": "issue 2558", "href": "https://github.com/scrapy/scrapy/issues/2558"}, {"text": "issue 2519", "href": "https://github.com/scrapy/scrapy/issues/2519"}, {"text": "XPath variables", "href": "topics/selectors.html#topics-selectors-xpath-variables"}, {"text": "issue 2457", "href": "https://github.com/scrapy/scrapy/issues/2457"}, {"text": "issue 2485", "href": "https://github.com/scrapy/scrapy/issues/2485"}, {"text": "issue 2496", "href": "https://github.com/scrapy/scrapy/issues/2496"}, {"text": "view", "href": "topics/commands.html#std-command-view"}, {"text": "issue 2503", "href": "https://github.com/scrapy/scrapy/issues/2503"}, {"text": "issue 2460", "href": "https://github.com/scrapy/scrapy/issues/2460"}, {"text": "issue 2466", "href": "https://github.com/scrapy/scrapy/issues/2466"}, {"text": "issue 2496", "href": "https://github.com/scrapy/scrapy/issues/2496"}, {"text": "issue 2528", "href": "https://github.com/scrapy/scrapy/issues/2528"}, {"text": "issue 2511", "href": "https://github.com/scrapy/scrapy/issues/2511"}, {"text": "issue 2420", "href": "https://github.com/scrapy/scrapy/issues/2420"}, {"text": "issue 2469", "href": "https://github.com/scrapy/scrapy/issues/2469"}, {"text": "issue 2483", "href": "https://github.com/scrapy/scrapy/issues/2483"}, {"text": "issue 2497", "href": "https://github.com/scrapy/scrapy/issues/2497"}, {"text": "issue 2507", "href": "https://github.com/scrapy/scrapy/issues/2507"}, {"text": "issue 2525", "href": "https://github.com/scrapy/scrapy/issues/2525"}, {"text": "issue 2533", "href": "https://github.com/scrapy/scrapy/issues/2533"}, {"text": "issue 1704", "href": "https://github.com/scrapy/scrapy/issues/1704"}, {"text": "issue 2512", "href": "https://github.com/scrapy/scrapy/issues/2512"}, {"text": "issue 2534", "href": "https://github.com/scrapy/scrapy/issues/2534"}, {"text": "issue 2531", "href": "https://github.com/scrapy/scrapy/issues/2531"}, {"text": "issue 2542", "href": "https://github.com/scrapy/scrapy/issues/2542"}, {"text": "issue 2538", "href": "https://github.com/scrapy/scrapy/issues/2538"}, {"text": "issue 2544", "href": "https://github.com/scrapy/scrapy/issues/2544"}, {"text": "issue 2272", "href": "https://github.com/scrapy/scrapy/issues/2272"}, {"text": "issue 2290", "href": "https://github.com/scrapy/scrapy/issues/2290"}, {"text": "fetch", "href": "topics/commands.html#std-command-fetch"}, {"text": "shell", "href": "topics/commands.html#std-command-shell"}, {"text": "LOG_SHORT_NAMES", "href": "topics/settings.html#std-setting-LOG_SHORT_NAMES"}, {"text": "issue 2011", "href": "https://github.com/scrapy/scrapy/issues/2011"}, {"text": "issue 396", "href": "https://github.com/scrapy/scrapy/issues/396"}, {"text": "issue 2418", "href": "https://github.com/scrapy/scrapy/issues/2418"}, {"text": "issue 2390", "href": "https://github.com/scrapy/scrapy/issues/2390"}, {"text": "issue 2373", "href": "https://github.com/scrapy/scrapy/issues/2373"}, {"text": "issue 2033", "href": "https://github.com/scrapy/scrapy/issues/2033"}, {"text": "issue 2335", "href": "https://github.com/scrapy/scrapy/issues/2335"}, {"text": "issue 2346", "href": "https://github.com/scrapy/scrapy/issues/2346"}, {"text": "issue 2369", "href": "https://github.com/scrapy/scrapy/issues/2369"}, {"text": "issue 2369", "href": "https://github.com/scrapy/scrapy/issues/2369"}, {"text": "issue 2380", "href": "https://github.com/scrapy/scrapy/issues/2380"}, {"text": "issue 2354", "href": "https://github.com/scrapy/scrapy/issues/2354"}, {"text": "issue 2325", "href": "https://github.com/scrapy/scrapy/issues/2325"}, {"text": "issue 2414", "href": "https://github.com/scrapy/scrapy/issues/2414"}, {"text": "conda-forge", "href": "https://anaconda.org/conda-forge/scrapy"}, {"text": "issue 2387", "href": "https://github.com/scrapy/scrapy/issues/2387"}, {"text": "issue 2264", "href": "https://github.com/scrapy/scrapy/issues/2264"}, {"text": "issue 2335", "href": "https://github.com/scrapy/scrapy/issues/2335"}, {"text": "issue 2404", "href": "https://github.com/scrapy/scrapy/issues/2404"}, {"text": "issue 2386", "href": "https://github.com/scrapy/scrapy/issues/2386"}, {"text": "issue 2314", "href": "https://github.com/scrapy/scrapy/issues/2314"}, {"text": "issue 2321", "href": "https://github.com/scrapy/scrapy/issues/2321"}, {"text": "issue 2302", "href": "https://github.com/scrapy/scrapy/issues/2302"}, {"text": "issue 2330", "href": "https://github.com/scrapy/scrapy/issues/2330"}, {"text": "issue 2329", "href": "https://github.com/scrapy/scrapy/issues/2329"}, {"text": "issue 2327", "href": "https://github.com/scrapy/scrapy/issues/2327"}, {"text": "issue 2299", "href": "https://github.com/scrapy/scrapy/issues/2299"}, {"text": "FEED_EXPORT_ENCODING", "href": "topics/feed-exports.html#std-setting-FEED_EXPORT_ENCODING"}, {"text": "issue 2034", "href": "https://github.com/scrapy/scrapy/issues/2034"}, {"text": "issue 2005", "href": "https://github.com/scrapy/scrapy/issues/2005"}, {"text": "SCHEDULER_DEBUG", "href": "topics/settings.html#std-setting-SCHEDULER_DEBUG"}, {"text": "issue 1610", "href": "https://github.com/scrapy/scrapy/issues/1610"}, {"text": "issue 2058", "href": "https://github.com/scrapy/scrapy/issues/2058"}, {"text": "issue 1503", "href": "https://github.com/scrapy/scrapy/issues/1503"}, {"text": "shell", "href": "topics/commands.html#std-command-shell"}, {"text": "inspect_response", "href": "topics/shell.html#topics-shell-inspect-response"}, {"text": "issue 2248", "href": "https://github.com/scrapy/scrapy/issues/2248"}, {"text": "issue 2088", "href": "https://github.com/scrapy/scrapy/issues/2088"}, {"text": "issue 1581", "href": "https://github.com/scrapy/scrapy/issues/1581"}, {"text": "issue 2153", "href": "https://github.com/scrapy/scrapy/issues/2153"}, {"text": "issue 2169", "href": "https://github.com/scrapy/scrapy/issues/2169"}, {"text": "issue 1606", "href": "https://github.com/scrapy/scrapy/issues/1606"}, {"text": "scrapy parse", "href": "topics/commands.html#std-command-parse"}, {"text": "issue 2225", "href": "https://github.com/scrapy/scrapy/issues/2225"}, {"text": "issue 872", "href": "https://github.com/scrapy/scrapy/issues/872"}, {"text": "issue 2125", "href": "https://github.com/scrapy/scrapy/issues/2125"}, {"text": "w3lib.url", "href": "https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.url.canonicalize_url"}, {"text": "issue 2168", "href": "https://github.com/scrapy/scrapy/issues/2168"}, {"text": "issue 2128", "href": "https://github.com/scrapy/scrapy/issues/2128"}, {"text": "issue 1566", "href": "https://github.com/scrapy/scrapy/issues/1566"}, {"text": "issue 2160", "href": "https://github.com/scrapy/scrapy/issues/2160"}, {"text": "architecture diagram", "href": "topics/architecture.html#topics-architecture"}, {"text": "issue 2165", "href": "https://github.com/scrapy/scrapy/issues/2165"}, {"text": "issue 2197", "href": "https://github.com/scrapy/scrapy/issues/2197"}, {"text": "RANDOMIZE_DOWNLOAD_DELAY", "href": "topics/settings.html#std-setting-RANDOMIZE_DOWNLOAD_DELAY"}, {"text": "issue 2190", "href": "https://github.com/scrapy/scrapy/issues/2190"}, {"text": "issue 2257", "href": "https://github.com/scrapy/scrapy/issues/2257"}, {"text": "issue 2243", "href": "https://github.com/scrapy/scrapy/issues/2243"}, {"text": "issue 2198", "href": "https://github.com/scrapy/scrapy/issues/2198"}, {"text": "Overview", "href": "intro/overview.html#intro-overview"}, {"text": "tutorial", "href": "intro/tutorial.html#intro-tutorial"}, {"text": "http://toscrape.com", "href": "http://toscrape.com"}, {"text": "issue 2236", "href": "https://github.com/scrapy/scrapy/issues/2236"}, {"text": "issue 2249", "href": "https://github.com/scrapy/scrapy/issues/2249"}, {"text": "issue 2252", "href": "https://github.com/scrapy/scrapy/issues/2252"}, {"text": "IMAGES_STORE_S3_ACL", "href": "topics/media-pipeline.html#std-setting-IMAGES_STORE_S3_ACL"}, {"text": "IMAGES_EXPIRES", "href": "topics/media-pipeline.html#std-setting-IMAGES_EXPIRES"}, {"text": "issue 2069", "href": "https://github.com/scrapy/scrapy/issues/2069"}, {"text": "issue 2001", "href": "https://github.com/scrapy/scrapy/issues/2001"}, {"text": "issue 2000", "href": "https://github.com/scrapy/scrapy/issues/2000"}, {"text": "issue 2038", "href": "https://github.com/scrapy/scrapy/issues/2038"}, {"text": "issue 2010", "href": "https://github.com/scrapy/scrapy/issues/2010"}, {"text": "issue 2008", "href": "https://github.com/scrapy/scrapy/issues/2008"}, {"text": "issue 1899", "href": "https://github.com/scrapy/scrapy/issues/1899"}, {"text": "issue 2050", "href": "https://github.com/scrapy/scrapy/issues/2050"}, {"text": "issue 2049", "href": "https://github.com/scrapy/scrapy/issues/2049"}, {"text": "issue 2065", "href": "https://github.com/scrapy/scrapy/issues/2065"}, {"text": "issue 2063", "href": "https://github.com/scrapy/scrapy/issues/2063"}, {"text": "issue 2094", "href": "https://github.com/scrapy/scrapy/issues/2094"}, {"text": "issue 2092", "href": "https://github.com/scrapy/scrapy/issues/2092"}, {"text": "issue 1989", "href": "https://github.com/scrapy/scrapy/issues/1989"}, {"text": "issue 1985", "href": "https://github.com/scrapy/scrapy/issues/1985"}, {"text": "issue 2052", "href": "https://github.com/scrapy/scrapy/issues/2052"}, {"text": "issue 1974", "href": "https://github.com/scrapy/scrapy/issues/1974"}, {"text": "commit 9b3c72c", "href": "https://github.com/scrapy/scrapy/commit/9b3c72c"}, {"text": "issue 1994", "href": "https://github.com/scrapy/scrapy/issues/1994"}, {"text": "commit c2c8036", "href": "https://github.com/scrapy/scrapy/commit/c2c8036"}, {"text": "issue 1995", "href": "https://github.com/scrapy/scrapy/issues/1995"}, {"text": "issue 2015", "href": "https://github.com/scrapy/scrapy/issues/2015"}, {"text": "issue 2054", "href": "https://github.com/scrapy/scrapy/issues/2054"}, {"text": "issue 2120", "href": "https://github.com/scrapy/scrapy/issues/2120"}, {"text": "issue 2048", "href": "https://github.com/scrapy/scrapy/issues/2048"}, {"text": "issue 2060", "href": "https://github.com/scrapy/scrapy/issues/2060"}, {"text": "issue 2026", "href": "https://github.com/scrapy/scrapy/issues/2026"}, {"text": "issue 2095", "href": "https://github.com/scrapy/scrapy/issues/2095"}, {"text": "issue 1467", "href": "https://github.com/scrapy/scrapy/issues/1467"}, {"text": "issue 1382", "href": "https://github.com/scrapy/scrapy/issues/1382"}, {"text": "issue 1137", "href": "https://github.com/scrapy/scrapy/issues/1137"}, {"text": "AUTOTHROTTLE_TARGET_CONCURRENCY", "href": "topics/autothrottle.html#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY"}, {"text": "issue 1324", "href": "https://github.com/scrapy/scrapy/issues/1324"}, {"text": "issue 1730", "href": "https://github.com/scrapy/scrapy/issues/1730"}, {"text": "issue 1358", "href": "https://github.com/scrapy/scrapy/issues/1358"}, {"text": "issue 1473", "href": "https://github.com/scrapy/scrapy/issues/1473"}, {"text": "issue 1471", "href": "https://github.com/scrapy/scrapy/issues/1471"}, {"text": "HTTPCACHE_ALWAYS_STORE", "href": "topics/downloader-middleware.html#std-setting-HTTPCACHE_ALWAYS_STORE"}, {"text": "HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS", "href": "topics/downloader-middleware.html#std-setting-HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS"}, {"text": "issue 1151", "href": "https://github.com/scrapy/scrapy/issues/1151"}, {"text": "parsel", "href": "https://github.com/scrapy/parsel"}, {"text": "issue 1409", "href": "https://github.com/scrapy/scrapy/issues/1409"}, {"text": "DOWNLOADER_CLIENT_TLS_METHOD", "href": "topics/settings.html#std-setting-DOWNLOADER_CLIENT_TLS_METHOD"}, {"text": "issue 1289", "href": "https://github.com/scrapy/scrapy/issues/1289"}, {"text": "RETRY_HTTP_CODES", "href": "topics/downloader-middleware.html#std-setting-RETRY_HTTP_CODES"}, {"text": "issue 1710", "href": "https://github.com/scrapy/scrapy/issues/1710"}, {"text": "issue 1550", "href": "https://github.com/scrapy/scrapy/issues/1550"}, {"text": "http://index.html", "href": "http://index.html"}, {"text": "issue 1724", "href": "https://github.com/scrapy/scrapy/issues/1724"}, {"text": "issue 1735", "href": "https://github.com/scrapy/scrapy/issues/1735"}, {"text": "ROBOTSTXT_OBEY", "href": "topics/settings.html#std-setting-ROBOTSTXT_OBEY"}, {"text": "issue 1080", "href": "https://github.com/scrapy/scrapy/issues/1080"}, {"text": "PythonItemExporter", "href": "topics/exporters.html#scrapy.exporters.PythonItemExporter"}, {"text": "issue 1533", "href": "https://github.com/scrapy/scrapy/issues/1533"}, {"text": "FILES_STORE_S3_ACL", "href": "topics/media-pipeline.html#std-setting-FILES_STORE_S3_ACL"}, {"text": "issue 1947", "href": "https://github.com/scrapy/scrapy/issues/1947"}, {"text": "hard at work to make Scrapy run on Python 3", "href": "https://github.com/scrapy/scrapy/wiki/Python-3-Porting"}, {"text": "Code of Conduct", "href": "https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md"}, {"text": "issue 1681", "href": "https://github.com/scrapy/scrapy/issues/1681"}, {"text": "issue 934", "href": "https://github.com/scrapy/scrapy/issues/934"}, {"text": "issue 1100", "href": "https://github.com/scrapy/scrapy/issues/1100"}, {"text": "issue 1444", "href": "https://github.com/scrapy/scrapy/issues/1444"}, {"text": "issue 1498", "href": "https://github.com/scrapy/scrapy/issues/1498"}, {"text": "issue 1710", "href": "https://github.com/scrapy/scrapy/issues/1710"}, {"text": "issue 1550", "href": "https://github.com/scrapy/scrapy/issues/1550"}, {"text": "MEMUSAGE_CHECK_INTERVAL_SECONDS", "href": "topics/settings.html#std-setting-MEMUSAGE_CHECK_INTERVAL_SECONDS"}, {"text": "issue 1282", "href": "https://github.com/scrapy/scrapy/issues/1282"}, {"text": "issue 1390", "href": "https://github.com/scrapy/scrapy/issues/1390"}, {"text": "issue 1421", "href": "https://github.com/scrapy/scrapy/issues/1421"}, {"text": "issue 1794", "href": "https://github.com/scrapy/scrapy/issues/1794"}, {"text": "issue 1629", "href": "https://github.com/scrapy/scrapy/issues/1629"}, {"text": "issue 1334", "href": "https://github.com/scrapy/scrapy/issues/1334"}, {"text": "issue 1364", "href": "https://github.com/scrapy/scrapy/issues/1364"}, {"text": "issue 1447", "href": "https://github.com/scrapy/scrapy/issues/1447"}, {"text": "issue 1469", "href": "https://github.com/scrapy/scrapy/issues/1469"}, {"text": "issue 1472", "href": "https://github.com/scrapy/scrapy/issues/1472"}, {"text": "issue 1135", "href": "https://github.com/scrapy/scrapy/issues/1135"}, {"text": "issue 1149", "href": "https://github.com/scrapy/scrapy/issues/1149"}, {"text": "issue 1586", "href": "https://github.com/scrapy/scrapy/issues/1586"}, {"text": "issue 1662", "href": "https://github.com/scrapy/scrapy/issues/1662"}, {"text": "issue 1723", "href": "https://github.com/scrapy/scrapy/issues/1723"}, {"text": "issue 1725", "href": "https://github.com/scrapy/scrapy/issues/1725"}, {"text": "issue 1423", "href": "https://github.com/scrapy/scrapy/issues/1423"}, {"text": "issue 1528", "href": "https://github.com/scrapy/scrapy/issues/1528"}, {"text": "SCHEDULER_PRIORITY_QUEUE", "href": "topics/settings.html#std-setting-SCHEDULER_PRIORITY_QUEUE"}, {"text": "issue 1822", "href": "https://github.com/scrapy/scrapy/issues/1822"}, {"text": "issue 1835", "href": "https://github.com/scrapy/scrapy/issues/1835"}, {"text": "FEED_TEMPDIR", "href": "topics/settings.html#std-setting-FEED_TEMPDIR"}, {"text": "issue 1847", "href": "https://github.com/scrapy/scrapy/issues/1847"}, {"text": "issue 1891", "href": "https://github.com/scrapy/scrapy/issues/1891"}, {"text": "issue 1950", "href": "https://github.com/scrapy/scrapy/issues/1950"}, {"text": "issue 1761", "href": "https://github.com/scrapy/scrapy/issues/1761"}, {"text": "issue 1883", "href": "https://github.com/scrapy/scrapy/issues/1883"}, {"text": "issue 1291", "href": "https://github.com/scrapy/scrapy/issues/1291"}, {"text": "issue 1302", "href": "https://github.com/scrapy/scrapy/issues/1302"}, {"text": "issue 1335", "href": "https://github.com/scrapy/scrapy/issues/1335"}, {"text": "issue 1683", "href": "https://github.com/scrapy/scrapy/issues/1683"}, {"text": "issue 1660", "href": "https://github.com/scrapy/scrapy/issues/1660"}, {"text": "issue 1642", "href": "https://github.com/scrapy/scrapy/issues/1642"}, {"text": "issue 1721", "href": "https://github.com/scrapy/scrapy/issues/1721"}, {"text": "issue 1727", "href": "https://github.com/scrapy/scrapy/issues/1727"}, {"text": "issue 1879", "href": "https://github.com/scrapy/scrapy/issues/1879"}, {"text": "issue 1476", "href": "https://github.com/scrapy/scrapy/issues/1476"}, {"text": "issue 1481", "href": "https://github.com/scrapy/scrapy/issues/1481"}, {"text": "issue 1477", "href": "https://github.com/scrapy/scrapy/issues/1477"}, {"text": "issue 1315", "href": "https://github.com/scrapy/scrapy/issues/1315"}, {"text": "issue 1290", "href": "https://github.com/scrapy/scrapy/issues/1290"}, {"text": "issue 1750", "href": "https://github.com/scrapy/scrapy/issues/1750"}, {"text": "issue 1881", "href": "https://github.com/scrapy/scrapy/issues/1881"}, {"text": "issue 778", "href": "https://github.com/scrapy/scrapy/issues/778"}, {"text": "issue 1851", "href": "https://github.com/scrapy/scrapy/issues/1851"}, {"text": "issue 1359", "href": "https://github.com/scrapy/scrapy/issues/1359"}, {"text": "issue 1689", "href": "https://github.com/scrapy/scrapy/issues/1689"}, {"text": "issue 1720", "href": "https://github.com/scrapy/scrapy/issues/1720"}, {"text": "pydispatcher", "href": "https://pypi.org/project/PyDispatcher/"}, {"text": "issue 1524", "href": "https://github.com/scrapy/scrapy/issues/1524"}, {"text": "https://github.com/scrapy/scrapy/pull/1524#issuecomment-146985595", "href": "https://github.com/scrapy/scrapy/pull/1524#issuecomment-146985595"}, {"text": "issue 1289", "href": "https://github.com/scrapy/scrapy/issues/1289"}, {"text": "issue 1274", "href": "https://github.com/scrapy/scrapy/issues/1274"}, {"text": "issue 1333", "href": "https://github.com/scrapy/scrapy/issues/1333"}, {"text": "issue 1201", "href": "https://github.com/scrapy/scrapy/issues/1201"}, {"text": "issue 1564", "href": "https://github.com/scrapy/scrapy/issues/1564"}, {"text": "TEMPLATES_DIR", "href": "topics/settings.html#std-setting-TEMPLATES_DIR"}, {"text": "issue 1575", "href": "https://github.com/scrapy/scrapy/issues/1575"}, {"text": "issue 1595", "href": "https://github.com/scrapy/scrapy/issues/1595"}, {"text": "issue 1596", "href": "https://github.com/scrapy/scrapy/issues/1596"}, {"text": "issue 1597", "href": "https://github.com/scrapy/scrapy/issues/1597"}, {"text": "issue 1634", "href": "https://github.com/scrapy/scrapy/issues/1634"}, {"text": "issue 1738", "href": "https://github.com/scrapy/scrapy/issues/1738"}, {"text": "issue 1635", "href": "https://github.com/scrapy/scrapy/issues/1635"}, {"text": "PythonItemExporter", "href": "topics/exporters.html#scrapy.exporters.PythonItemExporter"}, {"text": "issue 1737", "href": "https://github.com/scrapy/scrapy/issues/1737"}, {"text": "issue 1294", "href": "https://github.com/scrapy/scrapy/issues/1294"}, {"text": "issue 1419", "href": "https://github.com/scrapy/scrapy/issues/1419"}, {"text": "issue 1263", "href": "https://github.com/scrapy/scrapy/issues/1263"}, {"text": "issue 1624", "href": "https://github.com/scrapy/scrapy/issues/1624"}, {"text": "issue 1654", "href": "https://github.com/scrapy/scrapy/issues/1654"}, {"text": "issue 1722", "href": "https://github.com/scrapy/scrapy/issues/1722"}, {"text": "issue 1726", "href": "https://github.com/scrapy/scrapy/issues/1726"}, {"text": "issue 1303", "href": "https://github.com/scrapy/scrapy/issues/1303"}, {"text": "issue 1212", "href": "https://github.com/scrapy/scrapy/issues/1212"}, {"text": "issue 1902", "href": "https://github.com/scrapy/scrapy/issues/1902"}, {"text": "issue 1912", "href": "https://github.com/scrapy/scrapy/issues/1912"}, {"text": "issue 1857", "href": "https://github.com/scrapy/scrapy/issues/1857"}, {"text": "issue 1875", "href": "https://github.com/scrapy/scrapy/issues/1875"}, {"text": "issue 1893", "href": "https://github.com/scrapy/scrapy/issues/1893"}, {"text": "issue 1869", "href": "https://github.com/scrapy/scrapy/issues/1869"}, {"text": "issue 907", "href": "https://github.com/scrapy/scrapy/issues/907"}, {"text": "commit 108195e", "href": "https://github.com/scrapy/scrapy/commit/108195e"}, {"text": "commit 1f3d90a", "href": "https://github.com/scrapy/scrapy/commit/1f3d90a"}, {"text": "commit 808a9ea", "href": "https://github.com/scrapy/scrapy/commit/808a9ea"}, {"text": "commit 803bd87", "href": "https://github.com/scrapy/scrapy/commit/803bd87"}, {"text": "commit aa94121", "href": "https://github.com/scrapy/scrapy/commit/aa94121"}, {"text": "commit 7dfa979", "href": "https://github.com/scrapy/scrapy/commit/7dfa979"}, {"text": "commit 6e42f0b", "href": "https://github.com/scrapy/scrapy/commit/6e42f0b"}, {"text": "commit 823a1cc", "href": "https://github.com/scrapy/scrapy/commit/823a1cc"}, {"text": "commit da3c155", "href": "https://github.com/scrapy/scrapy/commit/da3c155"}, {"text": "commit 4418fc3", "href": "https://github.com/scrapy/scrapy/commit/4418fc3"}, {"text": "commit a55078c", "href": "https://github.com/scrapy/scrapy/commit/a55078c"}, {"text": "commit 86fc330", "href": "https://github.com/scrapy/scrapy/commit/86fc330"}, {"text": "commit db4c9fe", "href": "https://github.com/scrapy/scrapy/commit/db4c9fe"}, {"text": "commit df2b944", "href": "https://github.com/scrapy/scrapy/commit/df2b944"}, {"text": "commit a83ab41", "href": "https://github.com/scrapy/scrapy/commit/a83ab41"}, {"text": "commit 73ac80d", "href": "https://github.com/scrapy/scrapy/commit/73ac80d"}, {"text": "commit 97d080e", "href": "https://github.com/scrapy/scrapy/commit/97d080e"}, {"text": "commit 97f2fb3", "href": "https://github.com/scrapy/scrapy/commit/97f2fb3"}, {"text": "file://", "href": "file://"}, {"text": "commit d9b4850", "href": "https://github.com/scrapy/scrapy/commit/d9b4850"}, {"text": "commit c0d0734", "href": "https://github.com/scrapy/scrapy/commit/c0d0734"}, {"text": "commit aa239ad", "href": "https://github.com/scrapy/scrapy/commit/aa239ad"}, {"text": "commit 10eb400", "href": "https://github.com/scrapy/scrapy/commit/10eb400"}, {"text": "commit 1c3600a", "href": "https://github.com/scrapy/scrapy/commit/1c3600a"}, {"text": "commit 7f4ddd5", "href": "https://github.com/scrapy/scrapy/commit/7f4ddd5"}, {"text": "commit b71f677", "href": "https://github.com/scrapy/scrapy/commit/b71f677"}, {"text": "commit 5456c0e", "href": "https://github.com/scrapy/scrapy/commit/5456c0e"}, {"text": "commit 0a1366e", "href": "https://github.com/scrapy/scrapy/commit/0a1366e"}, {"text": "commit ca8d60f", "href": "https://github.com/scrapy/scrapy/commit/ca8d60f"}, {"text": "commit 7067117", "href": "https://github.com/scrapy/scrapy/commit/7067117"}, {"text": "commit 32f115c", "href": "https://github.com/scrapy/scrapy/commit/32f115c"}, {"text": "commit 23fda69", "href": "https://github.com/scrapy/scrapy/commit/23fda69"}, {"text": "commit 98b63ee", "href": "https://github.com/scrapy/scrapy/commit/98b63ee"}, {"text": "commit 1925db1", "href": "https://github.com/scrapy/scrapy/commit/1925db1"}, {"text": "commit 5d10d6d", "href": "https://github.com/scrapy/scrapy/commit/5d10d6d"}, {"text": "commit 85c980e", "href": "https://github.com/scrapy/scrapy/commit/85c980e"}, {"text": "commit fbd010d", "href": "https://github.com/scrapy/scrapy/commit/fbd010d"}, {"text": "commit d8f4cba", "href": "https://github.com/scrapy/scrapy/commit/d8f4cba"}, {"text": "commit de73b1a", "href": "https://github.com/scrapy/scrapy/commit/de73b1a"}, {"text": "commit 1ddcc7b", "href": "https://github.com/scrapy/scrapy/commit/1ddcc7b"}, {"text": "commit 1b85bcf", "href": "https://github.com/scrapy/scrapy/commit/1b85bcf"}, {"text": "commit 55f7104", "href": "https://github.com/scrapy/scrapy/commit/55f7104"}, {"text": "commit b262411", "href": "https://github.com/scrapy/scrapy/commit/b262411"}, {"text": "commit a6535c2", "href": "https://github.com/scrapy/scrapy/commit/a6535c2"}, {"text": "commit 8876111", "href": "https://github.com/scrapy/scrapy/commit/8876111"}, {"text": "commit 5d4daf8", "href": "https://github.com/scrapy/scrapy/commit/5d4daf8"}, {"text": "commit f8d0682", "href": "https://github.com/scrapy/scrapy/commit/f8d0682"}, {"text": "commit 5f83a93", "href": "https://github.com/scrapy/scrapy/commit/5f83a93"}, {"text": "commit 3365c01", "href": "https://github.com/scrapy/scrapy/commit/3365c01"}, {"text": "commit 2d688cd", "href": "https://github.com/scrapy/scrapy/commit/2d688cd"}, {"text": "commit fbc1f25", "href": "https://github.com/scrapy/scrapy/commit/fbc1f25"}, {"text": "commit 7d6538c", "href": "https://github.com/scrapy/scrapy/commit/7d6538c"}, {"text": "commit 8752294", "href": "https://github.com/scrapy/scrapy/commit/8752294"}, {"text": "commit 13c45ac", "href": "https://github.com/scrapy/scrapy/commit/13c45ac"}, {"text": "commit cbc2501", "href": "https://github.com/scrapy/scrapy/commit/cbc2501"}, {"text": "commit 66af9cd", "href": "https://github.com/scrapy/scrapy/commit/66af9cd"}, {"text": "commit b04dd7d", "href": "https://github.com/scrapy/scrapy/commit/b04dd7d"}, {"text": "commit 6f85c7f", "href": "https://github.com/scrapy/scrapy/commit/6f85c7f"}, {"text": "commit 9c9d2e0", "href": "https://github.com/scrapy/scrapy/commit/9c9d2e0"}, {"text": "commit c63882b", "href": "https://github.com/scrapy/scrapy/commit/c63882b"}, {"text": "commit a9ae7b0", "href": "https://github.com/scrapy/scrapy/commit/a9ae7b0"}, {"text": "commit 7c8a4fe", "href": "https://github.com/scrapy/scrapy/commit/7c8a4fe"}, {"text": "commit cc00ad2", "href": "https://github.com/scrapy/scrapy/commit/cc00ad2"}, {"text": "commit eca227e", "href": "https://github.com/scrapy/scrapy/commit/eca227e"}, {"text": "commit b8567bc", "href": "https://github.com/scrapy/scrapy/commit/b8567bc"}, {"text": "commit 392233f", "href": "https://github.com/scrapy/scrapy/commit/392233f"}, {"text": "commit 5303c66", "href": "https://github.com/scrapy/scrapy/commit/5303c66"}, {"text": "commit c89fa29", "href": "https://github.com/scrapy/scrapy/commit/c89fa29"}, {"text": "overview", "href": "intro/overview.html#intro-overview"}, {"text": "tutorial", "href": "intro/tutorial.html#intro-tutorial"}, {"text": "Settings", "href": "topics/settings.html#topics-settings"}, {"text": "Logging", "href": "topics/logging.html#topics-logging"}, {"text": "Core API", "href": "topics/api.html#topics-api"}, {"text": "Common Practices", "href": "topics/practices.html#topics-practices"}, {"text": "scrapyd-client", "href": "https://github.com/scrapy/scrapyd-client"}, {"text": "Deploying Spiders", "href": "topics/deploy.html#topics-deploy"}, {"text": "scrapy-djangoitem", "href": "https://github.com/scrapy-plugins/scrapy-djangoitem"}, {"text": "scrapy-jsonrpc", "href": "https://github.com/scrapy-plugins/scrapy-jsonrpc"}, {"text": "issue 1060", "href": "https://github.com/scrapy/scrapy/issues/1060"}, {"text": "issue 1235", "href": "https://github.com/scrapy/scrapy/issues/1235"}, {"text": "issue 1236", "href": "https://github.com/scrapy/scrapy/issues/1236"}, {"text": "issue 1240", "href": "https://github.com/scrapy/scrapy/issues/1240"}, {"text": "issue 1259", "href": "https://github.com/scrapy/scrapy/issues/1259"}, {"text": "issue 1278", "href": "https://github.com/scrapy/scrapy/issues/1278"}, {"text": "issue 1286", "href": "https://github.com/scrapy/scrapy/issues/1286"}, {"text": "issue 1159", "href": "https://github.com/scrapy/scrapy/issues/1159"}, {"text": "issue 1224", "href": "https://github.com/scrapy/scrapy/issues/1224"}, {"text": "issue 1132", "href": "https://github.com/scrapy/scrapy/issues/1132"}, {"text": "issue 963", "href": "https://github.com/scrapy/scrapy/issues/963"}, {"text": "issue 1123", "href": "https://github.com/scrapy/scrapy/issues/1123"}, {"text": "issue 1081", "href": "https://github.com/scrapy/scrapy/issues/1081"}, {"text": "issue 1086", "href": "https://github.com/scrapy/scrapy/issues/1086"}, {"text": "issue 1098", "href": "https://github.com/scrapy/scrapy/issues/1098"}, {"text": "issue 1101", "href": "https://github.com/scrapy/scrapy/issues/1101"}, {"text": "issue 624", "href": "https://github.com/scrapy/scrapy/issues/624"}, {"text": "issue 1145", "href": "https://github.com/scrapy/scrapy/issues/1145"}, {"text": "issue 1016", "href": "https://github.com/scrapy/scrapy/issues/1016"}, {"text": "issue 1020", "href": "https://github.com/scrapy/scrapy/issues/1020"}, {"text": "issue 983", "href": "https://github.com/scrapy/scrapy/issues/983"}, {"text": "issue 821", "href": "https://github.com/scrapy/scrapy/issues/821"}, {"text": "issue 961", "href": "https://github.com/scrapy/scrapy/issues/961"}, {"text": "issue 946", "href": "https://github.com/scrapy/scrapy/issues/946"}, {"text": "issue 882", "href": "https://github.com/scrapy/scrapy/issues/882"}, {"text": "issue 795", "href": "https://github.com/scrapy/scrapy/issues/795"}, {"text": "issue 896", "href": "https://github.com/scrapy/scrapy/issues/896"}, {"text": "issue 854", "href": "https://github.com/scrapy/scrapy/issues/854"}, {"text": "issue 817", "href": "https://github.com/scrapy/scrapy/issues/817"}, {"text": "issue 816", "href": "https://github.com/scrapy/scrapy/issues/816"}, {"text": "issue 1128", "href": "https://github.com/scrapy/scrapy/issues/1128"}, {"text": "issue 1147", "href": "https://github.com/scrapy/scrapy/issues/1147"}, {"text": "issue 1148", "href": "https://github.com/scrapy/scrapy/issues/1148"}, {"text": "issue 1156", "href": "https://github.com/scrapy/scrapy/issues/1156"}, {"text": "issue 1185", "href": "https://github.com/scrapy/scrapy/issues/1185"}, {"text": "issue 1187", "href": "https://github.com/scrapy/scrapy/issues/1187"}, {"text": "issue 1258", "href": "https://github.com/scrapy/scrapy/issues/1258"}, {"text": "issue 1268", "href": "https://github.com/scrapy/scrapy/issues/1268"}, {"text": "issue 1276", "href": "https://github.com/scrapy/scrapy/issues/1276"}, {"text": "issue 1285", "href": "https://github.com/scrapy/scrapy/issues/1285"}, {"text": "issue 1284", "href": "https://github.com/scrapy/scrapy/issues/1284"}, {"text": "issue 1074", "href": "https://github.com/scrapy/scrapy/issues/1074"}, {"text": "issue 1075", "href": "https://github.com/scrapy/scrapy/issues/1075"}, {"text": "issue 1297", "href": "https://github.com/scrapy/scrapy/issues/1297"}, {"text": "issue 1205", "href": "https://github.com/scrapy/scrapy/issues/1205"}, {"text": "issue 1155", "href": "https://github.com/scrapy/scrapy/issues/1155"}, {"text": "issue 925", "href": "https://github.com/scrapy/scrapy/issues/925"}, {"text": "issue 895", "href": "https://github.com/scrapy/scrapy/issues/895"}, {"text": "issue 911", "href": "https://github.com/scrapy/scrapy/issues/911"}, {"text": "issue 777", "href": "https://github.com/scrapy/scrapy/issues/777"}, {"text": "issue 1242", "href": "https://github.com/scrapy/scrapy/issues/1242"}, {"text": "issue 1218", "href": "https://github.com/scrapy/scrapy/issues/1218"}, {"text": "issue 1233", "href": "https://github.com/scrapy/scrapy/issues/1233"}, {"text": "issue 1181", "href": "https://github.com/scrapy/scrapy/issues/1181"}, {"text": "issue 1210", "href": "https://github.com/scrapy/scrapy/issues/1210"}, {"text": "issue 1166", "href": "https://github.com/scrapy/scrapy/issues/1166"}, {"text": "issue 1177", "href": "https://github.com/scrapy/scrapy/issues/1177"}, {"text": "issue 1102", "href": "https://github.com/scrapy/scrapy/issues/1102"}, {"text": "issue 1134", "href": "https://github.com/scrapy/scrapy/issues/1134"}, {"text": "issue 914", "href": "https://github.com/scrapy/scrapy/issues/914"}, {"text": "issue 859", "href": "https://github.com/scrapy/scrapy/issues/859"}, {"text": "issue 827", "href": "https://github.com/scrapy/scrapy/issues/827"}, {"text": "issue 841", "href": "https://github.com/scrapy/scrapy/issues/841"}, {"text": "issue 1267", "href": "https://github.com/scrapy/scrapy/issues/1267"}, {"text": "issue 1190", "href": "https://github.com/scrapy/scrapy/issues/1190"}, {"text": "issue 1188", "href": "https://github.com/scrapy/scrapy/issues/1188"}, {"text": "issue 1180", "href": "https://github.com/scrapy/scrapy/issues/1180"}, {"text": "issue 1150", "href": "https://github.com/scrapy/scrapy/issues/1150"}, {"text": "issue 1164", "href": "https://github.com/scrapy/scrapy/issues/1164"}, {"text": "issue 1124", "href": "https://github.com/scrapy/scrapy/issues/1124"}, {"text": "issue 1073", "href": "https://github.com/scrapy/scrapy/issues/1073"}, {"text": "issue 1106", "href": "https://github.com/scrapy/scrapy/issues/1106"}, {"text": "issue 647", "href": "https://github.com/scrapy/scrapy/issues/647"}, {"text": "issue 1022", "href": "https://github.com/scrapy/scrapy/issues/1022"}, {"text": "issue 1071", "href": "https://github.com/scrapy/scrapy/issues/1071"}, {"text": "issue 898", "href": "https://github.com/scrapy/scrapy/issues/898"}, {"text": "issue 893", "href": "https://github.com/scrapy/scrapy/issues/893"}, {"text": "issue 894", "href": "https://github.com/scrapy/scrapy/issues/894"}, {"text": "issue 904", "href": "https://github.com/scrapy/scrapy/issues/904"}, {"text": "issue 1292", "href": "https://github.com/scrapy/scrapy/issues/1292"}, {"text": "issue 1220", "href": "https://github.com/scrapy/scrapy/issues/1220"}, {"text": "issue 1219", "href": "https://github.com/scrapy/scrapy/issues/1219"}, {"text": "issue 1196", "href": "https://github.com/scrapy/scrapy/issues/1196"}, {"text": "issue 1172", "href": "https://github.com/scrapy/scrapy/issues/1172"}, {"text": "issue 1171", "href": "https://github.com/scrapy/scrapy/issues/1171"}, {"text": "issue 1169", "href": "https://github.com/scrapy/scrapy/issues/1169"}, {"text": "issue 1160", "href": "https://github.com/scrapy/scrapy/issues/1160"}, {"text": "issue 1154", "href": "https://github.com/scrapy/scrapy/issues/1154"}, {"text": "issue 1127", "href": "https://github.com/scrapy/scrapy/issues/1127"}, {"text": "issue 1112", "href": "https://github.com/scrapy/scrapy/issues/1112"}, {"text": "issue 1105", "href": "https://github.com/scrapy/scrapy/issues/1105"}, {"text": "issue 1041", "href": "https://github.com/scrapy/scrapy/issues/1041"}, {"text": "issue 1082", "href": "https://github.com/scrapy/scrapy/issues/1082"}, {"text": "issue 1033", "href": "https://github.com/scrapy/scrapy/issues/1033"}, {"text": "issue 944", "href": "https://github.com/scrapy/scrapy/issues/944"}, {"text": "issue 866", "href": "https://github.com/scrapy/scrapy/issues/866"}, {"text": "issue 864", "href": "https://github.com/scrapy/scrapy/issues/864"}, {"text": "issue 796", "href": "https://github.com/scrapy/scrapy/issues/796"}, {"text": "issue 1260", "href": "https://github.com/scrapy/scrapy/issues/1260"}, {"text": "issue 1271", "href": "https://github.com/scrapy/scrapy/issues/1271"}, {"text": "issue 1293", "href": "https://github.com/scrapy/scrapy/issues/1293"}, {"text": "issue 1298", "href": "https://github.com/scrapy/scrapy/issues/1298"}, {"text": "issue 353", "href": "https://github.com/scrapy/scrapy/issues/353"}, {"text": "issue 1228", "href": "https://github.com/scrapy/scrapy/issues/1228"}, {"text": "issue 722", "href": "https://github.com/scrapy/scrapy/issues/722"}, {"text": "issue 1131", "href": "https://github.com/scrapy/scrapy/issues/1131"}, {"text": "issue 1197", "href": "https://github.com/scrapy/scrapy/issues/1197"}, {"text": "issue 954", "href": "https://github.com/scrapy/scrapy/issues/954"}, {"text": "issue 902", "href": "https://github.com/scrapy/scrapy/issues/902"}, {"text": "issue 878", "href": "https://github.com/scrapy/scrapy/issues/878"}, {"text": "issue 879", "href": "https://github.com/scrapy/scrapy/issues/879"}, {"text": "issue 846", "href": "https://github.com/scrapy/scrapy/issues/846"}, {"text": "issue 1161", "href": "https://github.com/scrapy/scrapy/issues/1161"}, {"text": "issue 1162", "href": "https://github.com/scrapy/scrapy/issues/1162"}, {"text": "issue 1121", "href": "https://github.com/scrapy/scrapy/issues/1121"}, {"text": "issue 1070", "href": "https://github.com/scrapy/scrapy/issues/1070"}, {"text": "issue 1066", "href": "https://github.com/scrapy/scrapy/issues/1066"}, {"text": "issue 909", "href": "https://github.com/scrapy/scrapy/issues/909"}, {"text": "issue 830", "href": "https://github.com/scrapy/scrapy/issues/830"}, {"text": "issue 810", "href": "https://github.com/scrapy/scrapy/issues/810"}, {"text": "issue 803", "href": "https://github.com/scrapy/scrapy/issues/803"}, {"text": "issue 801", "href": "https://github.com/scrapy/scrapy/issues/801"}, {"text": "issue 800", "href": "https://github.com/scrapy/scrapy/issues/800"}, {"text": "issue 799", "href": "https://github.com/scrapy/scrapy/issues/799"}, {"text": "issue 798", "href": "https://github.com/scrapy/scrapy/issues/798"}, {"text": "issue 797", "href": "https://github.com/scrapy/scrapy/issues/797"}, {"text": "issue 776", "href": "https://github.com/scrapy/scrapy/issues/776"}, {"text": "issue 1243", "href": "https://github.com/scrapy/scrapy/issues/1243"}, {"text": "issue 1206", "href": "https://github.com/scrapy/scrapy/issues/1206"}, {"text": "issue 1234", "href": "https://github.com/scrapy/scrapy/issues/1234"}, {"text": "issue 1165", "href": "https://github.com/scrapy/scrapy/issues/1165"}, {"text": "issue 1168", "href": "https://github.com/scrapy/scrapy/issues/1168"}, {"text": "issue 1152", "href": "https://github.com/scrapy/scrapy/issues/1152"}, {"text": "issue 1089", "href": "https://github.com/scrapy/scrapy/issues/1089"}, {"text": "issue 1044", "href": "https://github.com/scrapy/scrapy/issues/1044"}, {"text": "issue 835", "href": "https://github.com/scrapy/scrapy/issues/835"}, {"text": "issue 779", "href": "https://github.com/scrapy/scrapy/issues/779"}, {"text": "issue 1079", "href": "https://github.com/scrapy/scrapy/issues/1079"}, {"text": "issue 1078", "href": "https://github.com/scrapy/scrapy/issues/1078"}, {"text": "issue 992", "href": "https://github.com/scrapy/scrapy/issues/992"}, {"text": "issue 871", "href": "https://github.com/scrapy/scrapy/issues/871"}, {"text": "issue 805", "href": "https://github.com/scrapy/scrapy/issues/805"}, {"text": "issue 775", "href": "https://github.com/scrapy/scrapy/issues/775"}, {"text": "commit 07cb3e5", "href": "https://github.com/scrapy/scrapy/commit/07cb3e5"}, {"text": "commit 2c8e573", "href": "https://github.com/scrapy/scrapy/commit/2c8e573"}, {"text": "commit d694019", "href": "https://github.com/scrapy/scrapy/commit/d694019"}, {"text": "commit f92fa83", "href": "https://github.com/scrapy/scrapy/commit/f92fa83"}, {"text": "commit c2c6d15", "href": "https://github.com/scrapy/scrapy/commit/c2c6d15"}, {"text": "commit 540b9bc", "href": "https://github.com/scrapy/scrapy/commit/540b9bc"}, {"text": "commit b4c454b", "href": "https://github.com/scrapy/scrapy/commit/b4c454b"}, {"text": "commit e3c1260", "href": "https://github.com/scrapy/scrapy/commit/e3c1260"}, {"text": "commit 9e13f42", "href": "https://github.com/scrapy/scrapy/commit/9e13f42"}, {"text": "commit cdb9a0b", "href": "https://github.com/scrapy/scrapy/commit/cdb9a0b"}, {"text": "commit bb3a848", "href": "https://github.com/scrapy/scrapy/commit/bb3a848"}, {"text": "commit edb07a4", "href": "https://github.com/scrapy/scrapy/commit/edb07a4"}, {"text": "commit 7ee6f7a", "href": "https://github.com/scrapy/scrapy/commit/7ee6f7a"}, {"text": "commit 874fcdd", "href": "https://github.com/scrapy/scrapy/commit/874fcdd"}, {"text": "commit c6b21f0", "href": "https://github.com/scrapy/scrapy/commit/c6b21f0"}, {"text": "commit c3a6628", "href": "https://github.com/scrapy/scrapy/commit/c3a6628"}, {"text": "commit d0bf957", "href": "https://github.com/scrapy/scrapy/commit/d0bf957"}, {"text": "commit eeb589a", "href": "https://github.com/scrapy/scrapy/commit/eeb589a"}, {"text": "commit 5fdab02", "href": "https://github.com/scrapy/scrapy/commit/5fdab02"}, {"text": "commit b0ae199", "href": "https://github.com/scrapy/scrapy/commit/b0ae199"}, {"text": "commit 5cb0cfb", "href": "https://github.com/scrapy/scrapy/commit/5cb0cfb"}, {"text": "commit 781286b", "href": "https://github.com/scrapy/scrapy/commit/781286b"}, {"text": "commit b415d04", "href": "https://github.com/scrapy/scrapy/commit/b415d04"}, {"text": "commit 627b9ba", "href": "https://github.com/scrapy/scrapy/commit/627b9ba"}, {"text": "commit de909ad", "href": "https://github.com/scrapy/scrapy/commit/de909ad"}, {"text": "commit 3f3263d", "href": "https://github.com/scrapy/scrapy/commit/3f3263d"}, {"text": "commit 49b40f0", "href": "https://github.com/scrapy/scrapy/commit/49b40f0"}, {"text": "commit 5eddc68", "href": "https://github.com/scrapy/scrapy/commit/5eddc68"}, {"text": "commit d6cb999", "href": "https://github.com/scrapy/scrapy/commit/d6cb999"}, {"text": "commit 8e080c1", "href": "https://github.com/scrapy/scrapy/commit/8e080c1"}, {"text": "commit 1d0c096", "href": "https://github.com/scrapy/scrapy/commit/1d0c096"}, {"text": "commit 4c701d7", "href": "https://github.com/scrapy/scrapy/commit/4c701d7"}, {"text": "commit d109c13", "href": "https://github.com/scrapy/scrapy/commit/d109c13"}, {"text": "commit 39d2ce5", "href": "https://github.com/scrapy/scrapy/commit/39d2ce5"}, {"text": "commit 180d3ad", "href": "https://github.com/scrapy/scrapy/commit/180d3ad"}, {"text": "commit a51ee8b", "href": "https://github.com/scrapy/scrapy/commit/a51ee8b"}, {"text": "commit ee3b371", "href": "https://github.com/scrapy/scrapy/commit/ee3b371"}, {"text": "commit c3861cf", "href": "https://github.com/scrapy/scrapy/commit/c3861cf"}, {"text": "commit 362e322", "href": "https://github.com/scrapy/scrapy/commit/362e322"}, {"text": "commit 94a5c65", "href": "https://github.com/scrapy/scrapy/commit/94a5c65"}, {"text": "commit a274a7f", "href": "https://github.com/scrapy/scrapy/commit/a274a7f"}, {"text": "commit ae1e2cc", "href": "https://github.com/scrapy/scrapy/commit/ae1e2cc"}, {"text": "commit e49c96a", "href": "https://github.com/scrapy/scrapy/commit/e49c96a"}, {"text": "commit 1ca489d", "href": "https://github.com/scrapy/scrapy/commit/1ca489d"}, {"text": "commit 65c8f05", "href": "https://github.com/scrapy/scrapy/commit/65c8f05"}, {"text": "commit 037f6ab", "href": "https://github.com/scrapy/scrapy/commit/037f6ab"}, {"text": "commit 2d103e0", "href": "https://github.com/scrapy/scrapy/commit/2d103e0"}, {"text": "https://github.com/scrapy/w3lib/pull/23", "href": "https://github.com/scrapy/w3lib/pull/23"}, {"text": "commit f8d366a", "href": "https://github.com/scrapy/scrapy/commit/f8d366a"}, {"text": "commit 81344ea", "href": "https://github.com/scrapy/scrapy/commit/81344ea"}, {"text": "commit f7c4ea8", "href": "https://github.com/scrapy/scrapy/commit/f7c4ea8"}, {"text": "commit db59ed9", "href": "https://github.com/scrapy/scrapy/commit/db59ed9"}, {"text": "commit f090260", "href": "https://github.com/scrapy/scrapy/commit/f090260"}, {"text": "commit d8793af", "href": "https://github.com/scrapy/scrapy/commit/d8793af"}, {"text": "commit ed1f376", "href": "https://github.com/scrapy/scrapy/commit/ed1f376"}, {"text": "commit 91a1106", "href": "https://github.com/scrapy/scrapy/commit/91a1106"}, {"text": "commit 743e1e2", "href": "https://github.com/scrapy/scrapy/commit/743e1e2"}, {"text": "commit e22daaf", "href": "https://github.com/scrapy/scrapy/commit/e22daaf"}, {"text": "commit 5ec430b", "href": "https://github.com/scrapy/scrapy/commit/5ec430b"}, {"text": "commit e5e8133", "href": "https://github.com/scrapy/scrapy/commit/e5e8133"}, {"text": "commit 3cd6146", "href": "https://github.com/scrapy/scrapy/commit/3cd6146"}, {"text": "commit fa5d76b", "href": "https://github.com/scrapy/scrapy/commit/fa5d76b"}, {"text": "commit c6a9e20", "href": "https://github.com/scrapy/scrapy/commit/c6a9e20"}, {"text": "commit 8e3f20a", "href": "https://github.com/scrapy/scrapy/commit/8e3f20a"}, {"text": "issue 494", "href": "https://github.com/scrapy/scrapy/issues/494"}, {"text": "issue 684", "href": "https://github.com/scrapy/scrapy/issues/684"}, {"text": "issue 554", "href": "https://github.com/scrapy/scrapy/issues/554"}, {"text": "issue 690", "href": "https://github.com/scrapy/scrapy/issues/690"}, {"text": "issue 559", "href": "https://github.com/scrapy/scrapy/issues/559"}, {"text": "issue 761", "href": "https://github.com/scrapy/scrapy/issues/761"}, {"text": "issue 763", "href": "https://github.com/scrapy/scrapy/issues/763"}, {"text": "issue 737", "href": "https://github.com/scrapy/scrapy/issues/737"}, {"text": "issue 688", "href": "https://github.com/scrapy/scrapy/issues/688"}, {"text": "issue 762", "href": "https://github.com/scrapy/scrapy/issues/762"}, {"text": "issue 699", "href": "https://github.com/scrapy/scrapy/issues/699"}, {"text": "issue 509", "href": "https://github.com/scrapy/scrapy/issues/509"}, {"text": "issue 549", "href": "https://github.com/scrapy/scrapy/issues/549"}, {"text": "issue 535", "href": "https://github.com/scrapy/scrapy/issues/535"}, {"text": "issue 541", "href": "https://github.com/scrapy/scrapy/issues/541"}, {"text": "issue 500", "href": "https://github.com/scrapy/scrapy/issues/500"}, {"text": "issue 571", "href": "https://github.com/scrapy/scrapy/issues/571"}, {"text": "issue 557", "href": "https://github.com/scrapy/scrapy/issues/557"}, {"text": "issue 570", "href": "https://github.com/scrapy/scrapy/issues/570"}, {"text": "issue 566", "href": "https://github.com/scrapy/scrapy/issues/566"}, {"text": "issue 555", "href": "https://github.com/scrapy/scrapy/issues/555"}, {"text": "issue 553", "href": "https://github.com/scrapy/scrapy/issues/553"}, {"text": "issue 602", "href": "https://github.com/scrapy/scrapy/issues/602"}, {"text": "issue 622", "href": "https://github.com/scrapy/scrapy/issues/622"}, {"text": "issue 565", "href": "https://github.com/scrapy/scrapy/issues/565"}, {"text": "issue 629", "href": "https://github.com/scrapy/scrapy/issues/629"}, {"text": "issue 630", "href": "https://github.com/scrapy/scrapy/issues/630"}, {"text": "issue 638", "href": "https://github.com/scrapy/scrapy/issues/638"}, {"text": "issue 632", "href": "https://github.com/scrapy/scrapy/issues/632"}, {"text": "issue 636", "href": "https://github.com/scrapy/scrapy/issues/636"}, {"text": "issue 640", "href": "https://github.com/scrapy/scrapy/issues/640"}, {"text": "issue 635", "href": "https://github.com/scrapy/scrapy/issues/635"}, {"text": "issue 634", "href": "https://github.com/scrapy/scrapy/issues/634"}, {"text": "issue 639", "href": "https://github.com/scrapy/scrapy/issues/639"}, {"text": "issue 637", "href": "https://github.com/scrapy/scrapy/issues/637"}, {"text": "issue 631", "href": "https://github.com/scrapy/scrapy/issues/631"}, {"text": "issue 633", "href": "https://github.com/scrapy/scrapy/issues/633"}, {"text": "issue 641", "href": "https://github.com/scrapy/scrapy/issues/641"}, {"text": "issue 642", "href": "https://github.com/scrapy/scrapy/issues/642"}, {"text": "issue 646", "href": "https://github.com/scrapy/scrapy/issues/646"}, {"text": "issue 645", "href": "https://github.com/scrapy/scrapy/issues/645"}, {"text": "issue 650", "href": "https://github.com/scrapy/scrapy/issues/650"}, {"text": "issue 654", "href": "https://github.com/scrapy/scrapy/issues/654"}, {"text": "issue 612", "href": "https://github.com/scrapy/scrapy/issues/612"}, {"text": "issue 656", "href": "https://github.com/scrapy/scrapy/issues/656"}, {"text": "issue 193", "href": "https://github.com/scrapy/scrapy/issues/193"}, {"text": "issue 660", "href": "https://github.com/scrapy/scrapy/issues/660"}, {"text": "issue 674", "href": "https://github.com/scrapy/scrapy/issues/674"}, {"text": "issue 679", "href": "https://github.com/scrapy/scrapy/issues/679"}, {"text": "issue 687", "href": "https://github.com/scrapy/scrapy/issues/687"}, {"text": "issue 681", "href": "https://github.com/scrapy/scrapy/issues/681"}, {"text": "issue 692", "href": "https://github.com/scrapy/scrapy/issues/692"}, {"text": "issue 546", "href": "https://github.com/scrapy/scrapy/issues/546"}, {"text": "issue 659", "href": "https://github.com/scrapy/scrapy/issues/659"}, {"text": "issue 760", "href": "https://github.com/scrapy/scrapy/issues/760"}, {"text": "issue 693", "href": "https://github.com/scrapy/scrapy/issues/693"}, {"text": "issue 698", "href": "https://github.com/scrapy/scrapy/issues/698"}, {"text": "issue 597", "href": "https://github.com/scrapy/scrapy/issues/597"}, {"text": "issue 705", "href": "https://github.com/scrapy/scrapy/issues/705"}, {"text": "issue 727", "href": "https://github.com/scrapy/scrapy/issues/727"}, {"text": "issue 738", "href": "https://github.com/scrapy/scrapy/issues/738"}, {"text": "issue 724", "href": "https://github.com/scrapy/scrapy/issues/724"}, {"text": "issue 733", "href": "https://github.com/scrapy/scrapy/issues/733"}, {"text": "issue 752", "href": "https://github.com/scrapy/scrapy/issues/752"}, {"text": "issue 719", "href": "https://github.com/scrapy/scrapy/issues/719"}, {"text": "issue 746", "href": "https://github.com/scrapy/scrapy/issues/746"}, {"text": "issue 697", "href": "https://github.com/scrapy/scrapy/issues/697"}, {"text": "issue 626", "href": "https://github.com/scrapy/scrapy/issues/626"}, {"text": "issue 500", "href": "https://github.com/scrapy/scrapy/issues/500"}, {"text": "issue 742", "href": "https://github.com/scrapy/scrapy/issues/742"}, {"text": "issue 575", "href": "https://github.com/scrapy/scrapy/issues/575"}, {"text": "issue 587", "href": "https://github.com/scrapy/scrapy/issues/587"}, {"text": "issue 590", "href": "https://github.com/scrapy/scrapy/issues/590"}, {"text": "issue 596", "href": "https://github.com/scrapy/scrapy/issues/596"}, {"text": "issue 610", "href": "https://github.com/scrapy/scrapy/issues/610"}, {"text": "issue 617", "href": "https://github.com/scrapy/scrapy/issues/617"}, {"text": "issue 618", "href": "https://github.com/scrapy/scrapy/issues/618"}, {"text": "issue 627", "href": "https://github.com/scrapy/scrapy/issues/627"}, {"text": "issue 613", "href": "https://github.com/scrapy/scrapy/issues/613"}, {"text": "issue 643", "href": "https://github.com/scrapy/scrapy/issues/643"}, {"text": "issue 654", "href": "https://github.com/scrapy/scrapy/issues/654"}, {"text": "issue 675", "href": "https://github.com/scrapy/scrapy/issues/675"}, {"text": "issue 663", "href": "https://github.com/scrapy/scrapy/issues/663"}, {"text": "issue 711", "href": "https://github.com/scrapy/scrapy/issues/711"}, {"text": "issue 714", "href": "https://github.com/scrapy/scrapy/issues/714"}, {"text": "issue 561", "href": "https://github.com/scrapy/scrapy/issues/561"}, {"text": "issue 556", "href": "https://github.com/scrapy/scrapy/issues/556"}, {"text": "issue 485", "href": "https://github.com/scrapy/scrapy/issues/485"}, {"text": "issue 574", "href": "https://github.com/scrapy/scrapy/issues/574"}, {"text": "issue 581", "href": "https://github.com/scrapy/scrapy/issues/581"}, {"text": "issue 584", "href": "https://github.com/scrapy/scrapy/issues/584"}, {"text": "issue 582", "href": "https://github.com/scrapy/scrapy/issues/582"}, {"text": "issue 593", "href": "https://github.com/scrapy/scrapy/issues/593"}, {"text": "issue 594", "href": "https://github.com/scrapy/scrapy/issues/594"}, {"text": "issue 603", "href": "https://github.com/scrapy/scrapy/issues/603"}, {"text": "issue 628", "href": "https://github.com/scrapy/scrapy/issues/628"}, {"text": "issue 661", "href": "https://github.com/scrapy/scrapy/issues/661"}, {"text": "issue 676", "href": "https://github.com/scrapy/scrapy/issues/676"}, {"text": "issue 707", "href": "https://github.com/scrapy/scrapy/issues/707"}, {"text": "issue 745", "href": "https://github.com/scrapy/scrapy/issues/745"}, {"text": "issue 585", "href": "https://github.com/scrapy/scrapy/issues/585"}, {"text": "commit 13c099a", "href": "https://github.com/scrapy/scrapy/commit/13c099a"}, {"text": "commit 8ae11bf", "href": "https://github.com/scrapy/scrapy/commit/8ae11bf"}, {"text": "commit 1346037", "href": "https://github.com/scrapy/scrapy/commit/1346037"}, {"text": "commit 2ec2279", "href": "https://github.com/scrapy/scrapy/commit/2ec2279"}, {"text": "commit cc3eda3", "href": "https://github.com/scrapy/scrapy/commit/cc3eda3"}, {"text": "commit 8cb44f9", "href": "https://github.com/scrapy/scrapy/commit/8cb44f9"}, {"text": "commit 46d98d6", "href": "https://github.com/scrapy/scrapy/commit/46d98d6"}, {"text": "commit 13846de", "href": "https://github.com/scrapy/scrapy/commit/13846de"}, {"text": "commit 368a946", "href": "https://github.com/scrapy/scrapy/commit/368a946"}, {"text": "commit b566388", "href": "https://github.com/scrapy/scrapy/commit/b566388"}, {"text": "commit c1cb418", "href": "https://github.com/scrapy/scrapy/commit/c1cb418"}, {"text": "commit 7e4d627", "href": "https://github.com/scrapy/scrapy/commit/7e4d627"}, {"text": "commit 76c7e20", "href": "https://github.com/scrapy/scrapy/commit/76c7e20"}, {"text": "commit 5f87b17", "href": "https://github.com/scrapy/scrapy/commit/5f87b17"}, {"text": "commit d0ee545", "href": "https://github.com/scrapy/scrapy/commit/d0ee545"}, {"text": "commit 8da65de", "href": "https://github.com/scrapy/scrapy/commit/8da65de"}, {"text": "commit 875b9ab", "href": "https://github.com/scrapy/scrapy/commit/875b9ab"}, {"text": "commit f89efaf", "href": "https://github.com/scrapy/scrapy/commit/f89efaf"}, {"text": "commit 5349cec", "href": "https://github.com/scrapy/scrapy/commit/5349cec"}, {"text": "commit 387f414", "href": "https://github.com/scrapy/scrapy/commit/387f414"}, {"text": "commit 0632546", "href": "https://github.com/scrapy/scrapy/commit/0632546"}, {"text": "commit cde9a8c", "href": "https://github.com/scrapy/scrapy/commit/cde9a8c"}, {"text": "commit fb5c9c5", "href": "https://github.com/scrapy/scrapy/commit/fb5c9c5"}, {"text": "commit 70fb105", "href": "https://github.com/scrapy/scrapy/commit/70fb105"}, {"text": "commit 6f70b6a", "href": "https://github.com/scrapy/scrapy/commit/6f70b6a"}, {"text": "commit 725900d", "href": "https://github.com/scrapy/scrapy/commit/725900d"}, {"text": "commit af0219a", "href": "https://github.com/scrapy/scrapy/commit/af0219a"}, {"text": "commit b7f58f4", "href": "https://github.com/scrapy/scrapy/commit/b7f58f4"}, {"text": "issue 541", "href": "https://github.com/scrapy/scrapy/issues/541"}, {"text": "issue 392", "href": "https://github.com/scrapy/scrapy/issues/392"}, {"text": "issue 397", "href": "https://github.com/scrapy/scrapy/issues/397"}, {"text": "issue 343", "href": "https://github.com/scrapy/scrapy/issues/343"}, {"text": "issue 510", "href": "https://github.com/scrapy/scrapy/issues/510"}, {"text": "issue 519", "href": "https://github.com/scrapy/scrapy/issues/519"}, {"text": "issue 472", "href": "https://github.com/scrapy/scrapy/issues/472"}, {"text": "issue 461", "href": "https://github.com/scrapy/scrapy/issues/461"}, {"text": "issue 533", "href": "https://github.com/scrapy/scrapy/issues/533"}, {"text": "issue 525", "href": "https://github.com/scrapy/scrapy/issues/525"}, {"text": "issue 520", "href": "https://github.com/scrapy/scrapy/issues/520"}, {"text": "issue 506", "href": "https://github.com/scrapy/scrapy/issues/506"}, {"text": "issue 503", "href": "https://github.com/scrapy/scrapy/issues/503"}, {"text": "issue 498", "href": "https://github.com/scrapy/scrapy/issues/498"}, {"text": "issue 490", "href": "https://github.com/scrapy/scrapy/issues/490"}, {"text": "issue 478", "href": "https://github.com/scrapy/scrapy/issues/478"}, {"text": "issue 475", "href": "https://github.com/scrapy/scrapy/issues/475"}, {"text": "issue 469", "href": "https://github.com/scrapy/scrapy/issues/469"}, {"text": "issue 466", "href": "https://github.com/scrapy/scrapy/issues/466"}, {"text": "issue 497", "href": "https://github.com/scrapy/scrapy/issues/497"}, {"text": "issue 527", "href": "https://github.com/scrapy/scrapy/issues/527"}, {"text": "issue 524", "href": "https://github.com/scrapy/scrapy/issues/524"}, {"text": "issue 521", "href": "https://github.com/scrapy/scrapy/issues/521"}, {"text": "issue 517", "href": "https://github.com/scrapy/scrapy/issues/517"}, {"text": "issue 512", "href": "https://github.com/scrapy/scrapy/issues/512"}, {"text": "issue 505", "href": "https://github.com/scrapy/scrapy/issues/505"}, {"text": "issue 502", "href": "https://github.com/scrapy/scrapy/issues/502"}, {"text": "issue 489", "href": "https://github.com/scrapy/scrapy/issues/489"}, {"text": "issue 465", "href": "https://github.com/scrapy/scrapy/issues/465"}, {"text": "issue 460", "href": "https://github.com/scrapy/scrapy/issues/460"}, {"text": "issue 425", "href": "https://github.com/scrapy/scrapy/issues/425"}, {"text": "issue 536", "href": "https://github.com/scrapy/scrapy/issues/536"}, {"text": "issue 484", "href": "https://github.com/scrapy/scrapy/issues/484"}, {"text": "issue 464", "href": "https://github.com/scrapy/scrapy/issues/464"}, {"text": "issue 462", "href": "https://github.com/scrapy/scrapy/issues/462"}, {"text": "issue 523", "href": "https://github.com/scrapy/scrapy/issues/523"}, {"text": "issue 537", "href": "https://github.com/scrapy/scrapy/issues/537"}, {"text": "issue 531", "href": "https://github.com/scrapy/scrapy/issues/531"}, {"text": "issue 530", "href": "https://github.com/scrapy/scrapy/issues/530"}, {"text": "issue 529", "href": "https://github.com/scrapy/scrapy/issues/529"}, {"text": "issue 507", "href": "https://github.com/scrapy/scrapy/issues/507"}, {"text": "issue 513", "href": "https://github.com/scrapy/scrapy/issues/513"}, {"text": "issue 479", "href": "https://github.com/scrapy/scrapy/issues/479"}, {"text": "commit 6d1457d", "href": "https://github.com/scrapy/scrapy/commit/6d1457d"}, {"text": "commit b4fc359", "href": "https://github.com/scrapy/scrapy/commit/b4fc359"}, {"text": "commit 5ba1ad5", "href": "https://github.com/scrapy/scrapy/commit/5ba1ad5"}, {"text": "commit 419a780", "href": "https://github.com/scrapy/scrapy/commit/419a780"}, {"text": "issue 395", "href": "https://github.com/scrapy/scrapy/issues/395"}, {"text": "issue 426", "href": "https://github.com/scrapy/scrapy/issues/426"}, {"text": "ITEM_PIPELINES", "href": "topics/settings.html#std-setting-ITEM_PIPELINES"}, {"text": "issue 360", "href": "https://github.com/scrapy/scrapy/issues/360"}, {"text": "issue 416", "href": "https://github.com/scrapy/scrapy/issues/416"}, {"text": "issue 435", "href": "https://github.com/scrapy/scrapy/issues/435"}, {"text": "issue 436", "href": "https://github.com/scrapy/scrapy/issues/436"}, {"text": "issue 431", "href": "https://github.com/scrapy/scrapy/issues/431"}, {"text": "issue 452", "href": "https://github.com/scrapy/scrapy/issues/452"}, {"text": "issue 366", "href": "https://github.com/scrapy/scrapy/issues/366"}, {"text": "commit b43b5f575", "href": "https://github.com/scrapy/scrapy/commit/b43b5f575"}, {"text": "issue 327", "href": "https://github.com/scrapy/scrapy/issues/327"}, {"text": "issue 370", "href": "https://github.com/scrapy/scrapy/issues/370"}, {"text": "issue 409", "href": "https://github.com/scrapy/scrapy/issues/409"}, {"text": "issue 317", "href": "https://github.com/scrapy/scrapy/issues/317"}, {"text": "commit 86230c0", "href": "https://github.com/scrapy/scrapy/commit/86230c0"}, {"text": "issue 410", "href": "https://github.com/scrapy/scrapy/issues/410"}, {"text": "issue 422", "href": "https://github.com/scrapy/scrapy/issues/422"}, {"text": "issue 421", "href": "https://github.com/scrapy/scrapy/issues/421"}, {"text": "issue 420", "href": "https://github.com/scrapy/scrapy/issues/420"}, {"text": "issue 419", "href": "https://github.com/scrapy/scrapy/issues/419"}, {"text": "issue 423", "href": "https://github.com/scrapy/scrapy/issues/423"}, {"text": "issue 418", "href": "https://github.com/scrapy/scrapy/issues/418"}, {"text": "commit ecfa7431", "href": "https://github.com/scrapy/scrapy/commit/ecfa7431"}, {"text": "issue 430", "href": "https://github.com/scrapy/scrapy/issues/430"}, {"text": "issue 432", "href": "https://github.com/scrapy/scrapy/issues/432"}, {"text": "issue 445", "href": "https://github.com/scrapy/scrapy/issues/445"}, {"text": "issue 372", "href": "https://github.com/scrapy/scrapy/issues/372"}, {"text": "issue 450", "href": "https://github.com/scrapy/scrapy/issues/450"}, {"text": "commit b326b87", "href": "https://github.com/scrapy/scrapy/commit/b326b87"}, {"text": "commit 684cfc0", "href": "https://github.com/scrapy/scrapy/commit/684cfc0"}, {"text": "commit b6bed44c", "href": "https://github.com/scrapy/scrapy/commit/b6bed44c"}, {"text": "issue 406", "href": "https://github.com/scrapy/scrapy/issues/406"}, {"text": "issue 418", "href": "https://github.com/scrapy/scrapy/issues/418"}, {"text": "issue 407", "href": "https://github.com/scrapy/scrapy/issues/407"}, {"text": "issue 429", "href": "https://github.com/scrapy/scrapy/issues/429"}, {"text": "issue 387", "href": "https://github.com/scrapy/scrapy/issues/387"}, {"text": "issue 391", "href": "https://github.com/scrapy/scrapy/issues/391"}, {"text": "issue 399", "href": "https://github.com/scrapy/scrapy/issues/399"}, {"text": "issue 400", "href": "https://github.com/scrapy/scrapy/issues/400"}, {"text": "issue 401", "href": "https://github.com/scrapy/scrapy/issues/401"}, {"text": "issue 402", "href": "https://github.com/scrapy/scrapy/issues/402"}, {"text": "issue 404", "href": "https://github.com/scrapy/scrapy/issues/404"}, {"text": "commit 37c24e01d7", "href": "https://github.com/scrapy/scrapy/commit/37c24e01d7"}, {"text": "issue 226", "href": "https://github.com/scrapy/scrapy/issues/226"}, {"text": "issue 448", "href": "https://github.com/scrapy/scrapy/issues/448"}, {"text": "cssselect", "href": "https://cssselect.readthedocs.io/en/latest/index.html"}, {"text": "lxml", "href": "https://lxml.de/"}, {"text": "issue 390", "href": "https://github.com/scrapy/scrapy/issues/390"}, {"text": "commit 3d32c4f", "href": "https://github.com/scrapy/scrapy/commit/3d32c4f"}, {"text": "commit b1d8919", "href": "https://github.com/scrapy/scrapy/commit/b1d8919"}, {"text": "commit 89faf52", "href": "https://github.com/scrapy/scrapy/commit/89faf52"}, {"text": "commit 12693a5", "href": "https://github.com/scrapy/scrapy/commit/12693a5"}, {"text": "commit e429f63", "href": "https://github.com/scrapy/scrapy/commit/e429f63"}, {"text": "commit 912202e", "href": "https://github.com/scrapy/scrapy/commit/912202e"}, {"text": "commit cfc2d46", "href": "https://github.com/scrapy/scrapy/commit/cfc2d46"}, {"text": "commit 06149e0", "href": "https://github.com/scrapy/scrapy/commit/06149e0"}, {"text": "issue 339", "href": "https://github.com/scrapy/scrapy/issues/339"}, {"text": "commit d20304e", "href": "https://github.com/scrapy/scrapy/commit/d20304e"}, {"text": "commit 1994f38", "href": "https://github.com/scrapy/scrapy/commit/1994f38"}, {"text": "commit abf756f", "href": "https://github.com/scrapy/scrapy/commit/abf756f"}, {"text": "commit b15470d", "href": "https://github.com/scrapy/scrapy/commit/b15470d"}, {"text": "commit c4bf324", "href": "https://github.com/scrapy/scrapy/commit/c4bf324"}, {"text": "commit 6cbe684", "href": "https://github.com/scrapy/scrapy/commit/6cbe684"}, {"text": "commit 1a20bba", "href": "https://github.com/scrapy/scrapy/commit/1a20bba"}, {"text": "commit 3b01bb8", "href": "https://github.com/scrapy/scrapy/commit/3b01bb8"}, {"text": "commit fa766d7", "href": "https://github.com/scrapy/scrapy/commit/fa766d7"}, {"text": "commit 3283809", "href": "https://github.com/scrapy/scrapy/commit/3283809"}, {"text": "commit 1411923", "href": "https://github.com/scrapy/scrapy/commit/1411923"}, {"text": "commit bb35ed0", "href": "https://github.com/scrapy/scrapy/commit/bb35ed0"}, {"text": "commit de3e451", "href": "https://github.com/scrapy/scrapy/commit/de3e451"}, {"text": "commit c45e5f1", "href": "https://github.com/scrapy/scrapy/commit/c45e5f1"}, {"text": "commit 0b60031", "href": "https://github.com/scrapy/scrapy/commit/0b60031"}, {"text": "commit 3fe2a32", "href": "https://github.com/scrapy/scrapy/commit/3fe2a32"}, {"text": "issue 347", "href": "https://github.com/scrapy/scrapy/issues/347"}, {"text": "issue 352", "href": "https://github.com/scrapy/scrapy/issues/352"}, {"text": "issue 359", "href": "https://github.com/scrapy/scrapy/issues/359"}, {"text": "issue 12", "href": "https://github.com/scrapy/scrapy/issues/12"}, {"text": "issue 19", "href": "https://github.com/scrapy/scrapy/issues/19"}, {"text": "commit 4dc76e", "href": "https://github.com/scrapy/scrapy/commit/4dc76e"}, {"text": "issue 24", "href": "https://github.com/scrapy/scrapy/issues/24"}, {"text": "issue 59", "href": "https://github.com/scrapy/scrapy/issues/59"}, {"text": "issue 66", "href": "https://github.com/scrapy/scrapy/issues/66"}, {"text": "issue 77", "href": "https://github.com/scrapy/scrapy/issues/77"}, {"text": "issue 105", "href": "https://github.com/scrapy/scrapy/issues/105"}, {"text": "issue 78", "href": "https://github.com/scrapy/scrapy/issues/78"}, {"text": "issue 109", "href": "https://github.com/scrapy/scrapy/issues/109"}, {"text": "issue 318", "href": "https://github.com/scrapy/scrapy/issues/318"}, {"text": "issue 185", "href": "https://github.com/scrapy/scrapy/issues/185"}, {"text": "issue 199", "href": "https://github.com/scrapy/scrapy/issues/199"}, {"text": "issue 205", "href": "https://github.com/scrapy/scrapy/issues/205"}, {"text": "issue 206", "href": "https://github.com/scrapy/scrapy/issues/206"}, {"text": "issue 212", "href": "https://github.com/scrapy/scrapy/issues/212"}, {"text": "issue 214", "href": "https://github.com/scrapy/scrapy/issues/214"}, {"text": "issue 217", "href": "https://github.com/scrapy/scrapy/issues/217"}, {"text": "issue 218", "href": "https://github.com/scrapy/scrapy/issues/218"}, {"text": "issue 221", "href": "https://github.com/scrapy/scrapy/issues/221"}, {"text": "issue 260", "href": "https://github.com/scrapy/scrapy/issues/260"}, {"text": "issue 261", "href": "https://github.com/scrapy/scrapy/issues/261"}, {"text": "issue 269", "href": "https://github.com/scrapy/scrapy/issues/269"}, {"text": "issue 271", "href": "https://github.com/scrapy/scrapy/issues/271"}, {"text": "issue 290", "href": "https://github.com/scrapy/scrapy/issues/290"}, {"text": "issue 297", "href": "https://github.com/scrapy/scrapy/issues/297"}, {"text": "issue 329", "href": "https://github.com/scrapy/scrapy/issues/329"}, {"text": "Benchmarking", "href": "topics/benchmarking.html#benchmarking"}, {"text": "queuelib", "href": "https://github.com/scrapy/queuelib"}, {"text": "issue 260", "href": "https://github.com/scrapy/scrapy/issues/260"}, {"text": "XPathSelector.remove_namespaces", "href": "topics/selectors.html#scrapy.selector.Selector.remove_namespaces"}, {"text": "Selectors", "href": "topics/selectors.html#topics-selectors"}, {"text": "commit 8c4fcee", "href": "https://github.com/scrapy/scrapy/commit/8c4fcee"}, {"text": "commit 40667cb", "href": "https://github.com/scrapy/scrapy/commit/40667cb"}, {"text": "commit bd58bfa", "href": "https://github.com/scrapy/scrapy/commit/bd58bfa"}, {"text": "commit e3d6945", "href": "https://github.com/scrapy/scrapy/commit/e3d6945"}, {"text": "commit a274276", "href": "https://github.com/scrapy/scrapy/commit/a274276"}, {"text": "commit 6d2b3aa", "href": "https://github.com/scrapy/scrapy/commit/6d2b3aa"}, {"text": "commit c90de33", "href": "https://github.com/scrapy/scrapy/commit/c90de33"}, {"text": "commit c16150c", "href": "https://github.com/scrapy/scrapy/commit/c16150c"}, {"text": "commit 56b45fc", "href": "https://github.com/scrapy/scrapy/commit/56b45fc"}, {"text": "commit 243be84", "href": "https://github.com/scrapy/scrapy/commit/243be84"}, {"text": "commit 1fbb715", "href": "https://github.com/scrapy/scrapy/commit/1fbb715"}, {"text": "commit c72e682", "href": "https://github.com/scrapy/scrapy/commit/c72e682"}, {"text": "commit 28eac7a", "href": "https://github.com/scrapy/scrapy/commit/28eac7a"}, {"text": "commit 487b9b5", "href": "https://github.com/scrapy/scrapy/commit/487b9b5"}, {"text": "commit 8232569", "href": "https://github.com/scrapy/scrapy/commit/8232569"}, {"text": "commit 8dcf8aa", "href": "https://github.com/scrapy/scrapy/commit/8dcf8aa"}, {"text": "commit 7b5310d", "href": "https://github.com/scrapy/scrapy/commit/7b5310d"}, {"text": "commit 80f9bb6", "href": "https://github.com/scrapy/scrapy/commit/80f9bb6"}, {"text": "commit 2aa491b", "href": "https://github.com/scrapy/scrapy/commit/2aa491b"}, {"text": "commit bdf61c4", "href": "https://github.com/scrapy/scrapy/commit/bdf61c4"}, {"text": "commit 7184094", "href": "https://github.com/scrapy/scrapy/commit/7184094"}, {"text": "commit a4a9199", "href": "https://github.com/scrapy/scrapy/commit/a4a9199"}, {"text": "commit ec41673", "href": "https://github.com/scrapy/scrapy/commit/ec41673"}, {"text": "commit 86635e4", "href": "https://github.com/scrapy/scrapy/commit/86635e4"}, {"text": "commit c9b690d", "href": "https://github.com/scrapy/scrapy/commit/c9b690d"}, {"text": "commit dd55067", "href": "https://github.com/scrapy/scrapy/commit/dd55067"}, {"text": "commit 58998f4", "href": "https://github.com/scrapy/scrapy/commit/58998f4"}, {"text": "commit 8c780fd", "href": "https://github.com/scrapy/scrapy/commit/8c780fd"}, {"text": "commit 3403089", "href": "https://github.com/scrapy/scrapy/commit/3403089"}, {"text": "commit c4da0b5", "href": "https://github.com/scrapy/scrapy/commit/c4da0b5"}, {"text": "commit d52c188", "href": "https://github.com/scrapy/scrapy/commit/d52c188"}, {"text": "commit fa4f7f9", "href": "https://github.com/scrapy/scrapy/commit/fa4f7f9"}, {"text": "commit e292246", "href": "https://github.com/scrapy/scrapy/commit/e292246"}, {"text": "Spiders Contracts", "href": "topics/contracts.html#topics-contracts"}, {"text": "runspider", "href": "topics/commands.html#std-command-runspider"}, {"text": "AutoThrottle extension", "href": "topics/autothrottle.html"}, {"text": "AUTOTHROTTLE_ENABLED", "href": "topics/autothrottle.html#std-setting-AUTOTHROTTLE_ENABLED"}, {"text": "process_start_requests()", "href": "topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_start_requests"}, {"text": "Core API", "href": "topics/api.html#topics-api"}, {"text": "lxml", "href": "https://lxml.de/"}, {"text": "ClientForm", "href": "http://wwwsearch.sourceforge.net/old/ClientForm/"}, {"text": "commit 10ed28b", "href": "https://github.com/scrapy/scrapy/commit/10ed28b"}, {"text": "commit fe2ce93", "href": "https://github.com/scrapy/scrapy/commit/fe2ce93"}, {"text": "cookiejar", "href": "topics/downloader-middleware.html#std-reqmeta-cookiejar"}, {"text": "w3lib.encoding", "href": "https://github.com/scrapy/w3lib/blob/master/w3lib/encoding.py"}, {"text": "https://blog.scrapinghub.com/2012/02/27/scrapy-0-15-dropping-support-for-python-2-5/", "href": "https://blog.scrapinghub.com/2012/02/27/scrapy-0-15-dropping-support-for-python-2-5/"}, {"text": "REFERER_ENABLED", "href": "topics/spider-middleware.html#std-setting-REFERER_ENABLED"}, {"text": "w3lib", "href": "https://github.com/scrapy/w3lib"}, {"text": "DjangoItem", "href": "topics/djangoitem.html#topics-djangoitem"}, {"text": "issue 164", "href": "https://github.com/scrapy/scrapy/issues/164"}, {"text": "commit dcef7b0", "href": "https://github.com/scrapy/scrapy/commit/dcef7b0"}, {"text": "DOWNLOAD_HANDLERS", "href": "topics/settings.html#std-setting-DOWNLOAD_HANDLERS"}, {"text": "resource", "href": "https://docs.python.org/2/library/resource.html"}, {"text": "trackrefs", "href": "topics/leaks.html#topics-leaks-trackrefs"}, {"text": "commit b7e46df", "href": "https://github.com/scrapy/scrapy/commit/b7e46df"}, {"text": "https://groups.google.com/forum/#!topic/scrapy-users/qgVBmFybNAQ/discussion", "href": "https://groups.google.com/forum/#!topic/scrapy-users/qgVBmFybNAQ/discussion"}, {"text": "commit 340fbdb", "href": "https://github.com/scrapy/scrapy/commit/340fbdb"}, {"text": "commit 0cb68af", "href": "https://github.com/scrapy/scrapy/commit/0cb68af"}, {"text": "commit 4d17048", "href": "https://github.com/scrapy/scrapy/commit/4d17048"}, {"text": "commit b7b2e7f", "href": "https://github.com/scrapy/scrapy/commit/b7b2e7f"}, {"text": "commit fd85f9c", "href": "https://github.com/scrapy/scrapy/commit/fd85f9c"}, {"text": "commit c897793", "href": "https://github.com/scrapy/scrapy/commit/c897793"}, {"text": "commit 2548dcc", "href": "https://github.com/scrapy/scrapy/commit/2548dcc"}, {"text": "commit 668e352", "href": "https://github.com/scrapy/scrapy/commit/668e352"}, {"text": "commit 8e9f607", "href": "https://github.com/scrapy/scrapy/commit/8e9f607"}, {"text": "commit b830e95", "href": "https://github.com/scrapy/scrapy/commit/b830e95"}, {"text": "commit bf3c9ee", "href": "https://github.com/scrapy/scrapy/commit/bf3c9ee"}, {"text": "commit ba14f38", "href": "https://github.com/scrapy/scrapy/commit/ba14f38"}, {"text": "commit 0665175", "href": "https://github.com/scrapy/scrapy/commit/0665175"}, {"text": "commit 6a5bef2", "href": "https://github.com/scrapy/scrapy/commit/6a5bef2"}, {"text": "commit 9817df1", "href": "https://github.com/scrapy/scrapy/commit/9817df1"}, {"text": "commit 673a120", "href": "https://github.com/scrapy/scrapy/commit/673a120"}, {"text": "commit 11133e9", "href": "https://github.com/scrapy/scrapy/commit/11133e9"}, {"text": "commit 1423140", "href": "https://github.com/scrapy/scrapy/commit/1423140"}, {"text": "commit 0de3fb4", "href": "https://github.com/scrapy/scrapy/commit/0de3fb4"}, {"text": "commit 454a21d", "href": "https://github.com/scrapy/scrapy/commit/454a21d"}, {"text": "commit 2fbd662", "href": "https://github.com/scrapy/scrapy/commit/2fbd662"}, {"text": "commit 0a070f5", "href": "https://github.com/scrapy/scrapy/commit/0a070f5"}, {"text": "commit 2b4e4c3", "href": "https://github.com/scrapy/scrapy/commit/2b4e4c3"}, {"text": "commit caffe0e", "href": "https://github.com/scrapy/scrapy/commit/caffe0e"}, {"text": "commit caffe0e", "href": "https://github.com/scrapy/scrapy/commit/caffe0e"}, {"text": "commit 6cb9e1c", "href": "https://github.com/scrapy/scrapy/commit/6cb9e1c"}, {"text": "commit 4b86bd6", "href": "https://github.com/scrapy/scrapy/commit/4b86bd6"}, {"text": "commit 1aeccdd", "href": "https://github.com/scrapy/scrapy/commit/1aeccdd"}, {"text": "commit 8bf19e6", "href": "https://github.com/scrapy/scrapy/commit/8bf19e6"}, {"text": "commit 14a8e6e", "href": "https://github.com/scrapy/scrapy/commit/14a8e6e"}, {"text": "commit 5223575", "href": "https://github.com/scrapy/scrapy/commit/5223575"}, {"text": "commit 63d583d", "href": "https://github.com/scrapy/scrapy/commit/63d583d"}, {"text": "commit bcb3198", "href": "https://github.com/scrapy/scrapy/commit/bcb3198"}, {"text": "commit 98f3f87", "href": "https://github.com/scrapy/scrapy/commit/98f3f87"}, {"text": "commit 175a4b5", "href": "https://github.com/scrapy/scrapy/commit/175a4b5"}, {"text": "AJAX crawlable urls", "href": "https://developers.google.com/search/docs/ajax-crawling/docs/getting-started?csw=1"}, {"text": "r2737", "href": "http://hg.scrapy.org/scrapy/changeset/2737"}, {"text": "r2779", "href": "http://hg.scrapy.org/scrapy/changeset/2779"}, {"text": "r2783", "href": "http://hg.scrapy.org/scrapy/changeset/2783"}, {"text": "chunked transfer encoding", "href": "https://en.wikipedia.org/wiki/Chunked_transfer_encoding"}, {"text": "r2769", "href": "http://hg.scrapy.org/scrapy/changeset/2769"}, {"text": "r2763", "href": "http://hg.scrapy.org/scrapy/changeset/2763"}, {"text": "marshal", "href": "https://docs.python.org/2/library/marshal.html"}, {"text": "r2744", "href": "http://hg.scrapy.org/scrapy/changeset/2744"}, {"text": "r2738", "href": "http://hg.scrapy.org/scrapy/changeset/2738"}, {"text": "r2732", "href": "http://hg.scrapy.org/scrapy/changeset/2732"}, {"text": "CONCURRENT_REQUESTS", "href": "topics/settings.html#std-setting-CONCURRENT_REQUESTS"}, {"text": "CONCURRENT_REQUESTS_PER_DOMAIN", "href": "topics/settings.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"}, {"text": "CONCURRENT_REQUESTS_PER_IP", "href": "topics/settings.html#std-setting-CONCURRENT_REQUESTS_PER_IP"}, {"text": "r2728", "href": "http://hg.scrapy.org/scrapy/changeset/2728"}, {"text": "https://github.com/scrapinghub/scaws", "href": "https://github.com/scrapinghub/scaws"}, {"text": "r2706", "href": "http://hg.scrapy.org/scrapy/changeset/2706"}, {"text": "r2714", "href": "http://hg.scrapy.org/scrapy/changeset/2714"}, {"text": "r2708", "href": "http://hg.scrapy.org/scrapy/changeset/2708"}, {"text": "r2781", "href": "http://hg.scrapy.org/scrapy/changeset/2781"}, {"text": "r2704", "href": "http://hg.scrapy.org/scrapy/changeset/2704"}, {"text": "REDIRECT_ENABLED", "href": "topics/downloader-middleware.html#std-setting-REDIRECT_ENABLED"}, {"text": "r2697", "href": "http://hg.scrapy.org/scrapy/changeset/2697"}, {"text": "RETRY_ENABLED", "href": "topics/downloader-middleware.html#std-setting-RETRY_ENABLED"}, {"text": "r2694", "href": "http://hg.scrapy.org/scrapy/changeset/2694"}, {"text": "r2691", "href": "http://hg.scrapy.org/scrapy/changeset/2691"}, {"text": "r2690", "href": "http://hg.scrapy.org/scrapy/changeset/2690"}, {"text": "r2688", "href": "http://hg.scrapy.org/scrapy/changeset/2688"}, {"text": "r2658", "href": "http://hg.scrapy.org/scrapy/changeset/2658"}, {"text": "r2657", "href": "http://hg.scrapy.org/scrapy/changeset/2657"}, {"text": "r2643", "href": "http://hg.scrapy.org/scrapy/changeset/2643"}, {"text": "r2639", "href": "http://hg.scrapy.org/scrapy/changeset/2639"}, {"text": "r2636", "href": "http://hg.scrapy.org/scrapy/changeset/2636"}, {"text": "r2653", "href": "http://hg.scrapy.org/scrapy/changeset/2653"}, {"text": "r2631", "href": "http://hg.scrapy.org/scrapy/changeset/2631"}, {"text": "spider_error", "href": "topics/signals.html#std-signal-spider_error"}, {"text": "r2628", "href": "http://hg.scrapy.org/scrapy/changeset/2628"}, {"text": "COOKIES_ENABLED", "href": "topics/downloader-middleware.html#std-setting-COOKIES_ENABLED"}, {"text": "r2625", "href": "http://hg.scrapy.org/scrapy/changeset/2625"}, {"text": "STATS_DUMP", "href": "topics/settings.html#std-setting-STATS_DUMP"}, {"text": "r2599", "href": "http://hg.scrapy.org/scrapy/changeset/2599"}, {"text": "r2576", "href": "http://hg.scrapy.org/scrapy/changeset/2576"}, {"text": "r2571", "href": "http://hg.scrapy.org/scrapy/changeset/2571"}, {"text": "r2578", "href": "http://hg.scrapy.org/scrapy/changeset/2578"}, {"text": "r2552", "href": "http://hg.scrapy.org/scrapy/changeset/2552"}, {"text": "r2579", "href": "http://hg.scrapy.org/scrapy/changeset/2579"}, {"text": "r2630", "href": "http://hg.scrapy.org/scrapy/changeset/2630"}, {"text": "w3lib", "href": "https://github.com/scrapy/w3lib"}, {"text": "r2584", "href": "http://hg.scrapy.org/scrapy/changeset/2584"}, {"text": "scrapely", "href": "https://github.com/scrapy/scrapely"}, {"text": "r2586", "href": "http://hg.scrapy.org/scrapy/changeset/2586"}, {"text": "r2577", "href": "http://hg.scrapy.org/scrapy/changeset/2577"}, {"text": "https://github.com/scrapy/dirbot", "href": "https://github.com/scrapy/dirbot"}, {"text": "r2616", "href": "http://hg.scrapy.org/scrapy/changeset/2616"}, {"text": "r2632", "href": "http://hg.scrapy.org/scrapy/changeset/2632"}, {"text": "r2640", "href": "http://hg.scrapy.org/scrapy/changeset/2640"}, {"text": "r2704", "href": "http://hg.scrapy.org/scrapy/changeset/2704"}, {"text": "r2704", "href": "http://hg.scrapy.org/scrapy/changeset/2704"}, {"text": "r2780", "href": "http://hg.scrapy.org/scrapy/changeset/2780"}, {"text": "r2789", "href": "http://hg.scrapy.org/scrapy/changeset/2789"}, {"text": "r2717", "href": "http://hg.scrapy.org/scrapy/changeset/2717"}, {"text": "r2718", "href": "http://hg.scrapy.org/scrapy/changeset/2718"}, {"text": "CLOSESPIDER_ITEMCOUNT", "href": "topics/extensions.html#std-setting-CLOSESPIDER_ITEMCOUNT"}, {"text": "r2655", "href": "http://hg.scrapy.org/scrapy/changeset/2655"}, {"text": "item_passed", "href": "topics/signals.html#std-signal-item_scraped"}, {"text": "CLOSESPIDER_PAGECOUNT", "href": "topics/extensions.html#std-setting-CLOSESPIDER_PAGECOUNT"}, {"text": "CLOSESPIDER_ERRORCOUNT", "href": "topics/extensions.html#std-setting-CLOSESPIDER_ERRORCOUNT"}, {"text": "http://localhost:6800", "href": "http://localhost:6800"}, {"text": "r2065", "href": "http://hg.scrapy.org/scrapy/changeset/2065"}, {"text": "r2039", "href": "http://hg.scrapy.org/scrapy/changeset/2039"}, {"text": "r2053", "href": "http://hg.scrapy.org/scrapy/changeset/2053"}, {"text": "r1988", "href": "http://hg.scrapy.org/scrapy/changeset/1988"}, {"text": "r2054", "href": "http://hg.scrapy.org/scrapy/changeset/2054"}, {"text": "r2055", "href": "http://hg.scrapy.org/scrapy/changeset/2055"}, {"text": "r2056", "href": "http://hg.scrapy.org/scrapy/changeset/2056"}, {"text": "r2057", "href": "http://hg.scrapy.org/scrapy/changeset/2057"}, {"text": "r2011", "href": "http://hg.scrapy.org/scrapy/changeset/2011"}, {"text": "r1961", "href": "http://hg.scrapy.org/scrapy/changeset/1961"}, {"text": "r1969", "href": "http://hg.scrapy.org/scrapy/changeset/1969"}, {"text": "r1956", "href": "http://hg.scrapy.org/scrapy/changeset/1956"}, {"text": "r1923", "href": "http://hg.scrapy.org/scrapy/changeset/1923"}, {"text": "r1955", "href": "http://hg.scrapy.org/scrapy/changeset/1955"}, {"text": "r1960", "href": "http://hg.scrapy.org/scrapy/changeset/1960"}, {"text": "r2022", "href": "http://hg.scrapy.org/scrapy/changeset/2022"}, {"text": "r2023", "href": "http://hg.scrapy.org/scrapy/changeset/2023"}, {"text": "r2024", "href": "http://hg.scrapy.org/scrapy/changeset/2024"}, {"text": "r2025", "href": "http://hg.scrapy.org/scrapy/changeset/2025"}, {"text": "r2026", "href": "http://hg.scrapy.org/scrapy/changeset/2026"}, {"text": "r2027", "href": "http://hg.scrapy.org/scrapy/changeset/2027"}, {"text": "r2028", "href": "http://hg.scrapy.org/scrapy/changeset/2028"}, {"text": "r2029", "href": "http://hg.scrapy.org/scrapy/changeset/2029"}, {"text": "r2030", "href": "http://hg.scrapy.org/scrapy/changeset/2030"}, {"text": "r2047", "href": "http://hg.scrapy.org/scrapy/changeset/2047"}, {"text": "r2050", "href": "http://hg.scrapy.org/scrapy/changeset/2050"}, {"text": "r1975", "href": "http://hg.scrapy.org/scrapy/changeset/1975"}, {"text": "r1961", "href": "http://hg.scrapy.org/scrapy/changeset/1961"}, {"text": "r2006", "href": "http://hg.scrapy.org/scrapy/changeset/2006"}, {"text": "r2035", "href": "http://hg.scrapy.org/scrapy/changeset/2035"}, {"text": "r2036", "href": "http://hg.scrapy.org/scrapy/changeset/2036"}, {"text": "r2037", "href": "http://hg.scrapy.org/scrapy/changeset/2037"}, {"text": "r2034", "href": "http://hg.scrapy.org/scrapy/changeset/2034"}, {"text": "r2039", "href": "http://hg.scrapy.org/scrapy/changeset/2039"}, {"text": "r2033", "href": "http://hg.scrapy.org/scrapy/changeset/2033"}, {"text": "r2047", "href": "http://hg.scrapy.org/scrapy/changeset/2047"}, {"text": "r1939", "href": "http://hg.scrapy.org/scrapy/changeset/1939"}, {"text": "r1809", "href": "http://hg.scrapy.org/scrapy/changeset/1809"}, {"text": "r1813", "href": "http://hg.scrapy.org/scrapy/changeset/1813"}, {"text": "r1816", "href": "http://hg.scrapy.org/scrapy/changeset/1816"}, {"text": "r1802", "href": "http://hg.scrapy.org/scrapy/changeset/1802"}, {"text": "r1803", "href": "http://hg.scrapy.org/scrapy/changeset/1803"}, {"text": "r1781", "href": "http://hg.scrapy.org/scrapy/changeset/1781"}, {"text": "r1785", "href": "http://hg.scrapy.org/scrapy/changeset/1785"}, {"text": "r1841", "href": "http://hg.scrapy.org/scrapy/changeset/1841"}, {"text": "r1804", "href": "http://hg.scrapy.org/scrapy/changeset/1804"}, {"text": "r1838", "href": "http://hg.scrapy.org/scrapy/changeset/1838"}, {"text": "r1836", "href": "http://hg.scrapy.org/scrapy/changeset/1836"}, {"text": "r1822", "href": "http://hg.scrapy.org/scrapy/changeset/1822"}, {"text": "r1822", "href": "http://hg.scrapy.org/scrapy/changeset/1822"}, {"text": "r1827", "href": "http://hg.scrapy.org/scrapy/changeset/1827"}, {"text": "r1849", "href": "http://hg.scrapy.org/scrapy/changeset/1849"}, {"text": "r1833", "href": "http://hg.scrapy.org/scrapy/changeset/1833"}, {"text": "r1840", "href": "http://hg.scrapy.org/scrapy/changeset/1840"}, {"text": "r1830", "href": "http://hg.scrapy.org/scrapy/changeset/1830"}, {"text": "r1844", "href": "http://hg.scrapy.org/scrapy/changeset/1844"}, {"text": "r1830", "href": "http://hg.scrapy.org/scrapy/changeset/1830"}, {"text": "r1843", "href": "http://hg.scrapy.org/scrapy/changeset/1843"}, {"text": "r1859", "href": "http://hg.scrapy.org/scrapy/changeset/1859"}, {"text": "r1861", "href": "http://hg.scrapy.org/scrapy/changeset/1861"}, {"text": "r1865", "href": "http://hg.scrapy.org/scrapy/changeset/1865"}], "timestamp": "2023-10-12T21:19:13.045356", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/api.html", "content": {"sections": [], "paragraphs": ["<p>This section documents the Scrapy core API, and it’s intended for developers of\nextensions and middlewares.</p>", "<p>The main entry point to Scrapy API is the <a class=\"reference internal\" href=\"#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a>\nobject, passed to extensions through the <code class=\"docutils literal notranslate\"><span class=\"pre\">from_crawler</span></code> class method. This\nobject provides access to all Scrapy core components, and it’s the only way for\nextensions to access them and hook their functionality into Scrapy.</p>", "<p>The Extension Manager is responsible for loading and keeping track of installed\nextensions and it’s configured through the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-EXTENSIONS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">EXTENSIONS</span></code></a> setting which\ncontains a dictionary of all available extensions and their order similar to\nhow you <a class=\"hoverxref tooltip reference internal\" href=\"downloader-middleware.html#topics-downloader-middleware-setting\"><span class=\"std std-ref\">configure the downloader middlewares</span></a>.</p>", "<p>The Crawler object must be instantiated with a\n<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.Spider</span></code></a> subclass and a\n<a class=\"reference internal\" href=\"#scrapy.settings.Settings\" title=\"scrapy.settings.Settings\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.settings.Settings</span></code></a> object.</p>", "<p>The request fingerprint builder of this crawler.</p>", "<p>This is used from extensions and middlewares to build short, unique\nidentifiers for requests. See <a class=\"hoverxref tooltip reference internal\" href=\"request-response.html#request-fingerprints\"><span class=\"std std-ref\">Request fingerprints</span></a>.</p>", "<p>The settings manager of this crawler.</p>", "<p>This is used by extensions &amp; middlewares to access the Scrapy settings\nof this crawler.</p>", "<p>For an introduction on Scrapy settings see <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#topics-settings\"><span class=\"std std-ref\">Settings</span></a>.</p>", "<p>For the API see <a class=\"reference internal\" href=\"#scrapy.settings.Settings\" title=\"scrapy.settings.Settings\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Settings</span></code></a> class.</p>", "<p>The signals manager of this crawler.</p>", "<p>This is used by extensions &amp; middlewares to hook themselves into Scrapy\nfunctionality.</p>", "<p>For an introduction on signals see <a class=\"hoverxref tooltip reference internal\" href=\"signals.html#topics-signals\"><span class=\"std std-ref\">Signals</span></a>.</p>", "<p>For the API see <a class=\"reference internal\" href=\"#scrapy.signalmanager.SignalManager\" title=\"scrapy.signalmanager.SignalManager\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">SignalManager</span></code></a> class.</p>", "<p>The stats collector of this crawler.</p>", "<p>This is used from extensions &amp; middlewares to record stats of their\nbehaviour, or access stats collected by other extensions.</p>", "<p>For an introduction on stats collection see <a class=\"hoverxref tooltip reference internal\" href=\"stats.html#topics-stats\"><span class=\"std std-ref\">Stats Collection</span></a>.</p>", "<p>For the API see <a class=\"reference internal\" href=\"#scrapy.statscollectors.StatsCollector\" title=\"scrapy.statscollectors.StatsCollector\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">StatsCollector</span></code></a> class.</p>", "<p>The extension manager that keeps track of enabled extensions.</p>", "<p>Most extensions won’t need to access this attribute.</p>", "<p>For an introduction on extensions and a list of available extensions on\nScrapy see <a class=\"hoverxref tooltip reference internal\" href=\"extensions.html#topics-extensions\"><span class=\"std std-ref\">Extensions</span></a>.</p>", "<p>The execution engine, which coordinates the core crawling logic\nbetween the scheduler, downloader and spiders.</p>", "<p>Some extension may want to access the Scrapy engine, to inspect  or\nmodify the downloader and scheduler behaviour, although this is an\nadvanced use and this API is not yet stable.</p>", "<p>Spider currently being crawled. This is an instance of the spider class\nprovided while constructing the crawler, and it is created after the\narguments given in the <a class=\"reference internal\" href=\"#scrapy.crawler.Crawler.crawl\" title=\"scrapy.crawler.Crawler.crawl\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">crawl()</span></code></a> method.</p>", "<p>Starts the crawler by instantiating its spider class with the given\n<code class=\"docutils literal notranslate\"><span class=\"pre\">args</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">kwargs</span></code> arguments, while setting the execution engine in\nmotion. Should be called only once.</p>", "<p>Returns a deferred that is fired when the crawl is finished.</p>", "<p>Starts a graceful stop of the crawler and returns a deferred that is\nfired when the crawler is stopped.</p>", "<p>This is a convenient helper class that keeps track of, manages and runs\ncrawlers inside an already setup <a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html\" title=\"(in Twisted)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">reactor</span></code></a>.</p>", "<p>The CrawlerRunner object must be instantiated with a\n<a class=\"reference internal\" href=\"#scrapy.settings.Settings\" title=\"scrapy.settings.Settings\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Settings</span></code></a> object.</p>", "<p>This class shouldn’t be needed (since Scrapy is responsible of using it\naccordingly) unless writing scripts that manually handle the crawling\nprocess. See <a class=\"hoverxref tooltip reference internal\" href=\"practices.html#run-from-script\"><span class=\"std std-ref\">Run Scrapy from a script</span></a> for an example.</p>", "<p>Run a crawler with the provided arguments.</p>", "<p>It will call the given Crawler’s <a class=\"reference internal\" href=\"#scrapy.crawler.Crawler.crawl\" title=\"scrapy.crawler.Crawler.crawl\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">crawl()</span></code></a> method, while\nkeeping track of it so it can be stopped later.</p>", "<p>If <code class=\"docutils literal notranslate\"><span class=\"pre\">crawler_or_spidercls</span></code> isn’t a <a class=\"reference internal\" href=\"#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a>\ninstance, this method will try to create one using this parameter as\nthe spider class given to it.</p>", "<p>Returns a deferred that is fired when the crawling is finished.</p>", "<p><strong>crawler_or_spidercls</strong> (<a class=\"reference internal\" href=\"#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a> instance,\n<a class=\"reference internal\" href=\"spiders.html#scrapy.spiders.Spider\" title=\"scrapy.spiders.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> subclass or string) – already created crawler, or a spider class\nor spider’s name inside the project to create it</p>", "<p><strong>args</strong> – arguments to initialize the spider</p>", "<p><strong>kwargs</strong> – keyword arguments to initialize the spider</p>", "<p>Set of <a class=\"reference internal\" href=\"#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">crawlers</span></code></a> started by <a class=\"reference internal\" href=\"#scrapy.crawler.CrawlerRunner.crawl\" title=\"scrapy.crawler.CrawlerRunner.crawl\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">crawl()</span></code></a> and managed by this class.</p>", "<p>Return a <a class=\"reference internal\" href=\"#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a> object.</p>", "<p>If <code class=\"docutils literal notranslate\"><span class=\"pre\">crawler_or_spidercls</span></code> is a Crawler, it is returned as-is.</p>", "<p>If <code class=\"docutils literal notranslate\"><span class=\"pre\">crawler_or_spidercls</span></code> is a Spider subclass, a new Crawler\nis constructed for it.</p>", "<p>If <code class=\"docutils literal notranslate\"><span class=\"pre\">crawler_or_spidercls</span></code> is a string, this function finds\na spider with this name in a Scrapy project (using spider loader),\nthen creates a Crawler instance for it.</p>", "<p>Returns a deferred that is fired when all managed <a class=\"reference internal\" href=\"#scrapy.crawler.CrawlerRunner.crawlers\" title=\"scrapy.crawler.CrawlerRunner.crawlers\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">crawlers</span></code></a> have\ncompleted their executions.</p>", "<p>Stops simultaneously all the crawling jobs taking place.</p>", "<p>Returns a deferred that is fired when they all have ended.</p>", "<p>Bases: <a class=\"reference internal\" href=\"#scrapy.crawler.CrawlerRunner\" title=\"scrapy.crawler.CrawlerRunner\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CrawlerRunner</span></code></a></p>", "<p>A class to run multiple scrapy crawlers in a process simultaneously.</p>", "<p>This class extends <a class=\"reference internal\" href=\"#scrapy.crawler.CrawlerRunner\" title=\"scrapy.crawler.CrawlerRunner\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CrawlerRunner</span></code></a> by adding support\nfor starting a <a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html\" title=\"(in Twisted)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">reactor</span></code></a> and handling shutdown\nsignals, like the keyboard interrupt command Ctrl-C. It also configures\ntop-level logging.</p>", "<p>This utility should be a better fit than\n<a class=\"reference internal\" href=\"#scrapy.crawler.CrawlerRunner\" title=\"scrapy.crawler.CrawlerRunner\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CrawlerRunner</span></code></a> if you aren’t running another\n<a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html\" title=\"(in Twisted)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">reactor</span></code></a> within your application.</p>", "<p>The CrawlerProcess object must be instantiated with a\n<a class=\"reference internal\" href=\"#scrapy.settings.Settings\" title=\"scrapy.settings.Settings\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Settings</span></code></a> object.</p>", "<p><strong>install_root_handler</strong> – whether to install root logging handler\n(default: True)</p>", "<p>This class shouldn’t be needed (since Scrapy is responsible of using it\naccordingly) unless writing scripts that manually handle the crawling\nprocess. See <a class=\"hoverxref tooltip reference internal\" href=\"practices.html#run-from-script\"><span class=\"std std-ref\">Run Scrapy from a script</span></a> for an example.</p>", "<p>Run a crawler with the provided arguments.</p>", "<p>It will call the given Crawler’s <a class=\"reference internal\" href=\"#scrapy.crawler.Crawler.crawl\" title=\"scrapy.crawler.Crawler.crawl\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">crawl()</span></code></a> method, while\nkeeping track of it so it can be stopped later.</p>", "<p>If <code class=\"docutils literal notranslate\"><span class=\"pre\">crawler_or_spidercls</span></code> isn’t a <a class=\"reference internal\" href=\"#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a>\ninstance, this method will try to create one using this parameter as\nthe spider class given to it.</p>", "<p>Returns a deferred that is fired when the crawling is finished.</p>", "<p><strong>crawler_or_spidercls</strong> (<a class=\"reference internal\" href=\"#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a> instance,\n<a class=\"reference internal\" href=\"spiders.html#scrapy.spiders.Spider\" title=\"scrapy.spiders.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> subclass or string) – already created crawler, or a spider class\nor spider’s name inside the project to create it</p>", "<p><strong>args</strong> – arguments to initialize the spider</p>", "<p><strong>kwargs</strong> – keyword arguments to initialize the spider</p>", "<p>Set of <a class=\"reference internal\" href=\"#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">crawlers</span></code></a> started by <a class=\"reference internal\" href=\"#scrapy.crawler.CrawlerProcess.crawl\" title=\"scrapy.crawler.CrawlerProcess.crawl\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">crawl()</span></code></a> and managed by this class.</p>", "<p>Return a <a class=\"reference internal\" href=\"#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a> object.</p>", "<p>If <code class=\"docutils literal notranslate\"><span class=\"pre\">crawler_or_spidercls</span></code> is a Crawler, it is returned as-is.</p>", "<p>If <code class=\"docutils literal notranslate\"><span class=\"pre\">crawler_or_spidercls</span></code> is a Spider subclass, a new Crawler\nis constructed for it.</p>", "<p>If <code class=\"docutils literal notranslate\"><span class=\"pre\">crawler_or_spidercls</span></code> is a string, this function finds\na spider with this name in a Scrapy project (using spider loader),\nthen creates a Crawler instance for it.</p>", "<p>Returns a deferred that is fired when all managed <a class=\"reference internal\" href=\"#scrapy.crawler.CrawlerProcess.crawlers\" title=\"scrapy.crawler.CrawlerProcess.crawlers\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">crawlers</span></code></a> have\ncompleted their executions.</p>", "<p>This method starts a <a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html\" title=\"(in Twisted)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">reactor</span></code></a>, adjusts its pool\nsize to <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-REACTOR_THREADPOOL_MAXSIZE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">REACTOR_THREADPOOL_MAXSIZE</span></code></a>, and installs a DNS cache\nbased on <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-DNSCACHE_ENABLED\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DNSCACHE_ENABLED</span></code></a> and <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-DNSCACHE_SIZE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DNSCACHE_SIZE</span></code></a>.</p>", "<p>If <code class=\"docutils literal notranslate\"><span class=\"pre\">stop_after_crawl</span></code> is True, the reactor will be stopped after all\ncrawlers have finished, using <a class=\"reference internal\" href=\"#scrapy.crawler.CrawlerProcess.join\" title=\"scrapy.crawler.CrawlerProcess.join\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">join()</span></code></a>.</p>", "<p><strong>stop_after_crawl</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.11)\"><em>bool</em></a>) – stop or not the reactor when all\ncrawlers have finished</p>", "<p><strong>install_signal_handlers</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.11)\"><em>bool</em></a>) – whether to install the shutdown\nhandlers (default: True)</p>", "<p>Stops simultaneously all the crawling jobs taking place.</p>", "<p>Returns a deferred that is fired when they all have ended.</p>", "<p>Dictionary that sets the key name and priority level of the default\nsettings priorities used in Scrapy.</p>", "<p>Each item defines a settings entry point, giving it a code name for\nidentification and an integer priority. Greater priorities take more\nprecedence over lesser ones when setting and retrieving values in the\n<a class=\"reference internal\" href=\"#scrapy.settings.Settings\" title=\"scrapy.settings.Settings\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Settings</span></code></a> class.</p>", "<p>For a detailed explanation on each settings sources, see:\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#topics-settings\"><span class=\"std std-ref\">Settings</span></a>.</p>", "<p>Small helper function that looks up a given string priority in the\n<a class=\"reference internal\" href=\"#scrapy.settings.SETTINGS_PRIORITIES\" title=\"scrapy.settings.SETTINGS_PRIORITIES\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">SETTINGS_PRIORITIES</span></code></a> dictionary and returns its\nnumerical value, or directly returns a given numerical priority.</p>", "<p>Bases: <a class=\"reference internal\" href=\"#scrapy.settings.BaseSettings\" title=\"scrapy.settings.BaseSettings\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">BaseSettings</span></code></a></p>", "<p>This object stores Scrapy settings for the configuration of internal\ncomponents, and can be used for any further customization.</p>", "<p>It is a direct subclass and supports all methods of\n<a class=\"reference internal\" href=\"#scrapy.settings.BaseSettings\" title=\"scrapy.settings.BaseSettings\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">BaseSettings</span></code></a>. Additionally, after instantiation\nof this class, the new object will have the global default settings\ndescribed on <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#topics-settings-ref\"><span class=\"std std-ref\">Built-in settings reference</span></a> already populated.</p>", "<p>Instances of this class behave like dictionaries, but store priorities\nalong with their <code class=\"docutils literal notranslate\"><span class=\"pre\">(key,</span> <span class=\"pre\">value)</span></code> pairs, and can be frozen (i.e. marked\nimmutable).</p>", "<p>Key-value entries can be passed on initialization with the <code class=\"docutils literal notranslate\"><span class=\"pre\">values</span></code>\nargument, and they would take the <code class=\"docutils literal notranslate\"><span class=\"pre\">priority</span></code> level (unless <code class=\"docutils literal notranslate\"><span class=\"pre\">values</span></code> is\nalready an instance of <a class=\"reference internal\" href=\"#scrapy.settings.BaseSettings\" title=\"scrapy.settings.BaseSettings\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">BaseSettings</span></code></a>, in which\ncase the existing priority levels will be kept).  If the <code class=\"docutils literal notranslate\"><span class=\"pre\">priority</span></code>\nargument is a string, the priority name will be looked up in\n<a class=\"reference internal\" href=\"#scrapy.settings.SETTINGS_PRIORITIES\" title=\"scrapy.settings.SETTINGS_PRIORITIES\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">SETTINGS_PRIORITIES</span></code></a>. Otherwise, a specific integer\nshould be provided.</p>", "<p>Once the object is created, new settings can be loaded or updated with the\n<a class=\"reference internal\" href=\"#scrapy.settings.BaseSettings.set\" title=\"scrapy.settings.BaseSettings.set\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">set()</span></code></a> method, and can be accessed with\nthe square bracket notation of dictionaries, or with the\n<a class=\"reference internal\" href=\"#scrapy.settings.BaseSettings.get\" title=\"scrapy.settings.BaseSettings.get\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">get()</span></code></a> method of the instance and its\nvalue conversion variants. When requesting a stored key, the value with the\nhighest priority will be retrieved.</p>", "<p>Make a deep copy of current settings.</p>", "<p>This method returns a new instance of the <a class=\"reference internal\" href=\"#scrapy.settings.Settings\" title=\"scrapy.settings.Settings\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Settings</span></code></a> class,\npopulated with the same values and their priorities.</p>", "<p>Modifications to the new object won’t be reflected on the original\nsettings.</p>", "<p>Make a copy of current settings and convert to a dict.</p>", "<p>This method returns a new dict populated with the same values\nand their priorities as the current settings.</p>", "<p>Modifications to the returned dict won’t be reflected on the original\nsettings.</p>", "<p>This method can be useful for example for printing settings\nin Scrapy shell.</p>", "<p>Disable further changes to the current settings.</p>", "<p>After calling this method, the present state of the settings will become\nimmutable. Trying to change values through the <a class=\"reference internal\" href=\"#scrapy.settings.BaseSettings.set\" title=\"scrapy.settings.BaseSettings.set\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">set()</span></code></a> method and\nits variants won’t be possible and will be alerted.</p>", "<p>Return an immutable copy of the current settings.</p>", "<p>Alias for a <a class=\"reference internal\" href=\"#scrapy.settings.BaseSettings.freeze\" title=\"scrapy.settings.BaseSettings.freeze\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">freeze()</span></code></a> call in the object returned by <a class=\"reference internal\" href=\"#scrapy.settings.BaseSettings.copy\" title=\"scrapy.settings.BaseSettings.copy\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">copy()</span></code></a>.</p>", "<p>Get a setting value without affecting its original type.</p>", "<p><strong>name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><em>str</em></a>) – the setting name</p>", "<p><strong>default</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#object\" title=\"(in Python v3.11)\"><em>object</em></a>) – the value to return if no setting is found</p>", "<p>Get a setting value as a boolean.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">1</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">'1'</span></code>, <cite>True`</cite> and <code class=\"docutils literal notranslate\"><span class=\"pre\">'True'</span></code> return <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code>,\nwhile <code class=\"docutils literal notranslate\"><span class=\"pre\">0</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">'0'</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">'False'</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> return <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code>.</p>", "<p>For example, settings populated through environment variables set to\n<code class=\"docutils literal notranslate\"><span class=\"pre\">'0'</span></code> will return <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code> when using this method.</p>", "<p><strong>name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><em>str</em></a>) – the setting name</p>", "<p><strong>default</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#object\" title=\"(in Python v3.11)\"><em>object</em></a>) – the value to return if no setting is found</p>", "<p>Get a setting value as a dictionary. If the setting original type is a\ndictionary, a copy of it will be returned. If it is a string it will be\nevaluated as a JSON dictionary. In the case that it is a\n<a class=\"reference internal\" href=\"#scrapy.settings.BaseSettings\" title=\"scrapy.settings.BaseSettings\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">BaseSettings</span></code></a> instance itself, it will be\nconverted to a dictionary, containing all its current settings values\nas they would be returned by <a class=\"reference internal\" href=\"#scrapy.settings.BaseSettings.get\" title=\"scrapy.settings.BaseSettings.get\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">get()</span></code></a>,\nand losing all information about priority and mutability.</p>", "<p><strong>name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><em>str</em></a>) – the setting name</p>", "<p><strong>default</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#object\" title=\"(in Python v3.11)\"><em>object</em></a>) – the value to return if no setting is found</p>", "<p>Get a setting value as either a <a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">dict</span></code></a> or a <a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">list</span></code></a>.</p>", "<p>If the setting is already a dict or a list, a copy of it will be\nreturned.</p>", "<p>If it is a string it will be evaluated as JSON, or as a comma-separated\nlist of strings as a fallback.</p>", "<p>For example, settings populated from the command line will return:</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">{'key1':</span> <span class=\"pre\">'value1',</span> <span class=\"pre\">'key2':</span> <span class=\"pre\">'value2'}</span></code> if set to\n<code class=\"docutils literal notranslate\"><span class=\"pre\">'{\"key1\":</span> <span class=\"pre\">\"value1\",</span> <span class=\"pre\">\"key2\":</span> <span class=\"pre\">\"value2\"}'</span></code></p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">['one',</span> <span class=\"pre\">'two']</span></code> if set to <code class=\"docutils literal notranslate\"><span class=\"pre\">'[\"one\",</span> <span class=\"pre\">\"two\"]'</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">'one,two'</span></code></p>", "<p><strong>name</strong> (<em>string</em>) – the setting name</p>", "<p><strong>default</strong> (<em>any</em>) – the value to return if no setting is found</p>", "<p>Get a setting value as a float.</p>", "<p><strong>name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><em>str</em></a>) – the setting name</p>", "<p><strong>default</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#object\" title=\"(in Python v3.11)\"><em>object</em></a>) – the value to return if no setting is found</p>", "<p>Get a setting value as an int.</p>", "<p><strong>name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><em>str</em></a>) – the setting name</p>", "<p><strong>default</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#object\" title=\"(in Python v3.11)\"><em>object</em></a>) – the value to return if no setting is found</p>", "<p>Get a setting value as a list. If the setting original type is a list, a\ncopy of it will be returned. If it’s a string it will be split by “,”.</p>", "<p>For example, settings populated through environment variables set to\n<code class=\"docutils literal notranslate\"><span class=\"pre\">'one,two'</span></code> will return a list [‘one’, ‘two’] when using this method.</p>", "<p><strong>name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><em>str</em></a>) – the setting name</p>", "<p><strong>default</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#object\" title=\"(in Python v3.11)\"><em>object</em></a>) – the value to return if no setting is found</p>", "<p>Return the current numerical priority value of a setting, or <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> if\nthe given <code class=\"docutils literal notranslate\"><span class=\"pre\">name</span></code> does not exist.</p>", "<p><strong>name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><em>str</em></a>) – the setting name</p>", "<p>Get a composition of a dictionary-like setting and its <cite>_BASE</cite>\ncounterpart.</p>", "<p><strong>name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><em>str</em></a>) – name of the dictionary-like setting</p>", "<p>Return the numerical value of the highest priority present throughout\nall settings, or the numerical value for <code class=\"docutils literal notranslate\"><span class=\"pre\">default</span></code> from\n<a class=\"reference internal\" href=\"#scrapy.settings.SETTINGS_PRIORITIES\" title=\"scrapy.settings.SETTINGS_PRIORITIES\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">SETTINGS_PRIORITIES</span></code></a> if there are no settings\nstored.</p>", "<p>If key is not found, d is returned if given, otherwise KeyError is raised.</p>", "<p>Store a key/value attribute with a given priority.</p>", "<p>Settings should be populated <em>before</em> configuring the Crawler object\n(through the <code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">configure()</span></code> method),\notherwise they won’t have any effect.</p>", "<p><strong>name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><em>str</em></a>) – the setting name</p>", "<p><strong>value</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#object\" title=\"(in Python v3.11)\"><em>object</em></a>) – the value to associate with the setting</p>", "<p><strong>priority</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><em>str</em></a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.11)\"><em>int</em></a>) – the priority of the setting. Should be a key of\n<a class=\"reference internal\" href=\"#scrapy.settings.SETTINGS_PRIORITIES\" title=\"scrapy.settings.SETTINGS_PRIORITIES\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">SETTINGS_PRIORITIES</span></code></a> or an integer</p>", "<p>Store settings from a module with a given priority.</p>", "<p>This is a helper function that calls\n<a class=\"reference internal\" href=\"#scrapy.settings.BaseSettings.set\" title=\"scrapy.settings.BaseSettings.set\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">set()</span></code></a> for every globally declared\nuppercase variable of <code class=\"docutils literal notranslate\"><span class=\"pre\">module</span></code> with the provided <code class=\"docutils literal notranslate\"><span class=\"pre\">priority</span></code>.</p>", "<p><strong>module</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/types.html#types.ModuleType\" title=\"(in Python v3.11)\"><em>types.ModuleType</em></a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><em>str</em></a>) – the module or the path of the module</p>", "<p><strong>priority</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><em>str</em></a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.11)\"><em>int</em></a>) – the priority of the settings. Should be a key of\n<a class=\"reference internal\" href=\"#scrapy.settings.SETTINGS_PRIORITIES\" title=\"scrapy.settings.SETTINGS_PRIORITIES\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">SETTINGS_PRIORITIES</span></code></a> or an integer</p>", "<p>Store key/value pairs with a given priority.</p>", "<p>This is a helper function that calls\n<a class=\"reference internal\" href=\"#scrapy.settings.BaseSettings.set\" title=\"scrapy.settings.BaseSettings.set\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">set()</span></code></a> for every item of <code class=\"docutils literal notranslate\"><span class=\"pre\">values</span></code>\nwith the provided <code class=\"docutils literal notranslate\"><span class=\"pre\">priority</span></code>.</p>", "<p>If <code class=\"docutils literal notranslate\"><span class=\"pre\">values</span></code> is a string, it is assumed to be JSON-encoded and parsed\ninto a dict with <code class=\"docutils literal notranslate\"><span class=\"pre\">json.loads()</span></code> first. If it is a\n<a class=\"reference internal\" href=\"#scrapy.settings.BaseSettings\" title=\"scrapy.settings.BaseSettings\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">BaseSettings</span></code></a> instance, the per-key priorities\nwill be used and the <code class=\"docutils literal notranslate\"><span class=\"pre\">priority</span></code> parameter ignored. This allows\ninserting/updating settings with different priorities with a single\ncommand.</p>", "<p><strong>values</strong> (dict or string or <a class=\"reference internal\" href=\"#scrapy.settings.BaseSettings\" title=\"scrapy.settings.BaseSettings\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">BaseSettings</span></code></a>) – the settings names and values</p>", "<p><strong>priority</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><em>str</em></a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.11)\"><em>int</em></a>) – the priority of the settings. Should be a key of\n<a class=\"reference internal\" href=\"#scrapy.settings.SETTINGS_PRIORITIES\" title=\"scrapy.settings.SETTINGS_PRIORITIES\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">SETTINGS_PRIORITIES</span></code></a> or an integer</p>", "<p>This class is in charge of retrieving and handling the spider classes\ndefined across the project.</p>", "<p>Custom spider loaders can be employed by specifying their path in the\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-SPIDER_LOADER_CLASS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SPIDER_LOADER_CLASS</span></code></a> project setting. They must fully implement\nthe <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.interfaces.ISpiderLoader</span></code> interface to guarantee an\nerrorless execution.</p>", "<p>This class method is used by Scrapy to create an instance of the class.\nIt’s called with the current project settings, and it loads the spiders\nfound recursively in the modules of the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-SPIDER_MODULES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SPIDER_MODULES</span></code></a>\nsetting.</p>", "<p><strong>settings</strong> (<a class=\"reference internal\" href=\"#scrapy.settings.Settings\" title=\"scrapy.settings.Settings\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Settings</span></code></a> instance) – project settings</p>", "<p>Get the Spider class with the given name. It’ll look into the previously\nloaded spiders for a spider class with name <code class=\"docutils literal notranslate\"><span class=\"pre\">spider_name</span></code> and will raise\na KeyError if not found.</p>", "<p><strong>spider_name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><em>str</em></a>) – spider class name</p>", "<p>Get the names of the available spiders in the project.</p>", "<p>List the spiders’ names that can handle the given request. Will try to\nmatch the request’s url against the domains of the spiders.</p>", "<p><strong>request</strong> (<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> instance) – queried request</p>", "<p>Connect a receiver function to a signal.</p>", "<p>The signal can be any object, although Scrapy comes with some\npredefined signals that are documented in the <a class=\"hoverxref tooltip reference internal\" href=\"signals.html#topics-signals\"><span class=\"std std-ref\">Signals</span></a>\nsection.</p>", "<p><strong>receiver</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable\" title=\"(in Python v3.11)\"><em>collections.abc.Callable</em></a>) – the function to be connected</p>", "<p><strong>signal</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#object\" title=\"(in Python v3.11)\"><em>object</em></a>) – the signal to connect to</p>", "<p>Disconnect a receiver function from a signal. This has the\nopposite effect of the <a class=\"reference internal\" href=\"#scrapy.signalmanager.SignalManager.connect\" title=\"scrapy.signalmanager.SignalManager.connect\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">connect()</span></code></a> method, and the arguments\nare the same.</p>", "<p>Disconnect all receivers from the given signal.</p>", "<p><strong>signal</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#object\" title=\"(in Python v3.11)\"><em>object</em></a>) – the signal to disconnect from</p>", "<p>Send a signal, catch exceptions and log them.</p>", "<p>The keyword arguments are passed to the signal handlers (connected\nthrough the <a class=\"reference internal\" href=\"#scrapy.signalmanager.SignalManager.connect\" title=\"scrapy.signalmanager.SignalManager.connect\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">connect()</span></code></a> method).</p>", "<p>Like <a class=\"reference internal\" href=\"#scrapy.signalmanager.SignalManager.send_catch_log\" title=\"scrapy.signalmanager.SignalManager.send_catch_log\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">send_catch_log()</span></code></a> but supports returning\n<a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html\" title=\"(in Twisted)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Deferred</span></code></a> objects from signal handlers.</p>", "<p>Returns a Deferred that gets fired once all signal handlers\ndeferreds were fired. Send a signal, catch exceptions and log them.</p>", "<p>The keyword arguments are passed to the signal handlers (connected\nthrough the <a class=\"reference internal\" href=\"#scrapy.signalmanager.SignalManager.connect\" title=\"scrapy.signalmanager.SignalManager.connect\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">connect()</span></code></a> method).</p>", "<p>There are several Stats Collectors available under the\n<a class=\"reference internal\" href=\"#module-scrapy.statscollectors\" title=\"scrapy.statscollectors: Stats Collectors\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">scrapy.statscollectors</span></code></a> module and they all implement the Stats\nCollector API defined by the <a class=\"reference internal\" href=\"#scrapy.statscollectors.StatsCollector\" title=\"scrapy.statscollectors.StatsCollector\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">StatsCollector</span></code></a>\nclass (which they all inherit from).</p>", "<p>Return the value for the given stats key or default if it doesn’t exist.</p>", "<p>Get all stats from the currently running spider as a dict.</p>", "<p>Set the given value for the given stats key.</p>", "<p>Override the current stats with the dict passed in <code class=\"docutils literal notranslate\"><span class=\"pre\">stats</span></code> argument.</p>", "<p>Increment the value of the given stats key, by the given count,\nassuming the start value given (when it’s not set).</p>", "<p>Set the given value for the given key only if current value for the\nsame key is lower than value. If there is no current value for the\ngiven key, the value is always set.</p>", "<p>Set the given value for the given key only if current value for the\nsame key is greater than value. If there is no current value for the\ngiven key, the value is always set.</p>", "<p>Clear all stats.</p>", "<p>The following methods are not part of the stats collection api but instead\nused when implementing custom stats collectors:</p>", "<p>Open the given spider for stats collection.</p>", "<p>Close the given spider. After this is called, no more specific stats\ncan be accessed or collected.</p>"]}, "code_blocks": ["SETTINGS_PRIORITIES = {\n    \"default\": 0,\n    \"command\": 10,\n    \"addon\": 15,\n    \"project\": 20,\n    \"spider\": 30,\n    \"cmdline\": 40,\n}\n</pre>"], "links": [{"text": "EXTENSIONS", "href": "settings.html#std-setting-EXTENSIONS"}, {"text": "configure the downloader middlewares", "href": "downloader-middleware.html#topics-downloader-middleware-setting"}, {"text": "[source]", "href": "../_modules/scrapy/crawler.html#Crawler"}, {"text": "scrapy.Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Request fingerprints", "href": "request-response.html#request-fingerprints"}, {"text": "Settings", "href": "settings.html#topics-settings"}, {"text": "Signals", "href": "signals.html#topics-signals"}, {"text": "Stats Collection", "href": "stats.html#topics-stats"}, {"text": "Extensions", "href": "extensions.html#topics-extensions"}, {"text": "[source]", "href": "../_modules/scrapy/crawler.html#Crawler.crawl"}, {"text": "Generator", "href": "https://docs.python.org/3/library/typing.html#typing.Generator"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "None", "href": "https://docs.python.org/3/library/constants.html#None"}, {"text": "[source]", "href": "../_modules/scrapy/crawler.html#Crawler.stop"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "Dict", "href": "https://docs.python.org/3/library/typing.html#typing.Dict"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "[source]", "href": "../_modules/scrapy/crawler.html#CrawlerRunner"}, {"text": "reactor", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html"}, {"text": "Run Scrapy from a script", "href": "practices.html#run-from-script"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "Type", "href": "https://docs.python.org/3/library/typing.html#typing.Type"}, {"text": "Spider", "href": "spiders.html#scrapy.spiders.Spider"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "[source]", "href": "../_modules/scrapy/crawler.html#CrawlerRunner.crawl"}, {"text": "Spider", "href": "spiders.html#scrapy.spiders.Spider"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "Type", "href": "https://docs.python.org/3/library/typing.html#typing.Type"}, {"text": "Spider", "href": "spiders.html#scrapy.spiders.Spider"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "[source]", "href": "../_modules/scrapy/crawler.html#CrawlerRunner.create_crawler"}, {"text": "[source]", "href": "../_modules/scrapy/crawler.html#CrawlerRunner.join"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "[source]", "href": "../_modules/scrapy/crawler.html#CrawlerRunner.stop"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "Dict", "href": "https://docs.python.org/3/library/typing.html#typing.Dict"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "[source]", "href": "../_modules/scrapy/crawler.html#CrawlerProcess"}, {"text": "reactor", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html"}, {"text": "reactor", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html"}, {"text": "Run Scrapy from a script", "href": "practices.html#run-from-script"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "Type", "href": "https://docs.python.org/3/library/typing.html#typing.Type"}, {"text": "Spider", "href": "spiders.html#scrapy.spiders.Spider"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "Spider", "href": "spiders.html#scrapy.spiders.Spider"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "Type", "href": "https://docs.python.org/3/library/typing.html#typing.Type"}, {"text": "Spider", "href": "spiders.html#scrapy.spiders.Spider"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "None", "href": "https://docs.python.org/3/library/constants.html#None"}, {"text": "[source]", "href": "../_modules/scrapy/crawler.html#CrawlerProcess.start"}, {"text": "reactor", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html"}, {"text": "REACTOR_THREADPOOL_MAXSIZE", "href": "settings.html#std-setting-REACTOR_THREADPOOL_MAXSIZE"}, {"text": "DNSCACHE_ENABLED", "href": "settings.html#std-setting-DNSCACHE_ENABLED"}, {"text": "DNSCACHE_SIZE", "href": "settings.html#std-setting-DNSCACHE_SIZE"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "Settings", "href": "settings.html#topics-settings"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#get_settings_priority"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#Settings"}, {"text": "Built-in settings reference", "href": "settings.html#topics-settings-ref"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.copy"}, {"text": "Dict", "href": "https://docs.python.org/3/library/typing.html#typing.Dict"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.copy_to_dict"}, {"text": "None", "href": "https://docs.python.org/3/library/constants.html#None"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.freeze"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.frozencopy"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.get"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "object", "href": "https://docs.python.org/3/library/functions.html#object"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.getbool"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "object", "href": "https://docs.python.org/3/library/functions.html#object"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Dict", "href": "https://docs.python.org/3/library/typing.html#typing.Dict"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Dict", "href": "https://docs.python.org/3/library/typing.html#typing.Dict"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.getdict"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "object", "href": "https://docs.python.org/3/library/functions.html#object"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "Dict", "href": "https://docs.python.org/3/library/typing.html#typing.Dict"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "List", "href": "https://docs.python.org/3/library/typing.html#typing.List"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Tuple", "href": "https://docs.python.org/3/library/typing.html#typing.Tuple"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "Dict", "href": "https://docs.python.org/3/library/typing.html#typing.Dict"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "List", "href": "https://docs.python.org/3/library/typing.html#typing.List"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.getdictorlist"}, {"text": "dict", "href": "https://docs.python.org/3/library/stdtypes.html#dict"}, {"text": "list", "href": "https://docs.python.org/3/library/stdtypes.html#list"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.getfloat"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "object", "href": "https://docs.python.org/3/library/functions.html#object"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.getint"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "object", "href": "https://docs.python.org/3/library/functions.html#object"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "List", "href": "https://docs.python.org/3/library/typing.html#typing.List"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "List", "href": "https://docs.python.org/3/library/typing.html#typing.List"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.getlist"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "object", "href": "https://docs.python.org/3/library/functions.html#object"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.getpriority"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.getwithbase"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.maxpriority"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.pop"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "float", "href": "https://docs.python.org/3/library/functions.html#float"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "None", "href": "https://docs.python.org/3/library/constants.html#None"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.set"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "object", "href": "https://docs.python.org/3/library/functions.html#object"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.setdefault"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "None", "href": "https://docs.python.org/3/library/constants.html#None"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.setmodule"}, {"text": "types.ModuleType", "href": "https://docs.python.org/3/library/types.html#types.ModuleType"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "None", "href": "https://docs.python.org/3/library/constants.html#None"}, {"text": "[source]", "href": "../_modules/scrapy/settings.html#BaseSettings.update"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "[source]", "href": "../_modules/scrapy/spiderloader.html#SpiderLoader"}, {"text": "SPIDER_LOADER_CLASS", "href": "settings.html#std-setting-SPIDER_LOADER_CLASS"}, {"text": "[source]", "href": "../_modules/scrapy/spiderloader.html#SpiderLoader.from_settings"}, {"text": "SPIDER_MODULES", "href": "settings.html#std-setting-SPIDER_MODULES"}, {"text": "[source]", "href": "../_modules/scrapy/spiderloader.html#SpiderLoader.load"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "[source]", "href": "../_modules/scrapy/spiderloader.html#SpiderLoader.list"}, {"text": "[source]", "href": "../_modules/scrapy/spiderloader.html#SpiderLoader.find_by_request"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "[source]", "href": "../_modules/scrapy/signalmanager.html#SignalManager"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "None", "href": "https://docs.python.org/3/library/constants.html#None"}, {"text": "[source]", "href": "../_modules/scrapy/signalmanager.html#SignalManager.connect"}, {"text": "Signals", "href": "signals.html#topics-signals"}, {"text": "collections.abc.Callable", "href": "https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable"}, {"text": "object", "href": "https://docs.python.org/3/library/functions.html#object"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "None", "href": "https://docs.python.org/3/library/constants.html#None"}, {"text": "[source]", "href": "../_modules/scrapy/signalmanager.html#SignalManager.disconnect"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "None", "href": "https://docs.python.org/3/library/constants.html#None"}, {"text": "[source]", "href": "../_modules/scrapy/signalmanager.html#SignalManager.disconnect_all"}, {"text": "object", "href": "https://docs.python.org/3/library/functions.html#object"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "List", "href": "https://docs.python.org/3/library/typing.html#typing.List"}, {"text": "Tuple", "href": "https://docs.python.org/3/library/typing.html#typing.Tuple"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "[source]", "href": "../_modules/scrapy/signalmanager.html#SignalManager.send_catch_log"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Any", "href": "https://docs.python.org/3/library/typing.html#typing.Any"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "[source]", "href": "../_modules/scrapy/signalmanager.html#SignalManager.send_catch_log_deferred"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "[source]", "href": "../_modules/scrapy/statscollectors.html#StatsCollector"}, {"text": "[source]", "href": "../_modules/scrapy/statscollectors.html#StatsCollector.get_value"}, {"text": "[source]", "href": "../_modules/scrapy/statscollectors.html#StatsCollector.get_stats"}, {"text": "[source]", "href": "../_modules/scrapy/statscollectors.html#StatsCollector.set_value"}, {"text": "[source]", "href": "../_modules/scrapy/statscollectors.html#StatsCollector.set_stats"}, {"text": "[source]", "href": "../_modules/scrapy/statscollectors.html#StatsCollector.inc_value"}, {"text": "[source]", "href": "../_modules/scrapy/statscollectors.html#StatsCollector.max_value"}, {"text": "[source]", "href": "../_modules/scrapy/statscollectors.html#StatsCollector.min_value"}, {"text": "[source]", "href": "../_modules/scrapy/statscollectors.html#StatsCollector.clear_stats"}, {"text": "[source]", "href": "../_modules/scrapy/statscollectors.html#StatsCollector.open_spider"}, {"text": "[source]", "href": "../_modules/scrapy/statscollectors.html#StatsCollector.close_spider"}], "timestamp": "2023-10-12T21:19:14.878905", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/components.html", "content": {"sections": [], "paragraphs": ["<p>A Scrapy component is any class whose objects are created using\n<code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.utils.misc.create_instance()</span></code>.</p>", "<p>That includes the classes that you may assign to the following settings:</p>", "<p>Third-party Scrapy components may also let you define additional Scrapy\ncomponents, usually configurable through <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#topics-settings\"><span class=\"std std-ref\">settings</span></a>, to\nmodify their behavior.</p>", "<p>Sometimes, your components may only be intended to work under certain\nconditions. For example, they may require a minimum version of Scrapy to work as\nintended, or they may require certain settings to have specific values.</p>", "<p>In addition to describing those conditions in the documentation of your\ncomponent, it is a good practice to raise an exception from the <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code>\nmethod of your component if those conditions are not met at run time.</p>", "<p>In the case of <a class=\"hoverxref tooltip reference internal\" href=\"downloader-middleware.html#topics-downloader-middleware\"><span class=\"std std-ref\">downloader middlewares</span></a>,\n<a class=\"hoverxref tooltip reference internal\" href=\"extensions.html#topics-extensions\"><span class=\"std std-ref\">extensions</span></a>, <a class=\"hoverxref tooltip reference internal\" href=\"item-pipeline.html#topics-item-pipeline\"><span class=\"std std-ref\">item pipelines</span></a>, and <a class=\"hoverxref tooltip reference internal\" href=\"spider-middleware.html#topics-spider-middleware\"><span class=\"std std-ref\">spider middlewares</span></a>, you should raise\n<a class=\"reference internal\" href=\"exceptions.html#scrapy.exceptions.NotConfigured\" title=\"scrapy.exceptions.NotConfigured\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">scrapy.exceptions.NotConfigured</span></code></a>, passing a description of the issue as a\nparameter to the exception so that it is printed in the logs, for the user to\nsee. For other components, feel free to raise whatever other exception feels\nright to you; for example, <a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#RuntimeError\" title=\"(in Python v3.11)\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">RuntimeError</span></code></a> would make sense for a Scrapy\nversion mismatch, while <a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#ValueError\" title=\"(in Python v3.11)\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">ValueError</span></code></a> may be better if the issue is the\nvalue of a setting.</p>", "<p>If your requirement is a minimum Scrapy version, you may use\n<code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">scrapy.__version__</span></code> to enforce your requirement. For example:</p>"]}, "code_blocks": ["from packaging.version import parse as parse_version\n\nimport scrapy\n\n\nclass MyComponent:\n    def __init__(self):\n        if parse_version(scrapy.__version__) < parse_version(\"2.7\"):\n            raise RuntimeError(\n                f\"{MyComponent.__qualname__} requires Scrapy 2.7 or \"\n                f\"later, which allow defining the process_spider_output \"\n                f\"method of spider middlewares as an asynchronous \"\n                f\"generator.\"\n            )\n</pre>"], "links": [{"text": "DNS_RESOLVER", "href": "settings.html#std-setting-DNS_RESOLVER"}, {"text": "DOWNLOAD_HANDLERS", "href": "settings.html#std-setting-DOWNLOAD_HANDLERS"}, {"text": "DOWNLOADER_CLIENTCONTEXTFACTORY", "href": "settings.html#std-setting-DOWNLOADER_CLIENTCONTEXTFACTORY"}, {"text": "DOWNLOADER_MIDDLEWARES", "href": "settings.html#std-setting-DOWNLOADER_MIDDLEWARES"}, {"text": "DUPEFILTER_CLASS", "href": "settings.html#std-setting-DUPEFILTER_CLASS"}, {"text": "EXTENSIONS", "href": "settings.html#std-setting-EXTENSIONS"}, {"text": "FEED_EXPORTERS", "href": "feed-exports.html#std-setting-FEED_EXPORTERS"}, {"text": "FEED_STORAGES", "href": "feed-exports.html#std-setting-FEED_STORAGES"}, {"text": "ITEM_PIPELINES", "href": "settings.html#std-setting-ITEM_PIPELINES"}, {"text": "SCHEDULER", "href": "settings.html#std-setting-SCHEDULER"}, {"text": "SCHEDULER_DISK_QUEUE", "href": "settings.html#std-setting-SCHEDULER_DISK_QUEUE"}, {"text": "SCHEDULER_MEMORY_QUEUE", "href": "settings.html#std-setting-SCHEDULER_MEMORY_QUEUE"}, {"text": "SCHEDULER_PRIORITY_QUEUE", "href": "settings.html#std-setting-SCHEDULER_PRIORITY_QUEUE"}, {"text": "SPIDER_MIDDLEWARES", "href": "settings.html#std-setting-SPIDER_MIDDLEWARES"}, {"text": "settings", "href": "settings.html#topics-settings"}, {"text": "downloader middlewares", "href": "downloader-middleware.html#topics-downloader-middleware"}, {"text": "extensions", "href": "extensions.html#topics-extensions"}, {"text": "item pipelines", "href": "item-pipeline.html#topics-item-pipeline"}, {"text": "spider middlewares", "href": "spider-middleware.html#topics-spider-middleware"}, {"text": "scrapy.exceptions.NotConfigured", "href": "exceptions.html#scrapy.exceptions.NotConfigured"}, {"text": "RuntimeError", "href": "https://docs.python.org/3/library/exceptions.html#RuntimeError"}, {"text": "ValueError", "href": "https://docs.python.org/3/library/exceptions.html#ValueError"}], "timestamp": "2023-10-12T21:19:18.299576", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/exporters.html", "content": {"sections": [], "paragraphs": ["<p>Once you have scraped your items, you often want to persist or export those\nitems, to use the data in some other application. That is, after all, the whole\npurpose of the scraping process.</p>", "<p>For this purpose Scrapy provides a collection of Item Exporters for different\noutput formats, such as XML, CSV or JSON.</p>", "<p>If you are in a hurry, and just want to use an Item Exporter to output scraped\ndata see the <a class=\"hoverxref tooltip reference internal\" href=\"feed-exports.html#topics-feed-exports\"><span class=\"std std-ref\">Feed exports</span></a>. Otherwise, if you want to know how\nItem Exporters work or need more custom functionality (not covered by the\ndefault exports), continue reading below.</p>", "<p>In order to use an Item Exporter, you  must instantiate it with its required\nargs. Each Item Exporter requires different arguments, so check each exporter\ndocumentation to be sure, in <a class=\"hoverxref tooltip reference internal\" href=\"#topics-exporters-reference\"><span class=\"std std-ref\">Built-in Item Exporters reference</span></a>. After you have\ninstantiated your exporter, you have to:</p>", "<p>1. call the method <a class=\"reference internal\" href=\"#scrapy.exporters.BaseItemExporter.start_exporting\" title=\"scrapy.exporters.BaseItemExporter.start_exporting\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">start_exporting()</span></code></a> in order to\nsignal the beginning of the exporting process</p>", "<p>2. call the <a class=\"reference internal\" href=\"#scrapy.exporters.BaseItemExporter.export_item\" title=\"scrapy.exporters.BaseItemExporter.export_item\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">export_item()</span></code></a> method for each item you want\nto export</p>", "<p>3. and finally call the <a class=\"reference internal\" href=\"#scrapy.exporters.BaseItemExporter.finish_exporting\" title=\"scrapy.exporters.BaseItemExporter.finish_exporting\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">finish_exporting()</span></code></a> to signal\nthe end of the exporting process</p>", "<p>Here you can see an <a class=\"reference internal\" href=\"item-pipeline.html\"><span class=\"doc\">Item Pipeline</span></a> which uses multiple\nItem Exporters to group scraped items to different files according to the\nvalue of one of their fields:</p>", "<p>By default, the field values are passed unmodified to the underlying\nserialization library, and the decision of how to serialize them is delegated\nto each particular serialization library.</p>", "<p>However, you can customize how each field value is serialized <em>before it is\npassed to the serialization library</em>.</p>", "<p>There are two ways to customize how a field will be serialized, which are\ndescribed next.</p>", "<p>If you use <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Item</span></code> you can declare a serializer in the\n<a class=\"hoverxref tooltip reference internal\" href=\"items.html#topics-items-fields\"><span class=\"std std-ref\">field metadata</span></a>. The serializer must be\na callable which receives a value and returns its serialized form.</p>", "<p>Example:</p>", "<p>You can also override the <a class=\"reference internal\" href=\"#scrapy.exporters.BaseItemExporter.serialize_field\" title=\"scrapy.exporters.BaseItemExporter.serialize_field\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">serialize_field()</span></code></a> method to\ncustomize how your field value will be exported.</p>", "<p>Make sure you call the base class <a class=\"reference internal\" href=\"#scrapy.exporters.BaseItemExporter.serialize_field\" title=\"scrapy.exporters.BaseItemExporter.serialize_field\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">serialize_field()</span></code></a> method\nafter your custom code.</p>", "<p>Example:</p>", "<p>Here is a list of the Item Exporters bundled with Scrapy. Some of them contain\noutput examples, which assume you’re exporting these two items:</p>", "<p>This is the (abstract) base class for all Item Exporters. It provides\nsupport for common features used by all (concrete) Item Exporters, such as\ndefining what fields to export, whether to export empty fields, or which\nencoding to use.</p>", "<p>These features can be configured through the <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method arguments which\npopulate their respective instance attributes: <a class=\"reference internal\" href=\"#scrapy.exporters.BaseItemExporter.fields_to_export\" title=\"scrapy.exporters.BaseItemExporter.fields_to_export\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">fields_to_export</span></code></a>,\n<a class=\"reference internal\" href=\"#scrapy.exporters.BaseItemExporter.export_empty_fields\" title=\"scrapy.exporters.BaseItemExporter.export_empty_fields\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">export_empty_fields</span></code></a>, <a class=\"reference internal\" href=\"#scrapy.exporters.BaseItemExporter.encoding\" title=\"scrapy.exporters.BaseItemExporter.encoding\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">encoding</span></code></a>, <a class=\"reference internal\" href=\"#scrapy.exporters.BaseItemExporter.indent\" title=\"scrapy.exporters.BaseItemExporter.indent\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">indent</span></code></a>.</p>", "<p><span class=\"versionmodified added\">New in version 2.0: </span>The <em>dont_fail</em> parameter.</p>", "<p>Exports the given item. This method must be implemented in subclasses.</p>", "<p>Return the serialized value for the given field. You can override this\nmethod (in your custom Item Exporters) if you want to control how a\nparticular field or value will be serialized/exported.</p>", "<p>By default, this method looks for a serializer <a class=\"hoverxref tooltip reference internal\" href=\"#topics-exporters-serializers\"><span class=\"std std-ref\">declared in the item\nfield</span></a> and returns the result of applying\nthat serializer to the value. If no serializer is found, it returns the\nvalue unchanged.</p>", "<p><strong>field</strong> (<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Field</span></code> object or a <a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">dict</span></code></a> instance) – the field being serialized. If the source <a class=\"hoverxref tooltip reference internal\" href=\"items.html#item-types\"><span class=\"std std-ref\">item object</span></a> does not define field metadata, <em>field</em> is an empty\n<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">dict</span></code></a>.</p>", "<p><strong>name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><em>str</em></a>) – the name of the field being serialized</p>", "<p><strong>value</strong> – the value being serialized</p>", "<p>Signal the beginning of the exporting process. Some exporters may use\nthis to generate some required header (for example, the\n<a class=\"reference internal\" href=\"#scrapy.exporters.XmlItemExporter\" title=\"scrapy.exporters.XmlItemExporter\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">XmlItemExporter</span></code></a>). You must call this method before exporting any\nitems.</p>", "<p>Signal the end of the exporting process. Some exporters may use this to\ngenerate some required footer (for example, the\n<a class=\"reference internal\" href=\"#scrapy.exporters.XmlItemExporter\" title=\"scrapy.exporters.XmlItemExporter\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">XmlItemExporter</span></code></a>). You must always call this method after you\nhave no more items to export.</p>", "<p>Fields to export, their order <a class=\"footnote-reference brackets\" href=\"#id3\" id=\"id1\">1</a> and their output names.</p>", "<p>Possible values are:</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> (all fields <a class=\"footnote-reference brackets\" href=\"#id4\" id=\"id2\">2</a>, default)</p>", "<p>A list of fields:</p>", "<p>A dict where keys are fields and values are output names:</p>", "<p>Not all exporters respect the specified field order.</p>", "<p>When using <a class=\"hoverxref tooltip reference internal\" href=\"items.html#item-types\"><span class=\"std std-ref\">item objects</span></a> that do not expose\nall their possible fields, exporters that do not support exporting\na different subset of fields per item will only export the fields\nfound in the first item exported.</p>", "<p>Whether to include empty/unpopulated item fields in the exported data.\nDefaults to <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code>. Some exporters (like <a class=\"reference internal\" href=\"#scrapy.exporters.CsvItemExporter\" title=\"scrapy.exporters.CsvItemExporter\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CsvItemExporter</span></code></a>)\nignore this attribute and always export all empty fields.</p>", "<p>This option is ignored for dict items.</p>", "<p>The output character encoding.</p>", "<p>Amount of spaces used to indent the output on each level. Defaults to <code class=\"docutils literal notranslate\"><span class=\"pre\">0</span></code>.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">indent=None</span></code> selects the most compact representation,\nall items in the same line with no indentation</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">indent&lt;=0</span></code> each item on its own line, no indentation</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">indent&gt;0</span></code> each item on its own line, indented with the provided numeric value</p>", "<p>This is a base class for item exporters that extends\n<a class=\"reference internal\" href=\"#scrapy.exporters.BaseItemExporter\" title=\"scrapy.exporters.BaseItemExporter\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">BaseItemExporter</span></code></a> with support for nested items.</p>", "<p>It serializes items to built-in Python types, so that any serialization\nlibrary (e.g. <a class=\"reference external\" href=\"https://docs.python.org/3/library/json.html#module-json\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">json</span></code></a> or <a class=\"reference external\" href=\"https://pypi.org/project/msgpack/\">msgpack</a>) can be used on top of it.</p>", "<p>Exports items in XML format to the specified file object.</p>", "<p><strong>file</strong> – the file-like object to use for exporting the data. Its <code class=\"docutils literal notranslate\"><span class=\"pre\">write</span></code> method should\naccept <code class=\"docutils literal notranslate\"><span class=\"pre\">bytes</span></code> (a disk file opened in binary mode, a <code class=\"docutils literal notranslate\"><span class=\"pre\">io.BytesIO</span></code> object, etc)</p>", "<p><strong>root_element</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><em>str</em></a>) – The name of root element in the exported XML.</p>", "<p><strong>item_element</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><em>str</em></a>) – The name of each item element in the exported XML.</p>", "<p>The additional keyword arguments of this <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method are passed to the\n<a class=\"reference internal\" href=\"#scrapy.exporters.BaseItemExporter\" title=\"scrapy.exporters.BaseItemExporter\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">BaseItemExporter</span></code></a> <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method.</p>", "<p>A typical output of this exporter would be:</p>", "<p>Unless overridden in the <code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">serialize_field()</span></code> method, multi-valued fields are\nexported by serializing each value inside a <code class=\"docutils literal notranslate\"><span class=\"pre\">&lt;value&gt;</span></code> element. This is for\nconvenience, as multi-valued fields are very common.</p>", "<p>For example, the item:</p>", "<p>Would be serialized as:</p>", "<p>Exports items in CSV format to the given file-like object. If the\n<code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">fields_to_export</span></code> attribute is set, it will be used to define the\nCSV columns, their order and their column names. The\n<code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">export_empty_fields</span></code> attribute has no effect on this exporter.</p>", "<p><strong>file</strong> – the file-like object to use for exporting the data. Its <code class=\"docutils literal notranslate\"><span class=\"pre\">write</span></code> method should\naccept <code class=\"docutils literal notranslate\"><span class=\"pre\">bytes</span></code> (a disk file opened in binary mode, a <code class=\"docutils literal notranslate\"><span class=\"pre\">io.BytesIO</span></code> object, etc)</p>", "<p><strong>include_headers_line</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><em>str</em></a>) – If enabled, makes the exporter output a header\nline with the field names taken from\n<a class=\"reference internal\" href=\"#scrapy.exporters.BaseItemExporter.fields_to_export\" title=\"scrapy.exporters.BaseItemExporter.fields_to_export\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">BaseItemExporter.fields_to_export</span></code></a> or the first exported item fields.</p>", "<p><strong>join_multivalued</strong> – The char (or chars) that will be used for joining\nmulti-valued fields, if found.</p>", "<p><strong>errors</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><em>str</em></a>) – The optional string that specifies how encoding and decoding\nerrors are to be handled. For more information see\n<a class=\"reference external\" href=\"https://docs.python.org/3/library/io.html#io.TextIOWrapper\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">io.TextIOWrapper</span></code></a>.</p>", "<p>The additional keyword arguments of this <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method are passed to the\n<a class=\"reference internal\" href=\"#scrapy.exporters.BaseItemExporter\" title=\"scrapy.exporters.BaseItemExporter\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">BaseItemExporter</span></code></a> <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method, and the leftover arguments to the\n<a class=\"reference external\" href=\"https://docs.python.org/3/library/csv.html#csv.writer\" title=\"(in Python v3.11)\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">csv.writer()</span></code></a> function, so you can use any <a class=\"reference external\" href=\"https://docs.python.org/3/library/csv.html#csv.writer\" title=\"(in Python v3.11)\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">csv.writer()</span></code></a> function\nargument to customize this exporter.</p>", "<p>A typical output of this exporter would be:</p>", "<p>Exports items in pickle format to the given file-like object.</p>", "<p><strong>file</strong> – the file-like object to use for exporting the data. Its <code class=\"docutils literal notranslate\"><span class=\"pre\">write</span></code> method should\naccept <code class=\"docutils literal notranslate\"><span class=\"pre\">bytes</span></code> (a disk file opened in binary mode, a <code class=\"docutils literal notranslate\"><span class=\"pre\">io.BytesIO</span></code> object, etc)</p>", "<p><strong>protocol</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.11)\"><em>int</em></a>) – The pickle protocol to use.</p>", "<p>For more information, see <a class=\"reference external\" href=\"https://docs.python.org/3/library/pickle.html#module-pickle\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">pickle</span></code></a>.</p>", "<p>The additional keyword arguments of this <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method are passed to the\n<a class=\"reference internal\" href=\"#scrapy.exporters.BaseItemExporter\" title=\"scrapy.exporters.BaseItemExporter\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">BaseItemExporter</span></code></a> <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method.</p>", "<p>Pickle isn’t a human readable format, so no output examples are provided.</p>", "<p>Exports items in pretty print format to the specified file object.</p>", "<p><strong>file</strong> – the file-like object to use for exporting the data. Its <code class=\"docutils literal notranslate\"><span class=\"pre\">write</span></code> method should\naccept <code class=\"docutils literal notranslate\"><span class=\"pre\">bytes</span></code> (a disk file opened in binary mode, a <code class=\"docutils literal notranslate\"><span class=\"pre\">io.BytesIO</span></code> object, etc)</p>", "<p>The additional keyword arguments of this <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method are passed to the\n<a class=\"reference internal\" href=\"#scrapy.exporters.BaseItemExporter\" title=\"scrapy.exporters.BaseItemExporter\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">BaseItemExporter</span></code></a> <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method.</p>", "<p>A typical output of this exporter would be:</p>", "<p>Longer lines (when present) are pretty-formatted.</p>", "<p>Exports items in JSON format to the specified file-like object, writing all\nobjects as a list of objects. The additional <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method arguments are\npassed to the <a class=\"reference internal\" href=\"#scrapy.exporters.BaseItemExporter\" title=\"scrapy.exporters.BaseItemExporter\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">BaseItemExporter</span></code></a> <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method, and the leftover\narguments to the <a class=\"reference external\" href=\"https://docs.python.org/3/library/json.html#json.JSONEncoder\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">JSONEncoder</span></code></a> <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method, so you can use any\n<a class=\"reference external\" href=\"https://docs.python.org/3/library/json.html#json.JSONEncoder\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">JSONEncoder</span></code></a> <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method argument to customize this exporter.</p>", "<p><strong>file</strong> – the file-like object to use for exporting the data. Its <code class=\"docutils literal notranslate\"><span class=\"pre\">write</span></code> method should\naccept <code class=\"docutils literal notranslate\"><span class=\"pre\">bytes</span></code> (a disk file opened in binary mode, a <code class=\"docutils literal notranslate\"><span class=\"pre\">io.BytesIO</span></code> object, etc)</p>", "<p>A typical output of this exporter would be:</p>", "<p class=\"admonition-title\">Warning</p>", "<p>JSON is very simple and flexible serialization format, but it\ndoesn’t scale well for large amounts of data since incremental (aka.\nstream-mode) parsing is not well supported (if at all) among JSON parsers\n(on any language), and most of them just parse the entire object in\nmemory. If you want the power and simplicity of JSON with a more\nstream-friendly format, consider using <a class=\"reference internal\" href=\"#scrapy.exporters.JsonLinesItemExporter\" title=\"scrapy.exporters.JsonLinesItemExporter\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">JsonLinesItemExporter</span></code></a>\ninstead, or splitting the output in multiple chunks.</p>", "<p>Exports items in JSON format to the specified file-like object, writing one\nJSON-encoded item per line. The additional <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method arguments are passed\nto the <a class=\"reference internal\" href=\"#scrapy.exporters.BaseItemExporter\" title=\"scrapy.exporters.BaseItemExporter\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">BaseItemExporter</span></code></a> <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method, and the leftover arguments to\nthe <a class=\"reference external\" href=\"https://docs.python.org/3/library/json.html#json.JSONEncoder\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">JSONEncoder</span></code></a> <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method, so you can use any\n<a class=\"reference external\" href=\"https://docs.python.org/3/library/json.html#json.JSONEncoder\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">JSONEncoder</span></code></a> <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method argument to customize this exporter.</p>", "<p><strong>file</strong> – the file-like object to use for exporting the data. Its <code class=\"docutils literal notranslate\"><span class=\"pre\">write</span></code> method should\naccept <code class=\"docutils literal notranslate\"><span class=\"pre\">bytes</span></code> (a disk file opened in binary mode, a <code class=\"docutils literal notranslate\"><span class=\"pre\">io.BytesIO</span></code> object, etc)</p>", "<p>A typical output of this exporter would be:</p>", "<p>Unlike the one produced by <a class=\"reference internal\" href=\"#scrapy.exporters.JsonItemExporter\" title=\"scrapy.exporters.JsonItemExporter\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">JsonItemExporter</span></code></a>, the format produced by\nthis exporter is well suited for serializing large amounts of data.</p>", "<p>Exports items in a Python-specific binary format (see\n<a class=\"reference external\" href=\"https://docs.python.org/3/library/marshal.html#module-marshal\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">marshal</span></code></a>).</p>", "<p><strong>file</strong> – The file-like object to use for exporting the data. Its\n<code class=\"docutils literal notranslate\"><span class=\"pre\">write</span></code> method should accept <a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#bytes\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">bytes</span></code></a> (a disk file\nopened in binary mode, a <a class=\"reference external\" href=\"https://docs.python.org/3/library/io.html#io.BytesIO\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">BytesIO</span></code></a> object, etc)</p>"]}, "code_blocks": ["from itemadapter import ItemAdapter\nfrom scrapy.exporters import XmlItemExporter\n\n\nclass PerYearXmlExportPipeline:\n    \"\"\"Distribute items across multiple XML files according to their 'year' field\"\"\"\n\n    def open_spider(self, spider):\n        self.year_to_exporter = {}\n\n    def close_spider(self, spider):\n        for exporter, xml_file in self.year_to_exporter.values():\n            exporter.finish_exporting()\n            xml_file.close()\n\n    def _exporter_for_item(self, item):\n        adapter = ItemAdapter(item)\n        year = adapter[\"year\"]\n        if year not in self.year_to_exporter:\n            xml_file = open(f\"{year}.xml\", \"wb\")\n            exporter = XmlItemExporter(xml_file)\n            exporter.start_exporting()\n            self.year_to_exporter[year] = (exporter, xml_file)\n        return self.year_to_exporter[year][0]\n\n    def process_item(self, item, spider):\n        exporter = self._exporter_for_item(item)\n        exporter.export_item(item)\n        return item\n</pre>", "import scrapy\n\n\ndef serialize_price(value):\n    return f\"$ {str(value)}\"\n\n\nclass Product(scrapy.Item):\n    name = scrapy.Field()\n    price = scrapy.Field(serializer=serialize_price)\n</pre>", "from scrapy.exporters import XmlItemExporter\n\n\nclass ProductXmlExporter(XmlItemExporter):\n    def serialize_field(self, field, name, value):\n        if name == \"price\":\n            return f\"$ {str(value)}\"\n        return super().serialize_field(field, name, value)\n</pre>", "Item(name=\"Color TV\", price=\"1200\")\nItem(name=\"DVD player\", price=\"200\")\n</pre>", "['field1', 'field2']\n</pre>", "{'field1': 'Field 1', 'field2': 'Field 2'}\n</pre>", "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<items>\n  <item>\n    <name>Color TV</name>\n    1200</price>\n </item>\n  <item>\n    <name>DVD player</name>\n    200</price>\n </item>\n</items>\n</pre>", "Item(name=['John', 'Doe'], age='23')\n</pre>", "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<items>\n  <item>\n    <name>\n      <value>John</value>\n      <value>Doe</value>\n    </name>\n    23</age>\n  </item>\n</items>\n</pre>", "product,price\nColor TV,1200\nDVD player,200\n</pre>", "{'name': 'Color TV', 'price': '1200'}\n{'name': 'DVD player', 'price': '200'}\n</pre>", "[{\"name\": \"Color TV\", \"price\": \"1200\"},\n{\"name\": \"DVD player\", \"price\": \"200\"}]\n</pre>", "{\"name\": \"Color TV\", \"price\": \"1200\"}\n{\"name\": \"DVD player\", \"price\": \"200\"}\n</pre>"], "links": [{"text": "Feed exports", "href": "feed-exports.html#topics-feed-exports"}, {"text": "Item Pipeline", "href": "item-pipeline.html"}, {"text": "field metadata", "href": "items.html#topics-items-fields"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#BaseItemExporter"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#BaseItemExporter.export_item"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#BaseItemExporter.serialize_field"}, {"text": "dict", "href": "https://docs.python.org/3/library/stdtypes.html#dict"}, {"text": "item object", "href": "items.html#item-types"}, {"text": "dict", "href": "https://docs.python.org/3/library/stdtypes.html#dict"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#BaseItemExporter.start_exporting"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#BaseItemExporter.finish_exporting"}, {"text": "item objects", "href": "items.html#item-types"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#PythonItemExporter"}, {"text": "json", "href": "https://docs.python.org/3/library/json.html#module-json"}, {"text": "msgpack", "href": "https://pypi.org/project/msgpack/"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#XmlItemExporter"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#CsvItemExporter"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "io.TextIOWrapper", "href": "https://docs.python.org/3/library/io.html#io.TextIOWrapper"}, {"text": "csv.writer()", "href": "https://docs.python.org/3/library/csv.html#csv.writer"}, {"text": "csv.writer()", "href": "https://docs.python.org/3/library/csv.html#csv.writer"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#PickleItemExporter"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "pickle", "href": "https://docs.python.org/3/library/pickle.html#module-pickle"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#PprintItemExporter"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#JsonItemExporter"}, {"text": "JSONEncoder", "href": "https://docs.python.org/3/library/json.html#json.JSONEncoder"}, {"text": "JSONEncoder", "href": "https://docs.python.org/3/library/json.html#json.JSONEncoder"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#JsonLinesItemExporter"}, {"text": "JSONEncoder", "href": "https://docs.python.org/3/library/json.html#json.JSONEncoder"}, {"text": "JSONEncoder", "href": "https://docs.python.org/3/library/json.html#json.JSONEncoder"}, {"text": "[source]", "href": "../_modules/scrapy/exporters.html#MarshalItemExporter"}, {"text": "marshal", "href": "https://docs.python.org/3/library/marshal.html#module-marshal"}, {"text": "bytes", "href": "https://docs.python.org/3/library/stdtypes.html#bytes"}, {"text": "BytesIO", "href": "https://docs.python.org/3/library/io.html#io.BytesIO"}], "timestamp": "2023-10-12T21:19:22.095804", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/scheduler.html", "content": {"sections": [], "paragraphs": ["<p>The scheduler component receives requests from the <a class=\"hoverxref tooltip reference internal\" href=\"architecture.html#component-engine\"><span class=\"std std-ref\">engine</span></a>\nand stores them into persistent and/or non-persistent data structures.\nIt also gets those requests and feeds them back to the engine when it\nasks for a next request to be downloaded.</p>", "<p>You can use your own custom scheduler class by supplying its full\nPython path in the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-SCHEDULER\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SCHEDULER</span></code></a> setting.</p>", "<p>The scheduler component is responsible for storing requests received from\nthe engine, and feeding them back upon request (also to the engine).</p>", "<p>The original sources of said requests are:</p>", "<p>Spider: <code class=\"docutils literal notranslate\"><span class=\"pre\">start_requests</span></code> method, requests created for URLs in the <code class=\"docutils literal notranslate\"><span class=\"pre\">start_urls</span></code> attribute, request callbacks</p>", "<p>Spider middleware: <code class=\"docutils literal notranslate\"><span class=\"pre\">process_spider_output</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">process_spider_exception</span></code> methods</p>", "<p>Downloader middleware: <code class=\"docutils literal notranslate\"><span class=\"pre\">process_request</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">process_response</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">process_exception</span></code> methods</p>", "<p>The order in which the scheduler returns its stored requests (via the <code class=\"docutils literal notranslate\"><span class=\"pre\">next_request</span></code> method)\nplays a great part in determining the order in which those requests are downloaded.</p>", "<p>The methods defined in this class constitute the minimal interface that the Scrapy engine will interact with.</p>", "<p>Called when the spider is closed by the engine. It receives the reason why the crawl\nfinished as argument and it’s useful to execute cleaning code.</p>", "<p><strong>reason</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">str</span></code></a>) – a string which describes the reason why the spider was closed</p>", "<p>Process a request received by the engine.</p>", "<p>Return <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code> if the request is stored correctly, <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code> otherwise.</p>", "<p>If <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code>, the engine will fire a <code class=\"docutils literal notranslate\"><span class=\"pre\">request_dropped</span></code> signal, and\nwill not make further attempts to schedule the request at a later time.\nFor reference, the default Scrapy scheduler returns <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code> when the\nrequest is rejected by the dupefilter.</p>", "<p>Factory method which receives the current <a class=\"reference internal\" href=\"api.html#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a> object as argument.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code> if the scheduler has enqueued requests, <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code> otherwise</p>", "<p>Return the next <a class=\"reference internal\" href=\"request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> to be processed, or <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>\nto indicate that there are no requests to be considered ready at the moment.</p>", "<p>Returning <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> implies that no request from the scheduler will be sent\nto the downloader in the current reactor cycle. The engine will continue\ncalling <code class=\"docutils literal notranslate\"><span class=\"pre\">next_request</span></code> until <code class=\"docutils literal notranslate\"><span class=\"pre\">has_pending_requests</span></code> is <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code>.</p>", "<p>Called when the spider is opened by the engine. It receives the spider\ninstance as argument and it’s useful to execute initialization code.</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.spiders.Spider\" title=\"scrapy.spiders.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a>) – the spider object for the current crawl</p>", "<p>Default Scrapy scheduler. This implementation also handles duplication\nfiltering via the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-DUPEFILTER_CLASS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">dupefilter</span></code></a>.</p>", "<p>This scheduler stores requests into several priority queues (defined by the\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-SCHEDULER_PRIORITY_QUEUE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SCHEDULER_PRIORITY_QUEUE</span></code></a> setting). In turn, said priority queues\nare backed by either memory or disk based queues (respectively defined by the\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-SCHEDULER_MEMORY_QUEUE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SCHEDULER_MEMORY_QUEUE</span></code></a> and <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-SCHEDULER_DISK_QUEUE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SCHEDULER_DISK_QUEUE</span></code></a> settings).</p>", "<p>Request prioritization is almost entirely delegated to the priority queue. The only\nprioritization performed by this scheduler is using the disk-based queue if present\n(i.e. if the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-JOBDIR\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">JOBDIR</span></code></a> setting is defined) and falling back to the memory-based\nqueue if a serialization error occurs. If the disk queue is not present, the memory one\nis used directly.</p>", "<p><strong>dupefilter</strong> (<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.dupefilters.BaseDupeFilter</span></code> instance or similar:\nany class that implements the <cite>BaseDupeFilter</cite> interface) – An object responsible for checking and filtering duplicate requests.\nThe value for the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-DUPEFILTER_CLASS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DUPEFILTER_CLASS</span></code></a> setting is used by default.</p>", "<p><strong>jobdir</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">str</span></code></a> or <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>) – The path of a directory to be used for persisting the crawl’s state.\nThe value for the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-JOBDIR\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">JOBDIR</span></code></a> setting is used by default.\nSee <a class=\"hoverxref tooltip reference internal\" href=\"jobs.html#topics-jobs\"><span class=\"std std-ref\">Jobs: pausing and resuming crawls</span></a>.</p>", "<p><strong>dqclass</strong> (<em>class</em>) – A class to be used as persistent request queue.\nThe value for the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-SCHEDULER_DISK_QUEUE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SCHEDULER_DISK_QUEUE</span></code></a> setting is used by default.</p>", "<p><strong>mqclass</strong> (<em>class</em>) – A class to be used as non-persistent request queue.\nThe value for the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-SCHEDULER_MEMORY_QUEUE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SCHEDULER_MEMORY_QUEUE</span></code></a> setting is used by default.</p>", "<p><strong>logunser</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.11)\"><em>bool</em></a>) – A boolean that indicates whether or not unserializable requests should be logged.\nThe value for the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-SCHEDULER_DEBUG\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SCHEDULER_DEBUG</span></code></a> setting is used by default.</p>", "<p><strong>stats</strong> (<a class=\"reference internal\" href=\"api.html#scrapy.statscollectors.StatsCollector\" title=\"scrapy.statscollectors.StatsCollector\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.statscollectors.StatsCollector</span></code></a> instance or similar:\nany class that implements the <cite>StatsCollector</cite> interface) – A stats collector object to record stats about the request scheduling process.\nThe value for the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-STATS_CLASS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">STATS_CLASS</span></code></a> setting is used by default.</p>", "<p><strong>pqclass</strong> (<em>class</em>) – A class to be used as priority queue for requests.\nThe value for the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-SCHEDULER_PRIORITY_QUEUE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SCHEDULER_PRIORITY_QUEUE</span></code></a> setting is used by default.</p>", "<p><strong>crawler</strong> (<a class=\"reference internal\" href=\"api.html#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.crawler.Crawler</span></code></a>) – The crawler object corresponding to the current crawl.</p>", "<p>Return the total amount of enqueued requests</p>", "<p>dump pending requests to disk if there is a disk queue</p>", "<p>return the result of the dupefilter’s <code class=\"docutils literal notranslate\"><span class=\"pre\">close</span></code> method</p>", "<p>Unless the received request is filtered out by the Dupefilter, attempt to push\nit into the disk queue, falling back to pushing it into the memory queue.</p>", "<p>Increment the appropriate stats, such as: <code class=\"docutils literal notranslate\"><span class=\"pre\">scheduler/enqueued</span></code>,\n<code class=\"docutils literal notranslate\"><span class=\"pre\">scheduler/enqueued/disk</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">scheduler/enqueued/memory</span></code>.</p>", "<p>Return <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code> if the request was stored successfully, <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code> otherwise.</p>", "<p>Factory method, initializes the scheduler with arguments taken from the crawl settings</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code> if the scheduler has enqueued requests, <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code> otherwise</p>", "<p>Return a <a class=\"reference internal\" href=\"request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a> object from the memory queue,\nfalling back to the disk queue if the memory queue is empty.\nReturn <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> if there are no more enqueued requests.</p>", "<p>Increment the appropriate stats, such as: <code class=\"docutils literal notranslate\"><span class=\"pre\">scheduler/dequeued</span></code>,\n<code class=\"docutils literal notranslate\"><span class=\"pre\">scheduler/dequeued/disk</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">scheduler/dequeued/memory</span></code>.</p>", "<p>initialize the memory queue</p>", "<p>initialize the disk queue if the <code class=\"docutils literal notranslate\"><span class=\"pre\">jobdir</span></code> attribute is a valid directory</p>", "<p>return the result of the dupefilter’s <code class=\"docutils literal notranslate\"><span class=\"pre\">open</span></code> method</p>"]}, "code_blocks": [], "links": [{"text": "engine", "href": "architecture.html#component-engine"}, {"text": "SCHEDULER", "href": "settings.html#std-setting-SCHEDULER"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#BaseScheduler"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#BaseScheduler.close"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Request", "href": "request-response.html#scrapy.http.Request"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#BaseScheduler.enqueue_request"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#BaseScheduler.from_crawler"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#BaseScheduler.has_pending_requests"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Request", "href": "request-response.html#scrapy.http.Request"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#BaseScheduler.next_request"}, {"text": "Request", "href": "request-response.html#scrapy.http.Request"}, {"text": "Spider", "href": "spiders.html#scrapy.spiders.Spider"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#BaseScheduler.open"}, {"text": "Spider", "href": "spiders.html#scrapy.spiders.Spider"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "StatsCollector", "href": "api.html#scrapy.statscollectors.StatsCollector"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#Scheduler"}, {"text": "dupefilter", "href": "settings.html#std-setting-DUPEFILTER_CLASS"}, {"text": "SCHEDULER_PRIORITY_QUEUE", "href": "settings.html#std-setting-SCHEDULER_PRIORITY_QUEUE"}, {"text": "SCHEDULER_MEMORY_QUEUE", "href": "settings.html#std-setting-SCHEDULER_MEMORY_QUEUE"}, {"text": "SCHEDULER_DISK_QUEUE", "href": "settings.html#std-setting-SCHEDULER_DISK_QUEUE"}, {"text": "JOBDIR", "href": "settings.html#std-setting-JOBDIR"}, {"text": "DUPEFILTER_CLASS", "href": "settings.html#std-setting-DUPEFILTER_CLASS"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "JOBDIR", "href": "settings.html#std-setting-JOBDIR"}, {"text": "Jobs: pausing and resuming crawls", "href": "jobs.html#topics-jobs"}, {"text": "SCHEDULER_DISK_QUEUE", "href": "settings.html#std-setting-SCHEDULER_DISK_QUEUE"}, {"text": "SCHEDULER_MEMORY_QUEUE", "href": "settings.html#std-setting-SCHEDULER_MEMORY_QUEUE"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "SCHEDULER_DEBUG", "href": "settings.html#std-setting-SCHEDULER_DEBUG"}, {"text": "scrapy.statscollectors.StatsCollector", "href": "api.html#scrapy.statscollectors.StatsCollector"}, {"text": "STATS_CLASS", "href": "settings.html#std-setting-STATS_CLASS"}, {"text": "SCHEDULER_PRIORITY_QUEUE", "href": "settings.html#std-setting-SCHEDULER_PRIORITY_QUEUE"}, {"text": "scrapy.crawler.Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "int", "href": "https://docs.python.org/3/library/functions.html#int"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#Scheduler.__len__"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#Scheduler.close"}, {"text": "Request", "href": "request-response.html#scrapy.http.Request"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#Scheduler.enqueue_request"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#Scheduler.from_crawler"}, {"text": "bool", "href": "https://docs.python.org/3/library/functions.html#bool"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#Scheduler.has_pending_requests"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Request", "href": "request-response.html#scrapy.http.Request"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#Scheduler.next_request"}, {"text": "Request", "href": "request-response.html#scrapy.http.Request"}, {"text": "Spider", "href": "spiders.html#scrapy.spiders.Spider"}, {"text": "Optional", "href": "https://docs.python.org/3/library/typing.html#typing.Optional"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "[source]", "href": "../_modules/scrapy/core/scheduler.html#Scheduler.open"}], "timestamp": "2023-10-12T21:19:26.032335", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/signals.html", "content": {"sections": [], "paragraphs": ["<p>Scrapy uses signals extensively to notify when certain events occur. You can\ncatch some of those signals in your Scrapy project (using an <a class=\"hoverxref tooltip reference internal\" href=\"extensions.html#topics-extensions\"><span class=\"std std-ref\">extension</span></a>, for example) to perform additional tasks or extend Scrapy\nto add functionality not provided out of the box.</p>", "<p>Even though signals provide several arguments, the handlers that catch them\ndon’t need to accept all of them - the signal dispatching mechanism will only\ndeliver the arguments that the handler receives.</p>", "<p>You can connect to signals (or send your own) through the\n<a class=\"hoverxref tooltip reference internal\" href=\"api.html#topics-api-signals\"><span class=\"std std-ref\">Signals API</span></a>.</p>", "<p>Here is a simple example showing how you can catch signals and perform some action:</p>", "<p>Some signals support returning <a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html\" title=\"(in Twisted)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Deferred</span></code></a>\nor <a class=\"reference external\" href=\"https://docs.python.org/3/glossary.html#term-awaitable\" title=\"(in Python v3.11)\"><span class=\"xref std std-term\">awaitable objects</span></a> from their handlers, allowing\nyou to run asynchronous code that does not block Scrapy. If a signal\nhandler returns one of these objects, Scrapy waits for that asynchronous\noperation to finish.</p>", "<p>Let’s take an example using <a class=\"hoverxref tooltip reference internal\" href=\"coroutines.html#topics-coroutines\"><span class=\"std std-ref\">coroutines</span></a>:</p>", "<p>See the <a class=\"hoverxref tooltip reference internal\" href=\"#topics-signals-ref\"><span class=\"std std-ref\">Built-in signals reference</span></a> below to know which signals support\n<a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html\" title=\"(in Twisted)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Deferred</span></code></a> and <a class=\"reference external\" href=\"https://docs.python.org/3/glossary.html#term-awaitable\" title=\"(in Python v3.11)\"><span class=\"xref std std-term\">awaitable objects</span></a>.</p>", "<p>Here’s the list of Scrapy built-in signals and their meaning.</p>", "<p>Sent when the Scrapy engine has started crawling.</p>", "<p>This signal supports returning deferreds from its handlers.</p>", "<p class=\"admonition-title\">Note</p>", "<p>This signal may be fired <em>after</em> the <a class=\"hoverxref tooltip reference internal\" href=\"#std-signal-spider_opened\"><code class=\"xref std std-signal docutils literal notranslate\"><span class=\"pre\">spider_opened</span></code></a> signal,\ndepending on how the spider was started. So <strong>don’t</strong> rely on this signal\ngetting fired before <a class=\"hoverxref tooltip reference internal\" href=\"#std-signal-spider_opened\"><code class=\"xref std std-signal docutils literal notranslate\"><span class=\"pre\">spider_opened</span></code></a>.</p>", "<p>Sent when the Scrapy engine is stopped (for example, when a crawling\nprocess has finished).</p>", "<p>This signal supports returning deferreds from its handlers.</p>", "<p class=\"admonition-title\">Note</p>", "<p>As at max <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-CONCURRENT_ITEMS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">CONCURRENT_ITEMS</span></code></a> items are processed in\nparallel, many deferreds are fired together using\n<a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.internet.defer.DeferredList.html\" title=\"(in Twisted)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">DeferredList</span></code></a>. Hence the next\nbatch waits for the <a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.internet.defer.DeferredList.html\" title=\"(in Twisted)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">DeferredList</span></code></a>\nto fire and then runs the respective item signal handler for\nthe next batch of scraped items.</p>", "<p>Sent when an item has been scraped, after it has passed all the\n<a class=\"hoverxref tooltip reference internal\" href=\"item-pipeline.html#topics-item-pipeline\"><span class=\"std std-ref\">Item Pipeline</span></a> stages (without being dropped).</p>", "<p>This signal supports returning deferreds from its handlers.</p>", "<p><strong>item</strong> (<a class=\"hoverxref tooltip reference internal\" href=\"items.html#item-types\"><span class=\"std std-ref\">item object</span></a>) – the scraped item</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider which scraped the item</p>", "<p><strong>response</strong> (<a class=\"reference internal\" href=\"request-response.html#scrapy.http.Response\" title=\"scrapy.http.Response\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response</span></code></a> object) – the response from where the item was scraped</p>", "<p>Sent after an item has been dropped from the <a class=\"hoverxref tooltip reference internal\" href=\"item-pipeline.html#topics-item-pipeline\"><span class=\"std std-ref\">Item Pipeline</span></a>\nwhen some stage raised a <a class=\"reference internal\" href=\"exceptions.html#scrapy.exceptions.DropItem\" title=\"scrapy.exceptions.DropItem\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">DropItem</span></code></a> exception.</p>", "<p>This signal supports returning deferreds from its handlers.</p>", "<p><strong>item</strong> (<a class=\"hoverxref tooltip reference internal\" href=\"items.html#item-types\"><span class=\"std std-ref\">item object</span></a>) – the item dropped from the <a class=\"hoverxref tooltip reference internal\" href=\"item-pipeline.html#topics-item-pipeline\"><span class=\"std std-ref\">Item Pipeline</span></a></p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider which scraped the item</p>", "<p><strong>response</strong> (<a class=\"reference internal\" href=\"request-response.html#scrapy.http.Response\" title=\"scrapy.http.Response\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response</span></code></a> object) – the response from where the item was dropped</p>", "<p><strong>exception</strong> (<a class=\"reference internal\" href=\"exceptions.html#scrapy.exceptions.DropItem\" title=\"scrapy.exceptions.DropItem\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">DropItem</span></code></a> exception) – the exception (which must be a\n<a class=\"reference internal\" href=\"exceptions.html#scrapy.exceptions.DropItem\" title=\"scrapy.exceptions.DropItem\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">DropItem</span></code></a> subclass) which caused the item\nto be dropped</p>", "<p>Sent when a <a class=\"hoverxref tooltip reference internal\" href=\"item-pipeline.html#topics-item-pipeline\"><span class=\"std std-ref\">Item Pipeline</span></a> generates an error (i.e. raises\nan exception), except <a class=\"reference internal\" href=\"exceptions.html#scrapy.exceptions.DropItem\" title=\"scrapy.exceptions.DropItem\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">DropItem</span></code></a> exception.</p>", "<p>This signal supports returning deferreds from its handlers.</p>", "<p><strong>item</strong> (<a class=\"hoverxref tooltip reference internal\" href=\"items.html#item-types\"><span class=\"std std-ref\">item object</span></a>) – the item that caused the error in the <a class=\"hoverxref tooltip reference internal\" href=\"item-pipeline.html#topics-item-pipeline\"><span class=\"std std-ref\">Item Pipeline</span></a></p>", "<p><strong>response</strong> (<a class=\"reference internal\" href=\"request-response.html#scrapy.http.Response\" title=\"scrapy.http.Response\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response</span></code></a> object) – the response being processed when the exception was raised</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider which raised the exception</p>", "<p><strong>failure</strong> (<a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html\" title=\"(in Twisted)\"><em>twisted.python.failure.Failure</em></a>) – the exception raised</p>", "<p>Sent after a spider has been closed. This can be used to release per-spider\nresources reserved on <a class=\"hoverxref tooltip reference internal\" href=\"#std-signal-spider_opened\"><code class=\"xref std std-signal docutils literal notranslate\"><span class=\"pre\">spider_opened</span></code></a>.</p>", "<p>This signal supports returning deferreds from its handlers.</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider which has been closed</p>", "<p><strong>reason</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><em>str</em></a>) – a string which describes the reason why the spider was closed. If\nit was closed because the spider has completed scraping, the reason\nis <code class=\"docutils literal notranslate\"><span class=\"pre\">'finished'</span></code>. Otherwise, if the spider was manually closed by\ncalling the <code class=\"docutils literal notranslate\"><span class=\"pre\">close_spider</span></code> engine method, then the reason is the one\npassed in the <code class=\"docutils literal notranslate\"><span class=\"pre\">reason</span></code> argument of that method (which defaults to\n<code class=\"docutils literal notranslate\"><span class=\"pre\">'cancelled'</span></code>). If the engine was shutdown (for example, by hitting\nCtrl-C to stop it) the reason will be <code class=\"docutils literal notranslate\"><span class=\"pre\">'shutdown'</span></code>.</p>", "<p>Sent after a spider has been opened for crawling. This is typically used to\nreserve per-spider resources, but can be used for any task that needs to be\nperformed when a spider is opened.</p>", "<p>This signal supports returning deferreds from its handlers.</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider which has been opened</p>", "<p>Sent when a spider has gone idle, which means the spider has no further:</p>", "<p>requests waiting to be downloaded</p>", "<p>requests scheduled</p>", "<p>items being processed in the item pipeline</p>", "<p>If the idle state persists after all handlers of this signal have finished,\nthe engine starts closing the spider. After the spider has finished\nclosing, the <a class=\"hoverxref tooltip reference internal\" href=\"#std-signal-spider_closed\"><code class=\"xref std std-signal docutils literal notranslate\"><span class=\"pre\">spider_closed</span></code></a> signal is sent.</p>", "<p>You may raise a <a class=\"reference internal\" href=\"exceptions.html#scrapy.exceptions.DontCloseSpider\" title=\"scrapy.exceptions.DontCloseSpider\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">DontCloseSpider</span></code></a> exception to\nprevent the spider from being closed.</p>", "<p>Alternatively, you may raise a <a class=\"reference internal\" href=\"exceptions.html#scrapy.exceptions.CloseSpider\" title=\"scrapy.exceptions.CloseSpider\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">CloseSpider</span></code></a>\nexception to provide a custom spider closing reason. An\nidle handler is the perfect place to put some code that assesses\nthe final spider results and update the final closing reason\naccordingly (e.g. setting it to ‘too_few_results’ instead of\n‘finished’).</p>", "<p>This signal does not support returning deferreds from its handlers.</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider which has gone idle</p>", "<p class=\"admonition-title\">Note</p>", "<p>Scheduling some requests in your <a class=\"hoverxref tooltip reference internal\" href=\"#std-signal-spider_idle\"><code class=\"xref std std-signal docutils literal notranslate\"><span class=\"pre\">spider_idle</span></code></a> handler does\n<strong>not</strong> guarantee that it can prevent the spider from being closed,\nalthough it sometimes can. That’s because the spider may still remain idle\nif all the scheduled requests are rejected by the scheduler (e.g. filtered\ndue to duplication).</p>", "<p>Sent when a spider callback generates an error (i.e. raises an exception).</p>", "<p>This signal does not support returning deferreds from its handlers.</p>", "<p><strong>failure</strong> (<a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html\" title=\"(in Twisted)\"><em>twisted.python.failure.Failure</em></a>) – the exception raised</p>", "<p><strong>response</strong> (<a class=\"reference internal\" href=\"request-response.html#scrapy.http.Response\" title=\"scrapy.http.Response\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response</span></code></a> object) – the response being processed when the exception was raised</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider which raised the exception</p>", "<p>Sent when a <a class=\"hoverxref tooltip reference internal\" href=\"feed-exports.html#topics-feed-exports\"><span class=\"std std-ref\">feed exports</span></a> slot is closed.</p>", "<p>This signal supports returning deferreds from its handlers.</p>", "<p><strong>slot</strong> (<em>scrapy.extensions.feedexport.FeedSlot</em>) – the slot closed</p>", "<p>Sent when the <a class=\"hoverxref tooltip reference internal\" href=\"feed-exports.html#topics-feed-exports\"><span class=\"std std-ref\">feed exports</span></a> extension is closed,\nduring the handling of the <a class=\"hoverxref tooltip reference internal\" href=\"#std-signal-spider_closed\"><code class=\"xref std std-signal docutils literal notranslate\"><span class=\"pre\">spider_closed</span></code></a> signal by the extension,\nafter all feed exporting has been handled.</p>", "<p>This signal supports returning deferreds from its handlers.</p>", "<p>Sent when the engine schedules a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code>, to be\ndownloaded later.</p>", "<p>This signal does not support returning deferreds from its handlers.</p>", "<p><strong>request</strong> (<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> object) – the request that reached the scheduler</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider that yielded the request</p>", "<p>Sent when a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code>, scheduled by the engine to be\ndownloaded later, is rejected by the scheduler.</p>", "<p>This signal does not support returning deferreds from its handlers.</p>", "<p><strong>request</strong> (<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> object) – the request that reached the scheduler</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider that yielded the request</p>", "<p>Sent when a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> reached downloader.</p>", "<p>This signal does not support returning deferreds from its handlers.</p>", "<p><strong>request</strong> (<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> object) – the request that reached downloader</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider that yielded the request</p>", "<p>Sent when a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> leaves the downloader, even in case of\nfailure.</p>", "<p>This signal does not support returning deferreds from its handlers.</p>", "<p><strong>request</strong> (<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> object) – the request that reached the downloader</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider that yielded the request</p>", "<p>Sent by the HTTP 1.1 and S3 download handlers when a group of bytes is\nreceived for a specific request. This signal might be fired multiple\ntimes for the same request, with partial data each time. For instance,\na possible scenario for a 25 kb response would be two signals fired\nwith 10 kb of data, and a final one with 5 kb of data.</p>", "<p>Handlers for this signal can stop the download of a response while it\nis in progress by raising the <a class=\"reference internal\" href=\"exceptions.html#scrapy.exceptions.StopDownload\" title=\"scrapy.exceptions.StopDownload\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">StopDownload</span></code></a>\nexception. Please refer to the <a class=\"hoverxref tooltip reference internal\" href=\"request-response.html#topics-stop-response-download\"><span class=\"std std-ref\">Stopping the download of a Response</span></a> topic\nfor additional information and examples.</p>", "<p>This signal does not support returning deferreds from its handlers.</p>", "<p><strong>data</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#bytes\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">bytes</span></code></a> object) – the data received by the download handler</p>", "<p><strong>request</strong> (<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> object) – the request that generated the download</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider associated with the response</p>", "<p>Sent by the HTTP 1.1 and S3 download handlers when the response headers are\navailable for a given request, before downloading any additional content.</p>", "<p>Handlers for this signal can stop the download of a response while it\nis in progress by raising the <a class=\"reference internal\" href=\"exceptions.html#scrapy.exceptions.StopDownload\" title=\"scrapy.exceptions.StopDownload\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">StopDownload</span></code></a>\nexception. Please refer to the <a class=\"hoverxref tooltip reference internal\" href=\"request-response.html#topics-stop-response-download\"><span class=\"std std-ref\">Stopping the download of a Response</span></a> topic\nfor additional information and examples.</p>", "<p>This signal does not support returning deferreds from its handlers.</p>", "<p><strong>headers</strong> (<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">scrapy.http.headers.Headers</span></code> object) – the headers received by the download handler</p>", "<p><strong>body_length</strong> (<cite>int</cite>) – expected size of the response body, in bytes</p>", "<p><strong>request</strong> (<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> object) – the request that generated the download</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider associated with the response</p>", "<p>Sent when the engine receives a new <a class=\"reference internal\" href=\"request-response.html#scrapy.http.Response\" title=\"scrapy.http.Response\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response</span></code></a> from the\ndownloader.</p>", "<p>This signal does not support returning deferreds from its handlers.</p>", "<p><strong>response</strong> (<a class=\"reference internal\" href=\"request-response.html#scrapy.http.Response\" title=\"scrapy.http.Response\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response</span></code></a> object) – the response received</p>", "<p><strong>request</strong> (<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> object) – the request that generated the response</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider for which the response is intended</p>", "<p class=\"admonition-title\">Note</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">request</span></code> argument might not contain the original request that\nreached the downloader, if a <a class=\"hoverxref tooltip reference internal\" href=\"downloader-middleware.html#topics-downloader-middleware\"><span class=\"std std-ref\">Downloader Middleware</span></a> modifies\nthe <a class=\"reference internal\" href=\"request-response.html#scrapy.http.Response\" title=\"scrapy.http.Response\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response</span></code></a> object and sets a specific <code class=\"docutils literal notranslate\"><span class=\"pre\">request</span></code>\nattribute.</p>", "<p>Sent by the downloader right after a <code class=\"docutils literal notranslate\"><span class=\"pre\">HTTPResponse</span></code> is downloaded.</p>", "<p>This signal does not support returning deferreds from its handlers.</p>", "<p><strong>response</strong> (<a class=\"reference internal\" href=\"request-response.html#scrapy.http.Response\" title=\"scrapy.http.Response\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response</span></code></a> object) – the response downloaded</p>", "<p><strong>request</strong> (<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> object) – the request that generated the response</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider for which the response is intended</p>"]}, "code_blocks": ["from scrapy import signals\nfrom scrapy import Spider\n\n\nclass DmozSpider(Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\",\n    ]\n\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        spider = super(DmozSpider, cls).from_crawler(crawler, *args, **kwargs)\n        crawler.signals.connect(spider.spider_closed, signal=signals.spider_closed)\n        return spider\n\n    def spider_closed(self, spider):\n        spider.logger.info(\"Spider closed: %s\", spider.name)\n\n    def parse(self, response):\n        pass\n</pre>", "import scrapy\n\n\nclass SignalSpider(scrapy.Spider):\n    name = \"signals\"\n    start_urls = [\"https://quotes.toscrape.com/page/1/\"]\n\n    @classmethod\n    def from_crawler(cls, crawler, *args, **kwargs):\n        spider = super(SignalSpider, cls).from_crawler(crawler, *args, **kwargs)\n        crawler.signals.connect(spider.item_scraped, signal=signals.item_scraped)\n        return spider\n\n    async def item_scraped(self, item):\n        # Send the scraped item to the server\n        response = await treq.post(\n            \"http://example.com/post\",\n            json.dumps(item).encode(\"ascii\"),\n            headers={b\"Content-Type\": [b\"application/json\"]},\n        )\n\n        return response\n\n    def parse(self, response):\n        for quote in response.css(\"div.quote\"):\n            yield {\n                \"text\": quote.css(\"span.text::text\").get(),\n                \"author\": quote.css(\"small.author::text\").get(),\n                \"tags\": quote.css(\"div.tags a.tag::text\").getall(),\n            }\n</pre>"], "links": [{"text": "extension", "href": "extensions.html#topics-extensions"}, {"text": "Signals API", "href": "api.html#topics-api-signals"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "awaitable objects", "href": "https://docs.python.org/3/glossary.html#term-awaitable"}, {"text": "coroutines", "href": "coroutines.html#topics-coroutines"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "awaitable objects", "href": "https://docs.python.org/3/glossary.html#term-awaitable"}, {"text": "CONCURRENT_ITEMS", "href": "settings.html#std-setting-CONCURRENT_ITEMS"}, {"text": "DeferredList", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.DeferredList.html"}, {"text": "DeferredList", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.DeferredList.html"}, {"text": "Item Pipeline", "href": "item-pipeline.html#topics-item-pipeline"}, {"text": "item object", "href": "items.html#item-types"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Item Pipeline", "href": "item-pipeline.html#topics-item-pipeline"}, {"text": "DropItem", "href": "exceptions.html#scrapy.exceptions.DropItem"}, {"text": "item object", "href": "items.html#item-types"}, {"text": "Item Pipeline", "href": "item-pipeline.html#topics-item-pipeline"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "DropItem", "href": "exceptions.html#scrapy.exceptions.DropItem"}, {"text": "DropItem", "href": "exceptions.html#scrapy.exceptions.DropItem"}, {"text": "Item Pipeline", "href": "item-pipeline.html#topics-item-pipeline"}, {"text": "DropItem", "href": "exceptions.html#scrapy.exceptions.DropItem"}, {"text": "item object", "href": "items.html#item-types"}, {"text": "Item Pipeline", "href": "item-pipeline.html#topics-item-pipeline"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "twisted.python.failure.Failure", "href": "https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "DontCloseSpider", "href": "exceptions.html#scrapy.exceptions.DontCloseSpider"}, {"text": "CloseSpider", "href": "exceptions.html#scrapy.exceptions.CloseSpider"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "twisted.python.failure.Failure", "href": "https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "feed exports", "href": "feed-exports.html#topics-feed-exports"}, {"text": "feed exports", "href": "feed-exports.html#topics-feed-exports"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "StopDownload", "href": "exceptions.html#scrapy.exceptions.StopDownload"}, {"text": "Stopping the download of a Response", "href": "request-response.html#topics-stop-response-download"}, {"text": "bytes", "href": "https://docs.python.org/3/library/stdtypes.html#bytes"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "StopDownload", "href": "exceptions.html#scrapy.exceptions.StopDownload"}, {"text": "Stopping the download of a Response", "href": "request-response.html#topics-stop-response-download"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Downloader Middleware", "href": "downloader-middleware.html#topics-downloader-middleware"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}], "timestamp": "2023-10-12T21:19:28.826675", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/extensions.html", "content": {"sections": [], "paragraphs": ["<p>The extensions framework provides a mechanism for inserting your own\ncustom functionality into Scrapy.</p>", "<p>Extensions are just regular classes.</p>", "<p>Extensions use the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#topics-settings\"><span class=\"std std-ref\">Scrapy settings</span></a> to manage their\nsettings, just like any other Scrapy code.</p>", "<p>It is customary for extensions to prefix their settings with their own name, to\navoid collision with existing (and future) extensions. For example, a\nhypothetical extension to handle <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Sitemaps\">Google Sitemaps</a> would use settings like\n<code class=\"docutils literal notranslate\"><span class=\"pre\">GOOGLESITEMAP_ENABLED</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">GOOGLESITEMAP_DEPTH</span></code>, and so on.</p>", "<p>Extensions are loaded and activated at startup by instantiating a single\ninstance of the extension class per spider being run. All the extension\ninitialization code must be performed in the class <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method.</p>", "<p>To make an extension available, add it to the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-EXTENSIONS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">EXTENSIONS</span></code></a> setting in\nyour Scrapy settings. In <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-EXTENSIONS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">EXTENSIONS</span></code></a>, each extension is represented\nby a string: the full Python path to the extension’s class name. For example:</p>", "<p>As you can see, the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-EXTENSIONS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">EXTENSIONS</span></code></a> setting is a dict where the keys are\nthe extension paths, and their values are the orders, which define the\nextension <em>loading</em> order. The <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-EXTENSIONS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">EXTENSIONS</span></code></a> setting is merged with the\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-EXTENSIONS_BASE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">EXTENSIONS_BASE</span></code></a> setting defined in Scrapy (and not meant to be\noverridden) and then sorted by order to get the final sorted list of enabled\nextensions.</p>", "<p>As extensions typically do not depend on each other, their loading order is\nirrelevant in most cases. This is why the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-EXTENSIONS_BASE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">EXTENSIONS_BASE</span></code></a> setting\ndefines all extensions with the same order (<code class=\"docutils literal notranslate\"><span class=\"pre\">0</span></code>). However, this feature can\nbe exploited if you need to add an extension which depends on other extensions\nalready loaded.</p>", "<p>Not all available extensions will be enabled. Some of them usually depend on a\nparticular setting. For example, the HTTP Cache extension is available by default\nbut disabled unless the <a class=\"hoverxref tooltip reference internal\" href=\"downloader-middleware.html#std-setting-HTTPCACHE_ENABLED\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">HTTPCACHE_ENABLED</span></code></a> setting is set.</p>", "<p>In order to disable an extension that comes enabled by default (i.e. those\nincluded in the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-EXTENSIONS_BASE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">EXTENSIONS_BASE</span></code></a> setting) you must set its order to\n<code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>. For example:</p>", "<p>Each extension is a Python class. The main entry point for a Scrapy extension\n(this also includes middlewares and pipelines) is the <code class=\"docutils literal notranslate\"><span class=\"pre\">from_crawler</span></code>\nclass method which receives a <code class=\"docutils literal notranslate\"><span class=\"pre\">Crawler</span></code> instance. Through the Crawler object\nyou can access settings, signals, stats, and also control the crawling behaviour.</p>", "<p>Typically, extensions connect to <a class=\"hoverxref tooltip reference internal\" href=\"signals.html#topics-signals\"><span class=\"std std-ref\">signals</span></a> and perform\ntasks triggered by them.</p>", "<p>Finally, if the <code class=\"docutils literal notranslate\"><span class=\"pre\">from_crawler</span></code> method raises the\n<a class=\"reference internal\" href=\"exceptions.html#scrapy.exceptions.NotConfigured\" title=\"scrapy.exceptions.NotConfigured\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">NotConfigured</span></code></a> exception, the extension will be\ndisabled. Otherwise, the extension will be enabled.</p>", "<p>Here we will implement a simple extension to illustrate the concepts described\nin the previous section. This extension will log a message every time:</p>", "<p>a spider is opened</p>", "<p>a spider is closed</p>", "<p>a specific number of items are scraped</p>", "<p>The extension will be enabled through the <code class=\"docutils literal notranslate\"><span class=\"pre\">MYEXT_ENABLED</span></code> setting and the\nnumber of items will be specified through the <code class=\"docutils literal notranslate\"><span class=\"pre\">MYEXT_ITEMCOUNT</span></code> setting.</p>", "<p>Here is the code of such extension:</p>", "<p>Log basic stats like crawled pages and scraped items.</p>", "<p>Enable the collection of core statistics, provided the stats collection is\nenabled (see <a class=\"hoverxref tooltip reference internal\" href=\"stats.html#topics-stats\"><span class=\"std std-ref\">Stats Collection</span></a>).</p>", "<p>Provides a telnet console for getting into a Python interpreter inside the\ncurrently running Scrapy process, which can be very useful for debugging.</p>", "<p>The telnet console must be enabled by the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-TELNETCONSOLE_ENABLED\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">TELNETCONSOLE_ENABLED</span></code></a>\nsetting, and the server will listen in the port specified in\n<a class=\"hoverxref tooltip reference internal\" href=\"telnetconsole.html#std-setting-TELNETCONSOLE_PORT\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">TELNETCONSOLE_PORT</span></code></a>.</p>", "<p class=\"admonition-title\">Note</p>", "<p>This extension does not work in Windows.</p>", "<p>Monitors the memory used by the Scrapy process that runs the spider and:</p>", "<p>sends a notification e-mail when it exceeds a certain value</p>", "<p>closes the spider when it exceeds a certain value</p>", "<p>The notification e-mails can be triggered when a certain warning value is\nreached (<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-MEMUSAGE_WARNING_MB\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">MEMUSAGE_WARNING_MB</span></code></a>) and when the maximum value is reached\n(<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-MEMUSAGE_LIMIT_MB\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">MEMUSAGE_LIMIT_MB</span></code></a>) which will also cause the spider to be closed\nand the Scrapy process to be terminated.</p>", "<p>This extension is enabled by the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-MEMUSAGE_ENABLED\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">MEMUSAGE_ENABLED</span></code></a> setting and\ncan be configured with the following settings:</p>", "<p>An extension for debugging memory usage. It collects information about:</p>", "<p>objects uncollected by the Python garbage collector</p>", "<p>objects left alive that shouldn’t. For more info, see <a class=\"hoverxref tooltip reference internal\" href=\"leaks.html#topics-leaks-trackrefs\"><span class=\"std std-ref\">Debugging memory leaks with trackref</span></a></p>", "<p>To enable this extension, turn on the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-MEMDEBUG_ENABLED\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">MEMDEBUG_ENABLED</span></code></a> setting. The\ninfo will be stored in the stats.</p>", "<p>Closes a spider automatically when some conditions are met, using a specific\nclosing reason for each condition.</p>", "<p>The conditions for closing a spider can be configured through the following\nsettings:</p>", "<p class=\"admonition-title\">Note</p>", "<p>When a certain closing condition is met, requests which are\ncurrently in the downloader queue (up to <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-CONCURRENT_REQUESTS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">CONCURRENT_REQUESTS</span></code></a>\nrequests) are still processed.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">0</span></code></p>", "<p>An integer which specifies a number of seconds. If the spider remains open for\nmore than that number of second, it will be automatically closed with the\nreason <code class=\"docutils literal notranslate\"><span class=\"pre\">closespider_timeout</span></code>. If zero (or non set), spiders won’t be closed by\ntimeout.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">0</span></code></p>", "<p>An integer which specifies a number of seconds. If the spider has not produced\nany items in the last number of seconds, it will be closed with the reason\n<code class=\"docutils literal notranslate\"><span class=\"pre\">closespider_timeout_no_item</span></code>. If zero (or non set), spiders won’t be closed\nregardless if it hasn’t produced any items.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">0</span></code></p>", "<p>An integer which specifies a number of items. If the spider scrapes more than\nthat amount and those items are passed by the item pipeline, the\nspider will be closed with the reason <code class=\"docutils literal notranslate\"><span class=\"pre\">closespider_itemcount</span></code>.\nIf zero (or non set), spiders won’t be closed by number of passed items.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">0</span></code></p>", "<p>An integer which specifies the maximum number of responses to crawl. If the spider\ncrawls more than that, the spider will be closed with the reason\n<code class=\"docutils literal notranslate\"><span class=\"pre\">closespider_pagecount</span></code>. If zero (or non set), spiders won’t be closed by\nnumber of crawled responses.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">0</span></code></p>", "<p>An integer which specifies the maximum number of errors to receive before\nclosing the spider. If the spider generates more than that number of errors,\nit will be closed with the reason <code class=\"docutils literal notranslate\"><span class=\"pre\">closespider_errorcount</span></code>. If zero (or non\nset), spiders won’t be closed by number of errors.</p>", "<p>This simple extension can be used to send a notification e-mail every time a\ndomain has finished scraping, including the Scrapy stats collected. The email\nwill be sent to all recipients specified in the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-STATSMAILER_RCPTS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">STATSMAILER_RCPTS</span></code></a>\nsetting.</p>", "<p>Emails can be sent using the <a class=\"reference internal\" href=\"email.html#scrapy.mail.MailSender\" title=\"scrapy.mail.MailSender\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MailSender</span></code></a> class. To see a\nfull list of parameters, including examples on how to instantiate\n<a class=\"reference internal\" href=\"email.html#scrapy.mail.MailSender\" title=\"scrapy.mail.MailSender\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MailSender</span></code></a> and use mail settings, see\n<a class=\"hoverxref tooltip reference internal\" href=\"email.html#topics-email\"><span class=\"std std-ref\">Sending e-mail</span></a>.</p>", "<p>This extension periodically logs rich stat data as a JSON object:</p>", "<p>This extension logs the following configurable sections:</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">\"delta\"</span></code> shows how some numeric stats have changed since the last stats\nlog message.</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"#std-setting-PERIODIC_LOG_DELTA\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">PERIODIC_LOG_DELTA</span></code></a> setting determines the target stats. They\nmust have <code class=\"docutils literal notranslate\"><span class=\"pre\">int</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">float</span></code> values.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">\"stats\"</span></code> shows the current value of some stats.</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"#std-setting-PERIODIC_LOG_STATS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">PERIODIC_LOG_STATS</span></code></a> setting determines the target stats.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">\"time\"</span></code> shows detailed timing data.</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"#std-setting-PERIODIC_LOG_TIMING_ENABLED\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">PERIODIC_LOG_TIMING_ENABLED</span></code></a> setting determines whether or\nnot to show this section.</p>", "<p>This extension logs data at the start, then on a fixed time interval\nconfigurable through the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-LOGSTATS_INTERVAL\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">LOGSTATS_INTERVAL</span></code></a> setting, and finally\nright before the crawl ends.</p>", "<p>Example extension configuration:</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code></p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">\"PERIODIC_LOG_DELTA\":</span> <span class=\"pre\">True</span></code> - show deltas for all <code class=\"docutils literal notranslate\"><span class=\"pre\">int</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">float</span></code> stat values.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">\"PERIODIC_LOG_DELTA\":</span> <span class=\"pre\">{\"include\":</span> <span class=\"pre\">[\"downloader/\",</span> <span class=\"pre\">\"scheduler/\"]}</span></code> - show deltas for stats with names containing any configured substring.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">\"PERIODIC_LOG_DELTA\":</span> <span class=\"pre\">{\"exclude\":</span> <span class=\"pre\">[\"downloader/\"]}</span></code> - show deltas for all stats with names not containing any configured substring.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code></p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">\"PERIODIC_LOG_STATS\":</span> <span class=\"pre\">True</span></code> - show the current value of all stats.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">\"PERIODIC_LOG_STATS\":</span> <span class=\"pre\">{\"include\":</span> <span class=\"pre\">[\"downloader/\",</span> <span class=\"pre\">\"scheduler/\"]}</span></code> - show current values for stats with names containing any configured substring.</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">\"PERIODIC_LOG_STATS\":</span> <span class=\"pre\">{\"exclude\":</span> <span class=\"pre\">[\"downloader/\"]}</span></code> - show current values for all stats with names not containing any configured substring.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code></p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code> enables logging of timing data (i.e. the <code class=\"docutils literal notranslate\"><span class=\"pre\">\"time\"</span></code> section).</p>", "<p>Dumps information about the running process when a <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/SIGQUIT\">SIGQUIT</a> or <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2\">SIGUSR2</a>\nsignal is received. The information dumped is the following:</p>", "<p>engine status (using <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.engine.get_engine_status()</span></code>)</p>", "<p>live references (see <a class=\"hoverxref tooltip reference internal\" href=\"leaks.html#topics-leaks-trackrefs\"><span class=\"std std-ref\">Debugging memory leaks with trackref</span></a>)</p>", "<p>stack trace of all threads</p>", "<p>After the stack trace and engine status is dumped, the Scrapy process continues\nrunning normally.</p>", "<p>This extension only works on POSIX-compliant platforms (i.e. not Windows),\nbecause the <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/SIGQUIT\">SIGQUIT</a> and <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2\">SIGUSR2</a> signals are not available on Windows.</p>", "<p>There are at least two ways to send Scrapy the <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/SIGQUIT\">SIGQUIT</a> signal:</p>", "<p>By pressing Ctrl-while a Scrapy process is running (Linux only?)</p>", "<p>By running this command (assuming <code class=\"docutils literal notranslate\"><span class=\"pre\">&lt;pid&gt;</span></code> is the process id of the Scrapy\nprocess):</p>", "<p>Invokes a <a class=\"reference external\" href=\"https://docs.python.org/3/library/pdb.html\" title=\"(in Python v3.11)\"><span class=\"xref std std-doc\">Python debugger</span></a> inside a running Scrapy process when a <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2\">SIGUSR2</a>\nsignal is received. After the debugger is exited, the Scrapy process continues\nrunning normally.</p>", "<p>For more info see <a class=\"reference external\" href=\"https://pythonconquerstheuniverse.wordpress.com/2009/09/10/debugging-in-python/\">Debugging in Python</a>.</p>", "<p>This extension only works on POSIX-compliant platforms (i.e. not Windows).</p>"]}, "code_blocks": ["EXTENSIONS = {\n    \"scrapy.extensions.corestats.CoreStats\": 500,\n    \"scrapy.extensions.telnet.TelnetConsole\": 500,\n}\n</pre>", "EXTENSIONS = {\n    \"scrapy.extensions.corestats.CoreStats\": None,\n}\n</pre>", "import logging\nfrom scrapy import signals\nfrom scrapy.exceptions import NotConfigured\n\nlogger = logging.getLogger(__name__)\n\n\nclass SpiderOpenCloseLogging:\n    def __init__(self, item_count):\n        self.item_count = item_count\n        self.items_scraped = 0\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # first check if the extension should be enabled and raise\n        # NotConfigured otherwise\n        if not crawler.settings.getbool(\"MYEXT_ENABLED\"):\n            raise NotConfigured\n\n        # get the number of items from settings\n        item_count = crawler.settings.getint(\"MYEXT_ITEMCOUNT\", 1000)\n\n        # instantiate the extension object\n        ext = cls(item_count)\n\n        # connect the extension object to signals\n        crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(ext.item_scraped, signal=signals.item_scraped)\n\n        # return the extension object\n        return ext\n\n    def spider_opened(self, spider):\n        logger.info(\"opened spider %s\", spider.name)\n\n    def spider_closed(self, spider):\n        logger.info(\"closed spider %s\", spider.name)\n\n    def item_scraped(self, item, spider):\n        self.items_scraped += 1\n        if self.items_scraped % self.item_count == 0:\n            logger.info(\"scraped %d items\", self.items_scraped)\n</pre>", "2023-08-04 02:30:57 [scrapy.extensions.logstats] INFO: Crawled 976 pages (at 162 pages/min), scraped 925 items (at 161 items/min)\n2023-08-04 02:30:57 [scrapy.extensions.periodic_log] INFO: {\n    \"delta\": {\n        \"downloader/request_bytes\": 55582,\n        \"downloader/request_count\": 162,\n        \"downloader/request_method_count/GET\": 162,\n        \"downloader/response_bytes\": 618133,\n        \"downloader/response_count\": 162,\n        \"downloader/response_status_count/200\": 162,\n        \"item_scraped_count\": 161\n    },\n    \"stats\": {\n        \"downloader/request_bytes\": 338243,\n        \"downloader/request_count\": 992,\n        \"downloader/request_method_count/GET\": 992,\n        \"downloader/response_bytes\": 3836736,\n        \"downloader/response_count\": 976,\n        \"downloader/response_status_count/200\": 976,\n        \"item_scraped_count\": 925,\n        \"log_count/INFO\": 21,\n        \"log_count/WARNING\": 1,\n        \"scheduler/dequeued\": 992,\n        \"scheduler/dequeued/memory\": 992,\n        \"scheduler/enqueued\": 1050,\n        \"scheduler/enqueued/memory\": 1050\n    },\n    \"time\": {\n        \"elapsed\": 360.008903,\n        \"log_interval\": 60.0,\n        \"log_interval_real\": 60.006694,\n        \"start_time\": \"2023-08-03 23:24:57\",\n        \"utcnow\": \"2023-08-03 23:30:57\"\n    }\n}\n</pre>", "custom_settings = {\n    \"LOG_LEVEL\": \"INFO\",\n    \"PERIODIC_LOG_STATS\": {\n        \"include\": [\"downloader/\", \"scheduler/\", \"log_count/\", \"item_scraped_count/\"],\n    },\n    \"PERIODIC_LOG_DELTA\": {\"include\": [\"downloader/\"]},\n    \"PERIODIC_LOG_TIMING_ENABLED\": True,\n    \"EXTENSIONS\": {\n        \"scrapy.extensions.periodic_log.PeriodicLog\": 0,\n    },\n}\n</pre>", "kill -QUIT <pid>\n</pre>"], "links": [{"text": "Scrapy settings", "href": "settings.html#topics-settings"}, {"text": "Google Sitemaps", "href": "https://en.wikipedia.org/wiki/Sitemaps"}, {"text": "EXTENSIONS", "href": "settings.html#std-setting-EXTENSIONS"}, {"text": "EXTENSIONS", "href": "settings.html#std-setting-EXTENSIONS"}, {"text": "EXTENSIONS", "href": "settings.html#std-setting-EXTENSIONS"}, {"text": "EXTENSIONS", "href": "settings.html#std-setting-EXTENSIONS"}, {"text": "EXTENSIONS_BASE", "href": "settings.html#std-setting-EXTENSIONS_BASE"}, {"text": "EXTENSIONS_BASE", "href": "settings.html#std-setting-EXTENSIONS_BASE"}, {"text": "HTTPCACHE_ENABLED", "href": "downloader-middleware.html#std-setting-HTTPCACHE_ENABLED"}, {"text": "EXTENSIONS_BASE", "href": "settings.html#std-setting-EXTENSIONS_BASE"}, {"text": "signals", "href": "signals.html#topics-signals"}, {"text": "NotConfigured", "href": "exceptions.html#scrapy.exceptions.NotConfigured"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/logstats.html#LogStats"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/corestats.html#CoreStats"}, {"text": "Stats Collection", "href": "stats.html#topics-stats"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/telnet.html#TelnetConsole"}, {"text": "TELNETCONSOLE_ENABLED", "href": "settings.html#std-setting-TELNETCONSOLE_ENABLED"}, {"text": "TELNETCONSOLE_PORT", "href": "telnetconsole.html#std-setting-TELNETCONSOLE_PORT"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/memusage.html#MemoryUsage"}, {"text": "MEMUSAGE_WARNING_MB", "href": "settings.html#std-setting-MEMUSAGE_WARNING_MB"}, {"text": "MEMUSAGE_LIMIT_MB", "href": "settings.html#std-setting-MEMUSAGE_LIMIT_MB"}, {"text": "MEMUSAGE_ENABLED", "href": "settings.html#std-setting-MEMUSAGE_ENABLED"}, {"text": "MEMUSAGE_LIMIT_MB", "href": "settings.html#std-setting-MEMUSAGE_LIMIT_MB"}, {"text": "MEMUSAGE_WARNING_MB", "href": "settings.html#std-setting-MEMUSAGE_WARNING_MB"}, {"text": "MEMUSAGE_NOTIFY_MAIL", "href": "settings.html#std-setting-MEMUSAGE_NOTIFY_MAIL"}, {"text": "MEMUSAGE_CHECK_INTERVAL_SECONDS", "href": "settings.html#std-setting-MEMUSAGE_CHECK_INTERVAL_SECONDS"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/memdebug.html#MemoryDebugger"}, {"text": "Debugging memory leaks with trackref", "href": "leaks.html#topics-leaks-trackrefs"}, {"text": "MEMDEBUG_ENABLED", "href": "settings.html#std-setting-MEMDEBUG_ENABLED"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/closespider.html#CloseSpider"}, {"text": "CONCURRENT_REQUESTS", "href": "settings.html#std-setting-CONCURRENT_REQUESTS"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/statsmailer.html#StatsMailer"}, {"text": "STATSMAILER_RCPTS", "href": "settings.html#std-setting-STATSMAILER_RCPTS"}, {"text": "MailSender", "href": "email.html#scrapy.mail.MailSender"}, {"text": "MailSender", "href": "email.html#scrapy.mail.MailSender"}, {"text": "Sending e-mail", "href": "email.html#topics-email"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/periodic_log.html#PeriodicLog"}, {"text": "LOGSTATS_INTERVAL", "href": "settings.html#std-setting-LOGSTATS_INTERVAL"}, {"text": "SIGQUIT", "href": "https://en.wikipedia.org/wiki/SIGQUIT"}, {"text": "SIGUSR2", "href": "https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2"}, {"text": "Debugging memory leaks with trackref", "href": "leaks.html#topics-leaks-trackrefs"}, {"text": "SIGQUIT", "href": "https://en.wikipedia.org/wiki/SIGQUIT"}, {"text": "SIGUSR2", "href": "https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2"}, {"text": "SIGQUIT", "href": "https://en.wikipedia.org/wiki/SIGQUIT"}, {"text": "Python debugger", "href": "https://docs.python.org/3/library/pdb.html"}, {"text": "SIGUSR2", "href": "https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2"}, {"text": "Debugging in Python", "href": "https://pythonconquerstheuniverse.wordpress.com/2009/09/10/debugging-in-python/"}], "timestamp": "2023-10-12T21:19:33.152543", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/spider-middleware.html", "content": {"sections": [], "paragraphs": ["<p>The spider middleware is a framework of hooks into Scrapy’s spider processing\nmechanism where you can plug custom functionality to process the responses that\nare sent to <a class=\"hoverxref tooltip reference internal\" href=\"spiders.html#topics-spiders\"><span class=\"std std-ref\">Spiders</span></a> for processing and to process the requests\nand items that are generated from spiders.</p>", "<p>To activate a spider middleware component, add it to the\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-SPIDER_MIDDLEWARES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SPIDER_MIDDLEWARES</span></code></a> setting, which is a dict whose keys are the\nmiddleware class path and their values are the middleware orders.</p>", "<p>Here’s an example:</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-SPIDER_MIDDLEWARES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SPIDER_MIDDLEWARES</span></code></a> setting is merged with the\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-SPIDER_MIDDLEWARES_BASE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SPIDER_MIDDLEWARES_BASE</span></code></a> setting defined in Scrapy (and not meant to\nbe overridden) and then sorted by order to get the final sorted list of enabled\nmiddlewares: the first middleware is the one closer to the engine and the last\nis the one closer to the spider. In other words,\nthe <a class=\"reference internal\" href=\"#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input\" title=\"scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_spider_input()</span></code></a>\nmethod of each middleware will be invoked in increasing\nmiddleware order (100, 200, 300, …), and the\n<a class=\"reference internal\" href=\"#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\" title=\"scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_spider_output()</span></code></a> method\nof each middleware will be invoked in decreasing order.</p>", "<p>To decide which order to assign to your middleware see the\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-SPIDER_MIDDLEWARES_BASE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SPIDER_MIDDLEWARES_BASE</span></code></a> setting and pick a value according to where\nyou want to insert the middleware. The order does matter because each\nmiddleware performs a different action and your middleware could depend on some\nprevious (or subsequent) middleware being applied.</p>", "<p>If you want to disable a builtin middleware (the ones defined in\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-SPIDER_MIDDLEWARES_BASE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SPIDER_MIDDLEWARES_BASE</span></code></a>, and enabled by default) you must define it\nin your project <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-SPIDER_MIDDLEWARES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SPIDER_MIDDLEWARES</span></code></a> setting and assign <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> as its\nvalue.  For example, if you want to disable the off-site middleware:</p>", "<p>Finally, keep in mind that some middlewares may need to be enabled through a\nparticular setting. See each middleware documentation for more info.</p>", "<p>Each spider middleware is a Python class that defines one or more of the\nmethods defined below.</p>", "<p>The main entry point is the <code class=\"docutils literal notranslate\"><span class=\"pre\">from_crawler</span></code> class method, which receives a\n<a class=\"reference internal\" href=\"api.html#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a> instance. The <a class=\"reference internal\" href=\"api.html#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a>\nobject gives you access, for example, to the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#topics-settings\"><span class=\"std std-ref\">settings</span></a>.</p>", "<p>This method is called for each response that goes through the spider\nmiddleware and into the spider, for processing.</p>", "<p><a class=\"reference internal\" href=\"#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input\" title=\"scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_spider_input()</span></code></a> should return <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> or raise an\nexception.</p>", "<p>If it returns <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>, Scrapy will continue processing this response,\nexecuting all other middlewares until, finally, the response is handed\nto the spider for processing.</p>", "<p>If it raises an exception, Scrapy won’t bother calling any other spider\nmiddleware <a class=\"reference internal\" href=\"#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input\" title=\"scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_spider_input()</span></code></a> and will call the request\nerrback if there is one, otherwise it will start the <a class=\"reference internal\" href=\"#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception\" title=\"scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_spider_exception()</span></code></a>\nchain. The output of the errback is chained back in the other\ndirection for <a class=\"reference internal\" href=\"#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\" title=\"scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_spider_output()</span></code></a> to process it, or\n<a class=\"reference internal\" href=\"#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception\" title=\"scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_spider_exception()</span></code></a> if it raised an exception.</p>", "<p><strong>response</strong> (<a class=\"reference internal\" href=\"request-response.html#scrapy.http.Response\" title=\"scrapy.http.Response\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response</span></code></a> object) – the response being processed</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider for which this response is intended</p>", "<p>This method is called with the results returned from the Spider, after\nit has processed the response.</p>", "<p><a class=\"reference internal\" href=\"#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\" title=\"scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_spider_output()</span></code></a> must return an iterable of\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> objects and <a class=\"hoverxref tooltip reference internal\" href=\"items.html#topics-items\"><span class=\"std std-ref\">item objects</span></a>.</p>", "<p><span class=\"versionmodified changed\">Changed in version 2.7: </span>This method may be defined as an <a class=\"reference external\" href=\"https://docs.python.org/3/glossary.html#term-asynchronous-generator\" title=\"(in Python v3.11)\"><span class=\"xref std std-term\">asynchronous generator</span></a>, in\nwhich case <code class=\"docutils literal notranslate\"><span class=\"pre\">result</span></code> is an <a class=\"reference external\" href=\"https://docs.python.org/3/glossary.html#term-asynchronous-iterable\" title=\"(in Python v3.11)\"><span class=\"xref std std-term\">asynchronous iterable</span></a>.</p>", "<p>Consider defining this method as an <a class=\"reference external\" href=\"https://docs.python.org/3/glossary.html#term-asynchronous-generator\" title=\"(in Python v3.11)\"><span class=\"xref std std-term\">asynchronous generator</span></a>,\nwhich will be a requirement in a future version of Scrapy. However, if\nyou plan on sharing your spider middleware with other people, consider\neither <a class=\"hoverxref tooltip reference internal\" href=\"components.html#enforce-component-requirements\"><span class=\"std std-ref\">enforcing Scrapy 2.7</span></a>\nas a minimum requirement of your spider middleware, or <a class=\"hoverxref tooltip reference internal\" href=\"coroutines.html#universal-spider-middleware\"><span class=\"std std-ref\">making\nyour spider middleware universal</span></a> so that\nit works with Scrapy versions earlier than Scrapy 2.7.</p>", "<p><strong>response</strong> (<a class=\"reference internal\" href=\"request-response.html#scrapy.http.Response\" title=\"scrapy.http.Response\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response</span></code></a> object) – the response which generated this output from the\nspider</p>", "<p><strong>result</strong> (an iterable of <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> objects and\n<a class=\"hoverxref tooltip reference internal\" href=\"items.html#topics-items\"><span class=\"std std-ref\">item objects</span></a>) – the result returned by the spider</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider whose result is being processed</p>", "<p>If defined, this method must be an <a class=\"reference external\" href=\"https://docs.python.org/3/glossary.html#term-asynchronous-generator\" title=\"(in Python v3.11)\"><span class=\"xref std std-term\">asynchronous generator</span></a>,\nwhich will be called instead of <a class=\"reference internal\" href=\"#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\" title=\"scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_spider_output()</span></code></a> if\n<code class=\"docutils literal notranslate\"><span class=\"pre\">result</span></code> is an <a class=\"reference external\" href=\"https://docs.python.org/3/glossary.html#term-asynchronous-iterable\" title=\"(in Python v3.11)\"><span class=\"xref std std-term\">asynchronous iterable</span></a>.</p>", "<p>This method is called when a spider or <a class=\"reference internal\" href=\"#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\" title=\"scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_spider_output()</span></code></a>\nmethod (from a previous spider middleware) raises an exception.</p>", "<p><a class=\"reference internal\" href=\"#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception\" title=\"scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_spider_exception()</span></code></a> should return either <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> or an\niterable of <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> or <a class=\"hoverxref tooltip reference internal\" href=\"items.html#topics-items\"><span class=\"std std-ref\">item</span></a>\nobjects.</p>", "<p>If it returns <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>, Scrapy will continue processing this exception,\nexecuting any other <a class=\"reference internal\" href=\"#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception\" title=\"scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_spider_exception()</span></code></a> in the following\nmiddleware components, until no middleware components are left and the\nexception reaches the engine (where it’s logged and discarded).</p>", "<p>If it returns an iterable the <a class=\"reference internal\" href=\"#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\" title=\"scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_spider_output()</span></code></a> pipeline\nkicks in, starting from the next spider middleware, and no other\n<a class=\"reference internal\" href=\"#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception\" title=\"scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_spider_exception()</span></code></a> will be called.</p>", "<p><strong>response</strong> (<a class=\"reference internal\" href=\"request-response.html#scrapy.http.Response\" title=\"scrapy.http.Response\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response</span></code></a> object) – the response being processed when the exception was\nraised</p>", "<p><strong>exception</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#Exception\" title=\"(in Python v3.11)\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">Exception</span></code></a> object) – the exception raised</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider which raised the exception</p>", "<p>This method is called with the start requests of the spider, and works\nsimilarly to the <a class=\"reference internal\" href=\"#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\" title=\"scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_spider_output()</span></code></a> method, except that it\ndoesn’t have a response associated and must return only requests (not\nitems).</p>", "<p>It receives an iterable (in the <code class=\"docutils literal notranslate\"><span class=\"pre\">start_requests</span></code> parameter) and must\nreturn another iterable of <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> objects.</p>", "<p class=\"admonition-title\">Note</p>", "<p>When implementing this method in your spider middleware, you\nshould always return an iterable (that follows the input one) and\nnot consume all <code class=\"docutils literal notranslate\"><span class=\"pre\">start_requests</span></code> iterator because it can be very\nlarge (or even unbounded) and cause a memory overflow. The Scrapy\nengine is designed to pull start requests while it has capacity to\nprocess them, so the start requests iterator can be effectively\nendless where there is some other condition for stopping the spider\n(like a time limit or item/page count).</p>", "<p><strong>start_requests</strong> (an iterable of <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code>) – the start requests</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider to whom the start requests belong</p>", "<p>If present, this classmethod is called to create a middleware instance\nfrom a <a class=\"reference internal\" href=\"api.html#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a>. It must return a new instance\nof the middleware. Crawler object provides access to all Scrapy core\ncomponents like settings and signals; it is a way for middleware to\naccess them and hook its functionality into Scrapy.</p>", "<p><strong>crawler</strong> (<a class=\"reference internal\" href=\"api.html#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a> object) – crawler that uses this middleware</p>", "<p>This page describes all spider middleware components that come with Scrapy. For\ninformation on how to use them and how to write your own spider middleware, see\nthe <a class=\"hoverxref tooltip reference internal\" href=\"#topics-spider-middleware\"><span class=\"std std-ref\">spider middleware usage guide</span></a>.</p>", "<p>For a list of the components enabled by default (and their orders) see the\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-SPIDER_MIDDLEWARES_BASE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SPIDER_MIDDLEWARES_BASE</span></code></a> setting.</p>", "<p>DepthMiddleware is used for tracking the depth of each Request inside the\nsite being scraped. It works by setting <code class=\"docutils literal notranslate\"><span class=\"pre\">request.meta['depth']</span> <span class=\"pre\">=</span> <span class=\"pre\">0</span></code> whenever\nthere is no value previously set (usually just the first Request) and\nincrementing it by 1 otherwise.</p>", "<p>It can be used to limit the maximum depth to scrape, control Request\npriority based on their depth, and things like that.</p>", "<p>The <a class=\"reference internal\" href=\"#scrapy.spidermiddlewares.depth.DepthMiddleware\" title=\"scrapy.spidermiddlewares.depth.DepthMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">DepthMiddleware</span></code></a> can be configured through the following\nsettings (see the settings documentation for more info):</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-DEPTH_LIMIT\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DEPTH_LIMIT</span></code></a> - The maximum depth that will be allowed to\ncrawl for any site. If zero, no limit will be imposed.</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-DEPTH_STATS_VERBOSE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DEPTH_STATS_VERBOSE</span></code></a> - Whether to collect the number of\nrequests for each depth.</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-DEPTH_PRIORITY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DEPTH_PRIORITY</span></code></a> - Whether to prioritize the requests based on\ntheir depth.</p>", "<p>Filter out unsuccessful (erroneous) HTTP responses so that spiders don’t\nhave to deal with them, which (most of the time) imposes an overhead,\nconsumes more resources, and makes the spider logic more complex.</p>", "<p>According to the <a class=\"reference external\" href=\"https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html\">HTTP standard</a>, successful responses are those whose\nstatus codes are in the 200-300 range.</p>", "<p>If you still want to process response codes outside that range, you can\nspecify which response codes the spider is able to handle using the\n<code class=\"docutils literal notranslate\"><span class=\"pre\">handle_httpstatus_list</span></code> spider attribute or\n<a class=\"hoverxref tooltip reference internal\" href=\"#std-setting-HTTPERROR_ALLOWED_CODES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">HTTPERROR_ALLOWED_CODES</span></code></a> setting.</p>", "<p>For example, if you want your spider to handle 404 responses you can do\nthis:</p>", "<p id=\"std-reqmeta-handle_httpstatus_all\">The <code class=\"docutils literal notranslate\"><span class=\"pre\">handle_httpstatus_list</span></code> key of <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Request.meta</span></code> can also be used to specify which response codes to\nallow on a per-request basis. You can also set the meta key <code class=\"docutils literal notranslate\"><span class=\"pre\">handle_httpstatus_all</span></code>\nto <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code> if you want to allow any response code for a request, and <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code> to\ndisable the effects of the <code class=\"docutils literal notranslate\"><span class=\"pre\">handle_httpstatus_all</span></code> key.</p>", "<p>Keep in mind, however, that it’s usually a bad idea to handle non-200\nresponses, unless you really know what you’re doing.</p>", "<p>For more information see: <a class=\"reference external\" href=\"https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html\">HTTP Status Code Definitions</a>.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">[]</span></code></p>", "<p>Pass all responses with non-200 status codes contained in this list.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code></p>", "<p>Pass all responses, regardless of its status code.</p>", "<p>Filters out Requests for URLs outside the domains covered by the spider.</p>", "<p>This middleware filters out every request whose host names aren’t in the\nspider’s <a class=\"reference internal\" href=\"spiders.html#scrapy.Spider.allowed_domains\" title=\"scrapy.Spider.allowed_domains\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">allowed_domains</span></code></a> attribute.\nAll subdomains of any domain in the list are also allowed.\nE.g. the rule <code class=\"docutils literal notranslate\"><span class=\"pre\">www.example.org</span></code> will also allow <code class=\"docutils literal notranslate\"><span class=\"pre\">bob.www.example.org</span></code>\nbut not <code class=\"docutils literal notranslate\"><span class=\"pre\">www2.example.com</span></code> nor <code class=\"docutils literal notranslate\"><span class=\"pre\">example.com</span></code>.</p>", "<p>When your spider returns a request for a domain not belonging to those\ncovered by the spider, this middleware will log a debug message similar to\nthis one:</p>", "<p>To avoid filling the log with too much noise, it will only print one of\nthese messages for each new domain filtered. So, for example, if another\nrequest for <code class=\"docutils literal notranslate\"><span class=\"pre\">www.othersite.com</span></code> is filtered, no log message will be\nprinted. But if a request for <code class=\"docutils literal notranslate\"><span class=\"pre\">someothersite.com</span></code> is filtered, a message\nwill be printed (but only for the first request filtered).</p>", "<p>If the spider doesn’t define an\n<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider.allowed_domains\" title=\"scrapy.Spider.allowed_domains\"><code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">allowed_domains</span></code></a> attribute, or the\nattribute is empty, the offsite middleware will allow all requests.</p>", "<p>If the request has the <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">dont_filter</span></code> attribute\nset, the offsite middleware will allow the request even if its domain is not\nlisted in allowed domains.</p>", "<p>Populates Request <code class=\"docutils literal notranslate\"><span class=\"pre\">Referer</span></code> header, based on the URL of the Response which\ngenerated it.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code></p>", "<p>Whether to enable referer middleware.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">'scrapy.spidermiddlewares.referer.DefaultReferrerPolicy'</span></code></p>", "<p id=\"std-reqmeta-referrer_policy\"><a class=\"reference external\" href=\"https://www.w3.org/TR/referrer-policy\">Referrer Policy</a> to apply when populating Request “Referer” header.</p>", "<p class=\"admonition-title\">Note</p>", "<p>You can also set the Referrer Policy per request,\nusing the special <code class=\"docutils literal notranslate\"><span class=\"pre\">\"referrer_policy\"</span></code> <a class=\"hoverxref tooltip reference internal\" href=\"request-response.html#topics-request-meta\"><span class=\"std std-ref\">Request.meta</span></a> key,\nwith the same acceptable values as for the <code class=\"docutils literal notranslate\"><span class=\"pre\">REFERRER_POLICY</span></code> setting.</p>", "<p>either a path to a <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.spidermiddlewares.referer.ReferrerPolicy</span></code>\nsubclass — a custom policy or one of the built-in ones (see classes below),</p>", "<p>or one of the standard W3C-defined string values,</p>", "<p>or the special <code class=\"docutils literal notranslate\"><span class=\"pre\">\"scrapy-default\"</span></code>.</p>", "<p>String value</p>", "<p>Class name (as a string)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">\"scrapy-default\"</span></code> (default)</p>", "<p>A variant of “no-referrer-when-downgrade”,\nwith the addition that “Referer” is not sent if the parent request was\nusing <code class=\"docutils literal notranslate\"><span class=\"pre\">file://</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">s3://</span></code> scheme.</p>", "<p class=\"admonition-title\">Warning</p>", "<p>Scrapy’s default referrer policy — just like <a class=\"reference external\" href=\"https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade\">“no-referrer-when-downgrade”</a>,\nthe W3C-recommended value for browsers — will send a non-empty\n“Referer” header from any <code class=\"docutils literal notranslate\"><span class=\"pre\">http(s)://</span></code> to any <code class=\"docutils literal notranslate\"><span class=\"pre\">https://</span></code> URL,\neven if the domain is different.</p>", "<p><a class=\"reference external\" href=\"https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin\">“same-origin”</a> may be a better choice if you want to remove referrer\ninformation for cross-domain requests.</p>", "<p>The simplest policy is “no-referrer”, which specifies that no referrer information\nis to be sent along with requests made from a particular request client to any origin.\nThe header will be omitted entirely.</p>", "<p>The “no-referrer-when-downgrade” policy sends a full URL along with requests\nfrom a TLS-protected environment settings object to a potentially trustworthy URL,\nand requests from clients which are not TLS-protected to any origin.</p>", "<p>Requests from TLS-protected clients to non-potentially trustworthy URLs,\non the other hand, will contain no referrer information.\nA Referer HTTP header will not be sent.</p>", "<p>This is a user agent’s default behavior, if no policy is otherwise specified.</p>", "<p class=\"admonition-title\">Note</p>", "<p>“no-referrer-when-downgrade” policy is the W3C-recommended default,\nand is used by major web browsers.</p>", "<p>However, it is NOT Scrapy’s default referrer policy (see <a class=\"reference internal\" href=\"#scrapy.spidermiddlewares.referer.DefaultReferrerPolicy\" title=\"scrapy.spidermiddlewares.referer.DefaultReferrerPolicy\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">DefaultReferrerPolicy</span></code></a>).</p>", "<p>The “same-origin” policy specifies that a full URL, stripped for use as a referrer,\nis sent as referrer information when making same-origin requests from a particular request client.</p>", "<p>Cross-origin requests, on the other hand, will contain no referrer information.\nA Referer HTTP header will not be sent.</p>", "<p>The “origin” policy specifies that only the ASCII serialization\nof the origin of the request client is sent as referrer information\nwhen making both same-origin requests and cross-origin requests\nfrom a particular request client.</p>", "<p>The “strict-origin” policy sends the ASCII serialization\nof the origin of the request client when making requests:\n- from a TLS-protected environment settings object to a potentially trustworthy URL, and\n- from non-TLS-protected environment settings objects to any origin.</p>", "<p>Requests from TLS-protected request clients to non- potentially trustworthy URLs,\non the other hand, will contain no referrer information.\nA Referer HTTP header will not be sent.</p>", "<p>The “origin-when-cross-origin” policy specifies that a full URL,\nstripped for use as a referrer, is sent as referrer information\nwhen making same-origin requests from a particular request client,\nand only the ASCII serialization of the origin of the request client\nis sent as referrer information when making cross-origin requests\nfrom a particular request client.</p>", "<p>The “strict-origin-when-cross-origin” policy specifies that a full URL,\nstripped for use as a referrer, is sent as referrer information\nwhen making same-origin requests from a particular request client,\nand only the ASCII serialization of the origin of the request client\nwhen making cross-origin requests:</p>", "<p>from a TLS-protected environment settings object to a potentially trustworthy URL, and</p>", "<p>from non-TLS-protected environment settings objects to any origin.</p>", "<p>Requests from TLS-protected clients to non- potentially trustworthy URLs,\non the other hand, will contain no referrer information.\nA Referer HTTP header will not be sent.</p>", "<p>The “unsafe-url” policy specifies that a full URL, stripped for use as a referrer,\nis sent along with both cross-origin requests\nand same-origin requests made from a particular request client.</p>", "<p>Note: The policy’s name doesn’t lie; it is unsafe.\nThis policy will leak origins and paths from TLS-protected resources\nto insecure origins.\nCarefully consider the impact of setting such a policy for potentially sensitive documents.</p>", "<p class=\"admonition-title\">Warning</p>", "<p>“unsafe-url” policy is NOT recommended.</p>", "<p>Filters out requests with URLs longer than URLLENGTH_LIMIT</p>", "<p>The <a class=\"reference internal\" href=\"#scrapy.spidermiddlewares.urllength.UrlLengthMiddleware\" title=\"scrapy.spidermiddlewares.urllength.UrlLengthMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">UrlLengthMiddleware</span></code></a> can be configured through the following\nsettings (see the settings documentation for more info):</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-URLLENGTH_LIMIT\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">URLLENGTH_LIMIT</span></code></a> - The maximum URL length to allow for crawled URLs.</p>"]}, "code_blocks": ["SPIDER_MIDDLEWARES = {\n    \"myproject.middlewares.CustomSpiderMiddleware\": 543,\n}\n</pre>", "SPIDER_MIDDLEWARES = {\n    \"myproject.middlewares.CustomSpiderMiddleware\": 543,\n    \"scrapy.spidermiddlewares.offsite.OffsiteMiddleware\": None,\n}\n</pre>", "from scrapy.spiders import CrawlSpider\n\n\nclass MySpider(CrawlSpider):\n    handle_httpstatus_list = [404]\n</pre>", "DEBUG: Filtered offsite request to 'www.othersite.com': <GET http://www.othersite.com/some/page.html>\n</pre>"], "links": [{"text": "Spiders", "href": "spiders.html#topics-spiders"}, {"text": "SPIDER_MIDDLEWARES", "href": "settings.html#std-setting-SPIDER_MIDDLEWARES"}, {"text": "SPIDER_MIDDLEWARES", "href": "settings.html#std-setting-SPIDER_MIDDLEWARES"}, {"text": "SPIDER_MIDDLEWARES_BASE", "href": "settings.html#std-setting-SPIDER_MIDDLEWARES_BASE"}, {"text": "SPIDER_MIDDLEWARES_BASE", "href": "settings.html#std-setting-SPIDER_MIDDLEWARES_BASE"}, {"text": "SPIDER_MIDDLEWARES_BASE", "href": "settings.html#std-setting-SPIDER_MIDDLEWARES_BASE"}, {"text": "SPIDER_MIDDLEWARES", "href": "settings.html#std-setting-SPIDER_MIDDLEWARES"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "settings", "href": "settings.html#topics-settings"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "item objects", "href": "items.html#topics-items"}, {"text": "asynchronous generator", "href": "https://docs.python.org/3/glossary.html#term-asynchronous-generator"}, {"text": "asynchronous iterable", "href": "https://docs.python.org/3/glossary.html#term-asynchronous-iterable"}, {"text": "asynchronous generator", "href": "https://docs.python.org/3/glossary.html#term-asynchronous-generator"}, {"text": "enforcing Scrapy 2.7", "href": "components.html#enforce-component-requirements"}, {"text": "making\nyour spider middleware universal", "href": "coroutines.html#universal-spider-middleware"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "item objects", "href": "items.html#topics-items"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "asynchronous generator", "href": "https://docs.python.org/3/glossary.html#term-asynchronous-generator"}, {"text": "asynchronous iterable", "href": "https://docs.python.org/3/glossary.html#term-asynchronous-iterable"}, {"text": "item", "href": "items.html#topics-items"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Exception", "href": "https://docs.python.org/3/library/exceptions.html#Exception"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "SPIDER_MIDDLEWARES_BASE", "href": "settings.html#std-setting-SPIDER_MIDDLEWARES_BASE"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/depth.html#DepthMiddleware"}, {"text": "DEPTH_LIMIT", "href": "settings.html#std-setting-DEPTH_LIMIT"}, {"text": "DEPTH_STATS_VERBOSE", "href": "settings.html#std-setting-DEPTH_STATS_VERBOSE"}, {"text": "DEPTH_PRIORITY", "href": "settings.html#std-setting-DEPTH_PRIORITY"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/httperror.html#HttpErrorMiddleware"}, {"text": "HTTP standard", "href": "https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html"}, {"text": "HTTP Status Code Definitions", "href": "https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/offsite.html#OffsiteMiddleware"}, {"text": "allowed_domains", "href": "spiders.html#scrapy.Spider.allowed_domains"}, {"text": "allowed_domains", "href": "spiders.html#scrapy.Spider.allowed_domains"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/referer.html#RefererMiddleware"}, {"text": "Referrer Policy", "href": "https://www.w3.org/TR/referrer-policy"}, {"text": "Request.meta", "href": "request-response.html#topics-request-meta"}, {"text": "“no-referrer”", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer"}, {"text": "“no-referrer-when-downgrade”", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade"}, {"text": "“same-origin”", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin"}, {"text": "“origin”", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-origin"}, {"text": "“strict-origin”", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin"}, {"text": "“origin-when-cross-origin”", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin"}, {"text": "“strict-origin-when-cross-origin”", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin"}, {"text": "“unsafe-url”", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/referer.html#DefaultReferrerPolicy"}, {"text": "“no-referrer-when-downgrade”", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade"}, {"text": "“same-origin”", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/referer.html#NoReferrerPolicy"}, {"text": "https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/referer.html#NoReferrerWhenDowngradePolicy"}, {"text": "https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/referer.html#SameOriginPolicy"}, {"text": "https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/referer.html#OriginPolicy"}, {"text": "https://www.w3.org/TR/referrer-policy/#referrer-policy-origin", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-origin"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/referer.html#StrictOriginPolicy"}, {"text": "https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/referer.html#OriginWhenCrossOriginPolicy"}, {"text": "https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/referer.html#StrictOriginWhenCrossOriginPolicy"}, {"text": "https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/referer.html#UnsafeUrlPolicy"}, {"text": "https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url", "href": "https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url"}, {"text": "[source]", "href": "../_modules/scrapy/spidermiddlewares/urllength.html#UrlLengthMiddleware"}, {"text": "URLLENGTH_LIMIT", "href": "settings.html#std-setting-URLLENGTH_LIMIT"}], "timestamp": "2023-10-12T21:19:37.164212", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/downloader-middleware.html", "content": {"sections": [], "paragraphs": ["<p>The downloader middleware is a framework of hooks into Scrapy’s\nrequest/response processing.  It’s a light, low-level system for globally\naltering Scrapy’s requests and responses.</p>", "<p>To activate a downloader middleware component, add it to the\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-DOWNLOADER_MIDDLEWARES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOADER_MIDDLEWARES</span></code></a> setting, which is a dict whose keys are the\nmiddleware class paths and their values are the middleware orders.</p>", "<p>Here’s an example:</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-DOWNLOADER_MIDDLEWARES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOADER_MIDDLEWARES</span></code></a> setting is merged with the\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> setting defined in Scrapy (and not meant\nto be overridden) and then sorted by order to get the final sorted list of\nenabled middlewares: the first middleware is the one closer to the engine and\nthe last is the one closer to the downloader. In other words,\nthe <a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_request\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_request()</span></code></a>\nmethod of each middleware will be invoked in increasing\nmiddleware order (100, 200, 300, …) and the <a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_response\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_response()</span></code></a> method\nof each middleware will be invoked in decreasing order.</p>", "<p>To decide which order to assign to your middleware see the\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> setting and pick a value according to\nwhere you want to insert the middleware. The order does matter because each\nmiddleware performs a different action and your middleware could depend on some\nprevious (or subsequent) middleware being applied.</p>", "<p>If you want to disable a built-in middleware (the ones defined in\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> and enabled by default) you must define it\nin your project’s <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-DOWNLOADER_MIDDLEWARES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOADER_MIDDLEWARES</span></code></a> setting and assign <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>\nas its value.  For example, if you want to disable the user-agent middleware:</p>", "<p>Finally, keep in mind that some middlewares may need to be enabled through a\nparticular setting. See each middleware documentation for more info.</p>", "<p>Each downloader middleware is a Python class that defines one or more of the\nmethods defined below.</p>", "<p>The main entry point is the <code class=\"docutils literal notranslate\"><span class=\"pre\">from_crawler</span></code> class method, which receives a\n<a class=\"reference internal\" href=\"api.html#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a> instance. The <a class=\"reference internal\" href=\"api.html#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a>\nobject gives you access, for example, to the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#topics-settings\"><span class=\"std std-ref\">settings</span></a>.</p>", "<p class=\"admonition-title\">Note</p>", "<p>Any of the downloader middleware methods may also return a deferred.</p>", "<p>This method is called for each request that goes through the download\nmiddleware.</p>", "<p><a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_request\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_request()</span></code></a> should either: return <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>, return a\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response</span></code> object, return a <a class=\"reference internal\" href=\"request-response.html#scrapy.http.Request\" title=\"scrapy.http.Request\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code></a>\nobject, or raise <a class=\"reference internal\" href=\"exceptions.html#scrapy.exceptions.IgnoreRequest\" title=\"scrapy.exceptions.IgnoreRequest\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">IgnoreRequest</span></code></a>.</p>", "<p>If it returns <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>, Scrapy will continue processing this request, executing all\nother middlewares until, finally, the appropriate downloader handler is called\nthe request performed (and its response downloaded).</p>", "<p>If it returns a <a class=\"reference internal\" href=\"request-response.html#scrapy.http.Response\" title=\"scrapy.http.Response\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response</span></code></a> object, Scrapy won’t bother\ncalling <em>any</em> other <a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_request\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_request()</span></code></a> or <a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_exception()</span></code></a> methods,\nor the appropriate download function; it’ll return that response. The <a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_response\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_response()</span></code></a>\nmethods of installed middleware is always called on every response.</p>", "<p>If it returns a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> object, Scrapy will stop calling\n<a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_request\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_request()</span></code></a> methods and reschedule the returned request. Once the newly returned\nrequest is performed, the appropriate middleware chain will be called on\nthe downloaded response.</p>", "<p>If it raises an <a class=\"reference internal\" href=\"exceptions.html#scrapy.exceptions.IgnoreRequest\" title=\"scrapy.exceptions.IgnoreRequest\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">IgnoreRequest</span></code></a> exception, the\n<a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_exception()</span></code></a> methods of installed downloader middleware will be called.\nIf none of them handle the exception, the errback function of the request\n(<code class=\"docutils literal notranslate\"><span class=\"pre\">Request.errback</span></code>) is called. If no code handles the raised exception, it is\nignored and not logged (unlike other exceptions).</p>", "<p><strong>request</strong> (<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> object) – the request being processed</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider for which this request is intended</p>", "<p><a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_response\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_response()</span></code></a> should either: return a <a class=\"reference internal\" href=\"request-response.html#scrapy.http.Response\" title=\"scrapy.http.Response\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response</span></code></a>\nobject, return a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> object or\nraise a <a class=\"reference internal\" href=\"exceptions.html#scrapy.exceptions.IgnoreRequest\" title=\"scrapy.exceptions.IgnoreRequest\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">IgnoreRequest</span></code></a> exception.</p>", "<p>If it returns a <a class=\"reference internal\" href=\"request-response.html#scrapy.http.Response\" title=\"scrapy.http.Response\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response</span></code></a> (it could be the same given\nresponse, or a brand-new one), that response will continue to be processed\nwith the <a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_response\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_response()</span></code></a> of the next middleware in the chain.</p>", "<p>If it returns a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> object, the middleware chain is\nhalted and the returned request is rescheduled to be downloaded in the future.\nThis is the same behavior as if a request is returned from <a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_request\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_request()</span></code></a>.</p>", "<p>If it raises an <a class=\"reference internal\" href=\"exceptions.html#scrapy.exceptions.IgnoreRequest\" title=\"scrapy.exceptions.IgnoreRequest\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">IgnoreRequest</span></code></a> exception, the errback\nfunction of the request (<code class=\"docutils literal notranslate\"><span class=\"pre\">Request.errback</span></code>) is called. If no code handles the raised\nexception, it is ignored and not logged (unlike other exceptions).</p>", "<p><strong>request</strong> (is a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> object) – the request that originated the response</p>", "<p><strong>response</strong> (<a class=\"reference internal\" href=\"request-response.html#scrapy.http.Response\" title=\"scrapy.http.Response\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response</span></code></a> object) – the response being processed</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider for which this response is intended</p>", "<p>Scrapy calls <a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_exception()</span></code></a> when a download handler\nor a <a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_request\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_request()</span></code></a> (from a downloader middleware) raises an\nexception (including an <a class=\"reference internal\" href=\"exceptions.html#scrapy.exceptions.IgnoreRequest\" title=\"scrapy.exceptions.IgnoreRequest\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">IgnoreRequest</span></code></a> exception)</p>", "<p><a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_exception()</span></code></a> should return: either <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>,\na <a class=\"reference internal\" href=\"request-response.html#scrapy.http.Response\" title=\"scrapy.http.Response\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response</span></code></a> object, or a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> object.</p>", "<p>If it returns <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>, Scrapy will continue processing this exception,\nexecuting any other <a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_exception()</span></code></a> methods of installed middleware,\nuntil no middleware is left and the default exception handling kicks in.</p>", "<p>If it returns a <a class=\"reference internal\" href=\"request-response.html#scrapy.http.Response\" title=\"scrapy.http.Response\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response</span></code></a> object, the <a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_response\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_response()</span></code></a>\nmethod chain of installed middleware is started, and Scrapy won’t bother calling\nany other <a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_exception()</span></code></a> methods of middleware.</p>", "<p>If it returns a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> object, the returned request is\nrescheduled to be downloaded in the future. This stops the execution of\n<a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_exception()</span></code></a> methods of the middleware the same as returning a\nresponse would.</p>", "<p><strong>request</strong> (is a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> object) – the request that generated the exception</p>", "<p><strong>exception</strong> (an <code class=\"docutils literal notranslate\"><span class=\"pre\">Exception</span></code> object) – the raised exception</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider for which this request is intended</p>", "<p>If present, this classmethod is called to create a middleware instance\nfrom a <a class=\"reference internal\" href=\"api.html#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a>. It must return a new instance\nof the middleware. Crawler object provides access to all Scrapy core\ncomponents like settings and signals; it is a way for middleware to\naccess them and hook its functionality into Scrapy.</p>", "<p><strong>crawler</strong> (<a class=\"reference internal\" href=\"api.html#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a> object) – crawler that uses this middleware</p>", "<p>This page describes all downloader middleware components that come with\nScrapy. For information on how to use them and how to write your own downloader\nmiddleware, see the <a class=\"hoverxref tooltip reference internal\" href=\"#topics-downloader-middleware\"><span class=\"std std-ref\">downloader middleware usage guide</span></a>.</p>", "<p>For a list of the components enabled by default (and their orders) see the\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> setting.</p>", "<p>This middleware enables working with sites that require cookies, such as\nthose that use sessions. It keeps track of cookies sent by web servers, and\nsends them back on subsequent requests (from that spider), just like web\nbrowsers do.</p>", "<p class=\"admonition-title\">Caution</p>", "<p>When non-UTF8 encoded byte sequences are passed to a\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code>, the <code class=\"docutils literal notranslate\"><span class=\"pre\">CookiesMiddleware</span></code> will log\na warning. Refer to <a class=\"hoverxref tooltip reference internal\" href=\"logging.html#topics-logging-advanced-customization\"><span class=\"std std-ref\">Advanced customization</span></a>\nto customize the logging behaviour.</p>", "<p class=\"admonition-title\">Caution</p>", "<p>Cookies set via the <code class=\"docutils literal notranslate\"><span class=\"pre\">Cookie</span></code> header are not considered by the\n<a class=\"hoverxref tooltip reference internal\" href=\"#cookies-mw\"><span class=\"std std-ref\">CookiesMiddleware</span></a>. If you need to set cookies for a request, use the\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request.cookies</span></code> parameter. This is a known\ncurrent limitation that is being worked on.</p>", "<p>The following settings can be used to configure the cookie middleware:</p>", "<p>There is support for keeping multiple cookie sessions per spider by using the\n<a class=\"hoverxref tooltip reference internal\" href=\"#std-reqmeta-cookiejar\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">cookiejar</span></code></a> Request meta key. By default it uses a single cookie jar\n(session), but you can pass an identifier to use different ones.</p>", "<p>For example:</p>", "<p>Keep in mind that the <a class=\"hoverxref tooltip reference internal\" href=\"#std-reqmeta-cookiejar\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">cookiejar</span></code></a> meta key is not “sticky”. You need to keep\npassing it along on subsequent requests. For example:</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code></p>", "<p>Whether to enable the cookies middleware. If disabled, no cookies will be sent\nto web servers.</p>", "<p>Notice that despite the value of <a class=\"hoverxref tooltip reference internal\" href=\"#std-setting-COOKIES_ENABLED\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">COOKIES_ENABLED</span></code></a> setting if\n<code class=\"docutils literal notranslate\"><span class=\"pre\">Request.</span></code><a class=\"hoverxref tooltip reference internal\" href=\"request-response.html#std-reqmeta-dont_merge_cookies\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">meta['dont_merge_cookies']</span></code></a>\nevaluates to <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code> the request cookies will <strong>not</strong> be sent to the\nweb server and received cookies in <a class=\"reference internal\" href=\"request-response.html#scrapy.http.Response\" title=\"scrapy.http.Response\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response</span></code></a> will\n<strong>not</strong> be merged with the existing cookies.</p>", "<p>For more detailed information see the <code class=\"docutils literal notranslate\"><span class=\"pre\">cookies</span></code> parameter in\n<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code>.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code></p>", "<p>If enabled, Scrapy will log all cookies sent in requests (i.e. <code class=\"docutils literal notranslate\"><span class=\"pre\">Cookie</span></code>\nheader) and all cookies received in responses (i.e. <code class=\"docutils literal notranslate\"><span class=\"pre\">Set-Cookie</span></code> header).</p>", "<p>Here’s an example of a log with <a class=\"hoverxref tooltip reference internal\" href=\"#std-setting-COOKIES_DEBUG\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">COOKIES_DEBUG</span></code></a> enabled:</p>", "<p>This middleware sets all default requests headers specified in the\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-DEFAULT_REQUEST_HEADERS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DEFAULT_REQUEST_HEADERS</span></code></a> setting.</p>", "<p>This middleware sets the download timeout for requests specified in the\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-DOWNLOAD_TIMEOUT\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOAD_TIMEOUT</span></code></a> setting or <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">download_timeout</span></code>\nspider attribute.</p>", "<p class=\"admonition-title\">Note</p>", "<p>You can also set download timeout per-request using\n<a class=\"hoverxref tooltip reference internal\" href=\"request-response.html#std-reqmeta-download_timeout\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">download_timeout</span></code></a> Request.meta key; this is supported\neven when DownloadTimeoutMiddleware is disabled.</p>", "<p>This middleware authenticates all requests generated from certain spiders\nusing <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Basic_access_authentication\">Basic access authentication</a> (aka. HTTP auth).</p>", "<p>To enable HTTP authentication for a spider, set the <code class=\"docutils literal notranslate\"><span class=\"pre\">http_user</span></code> and\n<code class=\"docutils literal notranslate\"><span class=\"pre\">http_pass</span></code> spider attributes to the authentication data and the\n<code class=\"docutils literal notranslate\"><span class=\"pre\">http_auth_domain</span></code> spider attribute to the domain which requires this\nauthentication (its subdomains will be also handled in the same way).\nYou can set <code class=\"docutils literal notranslate\"><span class=\"pre\">http_auth_domain</span></code> to <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> to enable the\nauthentication for all requests but you risk leaking your authentication\ncredentials to unrelated domains.</p>", "<p class=\"admonition-title\">Warning</p>", "<p>In previous Scrapy versions HttpAuthMiddleware sent the authentication\ndata with all requests, which is a security problem if the spider\nmakes requests to several different domains. Currently if the\n<code class=\"docutils literal notranslate\"><span class=\"pre\">http_auth_domain</span></code> attribute is not set, the middleware will use the\ndomain of the first request, which will work for some spiders but not\nfor others. In the future the middleware will produce an error instead.</p>", "<p>Example:</p>", "<p>This middleware provides low-level cache to all HTTP requests and responses.\nIt has to be combined with a cache storage backend as well as a cache policy.</p>", "<p>Scrapy ships with the following HTTP cache storage backends:</p>", "<p>You can change the HTTP cache storage backend with the <a class=\"hoverxref tooltip reference internal\" href=\"#std-setting-HTTPCACHE_STORAGE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">HTTPCACHE_STORAGE</span></code></a>\nsetting. Or you can also <a class=\"hoverxref tooltip reference internal\" href=\"#httpcache-storage-custom\"><span class=\"std std-ref\">implement your own storage backend.</span></a></p>", "<p>Scrapy ships with two HTTP cache policies:</p>", "<p>You can change the HTTP cache policy with the <a class=\"hoverxref tooltip reference internal\" href=\"#std-setting-HTTPCACHE_POLICY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">HTTPCACHE_POLICY</span></code></a>\nsetting. Or you can also implement your own policy.</p>", "<p id=\"std-reqmeta-dont_cache\">You can also avoid caching a response on every policy using <a class=\"hoverxref tooltip reference internal\" href=\"#std-reqmeta-dont_cache\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">dont_cache</span></code></a> meta key equals <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code>.</p>", "<p>This policy has no awareness of any HTTP Cache-Control directives.\nEvery request and its corresponding response are cached.  When the same\nrequest is seen again, the response is returned without transferring\nanything from the Internet.</p>", "<p>The Dummy policy is useful for testing spiders faster (without having\nto wait for downloads every time) and for trying your spider offline,\nwhen an Internet connection is not available. The goal is to be able to\n“replay” a spider run <em>exactly as it ran before</em>.</p>", "<p>This policy provides a RFC2616 compliant HTTP cache, i.e. with HTTP\nCache-Control awareness, aimed at production and used in continuous\nruns to avoid downloading unmodified data (to save bandwidth and speed up\ncrawls).</p>", "<p>What is implemented:</p>", "<p>Do not attempt to store responses/requests with <code class=\"docutils literal notranslate\"><span class=\"pre\">no-store</span></code> cache-control directive set</p>", "<p>Do not serve responses from cache if <code class=\"docutils literal notranslate\"><span class=\"pre\">no-cache</span></code> cache-control directive is set even for fresh responses</p>", "<p>Compute freshness lifetime from <code class=\"docutils literal notranslate\"><span class=\"pre\">max-age</span></code> cache-control directive</p>", "<p>Compute freshness lifetime from <code class=\"docutils literal notranslate\"><span class=\"pre\">Expires</span></code> response header</p>", "<p>Compute freshness lifetime from <code class=\"docutils literal notranslate\"><span class=\"pre\">Last-Modified</span></code> response header (heuristic used by Firefox)</p>", "<p>Compute current age from <code class=\"docutils literal notranslate\"><span class=\"pre\">Age</span></code> response header</p>", "<p>Compute current age from <code class=\"docutils literal notranslate\"><span class=\"pre\">Date</span></code> header</p>", "<p>Revalidate stale responses based on <code class=\"docutils literal notranslate\"><span class=\"pre\">Last-Modified</span></code> response header</p>", "<p>Revalidate stale responses based on <code class=\"docutils literal notranslate\"><span class=\"pre\">ETag</span></code> response header</p>", "<p>Set <code class=\"docutils literal notranslate\"><span class=\"pre\">Date</span></code> header for any received response missing it</p>", "<p>Support <code class=\"docutils literal notranslate\"><span class=\"pre\">max-stale</span></code> cache-control directive in requests</p>", "<p>This allows spiders to be configured with the full RFC2616 cache policy,\nbut avoid revalidation on a request-by-request basis, while remaining\nconformant with the HTTP spec.</p>", "<p>Example:</p>", "<p>Add <code class=\"docutils literal notranslate\"><span class=\"pre\">Cache-Control:</span> <span class=\"pre\">max-stale=600</span></code> to Request headers to accept responses that\nhave exceeded their expiration time by no more than 600 seconds.</p>", "<p>See also: RFC2616, 14.9.3</p>", "<p>What is missing:</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">Pragma:</span> <span class=\"pre\">no-cache</span></code> support <a class=\"reference external\" href=\"https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1\">https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1</a></p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">Vary</span></code> header support <a class=\"reference external\" href=\"https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6\">https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6</a></p>", "<p>Invalidation after updates or deletes <a class=\"reference external\" href=\"https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10\">https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10</a></p>", "<p>… probably others ..</p>", "<p>File system storage backend is available for the HTTP cache middleware.</p>", "<p>Each request/response pair is stored in a different directory containing\nthe following files:</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">request_body</span></code> - the plain request body</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">request_headers</span></code> - the request headers (in raw HTTP format)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">response_body</span></code> - the plain response body</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">response_headers</span></code> - the request headers (in raw HTTP format)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">meta</span></code> - some metadata of this cache resource in Python <code class=\"docutils literal notranslate\"><span class=\"pre\">repr()</span></code>\nformat (grep-friendly format)</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">pickled_meta</span></code> - the same metadata in <code class=\"docutils literal notranslate\"><span class=\"pre\">meta</span></code> but pickled for more\nefficient deserialization</p>", "<p>The directory name is made from the request fingerprint (see\n<code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.utils.request.fingerprint</span></code>), and one level of subdirectories is\nused to avoid creating too many files into the same directory (which is\ninefficient in many file systems). An example directory could be:</p>", "<p>A <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Dbm\">DBM</a> storage backend is also available for the HTTP cache middleware.</p>", "<p>By default, it uses the <a class=\"reference external\" href=\"https://docs.python.org/3/library/dbm.html#module-dbm\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">dbm</span></code></a>, but you can change it with the\n<a class=\"hoverxref tooltip reference internal\" href=\"#std-setting-HTTPCACHE_DBM_MODULE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">HTTPCACHE_DBM_MODULE</span></code></a> setting.</p>", "<p>You can implement a cache storage backend by creating a Python class that\ndefines the methods described below.</p>", "<p>This method gets called after a spider has been opened for crawling. It handles\nthe <a class=\"hoverxref tooltip reference internal\" href=\"signals.html#std-signal-spider_opened\"><code class=\"xref std std-signal docutils literal notranslate\"><span class=\"pre\">open_spider</span></code></a> signal.</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider which has been opened</p>", "<p>This method gets called after a spider has been closed. It handles\nthe <a class=\"hoverxref tooltip reference internal\" href=\"signals.html#std-signal-spider_closed\"><code class=\"xref std std-signal docutils literal notranslate\"><span class=\"pre\">close_spider</span></code></a> signal.</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider which has been closed</p>", "<p>Return response if present in cache, or <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> otherwise.</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider which generated the request</p>", "<p><strong>request</strong> (<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> object) – the request to find cached response for</p>", "<p>Store the given response in the cache.</p>", "<p><strong>spider</strong> (<a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> object) – the spider for which the response is intended</p>", "<p><strong>request</strong> (<code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> object) – the corresponding request the spider generated</p>", "<p><strong>response</strong> (<a class=\"reference internal\" href=\"request-response.html#scrapy.http.Response\" title=\"scrapy.http.Response\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Response</span></code></a> object) – the response to store in the cache</p>", "<p>In order to use your storage backend, set:</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"#std-setting-HTTPCACHE_STORAGE\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">HTTPCACHE_STORAGE</span></code></a> to the Python import path of your custom storage class.</p>", "<p>The <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">HttpCacheMiddleware</span></code> can be configured through the following\nsettings:</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code></p>", "<p>Whether the HTTP cache will be enabled.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">0</span></code></p>", "<p>Expiration time for cached requests, in seconds.</p>", "<p>Cached requests older than this time will be re-downloaded. If zero, cached\nrequests will never expire.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">'httpcache'</span></code></p>", "<p>The directory to use for storing the (low-level) HTTP cache. If empty, the HTTP\ncache will be disabled. If a relative path is given, is taken relative to the\nproject data dir. For more info see: <a class=\"hoverxref tooltip reference internal\" href=\"commands.html#topics-project-structure\"><span class=\"std std-ref\">Default structure of Scrapy projects</span></a>.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">[]</span></code></p>", "<p>Don’t cache response with these HTTP codes.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code></p>", "<p>If enabled, requests not found in the cache will be ignored instead of downloaded.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">['file']</span></code></p>", "<p>Don’t cache responses with these URI schemes.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">'scrapy.extensions.httpcache.FilesystemCacheStorage'</span></code></p>", "<p>The class which implements the cache storage backend.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">'dbm'</span></code></p>", "<p>The database module to use in the <a class=\"hoverxref tooltip reference internal\" href=\"#httpcache-storage-dbm\"><span class=\"std std-ref\">DBM storage backend</span></a>. This setting is specific to the DBM backend.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">'scrapy.extensions.httpcache.DummyPolicy'</span></code></p>", "<p>The class which implements the cache policy.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code></p>", "<p>If enabled, will compress all cached data with gzip.\nThis setting is specific to the Filesystem backend.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code></p>", "<p>If enabled, will cache pages unconditionally.</p>", "<p>A spider may wish to have all responses available in the cache, for\nfuture use with <code class=\"docutils literal notranslate\"><span class=\"pre\">Cache-Control:</span> <span class=\"pre\">max-stale</span></code>, for instance. The\nDummyPolicy caches all responses but never revalidates them, and\nsometimes a more nuanced policy is desirable.</p>", "<p>This setting still respects <code class=\"docutils literal notranslate\"><span class=\"pre\">Cache-Control:</span> <span class=\"pre\">no-store</span></code> directives in responses.\nIf you don’t want that, filter <code class=\"docutils literal notranslate\"><span class=\"pre\">no-store</span></code> out of the Cache-Control headers in\nresponses you feed to the cache middleware.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">[]</span></code></p>", "<p>List of Cache-Control directives in responses to be ignored.</p>", "<p>Sites often set “no-store”, “no-cache”, “must-revalidate”, etc., but get\nupset at the traffic a spider can generate if it actually respects those\ndirectives. This allows to selectively ignore Cache-Control directives\nthat are known to be unimportant for the sites being crawled.</p>", "<p>We assume that the spider will not issue Cache-Control directives\nin requests unless it actually needs them, so directives in requests are\nnot filtered.</p>", "<p>This middleware allows compressed (gzip, deflate) traffic to be\nsent/received from web sites.</p>", "<p>This middleware also supports decoding <a class=\"reference external\" href=\"https://www.ietf.org/rfc/rfc7932.txt\">brotli-compressed</a> as well as\n<a class=\"reference external\" href=\"https://www.ietf.org/rfc/rfc8478.txt\">zstd-compressed</a> responses, provided that <a class=\"reference external\" href=\"https://pypi.org/project/Brotli/\">brotli</a> or <a class=\"reference external\" href=\"https://pypi.org/project/zstandard/\">zstandard</a> is\ninstalled, respectively.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code></p>", "<p>Whether the Compression middleware will be enabled.</p>", "<p>This middleware sets the HTTP proxy to use for requests, by setting the\n<code class=\"docutils literal notranslate\"><span class=\"pre\">proxy</span></code> meta value for <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> objects.</p>", "<p>Like the Python standard library module <a class=\"reference external\" href=\"https://docs.python.org/3/library/urllib.request.html#module-urllib.request\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">urllib.request</span></code></a>, it obeys\nthe following environment variables:</p>", "<p>You can also set the meta key <code class=\"docutils literal notranslate\"><span class=\"pre\">proxy</span></code> per-request, to a value like\n<code class=\"docutils literal notranslate\"><span class=\"pre\">http://some_proxy_server:port</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">http://username:password@some_proxy_server:port</span></code>.\nKeep in mind this value will take precedence over <code class=\"docutils literal notranslate\"><span class=\"pre\">http_proxy</span></code>/<code class=\"docutils literal notranslate\"><span class=\"pre\">https_proxy</span></code>\nenvironment variables, and it will also ignore <code class=\"docutils literal notranslate\"><span class=\"pre\">no_proxy</span></code> environment variable.</p>", "<p>This middleware handles redirection of requests based on response status.</p>", "<p id=\"std-reqmeta-redirect_urls\">The urls which the request goes through (while being redirected) can be found\nin the <code class=\"docutils literal notranslate\"><span class=\"pre\">redirect_urls</span></code> <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Request.meta</span></code> key.</p>", "<p id=\"std-reqmeta-redirect_reasons\">The reason behind each redirect in <a class=\"hoverxref tooltip reference internal\" href=\"#std-reqmeta-redirect_urls\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">redirect_urls</span></code></a> can be found in the\n<code class=\"docutils literal notranslate\"><span class=\"pre\">redirect_reasons</span></code> <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Request.meta</span></code> key. For\nexample: <code class=\"docutils literal notranslate\"><span class=\"pre\">[301,</span> <span class=\"pre\">302,</span> <span class=\"pre\">307,</span> <span class=\"pre\">'meta</span> <span class=\"pre\">refresh']</span></code>.</p>", "<p>The format of a reason depends on the middleware that handled the corresponding\nredirect. For example, <a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.redirect.RedirectMiddleware\" title=\"scrapy.downloadermiddlewares.redirect.RedirectMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RedirectMiddleware</span></code></a> indicates the triggering\nresponse status code as an integer, while <a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware\" title=\"scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MetaRefreshMiddleware</span></code></a>\nalways uses the <code class=\"docutils literal notranslate\"><span class=\"pre\">'meta</span> <span class=\"pre\">refresh'</span></code> string as reason.</p>", "<p>The <a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.redirect.RedirectMiddleware\" title=\"scrapy.downloadermiddlewares.redirect.RedirectMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RedirectMiddleware</span></code></a> can be configured through the following\nsettings (see the settings documentation for more info):</p>", "<p id=\"std-reqmeta-dont_redirect\">If <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Request.meta</span></code> has <code class=\"docutils literal notranslate\"><span class=\"pre\">dont_redirect</span></code>\nkey set to True, the request will be ignored by this middleware.</p>", "<p>If you want to handle some redirect status codes in your spider, you can\nspecify these in the <code class=\"docutils literal notranslate\"><span class=\"pre\">handle_httpstatus_list</span></code> spider attribute.</p>", "<p>For example, if you want the redirect middleware to ignore 301 and 302\nresponses (and pass them through to your spider) you can do this:</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">handle_httpstatus_list</span></code> key of <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Request.meta</span></code> can also be used to specify which response codes to\nallow on a per-request basis. You can also set the meta key\n<code class=\"docutils literal notranslate\"><span class=\"pre\">handle_httpstatus_all</span></code> to <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code> if you want to allow any response code\nfor a request.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code></p>", "<p>Whether the Redirect middleware will be enabled.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">20</span></code></p>", "<p>The maximum number of redirections that will be followed for a single request.\nAfter this maximum, the request’s response is returned as is.</p>", "<p>This middleware handles redirection of requests based on meta-refresh html tag.</p>", "<p>The <a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware\" title=\"scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">MetaRefreshMiddleware</span></code></a> can be configured through the following\nsettings (see the settings documentation for more info):</p>", "<p>This middleware obey <a class=\"hoverxref tooltip reference internal\" href=\"#std-setting-REDIRECT_MAX_TIMES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">REDIRECT_MAX_TIMES</span></code></a> setting, <a class=\"hoverxref tooltip reference internal\" href=\"#std-reqmeta-dont_redirect\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">dont_redirect</span></code></a>,\n<a class=\"hoverxref tooltip reference internal\" href=\"#std-reqmeta-redirect_urls\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">redirect_urls</span></code></a> and <a class=\"hoverxref tooltip reference internal\" href=\"#std-reqmeta-redirect_reasons\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">redirect_reasons</span></code></a> request meta keys as described\nfor <a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.redirect.RedirectMiddleware\" title=\"scrapy.downloadermiddlewares.redirect.RedirectMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RedirectMiddleware</span></code></a></p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code></p>", "<p>Whether the Meta Refresh middleware will be enabled.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">[]</span></code></p>", "<p>Meta tags within these tags are ignored.</p>", "<p><span class=\"versionmodified changed\">Changed in version 2.0: </span>The default value of <a class=\"hoverxref tooltip reference internal\" href=\"#std-setting-METAREFRESH_IGNORE_TAGS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">METAREFRESH_IGNORE_TAGS</span></code></a> changed from\n<code class=\"docutils literal notranslate\"><span class=\"pre\">['script',</span> <span class=\"pre\">'noscript']</span></code> to <code class=\"docutils literal notranslate\"><span class=\"pre\">[]</span></code>.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">100</span></code></p>", "<p>The maximum meta-refresh delay (in seconds) to follow the redirection.\nSome sites use meta-refresh for redirecting to a session expired page, so we\nrestrict automatic redirection to the maximum delay.</p>", "<p>A middleware to retry failed requests that are potentially caused by\ntemporary problems such as a connection timeout or HTTP 500 error.</p>", "<p>Failed pages are collected on the scraping process and rescheduled at the\nend, once the spider has finished crawling all regular (non failed) pages.</p>", "<p>The <a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.retry.RetryMiddleware\" title=\"scrapy.downloadermiddlewares.retry.RetryMiddleware\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RetryMiddleware</span></code></a> can be configured through the following\nsettings (see the settings documentation for more info):</p>", "<p id=\"std-reqmeta-dont_retry\">If <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Request.meta</span></code> has <code class=\"docutils literal notranslate\"><span class=\"pre\">dont_retry</span></code> key\nset to True, the request will be ignored by this middleware.</p>", "<p>To retry requests from a spider callback, you can use the\n<a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.retry.get_retry_request\" title=\"scrapy.downloadermiddlewares.retry.get_retry_request\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">get_retry_request()</span></code></a> function:</p>", "<p>Returns a new <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> object to retry the specified\nrequest, or <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code> if retries of the specified request have been\nexhausted.</p>", "<p>For example, in a <a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> callback, you could use it as\nfollows:</p>", "<p><em>spider</em> is the <a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> instance which is asking for the\nretry request. It is used to access the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#topics-settings\"><span class=\"std std-ref\">settings</span></a>\nand <a class=\"hoverxref tooltip reference internal\" href=\"stats.html#topics-stats\"><span class=\"std std-ref\">stats</span></a>, and to provide extra logging context (see\n<a class=\"reference external\" href=\"https://docs.python.org/3/library/logging.html#logging.debug\" title=\"(in Python v3.11)\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">logging.debug()</span></code></a>).</p>", "<p><em>reason</em> is a string or an <a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#Exception\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Exception</span></code></a> object that indicates the\nreason why the request needs to be retried. It is used to name retry stats.</p>", "<p><em>max_retry_times</em> is a number that determines the maximum number of times\nthat <em>request</em> can be retried. If not specified or <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>, the number is\nread from the <a class=\"hoverxref tooltip reference internal\" href=\"request-response.html#std-reqmeta-max_retry_times\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">max_retry_times</span></code></a> meta key of the request. If the\n<a class=\"hoverxref tooltip reference internal\" href=\"request-response.html#std-reqmeta-max_retry_times\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">max_retry_times</span></code></a> meta key is not defined or <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>, the number\nis read from the <a class=\"hoverxref tooltip reference internal\" href=\"#std-setting-RETRY_TIMES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">RETRY_TIMES</span></code></a> setting.</p>", "<p><em>priority_adjust</em> is a number that determines how the priority of the new\nrequest changes in relation to <em>request</em>. If not specified, the number is\nread from the <a class=\"hoverxref tooltip reference internal\" href=\"#std-setting-RETRY_PRIORITY_ADJUST\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">RETRY_PRIORITY_ADJUST</span></code></a> setting.</p>", "<p><em>logger</em> is the logging.Logger object to be used when logging messages</p>", "<p><em>stats_base_key</em> is a string to be used as the base key for the\nretry-related job stats</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code></p>", "<p>Whether the Retry middleware will be enabled.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">2</span></code></p>", "<p>Maximum number of times to retry, in addition to the first download.</p>", "<p>Maximum number of retries can also be specified per-request using\n<a class=\"hoverxref tooltip reference internal\" href=\"request-response.html#std-reqmeta-max_retry_times\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">max_retry_times</span></code></a> attribute of <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Request.meta</span></code>.\nWhen initialized, the <a class=\"hoverxref tooltip reference internal\" href=\"request-response.html#std-reqmeta-max_retry_times\"><code class=\"xref std std-reqmeta docutils literal notranslate\"><span class=\"pre\">max_retry_times</span></code></a> meta key takes higher\nprecedence over the <a class=\"hoverxref tooltip reference internal\" href=\"#std-setting-RETRY_TIMES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">RETRY_TIMES</span></code></a> setting.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">[500,</span> <span class=\"pre\">502,</span> <span class=\"pre\">503,</span> <span class=\"pre\">504,</span> <span class=\"pre\">522,</span> <span class=\"pre\">524,</span> <span class=\"pre\">408,</span> <span class=\"pre\">429]</span></code></p>", "<p>Which HTTP response codes to retry. Other errors (DNS lookup issues,\nconnections lost, etc) are always retried.</p>", "<p>In some cases you may want to add 400 to <a class=\"hoverxref tooltip reference internal\" href=\"#std-setting-RETRY_HTTP_CODES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">RETRY_HTTP_CODES</span></code></a> because\nit is a common code used to indicate server overload. It is not included by\ndefault because HTTP specs say so.</p>", "<p>Default:</p>", "<p>List of exceptions to retry.</p>", "<p>Each list entry may be an exception type or its import path as a string.</p>", "<p>An exception will not be caught when the exception type is not in\n<a class=\"hoverxref tooltip reference internal\" href=\"#std-setting-RETRY_EXCEPTIONS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">RETRY_EXCEPTIONS</span></code></a> or when the maximum number of retries for a request\nhas been exceeded (see <a class=\"hoverxref tooltip reference internal\" href=\"#std-setting-RETRY_TIMES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">RETRY_TIMES</span></code></a>). To learn about uncaught\nexception propagation, see\n<a class=\"reference internal\" href=\"#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_exception()</span></code></a>.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">-1</span></code></p>", "<p>Adjust retry request priority relative to original request:</p>", "<p>a positive priority adjust means higher priority.</p>", "<p>This middleware filters out requests forbidden by the robots.txt exclusion\nstandard.</p>", "<p>To make sure Scrapy respects robots.txt make sure the middleware is enabled\nand the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-ROBOTSTXT_OBEY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">ROBOTSTXT_OBEY</span></code></a> setting is enabled.</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-ROBOTSTXT_USER_AGENT\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">ROBOTSTXT_USER_AGENT</span></code></a> setting can be used to specify the\nuser agent string to use for matching in the <a class=\"reference external\" href=\"https://www.robotstxt.org/\">robots.txt</a> file. If it\nis <code class=\"docutils literal notranslate\"><span class=\"pre\">None</span></code>, the User-Agent header you are sending with the request or the\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-USER_AGENT\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">USER_AGENT</span></code></a> setting (in that order) will be used for determining\nthe user agent to use in the <a class=\"reference external\" href=\"https://www.robotstxt.org/\">robots.txt</a> file.</p>", "<p>This middleware has to be combined with a <a class=\"reference external\" href=\"https://www.robotstxt.org/\">robots.txt</a> parser.</p>", "<p>Scrapy ships with support for the following <a class=\"reference external\" href=\"https://www.robotstxt.org/\">robots.txt</a> parsers:</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"#protego-parser\"><span class=\"std std-ref\">Protego</span></a> (default)</p>", "<p>You can change the <a class=\"reference external\" href=\"https://www.robotstxt.org/\">robots.txt</a> parser with the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-ROBOTSTXT_PARSER\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">ROBOTSTXT_PARSER</span></code></a>\nsetting. Or you can also <a class=\"hoverxref tooltip reference internal\" href=\"#support-for-new-robots-parser\"><span class=\"std std-ref\">implement support for a new parser</span></a>.</p>", "<p id=\"std-reqmeta-dont_obey_robotstxt\">If <code class=\"xref py py-attr docutils literal notranslate\"><span class=\"pre\">Request.meta</span></code> has\n<code class=\"docutils literal notranslate\"><span class=\"pre\">dont_obey_robotstxt</span></code> key set to True\nthe request will be ignored by this middleware even if\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-ROBOTSTXT_OBEY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">ROBOTSTXT_OBEY</span></code></a> is enabled.</p>", "<p>Parsers vary in several aspects:</p>", "<p>Language of implementation</p>", "<p>Supported specification</p>", "<p>Support for wildcard matching</p>", "<p>Usage of <a class=\"reference external\" href=\"https://developers.google.com/search/reference/robots_txt#order-of-precedence-for-group-member-lines\">length based rule</a>:\nin particular for <code class=\"docutils literal notranslate\"><span class=\"pre\">Allow</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">Disallow</span></code> directives, where the most\nspecific rule based on the length of the path trumps the less specific\n(shorter) rule</p>", "<p>Performance comparison of different parsers is available at <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy/issues/3969\">the following link</a>.</p>", "<p>Based on <a class=\"reference external\" href=\"https://github.com/scrapy/protego\">Protego</a>:</p>", "<p>implemented in Python</p>", "<p>is compliant with <a class=\"reference external\" href=\"https://developers.google.com/search/reference/robots_txt\">Google’s Robots.txt Specification</a></p>", "<p>supports wildcard matching</p>", "<p>uses the length based rule</p>", "<p>Scrapy uses this parser by default.</p>", "<p>Based on <a class=\"reference external\" href=\"https://docs.python.org/3/library/urllib.robotparser.html#urllib.robotparser.RobotFileParser\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RobotFileParser</span></code></a>:</p>", "<p>is Python’s built-in <a class=\"reference external\" href=\"https://www.robotstxt.org/\">robots.txt</a> parser</p>", "<p>is compliant with <a class=\"reference external\" href=\"https://www.robotstxt.org/norobots-rfc.txt\">Martijn Koster’s 1996 draft specification</a></p>", "<p>lacks support for wildcard matching</p>", "<p>doesn’t use the length based rule</p>", "<p>It is faster than Protego and backward-compatible with versions of Scrapy before 1.8.0.</p>", "<p>In order to use this parser, set:</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-ROBOTSTXT_PARSER\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">ROBOTSTXT_PARSER</span></code></a> to <code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.robotstxt.PythonRobotParser</span></code></p>", "<p>Based on <a class=\"reference external\" href=\"https://github.com/seomoz/reppy/\">Reppy</a>:</p>", "<p>is a Python wrapper around <a class=\"reference external\" href=\"https://github.com/seomoz/rep-cpp\">Robots Exclusion Protocol Parser for C++</a></p>", "<p>is compliant with <a class=\"reference external\" href=\"https://www.robotstxt.org/norobots-rfc.txt\">Martijn Koster’s 1996 draft specification</a></p>", "<p>supports wildcard matching</p>", "<p>uses the length based rule</p>", "<p>Native implementation, provides better speed than Protego.</p>", "<p>In order to use this parser:</p>", "<p>Install <a class=\"reference external\" href=\"https://github.com/seomoz/reppy/\">Reppy</a> by running <code class=\"docutils literal notranslate\"><span class=\"pre\">pip</span> <span class=\"pre\">install</span> <span class=\"pre\">reppy</span></code></p>", "<p class=\"admonition-title\">Warning</p>", "<p><a class=\"reference external\" href=\"https://github.com/seomoz/reppy/issues/122\">Upstream issue #122</a> prevents reppy usage in Python 3.9+.</p>", "<p>Set <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-ROBOTSTXT_PARSER\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">ROBOTSTXT_PARSER</span></code></a> setting to\n<code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.robotstxt.ReppyRobotParser</span></code></p>", "<p>Based on <a class=\"reference external\" href=\"http://nikitathespider.com/python/rerp/\">Robotexclusionrulesparser</a>:</p>", "<p>implemented in Python</p>", "<p>is compliant with <a class=\"reference external\" href=\"https://www.robotstxt.org/norobots-rfc.txt\">Martijn Koster’s 1996 draft specification</a></p>", "<p>supports wildcard matching</p>", "<p>doesn’t use the length based rule</p>", "<p>In order to use this parser:</p>", "<p>Install <a class=\"reference external\" href=\"http://nikitathespider.com/python/rerp/\">Robotexclusionrulesparser</a> by running\n<code class=\"docutils literal notranslate\"><span class=\"pre\">pip</span> <span class=\"pre\">install</span> <span class=\"pre\">robotexclusionrulesparser</span></code></p>", "<p>Set <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-ROBOTSTXT_PARSER\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">ROBOTSTXT_PARSER</span></code></a> setting to\n<code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.robotstxt.RerpRobotParser</span></code></p>", "<p>You can implement support for a new <a class=\"reference external\" href=\"https://www.robotstxt.org/\">robots.txt</a> parser by subclassing\nthe abstract base class <a class=\"reference internal\" href=\"#scrapy.robotstxt.RobotParser\" title=\"scrapy.robotstxt.RobotParser\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">RobotParser</span></code></a> and\nimplementing the methods described below.</p>", "<p>Return <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code> if  <code class=\"docutils literal notranslate\"><span class=\"pre\">user_agent</span></code> is allowed to crawl <code class=\"docutils literal notranslate\"><span class=\"pre\">url</span></code>, otherwise return <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code>.</p>", "<p><strong>url</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><em>str</em></a>) – Absolute URL</p>", "<p><strong>user_agent</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.11)\"><em>str</em></a>) – User agent</p>", "<p>Parse the content of a <a class=\"reference external\" href=\"https://www.robotstxt.org/\">robots.txt</a> file as bytes. This must be a class method.\nIt must return a new instance of the parser backend.</p>", "<p><strong>crawler</strong> (<a class=\"reference internal\" href=\"api.html#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a> instance) – crawler which made the request</p>", "<p><strong>robotstxt_body</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#bytes\" title=\"(in Python v3.11)\"><em>bytes</em></a>) – content of a <a class=\"reference external\" href=\"https://www.robotstxt.org/\">robots.txt</a> file.</p>", "<p>Middleware that stores stats of all requests, responses and exceptions that\npass through it.</p>", "<p>To use this middleware you must enable the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-DOWNLOADER_STATS\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOADER_STATS</span></code></a>\nsetting.</p>", "<p>Middleware that allows spiders to override the default user agent.</p>", "<p>In order for a spider to override the default user agent, its <code class=\"docutils literal notranslate\"><span class=\"pre\">user_agent</span></code>\nattribute must be set.</p>", "<p>Middleware that finds ‘AJAX crawlable’ page variants based\non meta-fragment html tag. See\n<a class=\"reference external\" href=\"https://developers.google.com/search/docs/ajax-crawling/docs/getting-started\">https://developers.google.com/search/docs/ajax-crawling/docs/getting-started</a>\nfor more info.</p>", "<p class=\"admonition-title\">Note</p>", "<p>Scrapy finds ‘AJAX crawlable’ pages for URLs like\n<code class=\"docutils literal notranslate\"><span class=\"pre\">'http://example.com/!#foo=bar'</span></code> even without this middleware.\nAjaxCrawlMiddleware is necessary when URL doesn’t contain <code class=\"docutils literal notranslate\"><span class=\"pre\">'!#'</span></code>.\nThis is often a case for ‘index’ or ‘main’ website pages.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code></p>", "<p>Whether the AjaxCrawlMiddleware will be enabled. You may want to\nenable it for <a class=\"hoverxref tooltip reference internal\" href=\"broad-crawls.html#topics-broad-crawls\"><span class=\"std std-ref\">broad crawls</span></a>.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code></p>", "<p>Whether or not to enable the <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">HttpProxyMiddleware</span></code>.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">\"latin-1\"</span></code></p>", "<p>The default encoding for proxy authentication on <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">HttpProxyMiddleware</span></code>.</p>"]}, "code_blocks": ["DOWNLOADER_MIDDLEWARES = {\n    \"myproject.middlewares.CustomDownloaderMiddleware\": 543,\n}\n</pre>", "DOWNLOADER_MIDDLEWARES = {\n    \"myproject.middlewares.CustomDownloaderMiddleware\": 543,\n    \"scrapy.downloadermiddlewares.useragent.UserAgentMiddleware\": None,\n}\n</pre>", "for i, url in enumerate(urls):\n    yield scrapy.Request(url, meta={\"cookiejar\": i}, callback=self.parse_page)\n</pre>", "def parse_page(self, response):\n    # do some processing\n    return scrapy.Request(\n        \"http://www.example.com/otherpage\",\n        meta={\"cookiejar\": response.meta[\"cookiejar\"]},\n        callback=self.parse_other_page,\n    )\n</pre>", "2011-04-06 14:35:10-0300 [scrapy.core.engine] INFO: Spider opened\n2011-04-06 14:35:10-0300 [scrapy.downloadermiddlewares.cookies] DEBUG: Sending cookies to: <GET http://www.diningcity.com/netherlands/index.html>\n        Cookie: clientlanguage_nl=en_EN\n2011-04-06 14:35:14-0300 [scrapy.downloadermiddlewares.cookies] DEBUG: Received cookies from: <200 http://www.diningcity.com/netherlands/index.html>\n        Set-Cookie: JSESSIONID=B~FA4DC0C496C8762AE4F1A620EAB34F38; Path=/\n        Set-Cookie: ip_isocode=US\n        Set-Cookie: clientlanguage_nl=en_EN; Expires=Thu, 07-Apr-2011 21:21:34 GMT; Path=/\n2011-04-06 14:49:50-0300 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.diningcity.com/netherlands/index.html> (referer: None)\n[...]\n</pre>", "from scrapy.spiders import CrawlSpider\n\n\nclass SomeIntranetSiteSpider(CrawlSpider):\n    http_user = \"someuser\"\n    http_pass = \"somepass\"\n    http_auth_domain = \"intranet.example.com\"\n    name = \"intranet.example.com\"\n\n    # .. rest of the spider code omitted ...\n</pre>", "/path/to/cache/dir/example.com/72/72811f648e718090f041317756c03adb0ada46c7\n</pre>", "class MySpider(CrawlSpider):\n    handle_httpstatus_list = [301, 302]\n</pre>", "def parse(self, response):\n    if not response.text:\n        new_request_or_none = get_retry_request(\n            response.request,\n            spider=self,\n            reason='empty',\n        )\n        return new_request_or_none\n</pre>", "[\n    'twisted.internet.defer.TimeoutError',\n    'twisted.internet.error.TimeoutError',\n    'twisted.internet.error.DNSLookupError',\n    'twisted.internet.error.ConnectionRefusedError',\n    'twisted.internet.error.ConnectionDone',\n    'twisted.internet.error.ConnectError',\n    'twisted.internet.error.ConnectionLost',\n    'twisted.internet.error.TCPTimedOutError',\n    'twisted.web.client.ResponseFailed',\n    IOError,\n    'scrapy.core.downloader.handlers.http11.TunnelError',\n]\n</pre>"], "links": [{"text": "DOWNLOADER_MIDDLEWARES", "href": "settings.html#std-setting-DOWNLOADER_MIDDLEWARES"}, {"text": "DOWNLOADER_MIDDLEWARES", "href": "settings.html#std-setting-DOWNLOADER_MIDDLEWARES"}, {"text": "DOWNLOADER_MIDDLEWARES_BASE", "href": "settings.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE"}, {"text": "DOWNLOADER_MIDDLEWARES_BASE", "href": "settings.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE"}, {"text": "DOWNLOADER_MIDDLEWARES_BASE", "href": "settings.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE"}, {"text": "DOWNLOADER_MIDDLEWARES", "href": "settings.html#std-setting-DOWNLOADER_MIDDLEWARES"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "settings", "href": "settings.html#topics-settings"}, {"text": "Request", "href": "request-response.html#scrapy.http.Request"}, {"text": "IgnoreRequest", "href": "exceptions.html#scrapy.exceptions.IgnoreRequest"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "IgnoreRequest", "href": "exceptions.html#scrapy.exceptions.IgnoreRequest"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "IgnoreRequest", "href": "exceptions.html#scrapy.exceptions.IgnoreRequest"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "IgnoreRequest", "href": "exceptions.html#scrapy.exceptions.IgnoreRequest"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "IgnoreRequest", "href": "exceptions.html#scrapy.exceptions.IgnoreRequest"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "DOWNLOADER_MIDDLEWARES_BASE", "href": "settings.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/cookies.html#CookiesMiddleware"}, {"text": "Advanced customization", "href": "logging.html#topics-logging-advanced-customization"}, {"text": "meta['dont_merge_cookies']", "href": "request-response.html#std-reqmeta-dont_merge_cookies"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/defaultheaders.html#DefaultHeadersMiddleware"}, {"text": "DEFAULT_REQUEST_HEADERS", "href": "settings.html#std-setting-DEFAULT_REQUEST_HEADERS"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/downloadtimeout.html#DownloadTimeoutMiddleware"}, {"text": "DOWNLOAD_TIMEOUT", "href": "settings.html#std-setting-DOWNLOAD_TIMEOUT"}, {"text": "download_timeout", "href": "request-response.html#std-reqmeta-download_timeout"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/httpauth.html#HttpAuthMiddleware"}, {"text": "Basic access authentication", "href": "https://en.wikipedia.org/wiki/Basic_access_authentication"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/httpcache.html#HttpCacheMiddleware"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/httpcache.html#DummyPolicy"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/httpcache.html#RFC2616Policy"}, {"text": "https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1", "href": "https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1"}, {"text": "https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6", "href": "https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6"}, {"text": "https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10", "href": "https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/httpcache.html#FilesystemCacheStorage"}, {"text": "[source]", "href": "../_modules/scrapy/extensions/httpcache.html#DbmCacheStorage"}, {"text": "DBM", "href": "https://en.wikipedia.org/wiki/Dbm"}, {"text": "dbm", "href": "https://docs.python.org/3/library/dbm.html#module-dbm"}, {"text": "open_spider", "href": "signals.html#std-signal-spider_opened"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "close_spider", "href": "signals.html#std-signal-spider_closed"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Response", "href": "request-response.html#scrapy.http.Response"}, {"text": "Default structure of Scrapy projects", "href": "commands.html#topics-project-structure"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/httpcompression.html#HttpCompressionMiddleware"}, {"text": "brotli-compressed", "href": "https://www.ietf.org/rfc/rfc7932.txt"}, {"text": "zstd-compressed", "href": "https://www.ietf.org/rfc/rfc8478.txt"}, {"text": "brotli", "href": "https://pypi.org/project/Brotli/"}, {"text": "zstandard", "href": "https://pypi.org/project/zstandard/"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/httpproxy.html#HttpProxyMiddleware"}, {"text": "urllib.request", "href": "https://docs.python.org/3/library/urllib.request.html#module-urllib.request"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/redirect.html#RedirectMiddleware"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/redirect.html#MetaRefreshMiddleware"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/retry.html#RetryMiddleware"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/retry.html#get_retry_request"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "settings", "href": "settings.html#topics-settings"}, {"text": "stats", "href": "stats.html#topics-stats"}, {"text": "logging.debug()", "href": "https://docs.python.org/3/library/logging.html#logging.debug"}, {"text": "Exception", "href": "https://docs.python.org/3/library/exceptions.html#Exception"}, {"text": "max_retry_times", "href": "request-response.html#std-reqmeta-max_retry_times"}, {"text": "max_retry_times", "href": "request-response.html#std-reqmeta-max_retry_times"}, {"text": "max_retry_times", "href": "request-response.html#std-reqmeta-max_retry_times"}, {"text": "max_retry_times", "href": "request-response.html#std-reqmeta-max_retry_times"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/robotstxt.html#RobotsTxtMiddleware"}, {"text": "ROBOTSTXT_OBEY", "href": "settings.html#std-setting-ROBOTSTXT_OBEY"}, {"text": "ROBOTSTXT_USER_AGENT", "href": "settings.html#std-setting-ROBOTSTXT_USER_AGENT"}, {"text": "robots.txt", "href": "https://www.robotstxt.org/"}, {"text": "USER_AGENT", "href": "settings.html#std-setting-USER_AGENT"}, {"text": "robots.txt", "href": "https://www.robotstxt.org/"}, {"text": "robots.txt", "href": "https://www.robotstxt.org/"}, {"text": "robots.txt", "href": "https://www.robotstxt.org/"}, {"text": "robots.txt", "href": "https://www.robotstxt.org/"}, {"text": "ROBOTSTXT_PARSER", "href": "settings.html#std-setting-ROBOTSTXT_PARSER"}, {"text": "ROBOTSTXT_OBEY", "href": "settings.html#std-setting-ROBOTSTXT_OBEY"}, {"text": "length based rule", "href": "https://developers.google.com/search/reference/robots_txt#order-of-precedence-for-group-member-lines"}, {"text": "the following link", "href": "https://github.com/scrapy/scrapy/issues/3969"}, {"text": "Protego", "href": "https://github.com/scrapy/protego"}, {"text": "Google’s Robots.txt Specification", "href": "https://developers.google.com/search/reference/robots_txt"}, {"text": "RobotFileParser", "href": "https://docs.python.org/3/library/urllib.robotparser.html#urllib.robotparser.RobotFileParser"}, {"text": "robots.txt", "href": "https://www.robotstxt.org/"}, {"text": "Martijn Koster’s 1996 draft specification", "href": "https://www.robotstxt.org/norobots-rfc.txt"}, {"text": "ROBOTSTXT_PARSER", "href": "settings.html#std-setting-ROBOTSTXT_PARSER"}, {"text": "Reppy", "href": "https://github.com/seomoz/reppy/"}, {"text": "Robots Exclusion Protocol Parser for C++", "href": "https://github.com/seomoz/rep-cpp"}, {"text": "Martijn Koster’s 1996 draft specification", "href": "https://www.robotstxt.org/norobots-rfc.txt"}, {"text": "Reppy", "href": "https://github.com/seomoz/reppy/"}, {"text": "Upstream issue #122", "href": "https://github.com/seomoz/reppy/issues/122"}, {"text": "ROBOTSTXT_PARSER", "href": "settings.html#std-setting-ROBOTSTXT_PARSER"}, {"text": "Robotexclusionrulesparser", "href": "http://nikitathespider.com/python/rerp/"}, {"text": "Martijn Koster’s 1996 draft specification", "href": "https://www.robotstxt.org/norobots-rfc.txt"}, {"text": "Robotexclusionrulesparser", "href": "http://nikitathespider.com/python/rerp/"}, {"text": "ROBOTSTXT_PARSER", "href": "settings.html#std-setting-ROBOTSTXT_PARSER"}, {"text": "robots.txt", "href": "https://www.robotstxt.org/"}, {"text": "[source]", "href": "../_modules/scrapy/robotstxt.html#RobotParser"}, {"text": "[source]", "href": "../_modules/scrapy/robotstxt.html#RobotParser.allowed"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "str", "href": "https://docs.python.org/3/library/stdtypes.html#str"}, {"text": "[source]", "href": "../_modules/scrapy/robotstxt.html#RobotParser.from_crawler"}, {"text": "robots.txt", "href": "https://www.robotstxt.org/"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "bytes", "href": "https://docs.python.org/3/library/stdtypes.html#bytes"}, {"text": "robots.txt", "href": "https://www.robotstxt.org/"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/stats.html#DownloaderStats"}, {"text": "DOWNLOADER_STATS", "href": "settings.html#std-setting-DOWNLOADER_STATS"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/useragent.html#UserAgentMiddleware"}, {"text": "[source]", "href": "../_modules/scrapy/downloadermiddlewares/ajaxcrawl.html#AjaxCrawlMiddleware"}, {"text": "https://developers.google.com/search/docs/ajax-crawling/docs/getting-started", "href": "https://developers.google.com/search/docs/ajax-crawling/docs/getting-started"}, {"text": "broad crawls", "href": "broad-crawls.html#topics-broad-crawls"}], "timestamp": "2023-10-12T21:19:40.812727", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/addons.html", "content": {"sections": [], "paragraphs": ["<p>Scrapy’s add-on system is a framework which unifies managing and configuring\ncomponents that extend Scrapy’s core functionality, such as middlewares,\nextensions, or pipelines. It provides users with a plug-and-play experience in\nScrapy extension management, and grants extensive configuration control to\ndevelopers.</p>", "<p>During <a class=\"reference internal\" href=\"api.html#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a> initialization, the list of enabled\nadd-ons is read from your <code class=\"docutils literal notranslate\"><span class=\"pre\">ADDONS</span></code> setting.</p>", "<p>The <code class=\"docutils literal notranslate\"><span class=\"pre\">ADDONS</span></code> setting is a dict in which every key is an add-on class or its\nimport path and the value is its priority.</p>", "<p>This is an example where two add-ons are enabled in a project’s\n<code class=\"docutils literal notranslate\"><span class=\"pre\">settings.py</span></code>:</p>", "<p>Add-ons are Python classes that include the following method:</p>", "<p>This method is called during the initialization of the\n<a class=\"reference internal\" href=\"api.html#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a>. Here, you should perform dependency checks\n(e.g. for external Python libraries) and update the\n<a class=\"reference internal\" href=\"api.html#scrapy.settings.Settings\" title=\"scrapy.settings.Settings\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Settings</span></code></a> object as wished, e.g. enable components\nfor this add-on or set required configuration of other extensions.</p>", "<p><strong>settings</strong> (<a class=\"reference internal\" href=\"api.html#scrapy.settings.Settings\" title=\"scrapy.settings.Settings\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Settings</span></code></a>) – The settings object storing Scrapy/component configuration</p>", "<p>They can also have the following method:</p>", "<p>If present, this class method is called to create an add-on instance\nfrom a <a class=\"reference internal\" href=\"api.html#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a>. It must return a new instance\nof the add-on. The crawler object provides access to all Scrapy core\ncomponents like settings and signals; it is a way for the add-on to access\nthem and hook its functionality into Scrapy.</p>", "<p><strong>crawler</strong> (<a class=\"reference internal\" href=\"api.html#scrapy.crawler.Crawler\" title=\"scrapy.crawler.Crawler\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Crawler</span></code></a>) – The crawler that uses this add-on</p>", "<p>The settings set by the add-on should use the <code class=\"docutils literal notranslate\"><span class=\"pre\">addon</span></code> priority (see\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#populating-settings\"><span class=\"std std-ref\">Populating the settings</span></a> and <a class=\"reference internal\" href=\"api.html#scrapy.settings.BaseSettings.set\" title=\"scrapy.settings.BaseSettings.set\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.settings.BaseSettings.set()</span></code></a>):</p>", "<p>This allows users to override these settings in the project or spider\nconfiguration. This is not possible with settings that are mutable objects,\nsuch as the dict that is a value of <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-ITEM_PIPELINES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">ITEM_PIPELINES</span></code></a>. In these cases\nyou can provide an add-on-specific setting that governs whether the add-on will\nmodify <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-ITEM_PIPELINES\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">ITEM_PIPELINES</span></code></a>:</p>", "<p>If the <code class=\"docutils literal notranslate\"><span class=\"pre\">update_settings</span></code> method raises\n<a class=\"reference internal\" href=\"exceptions.html#scrapy.exceptions.NotConfigured\" title=\"scrapy.exceptions.NotConfigured\"><code class=\"xref py py-exc docutils literal notranslate\"><span class=\"pre\">scrapy.exceptions.NotConfigured</span></code></a>, the add-on will be skipped. This makes\nit easy to enable an add-on only when some conditions are met.</p>", "<p>Some components provided by add-ons need to fall back to “default”\nimplementations, e.g. a custom download handler needs to send the request that\nit doesn’t handle via the default download handler, or a stats collector that\nincludes some additional processing but otherwise uses the default stats\ncollector. And it’s possible that a project needs to use several custom\ncomponents of the same type, e.g. two custom download handlers that support\ndifferent kinds of custom requests and still need to use the default download\nhandler for other requests. To make such use cases easier to configure, we\nrecommend that such custom components should be written in the following way:</p>", "<p>The custom component (e.g. <code class=\"docutils literal notranslate\"><span class=\"pre\">MyDownloadHandler</span></code>) shouldn’t inherit from the\ndefault Scrapy one (e.g.\n<code class=\"docutils literal notranslate\"><span class=\"pre\">scrapy.core.downloader.handlers.http.HTTPDownloadHandler</span></code>), but instead\nbe able to load the class of the fallback component from a special setting\n(e.g. <code class=\"docutils literal notranslate\"><span class=\"pre\">MY_FALLBACK_DOWNLOAD_HANDLER</span></code>), create an instance of it and use\nit.</p>", "<p>The add-ons that include these components should read the current value of\nthe default setting (e.g. <code class=\"docutils literal notranslate\"><span class=\"pre\">DOWNLOAD_HANDLERS</span></code>) in their\n<code class=\"docutils literal notranslate\"><span class=\"pre\">update_settings()</span></code> methods, save that value into the fallback setting\n(<code class=\"docutils literal notranslate\"><span class=\"pre\">MY_FALLBACK_DOWNLOAD_HANDLER</span></code> mentioned earlier) and set the default\nsetting to the component provided by the add-on (e.g.\n<code class=\"docutils literal notranslate\"><span class=\"pre\">MyDownloadHandler</span></code>). If the fallback setting is already set by the user,\nthey shouldn’t change it.</p>", "<p>This way, if there are several add-ons that want to modify the same setting,\nall of them will fallback to the component from the previous one and then to\nthe Scrapy default. The order of that depends on the priority order in the\n<code class=\"docutils literal notranslate\"><span class=\"pre\">ADDONS</span></code> setting.</p>", "<p>Set some basic configuration:</p>", "<p>Check dependencies:</p>", "<p>Access the crawler instance:</p>", "<p>Use a fallback component:</p>"]}, "code_blocks": ["ADDONS = {\n    'path.to.someaddon': 0,\n    SomeAddonClass: 1,\n}\n</pre>", "class MyAddon:\n    def update_settings(self, settings):\n        settings.set(\"DNSCACHE_ENABLED\", True, \"addon\")\n</pre>", "class MyAddon:\n    def update_settings(self, settings):\n        if settings.getbool(\"MYADDON_ENABLE_PIPELINE\"):\n            settings[\"ITEM_PIPELINES\"][\"path.to.mypipeline\"] = 200\n</pre>", "class MyAddon:\n    def update_settings(self, settings):\n        settings[\"ITEM_PIPELINES\"][\"path.to.mypipeline\"] = 200\n        settings.set(\"DNSCACHE_ENABLED\", True, \"addon\")\n</pre>", "class MyAddon:\n    def update_settings(self, settings):\n        try:\n            import boto\n        except ImportError:\n            raise NotConfigured(\"MyAddon requires the boto library\")\n        ...\n</pre>", "class MyAddon:\n    def __init__(self, crawler) -> None:\n        super().__init__()\n        self.crawler = crawler\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(crawler)\n\n    def update_settings(self, settings):\n        ...\n</pre>", "from scrapy.core.downloader.handlers.http import HTTPDownloadHandler\n\n\nFALLBACK_SETTING = \"MY_FALLBACK_DOWNLOAD_HANDLER\"\n\n\nclass MyHandler:\n    lazy = False\n\n    def __init__(self, settings, crawler):\n        dhcls = load_object(settings.get(FALLBACK_SETTING))\n        self._fallback_handler = create_instance(\n            dhcls,\n            settings=None,\n            crawler=crawler,\n        )\n\n    def download_request(self, request, spider):\n        if request.meta.get(\"my_params\"):\n            # handle the request\n            ...\n        else:\n            return self._fallback_handler.download_request(request, spider)\n\n\nclass MyAddon:\n    def update_settings(self, settings):\n        if not settings.get(FALLBACK_SETTING):\n            settings.set(\n                FALLBACK_SETTING,\n                settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"],\n                \"addon\",\n            )\n        settings[\"DOWNLOAD_HANDLERS\"][\"https\"] = MyHandler\n</pre>"], "links": [{"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "Settings", "href": "api.html#scrapy.settings.Settings"}, {"text": "Settings", "href": "api.html#scrapy.settings.Settings"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "Crawler", "href": "api.html#scrapy.crawler.Crawler"}, {"text": "Populating the settings", "href": "settings.html#populating-settings"}, {"text": "scrapy.settings.BaseSettings.set()", "href": "api.html#scrapy.settings.BaseSettings.set"}, {"text": "ITEM_PIPELINES", "href": "settings.html#std-setting-ITEM_PIPELINES"}, {"text": "ITEM_PIPELINES", "href": "settings.html#std-setting-ITEM_PIPELINES"}, {"text": "scrapy.exceptions.NotConfigured", "href": "exceptions.html#scrapy.exceptions.NotConfigured"}], "timestamp": "2023-10-12T21:19:44.872398", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/architecture.html", "content": {"sections": [], "paragraphs": ["<p>This document describes the architecture of Scrapy and how its components\ninteract.</p>", "<p>The following diagram shows an overview of the Scrapy architecture with its\ncomponents and an outline of the data flow that takes place inside the system\n(shown by the red arrows). A brief description of the components is included\nbelow with links for more detailed information about them. The data flow is\nalso described below.</p>", "<p>The data flow in Scrapy is controlled by the execution engine, and goes like\nthis:</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"#component-engine\"><span class=\"std std-ref\">Engine</span></a> gets the initial Requests to crawl from the\n<a class=\"hoverxref tooltip reference internal\" href=\"#component-spiders\"><span class=\"std std-ref\">Spider</span></a>.</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"#component-engine\"><span class=\"std std-ref\">Engine</span></a> schedules the Requests in the\n<a class=\"hoverxref tooltip reference internal\" href=\"#component-scheduler\"><span class=\"std std-ref\">Scheduler</span></a> and asks for the\nnext Requests to crawl.</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"#component-scheduler\"><span class=\"std std-ref\">Scheduler</span></a> returns the next Requests\nto the <a class=\"hoverxref tooltip reference internal\" href=\"#component-engine\"><span class=\"std std-ref\">Engine</span></a>.</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"#component-engine\"><span class=\"std std-ref\">Engine</span></a> sends the Requests to the\n<a class=\"hoverxref tooltip reference internal\" href=\"#component-downloader\"><span class=\"std std-ref\">Downloader</span></a>, passing through the\n<a class=\"hoverxref tooltip reference internal\" href=\"#component-downloader-middleware\"><span class=\"std std-ref\">Downloader Middlewares</span></a> (see\n<a class=\"reference internal\" href=\"downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_request\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_request()</span></code></a>).</p>", "<p>Once the page finishes downloading the\n<a class=\"hoverxref tooltip reference internal\" href=\"#component-downloader\"><span class=\"std std-ref\">Downloader</span></a> generates a Response (with\nthat page) and sends it to the Engine, passing through the\n<a class=\"hoverxref tooltip reference internal\" href=\"#component-downloader-middleware\"><span class=\"std std-ref\">Downloader Middlewares</span></a> (see\n<a class=\"reference internal\" href=\"downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_response\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_response()</span></code></a>).</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"#component-engine\"><span class=\"std std-ref\">Engine</span></a> receives the Response from the\n<a class=\"hoverxref tooltip reference internal\" href=\"#component-downloader\"><span class=\"std std-ref\">Downloader</span></a> and sends it to the\n<a class=\"hoverxref tooltip reference internal\" href=\"#component-spiders\"><span class=\"std std-ref\">Spider</span></a> for processing, passing\nthrough the <a class=\"hoverxref tooltip reference internal\" href=\"#component-spider-middleware\"><span class=\"std std-ref\">Spider Middleware</span></a> (see\n<a class=\"reference internal\" href=\"spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input\" title=\"scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_spider_input()</span></code></a>).</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"#component-spiders\"><span class=\"std std-ref\">Spider</span></a> processes the Response and returns\nscraped items and new Requests (to follow) to the\n<a class=\"hoverxref tooltip reference internal\" href=\"#component-engine\"><span class=\"std std-ref\">Engine</span></a>, passing through the\n<a class=\"hoverxref tooltip reference internal\" href=\"#component-spider-middleware\"><span class=\"std std-ref\">Spider Middleware</span></a> (see\n<a class=\"reference internal\" href=\"spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\" title=\"scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_spider_output()</span></code></a>).</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"#component-engine\"><span class=\"std std-ref\">Engine</span></a> sends processed items to\n<a class=\"hoverxref tooltip reference internal\" href=\"#component-pipelines\"><span class=\"std std-ref\">Item Pipelines</span></a>, then send processed Requests to\nthe <a class=\"hoverxref tooltip reference internal\" href=\"#component-scheduler\"><span class=\"std std-ref\">Scheduler</span></a> and asks for possible next Requests\nto crawl.</p>", "<p>The process repeats (from step 3) until there are no more requests from the\n<a class=\"hoverxref tooltip reference internal\" href=\"#component-scheduler\"><span class=\"std std-ref\">Scheduler</span></a>.</p>", "<p>The engine is responsible for controlling the data flow between all components\nof the system, and triggering events when certain actions occur. See the\n<a class=\"hoverxref tooltip reference internal\" href=\"#data-flow\"><span class=\"std std-ref\">Data Flow</span></a> section above for more details.</p>", "<p>The <a class=\"hoverxref tooltip reference internal\" href=\"scheduler.html#topics-scheduler\"><span class=\"std std-ref\">scheduler</span></a> receives requests from the engine and\nenqueues them for feeding them later (also to the engine) when the engine\nrequests them.</p>", "<p>The Downloader is responsible for fetching web pages and feeding them to the\nengine which, in turn, feeds them to the spiders.</p>", "<p>Spiders are custom classes written by Scrapy users to parse responses and\nextract <a class=\"hoverxref tooltip reference internal\" href=\"items.html#topics-items\"><span class=\"std std-ref\">items</span></a> from them or additional requests to\nfollow. For more information see <a class=\"hoverxref tooltip reference internal\" href=\"spiders.html#topics-spiders\"><span class=\"std std-ref\">Spiders</span></a>.</p>", "<p>The Item Pipeline is responsible for processing the items once they have been\nextracted (or scraped) by the spiders. Typical tasks include cleansing,\nvalidation and persistence (like storing the item in a database). For more\ninformation see <a class=\"hoverxref tooltip reference internal\" href=\"item-pipeline.html#topics-item-pipeline\"><span class=\"std std-ref\">Item Pipeline</span></a>.</p>", "<p>Downloader middlewares are specific hooks that sit between the Engine and the\nDownloader and process requests when they pass from the Engine to the\nDownloader, and responses that pass from Downloader to the Engine.</p>", "<p>Use a Downloader middleware if you need to do one of the following:</p>", "<p>process a request just before it is sent to the Downloader\n(i.e. right before Scrapy sends the request to the website);</p>", "<p>change received response before passing it to a spider;</p>", "<p>send a new Request instead of passing received response to a spider;</p>", "<p>pass response to a spider without fetching a web page;</p>", "<p>silently drop some requests.</p>", "<p>For more information see <a class=\"hoverxref tooltip reference internal\" href=\"downloader-middleware.html#topics-downloader-middleware\"><span class=\"std std-ref\">Downloader Middleware</span></a>.</p>", "<p>Spider middlewares are specific hooks that sit between the Engine and the\nSpiders and are able to process spider input (responses) and output (items and\nrequests).</p>", "<p>Use a Spider middleware if you need to</p>", "<p>post-process output of spider callbacks - change/add/remove requests or items;</p>", "<p>post-process start_requests;</p>", "<p>handle spider exceptions;</p>", "<p>call errback instead of callback for some of the requests based on response\ncontent.</p>", "<p>For more information see <a class=\"hoverxref tooltip reference internal\" href=\"spider-middleware.html#topics-spider-middleware\"><span class=\"std std-ref\">Spider Middleware</span></a>.</p>", "<p>Scrapy is written with <a class=\"reference external\" href=\"https://twistedmatrix.com/trac/\">Twisted</a>, a popular event-driven networking framework\nfor Python. Thus, it’s implemented using a non-blocking (aka asynchronous) code\nfor concurrency.</p>", "<p>For more information about asynchronous programming and Twisted see these\nlinks:</p>"]}, "code_blocks": [], "links": [{"text": "", "href": "../_images/scrapy_architecture_02.png"}, {"text": "process_request()", "href": "downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"}, {"text": "process_response()", "href": "downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"}, {"text": "process_spider_input()", "href": "spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input"}, {"text": "process_spider_output()", "href": "spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"}, {"text": "scheduler", "href": "scheduler.html#topics-scheduler"}, {"text": "items", "href": "items.html#topics-items"}, {"text": "Spiders", "href": "spiders.html#topics-spiders"}, {"text": "Item Pipeline", "href": "item-pipeline.html#topics-item-pipeline"}, {"text": "Downloader Middleware", "href": "downloader-middleware.html#topics-downloader-middleware"}, {"text": "Spider Middleware", "href": "spider-middleware.html#topics-spider-middleware"}, {"text": "Twisted", "href": "https://twistedmatrix.com/trac/"}, {"text": "Introduction to Deferreds", "href": "https://docs.twisted.org/en/stable/core/howto/defer-intro.html"}, {"text": "Twisted - hello, asynchronous programming", "href": "http://jessenoller.com/blog/2009/02/11/twisted-hello-asynchronous-programming/"}, {"text": "Twisted Introduction - Krondo", "href": "http://krondo.com/an-introduction-to-asynchronous-programming-and-twisted/"}], "timestamp": "2023-10-12T21:19:49.157336", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/asyncio.html", "content": {"sections": [], "paragraphs": ["<p>Scrapy has partial support for <a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio.html#module-asyncio\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">asyncio</span></code></a>. After you <a class=\"hoverxref tooltip reference internal\" href=\"#install-asyncio\"><span class=\"std std-ref\">install the\nasyncio reactor</span></a>, you may use <a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio.html#module-asyncio\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">asyncio</span></code></a> and\n<a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio.html#module-asyncio\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">asyncio</span></code></a>-powered libraries in any <a class=\"reference internal\" href=\"coroutines.html\"><span class=\"doc\">coroutine</span></a>.</p>", "<p>To enable <a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio.html#module-asyncio\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">asyncio</span></code></a> support, set the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-TWISTED_REACTOR\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">TWISTED_REACTOR</span></code></a> setting to\n<code class=\"docutils literal notranslate\"><span class=\"pre\">'twisted.internet.asyncioreactor.AsyncioSelectorReactor'</span></code>.</p>", "<p>If you are using <a class=\"reference internal\" href=\"api.html#scrapy.crawler.CrawlerRunner\" title=\"scrapy.crawler.CrawlerRunner\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CrawlerRunner</span></code></a>, you also need to\ninstall the <a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.internet.asyncioreactor.AsyncioSelectorReactor.html\" title=\"(in Twisted)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">AsyncioSelectorReactor</span></code></a>\nreactor manually. You can do that using\n<a class=\"reference internal\" href=\"settings.html#scrapy.utils.reactor.install_reactor\" title=\"scrapy.utils.reactor.install_reactor\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">install_reactor()</span></code></a>:</p>", "<p><code class=\"docutils literal notranslate\"><span class=\"pre\">twisted.internet.reactor</span></code> and some other Twisted imports install the default\nTwisted reactor as a side effect. Once a Twisted reactor is installed, it is\nnot possible to switch to a different reactor at run time.</p>", "<p>If you <a class=\"hoverxref tooltip reference internal\" href=\"#install-asyncio\"><span class=\"std std-ref\">configure the asyncio Twisted reactor</span></a> and, at\nrun time, Scrapy complains that a different reactor is already installed,\nchances are you have some such imports in your code.</p>", "<p>You can usually fix the issue by moving those offending module-level Twisted\nimports to the method or function definitions where they are used. For example,\nif you have something like:</p>", "<p>Switch to something like:</p>", "<p>Alternatively, you can try to <a class=\"hoverxref tooltip reference internal\" href=\"#install-asyncio\"><span class=\"std std-ref\">manually install the asyncio reactor</span></a>, with <a class=\"reference internal\" href=\"settings.html#scrapy.utils.reactor.install_reactor\" title=\"scrapy.utils.reactor.install_reactor\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">install_reactor()</span></code></a>, before\nthose imports happen.</p>", "<p>When the asyncio reactor isn’t installed, you can await on Deferreds in the\ncoroutines directly. When it is installed, this is not possible anymore, due to\nspecifics of the Scrapy coroutine integration (the coroutines are wrapped into\n<a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio-future.html#asyncio.Future\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">asyncio.Future</span></code></a> objects, not into\n<a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html\" title=\"(in Twisted)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Deferred</span></code></a> directly), and you need to wrap them into\nFutures. Scrapy provides two helpers for this:</p>", "<p>Return an <a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio-future.html#asyncio.Future\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">asyncio.Future</span></code></a> object that wraps <em>d</em>.</p>", "<p>When <a class=\"hoverxref tooltip reference internal\" href=\"#install-asyncio\"><span class=\"std std-ref\">using the asyncio reactor</span></a>, you cannot await\non <a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html\" title=\"(in Twisted)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Deferred</span></code></a> objects from <a class=\"hoverxref tooltip reference internal\" href=\"coroutines.html#coroutine-support\"><span class=\"std std-ref\">Scrapy\ncallables defined as coroutines</span></a>, you can only await on\n<code class=\"docutils literal notranslate\"><span class=\"pre\">Future</span></code> objects. Wrapping <code class=\"docutils literal notranslate\"><span class=\"pre\">Deferred</span></code> objects into <code class=\"docutils literal notranslate\"><span class=\"pre\">Future</span></code> objects\nallows you to wait on them:</p>", "<p>Return <em>d</em> as an object that can be awaited from a <a class=\"hoverxref tooltip reference internal\" href=\"coroutines.html#coroutine-support\"><span class=\"std std-ref\">Scrapy callable\ndefined as a coroutine</span></a>.</p>", "<p>What you can await in Scrapy callables defined as coroutines depends on the\nvalue of <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-TWISTED_REACTOR\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">TWISTED_REACTOR</span></code></a>:</p>", "<p>When not using the asyncio reactor, you can only await on\n<a class=\"reference external\" href=\"https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html\" title=\"(in Twisted)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Deferred</span></code></a> objects.</p>", "<p>When <a class=\"hoverxref tooltip reference internal\" href=\"#install-asyncio\"><span class=\"std std-ref\">using the asyncio reactor</span></a>, you can only\nawait on <a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio-future.html#asyncio.Future\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">asyncio.Future</span></code></a> objects.</p>", "<p>If you want to write code that uses <code class=\"docutils literal notranslate\"><span class=\"pre\">Deferred</span></code> objects but works with any\nreactor, use this function on all <code class=\"docutils literal notranslate\"><span class=\"pre\">Deferred</span></code> objects:</p>", "<p class=\"admonition-title\">Tip</p>", "<p>If you need to use these functions in code that aims to be compatible\nwith lower versions of Scrapy that do not provide these functions,\ndown to Scrapy 2.0 (earlier versions do not support\n<a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio.html#module-asyncio\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">asyncio</span></code></a>), you can copy the implementation of these functions\ninto your own code.</p>", "<p>If you are writing a <a class=\"hoverxref tooltip reference internal\" href=\"components.html#topics-components\"><span class=\"std std-ref\">component</span></a> that requires asyncio\nto work, use <code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">scrapy.utils.reactor.is_asyncio_reactor_installed()</span></code> to\n<a class=\"hoverxref tooltip reference internal\" href=\"components.html#enforce-component-requirements\"><span class=\"std std-ref\">enforce it as a requirement</span></a>. For\nexample:</p>", "<p>The Windows implementation of <a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio.html#module-asyncio\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">asyncio</span></code></a> can use two event loop\nimplementations, <a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.ProactorEventLoop\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ProactorEventLoop</span></code></a> (default) and\n<a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.SelectorEventLoop\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">SelectorEventLoop</span></code></a>. However, only\n<a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.SelectorEventLoop\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">SelectorEventLoop</span></code></a> works with Twisted.</p>", "<p>Scrapy changes the event loop class to <a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.SelectorEventLoop\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">SelectorEventLoop</span></code></a>\nautomatically when you change the <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-TWISTED_REACTOR\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">TWISTED_REACTOR</span></code></a> setting or call\n<a class=\"reference internal\" href=\"settings.html#scrapy.utils.reactor.install_reactor\" title=\"scrapy.utils.reactor.install_reactor\"><code class=\"xref py py-func docutils literal notranslate\"><span class=\"pre\">install_reactor()</span></code></a>.</p>", "<p class=\"admonition-title\">Note</p>", "<p>Other libraries you use may require\n<a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.ProactorEventLoop\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ProactorEventLoop</span></code></a>, e.g. because it supports\nsubprocesses (this is the case with <a class=\"reference external\" href=\"https://github.com/microsoft/playwright-python\">playwright</a>), so you cannot use\nthem together with Scrapy on Windows (but you should be able to use\nthem on WSL or native Linux).</p>", "<p>You can also use custom asyncio event loops with the asyncio reactor. Set the\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-ASYNCIO_EVENT_LOOP\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">ASYNCIO_EVENT_LOOP</span></code></a> setting to the import path of the desired event\nloop class to use it instead of the default asyncio event loop.</p>"]}, "code_blocks": ["install_reactor('twisted.internet.asyncioreactor.AsyncioSelectorReactor')\n</pre>", "from twisted.internet import reactor\n\n\ndef my_function():\n    reactor.callLater(...)\n</pre>", "def my_function():\n    from twisted.internet import reactor\n\n    reactor.callLater(...)\n</pre>", "class MySpider(Spider):\n    ...\n    async def parse(self, response):\n        additional_request = scrapy.Request('https://example.org/price')\n        deferred = self.crawler.engine.download(additional_request)\n        additional_response = await deferred_to_future(deferred)\n</pre>", "class MySpider(Spider):\n    ...\n    async def parse(self, response):\n        additional_request = scrapy.Request('https://example.org/price')\n        deferred = self.crawler.engine.download(additional_request)\n        additional_response = await maybe_deferred_to_future(deferred)\n</pre>", "from scrapy.utils.reactor import is_asyncio_reactor_installed\n\n\nclass MyComponent:\n    def __init__(self):\n        if not is_asyncio_reactor_installed():\n            raise ValueError(\n                f\"{MyComponent.__qualname__} requires the asyncio Twisted \"\n                f\"reactor. Make sure you have it configured in the \"\n                f\"TWISTED_REACTOR setting. See the asyncio documentation \"\n                f\"of Scrapy for more information.\"\n            )\n</pre>"], "links": [{"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "coroutine", "href": "coroutines.html"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "TWISTED_REACTOR", "href": "settings.html#std-setting-TWISTED_REACTOR"}, {"text": "CrawlerRunner", "href": "api.html#scrapy.crawler.CrawlerRunner"}, {"text": "AsyncioSelectorReactor", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.asyncioreactor.AsyncioSelectorReactor.html"}, {"text": "install_reactor()", "href": "settings.html#scrapy.utils.reactor.install_reactor"}, {"text": "install_reactor()", "href": "settings.html#scrapy.utils.reactor.install_reactor"}, {"text": "asyncio.Future", "href": "https://docs.python.org/3/library/asyncio-future.html#asyncio.Future"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "[source]", "href": "../_modules/scrapy/utils/defer.html#deferred_to_future"}, {"text": "asyncio.Future", "href": "https://docs.python.org/3/library/asyncio-future.html#asyncio.Future"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "Scrapy\ncallables defined as coroutines", "href": "coroutines.html#coroutine-support"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "Union", "href": "https://docs.python.org/3/library/typing.html#typing.Union"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "[source]", "href": "../_modules/scrapy/utils/defer.html#maybe_deferred_to_future"}, {"text": "Scrapy callable\ndefined as a coroutine", "href": "coroutines.html#coroutine-support"}, {"text": "TWISTED_REACTOR", "href": "settings.html#std-setting-TWISTED_REACTOR"}, {"text": "Deferred", "href": "https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html"}, {"text": "asyncio.Future", "href": "https://docs.python.org/3/library/asyncio-future.html#asyncio.Future"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "component", "href": "components.html#topics-components"}, {"text": "enforce it as a requirement", "href": "components.html#enforce-component-requirements"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "ProactorEventLoop", "href": "https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.ProactorEventLoop"}, {"text": "SelectorEventLoop", "href": "https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.SelectorEventLoop"}, {"text": "SelectorEventLoop", "href": "https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.SelectorEventLoop"}, {"text": "SelectorEventLoop", "href": "https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.SelectorEventLoop"}, {"text": "TWISTED_REACTOR", "href": "settings.html#std-setting-TWISTED_REACTOR"}, {"text": "install_reactor()", "href": "settings.html#scrapy.utils.reactor.install_reactor"}, {"text": "ProactorEventLoop", "href": "https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.ProactorEventLoop"}, {"text": "playwright", "href": "https://github.com/microsoft/playwright-python"}, {"text": "ASYNCIO_EVENT_LOOP", "href": "settings.html#std-setting-ASYNCIO_EVENT_LOOP"}], "timestamp": "2023-10-12T21:19:53.340204", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/coroutines.html", "content": {"sections": [], "paragraphs": ["<p>Scrapy has <a class=\"hoverxref tooltip reference internal\" href=\"#coroutine-support\"><span class=\"std std-ref\">partial support</span></a> for the\n<a class=\"reference external\" href=\"https://docs.python.org/3/reference/compound_stmts.html#async\" title=\"(in Python v3.11)\"><span class=\"xref std std-ref\">coroutine syntax</span></a>.</p>", "<p>The following callables may be defined as coroutines using <code class=\"docutils literal notranslate\"><span class=\"pre\">async</span> <span class=\"pre\">def</span></code>, and\nhence use coroutine syntax (e.g. <code class=\"docutils literal notranslate\"><span class=\"pre\">await</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">async</span> <span class=\"pre\">for</span></code>, <code class=\"docutils literal notranslate\"><span class=\"pre\">async</span> <span class=\"pre\">with</span></code>):</p>", "<p><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> callbacks.</p>", "<p>If you are using any custom or third-party <a class=\"hoverxref tooltip reference internal\" href=\"spider-middleware.html#topics-spider-middleware\"><span class=\"std std-ref\">spider middleware</span></a>, see <a class=\"hoverxref tooltip reference internal\" href=\"#sync-async-spider-middleware\"><span class=\"std std-ref\">Mixing synchronous and asynchronous spider middlewares</span></a>.</p>", "<p><span class=\"versionmodified changed\">Changed in version 2.7: </span>Output of async callbacks is now processed asynchronously instead of\ncollecting all of it first.</p>", "<p>The <a class=\"reference internal\" href=\"item-pipeline.html#process_item\" title=\"process_item\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_item()</span></code></a> method of\n<a class=\"hoverxref tooltip reference internal\" href=\"item-pipeline.html#topics-item-pipeline\"><span class=\"std std-ref\">item pipelines</span></a>.</p>", "<p>The\n<a class=\"reference internal\" href=\"downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_request\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_request()</span></code></a>,\n<a class=\"reference internal\" href=\"downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_response\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_response()</span></code></a>,\nand\n<a class=\"reference internal\" href=\"downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception\" title=\"scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_exception()</span></code></a>\nmethods of\n<a class=\"hoverxref tooltip reference internal\" href=\"downloader-middleware.html#topics-downloader-middleware-custom\"><span class=\"std std-ref\">downloader middlewares</span></a>.</p>", "<p><a class=\"hoverxref tooltip reference internal\" href=\"signals.html#signal-deferred\"><span class=\"std std-ref\">Signal handlers that support deferreds</span></a>.</p>", "<p>The\n<a class=\"reference internal\" href=\"spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\" title=\"scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_spider_output()</span></code></a>\nmethod of <a class=\"hoverxref tooltip reference internal\" href=\"spider-middleware.html#topics-spider-middleware\"><span class=\"std std-ref\">spider middlewares</span></a>.</p>", "<p>It must be defined as an <a class=\"reference external\" href=\"https://docs.python.org/3/glossary.html#term-asynchronous-generator\" title=\"(in Python v3.11)\"><span class=\"xref std std-term\">asynchronous generator</span></a>. The input\n<code class=\"docutils literal notranslate\"><span class=\"pre\">result</span></code> parameter is an <a class=\"reference external\" href=\"https://docs.python.org/3/glossary.html#term-asynchronous-iterable\" title=\"(in Python v3.11)\"><span class=\"xref std std-term\">asynchronous iterable</span></a>.</p>", "<p>See also <a class=\"hoverxref tooltip reference internal\" href=\"#sync-async-spider-middleware\"><span class=\"std std-ref\">Mixing synchronous and asynchronous spider middlewares</span></a> and\n<a class=\"hoverxref tooltip reference internal\" href=\"#universal-spider-middleware\"><span class=\"std std-ref\">Universal spider middlewares</span></a>.</p>", "<p>There are several use cases for coroutines in Scrapy.</p>", "<p>Code that would return Deferreds when written for previous Scrapy versions,\nsuch as downloader middlewares and signal handlers, can be rewritten to be\nshorter and cleaner:</p>", "<p>becomes:</p>", "<p>Coroutines may be used to call asynchronous code. This includes other\ncoroutines, functions that return Deferreds and functions that return\n<a class=\"reference external\" href=\"https://docs.python.org/3/glossary.html#term-awaitable\" title=\"(in Python v3.11)\"><span class=\"xref std std-term\">awaitable objects</span></a> such as <a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio-future.html#asyncio.Future\" title=\"(in Python v3.11)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Future</span></code></a>.\nThis means you can use many useful Python libraries providing such code:</p>", "<p class=\"admonition-title\">Note</p>", "<p>Many libraries that use coroutines, such as <a class=\"reference external\" href=\"https://github.com/aio-libs\">aio-libs</a>, require the\n<a class=\"reference external\" href=\"https://docs.python.org/3/library/asyncio.html#module-asyncio\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">asyncio</span></code></a> loop and to use them you need to\n<a class=\"reference internal\" href=\"asyncio.html\"><span class=\"doc\">enable asyncio support in Scrapy</span></a>.</p>", "<p class=\"admonition-title\">Note</p>", "<p>If you want to <code class=\"docutils literal notranslate\"><span class=\"pre\">await</span></code> on Deferreds while using the asyncio reactor,\nyou need to <a class=\"hoverxref tooltip reference internal\" href=\"asyncio.html#asyncio-await-dfd\"><span class=\"std std-ref\">wrap them</span></a>.</p>", "<p>Common use cases for asynchronous code include:</p>", "<p>requesting data from websites, databases and other services (in callbacks,\npipelines and middlewares);</p>", "<p>storing data in databases (in pipelines and middlewares);</p>", "<p>delaying the spider initialization until some external event (in the\n<a class=\"hoverxref tooltip reference internal\" href=\"signals.html#std-signal-spider_opened\"><code class=\"xref std std-signal docutils literal notranslate\"><span class=\"pre\">spider_opened</span></code></a> handler);</p>", "<p>calling asynchronous Scrapy methods like <code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">ExecutionEngine.download()</span></code>\n(see <a class=\"hoverxref tooltip reference internal\" href=\"item-pipeline.html#screenshotpipeline\"><span class=\"std std-ref\">the screenshot pipeline example</span></a>).</p>", "<p>The spider below shows how to send a request and await its response all from\nwithin a spider callback:</p>", "<p>You can also send multiple requests in parallel:</p>", "<p>The output of a <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> callback is passed as the <code class=\"docutils literal notranslate\"><span class=\"pre\">result</span></code>\nparameter to the\n<a class=\"reference internal\" href=\"spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\" title=\"scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output\"><code class=\"xref py py-meth docutils literal notranslate\"><span class=\"pre\">process_spider_output()</span></code></a> method\nof the first <a class=\"hoverxref tooltip reference internal\" href=\"spider-middleware.html#topics-spider-middleware\"><span class=\"std std-ref\">spider middleware</span></a> from the\n<a class=\"hoverxref tooltip reference internal\" href=\"spider-middleware.html#topics-spider-middleware-setting\"><span class=\"std std-ref\">list of active spider middlewares</span></a>.\nThen the output of that <code class=\"docutils literal notranslate\"><span class=\"pre\">process_spider_output</span></code> method is passed to the\n<code class=\"docutils literal notranslate\"><span class=\"pre\">process_spider_output</span></code> method of the next spider middleware, and so on for\nevery active spider middleware.</p>", "<p>Scrapy supports mixing <a class=\"reference external\" href=\"https://docs.python.org/3/reference/compound_stmts.html#async\" title=\"(in Python v3.11)\"><span class=\"xref std std-ref\">coroutine methods</span></a> and synchronous methods\nin this chain of calls.</p>", "<p>However, if any of the <code class=\"docutils literal notranslate\"><span class=\"pre\">process_spider_output</span></code> methods is defined as a\nsynchronous method, and the previous <code class=\"docutils literal notranslate\"><span class=\"pre\">Request</span></code> callback or\n<code class=\"docutils literal notranslate\"><span class=\"pre\">process_spider_output</span></code> method is a coroutine, there are some drawbacks to\nthe asynchronous-to-synchronous conversion that Scrapy does so that the\nsynchronous <code class=\"docutils literal notranslate\"><span class=\"pre\">process_spider_output</span></code> method gets a synchronous iterable as its\n<code class=\"docutils literal notranslate\"><span class=\"pre\">result</span></code> parameter:</p>", "<p>The whole output of the previous <code class=\"docutils literal notranslate\"><span class=\"pre\">Request</span></code> callback or\n<code class=\"docutils literal notranslate\"><span class=\"pre\">process_spider_output</span></code> method is awaited at this point.</p>", "<p>If an exception raises while awaiting the output of the previous\n<code class=\"docutils literal notranslate\"><span class=\"pre\">Request</span></code> callback or <code class=\"docutils literal notranslate\"><span class=\"pre\">process_spider_output</span></code> method, none of that\noutput will be processed.</p>", "<p>This contrasts with the regular behavior, where all items yielded before\nan exception raises are processed.</p>", "<p>Asynchronous-to-synchronous conversions are supported for backward\ncompatibility, but they are deprecated and will stop working in a future\nversion of Scrapy.</p>", "<p>To avoid asynchronous-to-synchronous conversions, when defining <code class=\"docutils literal notranslate\"><span class=\"pre\">Request</span></code>\ncallbacks as coroutine methods or when using spider middlewares whose\n<code class=\"docutils literal notranslate\"><span class=\"pre\">process_spider_output</span></code> method is an <a class=\"reference external\" href=\"https://docs.python.org/3/glossary.html#term-asynchronous-generator\" title=\"(in Python v3.11)\"><span class=\"xref std std-term\">asynchronous generator</span></a>, all\nactive spider middlewares must either have their <code class=\"docutils literal notranslate\"><span class=\"pre\">process_spider_output</span></code>\nmethod defined as an asynchronous generator or <a class=\"hoverxref tooltip reference internal\" href=\"#universal-spider-middleware\"><span class=\"std std-ref\">define a\nprocess_spider_output_async method</span></a>.</p>", "<p class=\"admonition-title\">Note</p>", "<p>When using third-party spider middlewares that only define a\nsynchronous <code class=\"docutils literal notranslate\"><span class=\"pre\">process_spider_output</span></code> method, consider\n<a class=\"hoverxref tooltip reference internal\" href=\"#universal-spider-middleware\"><span class=\"std std-ref\">making them universal</span></a> through\n<a class=\"reference external\" href=\"https://docs.python.org/3/tutorial/classes.html#tut-inheritance\" title=\"(in Python v3.11)\"><span class=\"xref std std-ref\">subclassing</span></a>.</p>", "<p>To allow writing a spider middleware that supports asynchronous execution of\nits <code class=\"docutils literal notranslate\"><span class=\"pre\">process_spider_output</span></code> method in Scrapy 2.7 and later (avoiding\n<a class=\"hoverxref tooltip reference internal\" href=\"#sync-async-spider-middleware\"><span class=\"std std-ref\">asynchronous-to-synchronous conversions</span></a>)\nwhile maintaining support for older Scrapy versions, you may define\n<code class=\"docutils literal notranslate\"><span class=\"pre\">process_spider_output</span></code> as a synchronous method and define an\n<a class=\"reference external\" href=\"https://docs.python.org/3/glossary.html#term-asynchronous-generator\" title=\"(in Python v3.11)\"><span class=\"xref std std-term\">asynchronous generator</span></a> version of that method with an alternative name:\n<code class=\"docutils literal notranslate\"><span class=\"pre\">process_spider_output_async</span></code>.</p>", "<p>For example:</p>", "<p class=\"admonition-title\">Note</p>", "<p>This is an interim measure to allow, for a time, to write code that\nworks in Scrapy 2.7 and later without requiring\nasynchronous-to-synchronous conversions, and works in earlier Scrapy\nversions as well.</p>", "<p>In some future version of Scrapy, however, this feature will be\ndeprecated and, eventually, in a later version of Scrapy, this\nfeature will be removed, and all spider middlewares will be expected\nto define their <code class=\"docutils literal notranslate\"><span class=\"pre\">process_spider_output</span></code> method as an asynchronous\ngenerator.</p>"]}, "code_blocks": ["from itemadapter import ItemAdapter\n\n\nclass DbPipeline:\n    def _update_item(self, data, item):\n        adapter = ItemAdapter(item)\n        adapter[\"field\"] = data\n        return item\n\n    def process_item(self, item, spider):\n        adapter = ItemAdapter(item)\n        dfd = db.get_some_data(adapter[\"id\"])\n        dfd.addCallback(self._update_item, item)\n        return dfd\n</pre>", "from itemadapter import ItemAdapter\n\n\nclass DbPipeline:\n    async def process_item(self, item, spider):\n        adapter = ItemAdapter(item)\n        adapter[\"field\"] = await db.get_some_data(adapter[\"id\"])\n        return item\n</pre>", "class MySpiderDeferred(Spider):\n    # ...\n    async def parse(self, response):\n        additional_response = await treq.get(\"https://additional.url\")\n        additional_data = await treq.content(additional_response)\n        # ... use response and additional_data to yield items and requests\n\n\nclass MySpiderAsyncio(Spider):\n    # ...\n    async def parse(self, response):\n        async with aiohttp.ClientSession() as session:\n            async with session.get(\"https://additional.url\") as additional_response:\n                additional_data = await additional_response.text()\n        # ... use response and additional_data to yield items and requests\n</pre>", "from scrapy import Spider, Request\nfrom scrapy.utils.defer import maybe_deferred_to_future\n\n\nclass SingleRequestSpider(Spider):\n    name = \"single\"\n    start_urls = [\"https://example.org/product\"]\n\n    async def parse(self, response, **kwargs):\n        additional_request = Request(\"https://example.org/price\")\n        deferred = self.crawler.engine.download(additional_request)\n        additional_response = await maybe_deferred_to_future(deferred)\n        yield {\n            \"h1\": response.css(\"h1\").get(),\n            \"price\": additional_response.css(\"#price\").get(),\n        }\n</pre>", "from scrapy import Spider, Request\nfrom scrapy.utils.defer import maybe_deferred_to_future\nfrom twisted.internet.defer import DeferredList\n\n\nclass MultipleRequestsSpider(Spider):\n    name = \"multiple\"\n    start_urls = [\"https://example.com/product\"]\n\n    async def parse(self, response, **kwargs):\n        additional_requests = [\n            Request(\"https://example.com/price\"),\n            Request(\"https://example.com/color\"),\n        ]\n        deferreds = []\n        for r in additional_requests:\n            deferred = self.crawler.engine.download(r)\n            deferreds.append(deferred)\n        responses = await maybe_deferred_to_future(DeferredList(deferreds))\n        yield {\n            \"h1\": response.css(\"h1::text\").get(),\n            \"price\": responses[0][1].css(\".price::text\").get(),\n            \"price2\": responses[1][1].css(\".color::text\").get(),\n        }\n</pre>", "class UniversalSpiderMiddleware:\n    def process_spider_output(self, response, result, spider):\n        for r in result:\n            # ... do something with r\n            yield r\n\n    async def process_spider_output_async(self, response, result, spider):\n        async for r in result:\n            # ... do something with r\n            yield r\n</pre>"], "links": [{"text": "coroutine syntax", "href": "https://docs.python.org/3/reference/compound_stmts.html#async"}, {"text": "spider middleware", "href": "spider-middleware.html#topics-spider-middleware"}, {"text": "process_item()", "href": "item-pipeline.html#process_item"}, {"text": "item pipelines", "href": "item-pipeline.html#topics-item-pipeline"}, {"text": "process_request()", "href": "downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"}, {"text": "process_response()", "href": "downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"}, {"text": "process_exception()", "href": "downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"}, {"text": "downloader middlewares", "href": "downloader-middleware.html#topics-downloader-middleware-custom"}, {"text": "Signal handlers that support deferreds", "href": "signals.html#signal-deferred"}, {"text": "process_spider_output()", "href": "spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"}, {"text": "spider middlewares", "href": "spider-middleware.html#topics-spider-middleware"}, {"text": "asynchronous generator", "href": "https://docs.python.org/3/glossary.html#term-asynchronous-generator"}, {"text": "asynchronous iterable", "href": "https://docs.python.org/3/glossary.html#term-asynchronous-iterable"}, {"text": "awaitable objects", "href": "https://docs.python.org/3/glossary.html#term-awaitable"}, {"text": "Future", "href": "https://docs.python.org/3/library/asyncio-future.html#asyncio.Future"}, {"text": "aio-libs", "href": "https://github.com/aio-libs"}, {"text": "asyncio", "href": "https://docs.python.org/3/library/asyncio.html#module-asyncio"}, {"text": "enable asyncio support in Scrapy", "href": "asyncio.html"}, {"text": "wrap them", "href": "asyncio.html#asyncio-await-dfd"}, {"text": "spider_opened", "href": "signals.html#std-signal-spider_opened"}, {"text": "the screenshot pipeline example", "href": "item-pipeline.html#screenshotpipeline"}, {"text": "process_spider_output()", "href": "spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"}, {"text": "spider middleware", "href": "spider-middleware.html#topics-spider-middleware"}, {"text": "list of active spider middlewares", "href": "spider-middleware.html#topics-spider-middleware-setting"}, {"text": "coroutine methods", "href": "https://docs.python.org/3/reference/compound_stmts.html#async"}, {"text": "asynchronous generator", "href": "https://docs.python.org/3/glossary.html#term-asynchronous-generator"}, {"text": "subclassing", "href": "https://docs.python.org/3/tutorial/classes.html#tut-inheritance"}, {"text": "asynchronous generator", "href": "https://docs.python.org/3/glossary.html#term-asynchronous-generator"}], "timestamp": "2023-10-12T21:19:57.809374", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/jobs.html", "content": {"sections": [], "paragraphs": ["<p>Sometimes, for big sites, it’s desirable to pause crawls and be able to resume\nthem later.</p>", "<p>Scrapy supports this functionality out of the box by providing the following\nfacilities:</p>", "<p>a scheduler that persists scheduled requests on disk</p>", "<p>a duplicates filter that persists visited requests on disk</p>", "<p>an extension that keeps some spider state (key/value pairs) persistent\nbetween batches</p>", "<p>To enable persistence support you just need to define a <em>job directory</em> through\nthe <code class=\"docutils literal notranslate\"><span class=\"pre\">JOBDIR</span></code> setting. This directory will be for storing all required data to\nkeep the state of a single job (i.e. a spider run).  It’s important to note that\nthis directory must not be shared by different spiders, or even different\njobs/runs of the same spider, as it’s meant to be used for storing the state of\na <em>single</em> job.</p>", "<p>To start a spider with persistence support enabled, run it like this:</p>", "<p>Then, you can stop the spider safely at any time (by pressing Ctrl-C or sending\na signal), and resume it later by issuing the same command:</p>", "<p>Sometimes you’ll want to keep some persistent spider state between pause/resume\nbatches. You can use the <code class=\"docutils literal notranslate\"><span class=\"pre\">spider.state</span></code> attribute for that, which should be a\ndict. There’s a built-in extension that takes care of serializing, storing and\nloading that attribute from the job directory, when the spider starts and\nstops.</p>", "<p>Here’s an example of a callback that uses the spider state (other spider code\nis omitted for brevity):</p>", "<p>There are a few things to keep in mind if you want to be able to use the Scrapy\npersistence support:</p>", "<p>Cookies may expire. So, if you don’t resume your spider quickly the requests\nscheduled may no longer work. This won’t be an issue if your spider doesn’t rely\non cookies.</p>", "<p>For persistence to work, <code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Request</span></code> objects must be\nserializable with <a class=\"reference external\" href=\"https://docs.python.org/3/library/pickle.html#module-pickle\" title=\"(in Python v3.11)\"><code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">pickle</span></code></a>, except for the <code class=\"docutils literal notranslate\"><span class=\"pre\">callback</span></code> and <code class=\"docutils literal notranslate\"><span class=\"pre\">errback</span></code>\nvalues passed to their <code class=\"docutils literal notranslate\"><span class=\"pre\">__init__</span></code> method, which must be methods of the\nrunning <a class=\"reference internal\" href=\"spiders.html#scrapy.Spider\" title=\"scrapy.Spider\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Spider</span></code></a> class.</p>", "<p>If you wish to log the requests that couldn’t be serialized, you can set the\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-SCHEDULER_DEBUG\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">SCHEDULER_DEBUG</span></code></a> setting to <code class=\"docutils literal notranslate\"><span class=\"pre\">True</span></code> in the project’s settings page.\nIt is <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code> by default.</p>"]}, "code_blocks": ["scrapy crawl somespider -s JOBDIR=crawls/somespider-1\n</pre>", "scrapy crawl somespider -s JOBDIR=crawls/somespider-1\n</pre>", "def parse_item(self, response):\n    # parse item here\n    self.state[\"items_count\"] = self.state.get(\"items_count\", 0) + 1\n</pre>"], "links": [{"text": "pickle", "href": "https://docs.python.org/3/library/pickle.html#module-pickle"}, {"text": "Spider", "href": "spiders.html#scrapy.Spider"}, {"text": "SCHEDULER_DEBUG", "href": "settings.html#std-setting-SCHEDULER_DEBUG"}], "timestamp": "2023-10-12T21:20:02.326176", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/benchmarking.html", "content": {"sections": [], "paragraphs": ["<p>Scrapy comes with a simple benchmarking suite that spawns a local HTTP server\nand crawls it at the maximum possible speed. The goal of this benchmarking is\nto get an idea of how Scrapy performs in your hardware, in order to have a\ncommon baseline for comparisons. It uses a simple spider that does nothing and\njust follows links.</p>", "<p>To run it use:</p>", "<p>You should see an output like this:</p>", "<p>That tells you that Scrapy is able to crawl about 3000 pages per minute in the\nhardware where you run it. Note that this is a very simple spider intended to\nfollow links, any custom spider you write will probably do more stuff which\nresults in slower crawl rates. How slower depends on how much your spider does\nand how well it’s written.</p>", "<p>Use <a class=\"reference external\" href=\"https://github.com/scrapy/scrapy-bench\">scrapy-bench</a> for more complex benchmarking.</p>"]}, "code_blocks": ["scrapy bench\n</pre>", "2016-12-16 21:18:48 [scrapy.utils.log] INFO: Scrapy 1.2.2 started (bot: quotesbot)\n2016-12-16 21:18:48 [scrapy.utils.log] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 10, 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['quotesbot.spiders'], 'LOGSTATS_INTERVAL': 1, 'BOT_NAME': 'quotesbot', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'quotesbot.spiders'}\n2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.closespider.CloseSpider',\n 'scrapy.extensions.logstats.LogStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.corestats.CoreStats']\n2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2016-12-16 21:18:49 [scrapy.core.engine] INFO: Spider opened\n2016-12-16 21:18:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-12-16 21:18:50 [scrapy.extensions.logstats] INFO: Crawled 70 pages (at 4200 pages/min), scraped 0 items (at 0 items/min)\n2016-12-16 21:18:51 [scrapy.extensions.logstats] INFO: Crawled 134 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)\n2016-12-16 21:18:52 [scrapy.extensions.logstats] INFO: Crawled 198 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)\n2016-12-16 21:18:53 [scrapy.extensions.logstats] INFO: Crawled 254 pages (at 3360 pages/min), scraped 0 items (at 0 items/min)\n2016-12-16 21:18:54 [scrapy.extensions.logstats] INFO: Crawled 302 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)\n2016-12-16 21:18:55 [scrapy.extensions.logstats] INFO: Crawled 358 pages (at 3360 pages/min), scraped 0 items (at 0 items/min)\n2016-12-16 21:18:56 [scrapy.extensions.logstats] INFO: Crawled 406 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)\n2016-12-16 21:18:57 [scrapy.extensions.logstats] INFO: Crawled 438 pages (at 1920 pages/min), scraped 0 items (at 0 items/min)\n2016-12-16 21:18:58 [scrapy.extensions.logstats] INFO: Crawled 470 pages (at 1920 pages/min), scraped 0 items (at 0 items/min)\n2016-12-16 21:18:59 [scrapy.core.engine] INFO: Closing spider (closespider_timeout)\n2016-12-16 21:18:59 [scrapy.extensions.logstats] INFO: Crawled 518 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)\n2016-12-16 21:19:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 229995,\n 'downloader/request_count': 534,\n 'downloader/request_method_count/GET': 534,\n 'downloader/response_bytes': 1565504,\n 'downloader/response_count': 534,\n 'downloader/response_status_count/200': 534,\n 'finish_reason': 'closespider_timeout',\n 'finish_time': datetime.datetime(2016, 12, 16, 16, 19, 0, 647725),\n 'log_count/INFO': 17,\n 'request_depth_max': 19,\n 'response_received_count': 534,\n 'scheduler/dequeued': 533,\n 'scheduler/dequeued/memory': 533,\n 'scheduler/enqueued': 10661,\n 'scheduler/enqueued/memory': 10661,\n 'start_time': datetime.datetime(2016, 12, 16, 16, 18, 49, 799869)}\n2016-12-16 21:19:00 [scrapy.core.engine] INFO: Spider closed (closespider_timeout)\n</pre>"], "links": [{"text": "scrapy-bench", "href": "https://github.com/scrapy/scrapy-bench"}], "timestamp": "2023-10-12T21:20:06.559050", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/autothrottle.html", "content": {"sections": [], "paragraphs": ["<p>This is an extension for automatically throttling crawling speed based on load\nof both the Scrapy server and the website you are crawling.</p>", "<p>be nicer to sites instead of using default download delay of zero</p>", "<p>automatically adjust Scrapy to the optimum crawling speed, so the user\ndoesn’t have to tune the download delays to find the optimum one.\nThe user only needs to specify the maximum concurrent requests\nit allows, and the extension does the rest.</p>", "<p>AutoThrottle extension adjusts download delays dynamically to make spider send\n<a class=\"hoverxref tooltip reference internal\" href=\"#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code></a> concurrent requests on average\nto each remote website.</p>", "<p>It uses download latency to compute the delays. The main idea is the\nfollowing: if a server needs <code class=\"docutils literal notranslate\"><span class=\"pre\">latency</span></code> seconds to respond, a client\nshould send a request each <code class=\"docutils literal notranslate\"><span class=\"pre\">latency/N</span></code> seconds to have <code class=\"docutils literal notranslate\"><span class=\"pre\">N</span></code> requests\nprocessed in parallel.</p>", "<p>Instead of adjusting the delays one can just set a small fixed\ndownload delay and impose hard limits on concurrency using\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> or\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-CONCURRENT_REQUESTS_PER_IP\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">CONCURRENT_REQUESTS_PER_IP</span></code></a> options. It will provide a similar\neffect, but there are some important differences:</p>", "<p>because the download delay is small there will be occasional bursts\nof requests;</p>", "<p>often non-200 (error) responses can be returned faster than regular\nresponses, so with a small download delay and a hard concurrency limit\ncrawler will be sending requests to server faster when server starts to\nreturn errors. But this is an opposite of what crawler should do - in case\nof errors it makes more sense to slow down: these errors may be caused by\nthe high request rate.</p>", "<p>AutoThrottle doesn’t have these issues.</p>", "<p>AutoThrottle algorithm adjusts download delays based on the following rules:</p>", "<p>spiders always start with a download delay of\n<a class=\"hoverxref tooltip reference internal\" href=\"#std-setting-AUTOTHROTTLE_START_DELAY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">AUTOTHROTTLE_START_DELAY</span></code></a>;</p>", "<p>when a response is received, the target download delay is calculated as\n<code class=\"docutils literal notranslate\"><span class=\"pre\">latency</span> <span class=\"pre\">/</span> <span class=\"pre\">N</span></code> where <code class=\"docutils literal notranslate\"><span class=\"pre\">latency</span></code> is a latency of the response,\nand <code class=\"docutils literal notranslate\"><span class=\"pre\">N</span></code> is <a class=\"hoverxref tooltip reference internal\" href=\"#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code></a>.</p>", "<p>download delay for next requests is set to the average of previous\ndownload delay and the target download delay;</p>", "<p>latencies of non-200 responses are not allowed to decrease the delay;</p>", "<p>download delay can’t become less than <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-DOWNLOAD_DELAY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOAD_DELAY</span></code></a> or greater\nthan <a class=\"hoverxref tooltip reference internal\" href=\"#std-setting-AUTOTHROTTLE_MAX_DELAY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">AUTOTHROTTLE_MAX_DELAY</span></code></a></p>", "<p class=\"admonition-title\">Note</p>", "<p>The AutoThrottle extension honours the standard Scrapy settings for\nconcurrency and delay. This means that it will respect\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> and\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-CONCURRENT_REQUESTS_PER_IP\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">CONCURRENT_REQUESTS_PER_IP</span></code></a> options and\nnever set a download delay lower than <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-DOWNLOAD_DELAY\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">DOWNLOAD_DELAY</span></code></a>.</p>", "<p id=\"download-latency\">In Scrapy, the download latency is measured as the time elapsed between\nestablishing the TCP connection and receiving the HTTP headers.</p>", "<p>Note that these latencies are very hard to measure accurately in a cooperative\nmultitasking environment because Scrapy may be busy processing a spider\ncallback, for example, and unable to attend downloads. However, these latencies\nshould still give a reasonable estimate of how busy Scrapy (and ultimately, the\nserver) is, and this extension builds on that premise.</p>", "<p>The settings used to control the AutoThrottle extension are:</p>", "<p>For more information see <a class=\"hoverxref tooltip reference internal\" href=\"#autothrottle-algorithm\"><span class=\"std std-ref\">How it works</span></a>.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code></p>", "<p>Enables the AutoThrottle extension.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">5.0</span></code></p>", "<p>The initial download delay (in seconds).</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">60.0</span></code></p>", "<p>The maximum download delay (in seconds) to be set in case of high latencies.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">1.0</span></code></p>", "<p>Average number of requests Scrapy should be sending in parallel to remote\nwebsites.</p>", "<p>By default, AutoThrottle adjusts the delay to send a single\nconcurrent request to each of the remote websites. Set this option to\na higher value (e.g. <code class=\"docutils literal notranslate\"><span class=\"pre\">2.0</span></code>) to increase the throughput and the load on remote\nservers. A lower <code class=\"docutils literal notranslate\"><span class=\"pre\">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code> value\n(e.g. <code class=\"docutils literal notranslate\"><span class=\"pre\">0.5</span></code>) makes the crawler more conservative and polite.</p>", "<p>Note that <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a>\nand <a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-CONCURRENT_REQUESTS_PER_IP\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">CONCURRENT_REQUESTS_PER_IP</span></code></a> options are still respected\nwhen AutoThrottle extension is enabled. This means that if\n<code class=\"docutils literal notranslate\"><span class=\"pre\">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code> is set to a value higher than\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> or\n<a class=\"hoverxref tooltip reference internal\" href=\"settings.html#std-setting-CONCURRENT_REQUESTS_PER_IP\"><code class=\"xref std std-setting docutils literal notranslate\"><span class=\"pre\">CONCURRENT_REQUESTS_PER_IP</span></code></a>, the crawler won’t reach this number\nof concurrent requests.</p>", "<p>At every given time point Scrapy can be sending more or less concurrent\nrequests than <code class=\"docutils literal notranslate\"><span class=\"pre\">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code>; it is a suggested\nvalue the crawler tries to approach, not a hard limit.</p>", "<p>Default: <code class=\"docutils literal notranslate\"><span class=\"pre\">False</span></code></p>", "<p>Enable AutoThrottle debug mode which will display stats on every response\nreceived, so you can see how the throttling parameters are being adjusted in\nreal time.</p>"]}, "code_blocks": [], "links": [{"text": "CONCURRENT_REQUESTS_PER_DOMAIN", "href": "settings.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"}, {"text": "CONCURRENT_REQUESTS_PER_IP", "href": "settings.html#std-setting-CONCURRENT_REQUESTS_PER_IP"}, {"text": "DOWNLOAD_DELAY", "href": "settings.html#std-setting-DOWNLOAD_DELAY"}, {"text": "CONCURRENT_REQUESTS_PER_DOMAIN", "href": "settings.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"}, {"text": "CONCURRENT_REQUESTS_PER_IP", "href": "settings.html#std-setting-CONCURRENT_REQUESTS_PER_IP"}, {"text": "DOWNLOAD_DELAY", "href": "settings.html#std-setting-DOWNLOAD_DELAY"}, {"text": "CONCURRENT_REQUESTS_PER_DOMAIN", "href": "settings.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"}, {"text": "CONCURRENT_REQUESTS_PER_IP", "href": "settings.html#std-setting-CONCURRENT_REQUESTS_PER_IP"}, {"text": "DOWNLOAD_DELAY", "href": "settings.html#std-setting-DOWNLOAD_DELAY"}, {"text": "CONCURRENT_REQUESTS_PER_DOMAIN", "href": "settings.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"}, {"text": "CONCURRENT_REQUESTS_PER_IP", "href": "settings.html#std-setting-CONCURRENT_REQUESTS_PER_IP"}, {"text": "CONCURRENT_REQUESTS_PER_DOMAIN", "href": "settings.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN"}, {"text": "CONCURRENT_REQUESTS_PER_IP", "href": "settings.html#std-setting-CONCURRENT_REQUESTS_PER_IP"}], "timestamp": "2023-10-12T21:20:10.790547", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/versioning.html", "content": {"sections": [], "paragraphs": ["<p>There are 3 numbers in a Scrapy version: <em>A.B.C</em></p>", "<p><em>A</em> is the major version. This will rarely change and will signify very\nlarge changes.</p>", "<p><em>B</em> is the release number. This will include many changes including features\nand things that possibly break backward compatibility, although we strive to\nkeep these cases at a minimum.</p>", "<p><em>C</em> is the bugfix release number.</p>", "<p>Backward-incompatibilities are explicitly mentioned in the <a class=\"hoverxref tooltip reference internal\" href=\"news.html#news\"><span class=\"std std-ref\">release notes</span></a>,\nand may require special attention before upgrading.</p>", "<p>Development releases do not follow 3-numbers version and are generally\nreleased as <code class=\"docutils literal notranslate\"><span class=\"pre\">dev</span></code> suffixed versions, e.g. <code class=\"docutils literal notranslate\"><span class=\"pre\">1.3dev</span></code>.</p>", "<p class=\"admonition-title\">Note</p>", "<p>With Scrapy 0.* series, Scrapy used <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Software_versioning#Odd-numbered_versions_for_development_releases\">odd-numbered versions for development releases</a>.\nThis is not the case anymore from Scrapy 1.0 onwards.</p>", "<p>Starting with Scrapy 1.0, all releases should be considered production-ready.</p>", "<p>For example:</p>", "<p><em>1.1.1</em> is the first bugfix release of the <em>1.1</em> series (safe to use in\nproduction)</p>", "<p>API stability was one of the major goals for the <em>1.0</em> release.</p>", "<p>Methods or functions that start with a single dash (<code class=\"docutils literal notranslate\"><span class=\"pre\">_</span></code>) are private and\nshould never be relied as stable.</p>", "<p>Also, keep in mind that stable doesn’t mean complete: stable APIs could grow\nnew methods or functionality but the existing methods should keep working the\nsame way.</p>", "<p>We aim to maintain support for deprecated Scrapy features for at least 1 year.</p>", "<p>For example, if a feature is deprecated in a Scrapy version released on\nJune 15th 2020, that feature should continue to work in versions released on\nJune 14th 2021 or before that.</p>", "<p>Any new Scrapy release after a year <em>may</em> remove support for that deprecated\nfeature.</p>", "<p>All deprecated features removed in a Scrapy release are explicitly mentioned in\nthe <a class=\"hoverxref tooltip reference internal\" href=\"news.html#news\"><span class=\"std std-ref\">release notes</span></a>.</p>"]}, "code_blocks": [], "links": [{"text": "release notes", "href": "news.html#news"}, {"text": "odd-numbered versions for development releases", "href": "https://en.wikipedia.org/wiki/Software_versioning#Odd-numbered_versions_for_development_releases"}, {"text": "release notes", "href": "news.html#news"}], "timestamp": "2023-10-12T21:20:12.913605", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/topics/djangoitem.html", "content": {"sections": [], "paragraphs": ["<p>DjangoItem has been moved into a separate project.</p>", "<p>It is hosted at:</p>"]}, "code_blocks": [], "links": [{"text": "https://github.com/scrapy-plugins/scrapy-djangoitem", "href": "https://github.com/scrapy-plugins/scrapy-djangoitem"}], "timestamp": "2023-10-12T21:20:15.138446", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/_modules/scrapy/statscollectors.html", "content": {"sections": [], "paragraphs": []}, "code_blocks": ["\n\"\"\"\nScrapy extension for collecting scraping stats\n\"\"\"\nimport logging\nimport pprint\nfrom typing import TYPE_CHECKING, Any, Dict, Optional\n\nfrom scrapy import Spider\n\nif TYPE_CHECKING:\n    from scrapy.crawler import Crawler\n\nlogger = logging.getLogger(__name__)\n\n\nStatsT = Dict[str, Any]\n\n\n[docs]class StatsCollector:\n    def __init__(self, crawler: \"Crawler\"):\n        self._dump: bool = crawler.settings.getbool(\"STATS_DUMP\")\n        self._stats: StatsT = {}\n\n[docs]    def get_value(\n        self, key: str, default: Any = None, spider: Optional[Spider] = None\n    ) -> Any:\n        return self._stats.get(key, default)\n\n[docs]    def get_stats(self, spider: Optional[Spider] = None) -> StatsT:\n        return self._stats\n\n[docs]    def set_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        self._stats[key] = value\n\n[docs]    def set_stats(self, stats: StatsT, spider: Optional[Spider] = None) -> None:\n        self._stats = stats\n\n[docs]    def inc_value(\n        self, key: str, count: int = 1, start: int = 0, spider: Optional[Spider] = None\n    ) -> None:\n        d = self._stats\n        d[key] = d.setdefault(key, start) + count\n\n[docs]    def max_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        self._stats[key] = max(self._stats.setdefault(key, value), value)\n\n[docs]    def min_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        self._stats[key] = min(self._stats.setdefault(key, value), value)\n\n[docs]    def clear_stats(self, spider: Optional[Spider] = None) -> None:\n        self._stats.clear()\n\n[docs]    def open_spider(self, spider: Spider) -> None:\n        pass\n\n[docs]    def close_spider(self, spider: Spider, reason: str) -> None:\n        if self._dump:\n            logger.info(\n                \"Dumping Scrapy stats:\\n\" + pprint.pformat(self._stats),\n                extra={\"spider\": spider},\n            )\n        self._persist_stats(self._stats, spider)\n\n    def _persist_stats(self, stats: StatsT, spider: Spider) -> None:\n        pass\n\n\n[docs]class MemoryStatsCollector(StatsCollector):\n    def __init__(self, crawler: \"Crawler\"):\n        super().__init__(crawler)\n        self.spider_stats: Dict[str, StatsT] = {}\n\n    def _persist_stats(self, stats: StatsT, spider: Spider) -> None:\n        self.spider_stats[spider.name] = stats\n\n\n[docs]class DummyStatsCollector(StatsCollector):\n    def get_value(\n        self, key: str, default: Any = None, spider: Optional[Spider] = None\n    ) -> Any:\n        return default\n\n    def set_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        pass\n\n    def set_stats(self, stats: StatsT, spider: Optional[Spider] = None) -> None:\n        pass\n\n    def inc_value(\n        self, key: str, count: int = 1, start: int = 0, spider: Optional[Spider] = None\n    ) -> None:\n        pass\n\n    def max_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        pass\n\n    def min_value(self, key: str, value: Any, spider: Optional[Spider] = None) -> None:\n        pass\n</pre>"], "links": [{"text": "[docs]", "href": "../../topics/api.html#scrapy.statscollectors.StatsCollector"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.statscollectors.StatsCollector.get_value"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.statscollectors.StatsCollector.get_stats"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.statscollectors.StatsCollector.set_value"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.statscollectors.StatsCollector.set_stats"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.statscollectors.StatsCollector.inc_value"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.statscollectors.StatsCollector.max_value"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.statscollectors.StatsCollector.min_value"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.statscollectors.StatsCollector.clear_stats"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.statscollectors.StatsCollector.open_spider"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.statscollectors.StatsCollector.close_spider"}, {"text": "[docs]", "href": "../../topics/stats.html#scrapy.statscollectors.MemoryStatsCollector"}, {"text": "[docs]", "href": "../../topics/stats.html#scrapy.statscollectors.DummyStatsCollector"}], "timestamp": "2023-10-12T21:20:18.419182", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/_modules/scrapy/signalmanager.html", "content": {"sections": [], "paragraphs": []}, "code_blocks": ["\nfrom typing import Any, List, Tuple\n\nfrom pydispatch import dispatcher\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy.utils import signal as _signal\n\n\n[docs]class SignalManager:\n    def __init__(self, sender: Any = dispatcher.Anonymous):\n        self.sender: Any = sender\n\n[docs]    def connect(self, receiver: Any, signal: Any, **kwargs: Any) -> None:\n        \"\"\"\n        Connect a receiver function to a signal.\n\n        The signal can be any object, although Scrapy comes with some\n        predefined signals that are documented in the :ref:`topics-signals`\n        section.\n\n        :param receiver: the function to be connected\n        :type receiver: collections.abc.Callable\n\n        :param signal: the signal to connect to\n        :type signal: object\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        dispatcher.connect(receiver, signal, **kwargs)\n\n[docs]    def disconnect(self, receiver: Any, signal: Any, **kwargs: Any) -> None:\n        \"\"\"\n        Disconnect a receiver function from a signal. This has the\n        opposite effect of the :meth:`connect` method, and the arguments\n        are the same.\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        dispatcher.disconnect(receiver, signal, **kwargs)\n\n[docs]    def send_catch_log(self, signal: Any, **kwargs: Any) -> List[Tuple[Any, Any]]:\n        \"\"\"\n        Send a signal, catch exceptions and log them.\n\n        The keyword arguments are passed to the signal handlers (connected\n        through the :meth:`connect` method).\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        return _signal.send_catch_log(signal, **kwargs)\n\n[docs]    def send_catch_log_deferred(self, signal: Any, **kwargs: Any) -> Deferred:\n        \"\"\"\n        Like :meth:`send_catch_log` but supports returning\n        :class:`~twisted.internet.defer.Deferred` objects from signal handlers.\n\n        Returns a Deferred that gets fired once all signal handlers\n        deferreds were fired. Send a signal, catch exceptions and log them.\n\n        The keyword arguments are passed to the signal handlers (connected\n        through the :meth:`connect` method).\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        return _signal.send_catch_log_deferred(signal, **kwargs)\n\n[docs]    def disconnect_all(self, signal: Any, **kwargs: Any) -> None:\n        \"\"\"\n        Disconnect all receivers from the given signal.\n\n        :param signal: the signal to disconnect from\n        :type signal: object\n        \"\"\"\n        kwargs.setdefault(\"sender\", self.sender)\n        _signal.disconnect_all(signal, **kwargs)\n</pre>"], "links": [{"text": "[docs]", "href": "../../topics/api.html#scrapy.signalmanager.SignalManager"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.signalmanager.SignalManager.connect"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.signalmanager.SignalManager.disconnect"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.signalmanager.SignalManager.send_catch_log"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.signalmanager.SignalManager.send_catch_log_deferred"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.signalmanager.SignalManager.disconnect_all"}], "timestamp": "2023-10-12T21:20:22.901725", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/_modules/scrapy/exporters.html", "content": {"sections": [], "paragraphs": []}, "code_blocks": ["\n\"\"\"\nItem Exporters are used to export/serialize items into different formats.\n\"\"\"\n\nimport csv\nimport io\nimport marshal\nimport pickle\nimport pprint\nfrom collections.abc import Mapping\nfrom xml.sax.saxutils import XMLGenerator\n\nfrom itemadapter import ItemAdapter, is_item\n\nfrom scrapy.item import Item\nfrom scrapy.utils.python import is_listlike, to_bytes, to_unicode\nfrom scrapy.utils.serialize import ScrapyJSONEncoder\n\n__all__ = [\n    \"BaseItemExporter\",\n    \"PprintItemExporter\",\n    \"PickleItemExporter\",\n    \"CsvItemExporter\",\n    \"XmlItemExporter\",\n    \"JsonLinesItemExporter\",\n    \"JsonItemExporter\",\n    \"MarshalItemExporter\",\n]\n\n\n[docs]class BaseItemExporter:\n    def __init__(self, *, dont_fail=False, **kwargs):\n        self._kwargs = kwargs\n        self._configure(kwargs, dont_fail=dont_fail)\n\n    def _configure(self, options, dont_fail=False):\n        \"\"\"Configure the exporter by popping options from the ``options`` dict.\n        If dont_fail is set, it won't raise an exception on unexpected options\n        (useful for using with keyword arguments in subclasses ``__init__`` methods)\n        \"\"\"\n        self.encoding = options.pop(\"encoding\", None)\n        self.fields_to_export = options.pop(\"fields_to_export\", None)\n        self.export_empty_fields = options.pop(\"export_empty_fields\", False)\n        self.indent = options.pop(\"indent\", None)\n        if not dont_fail and options:\n            raise TypeError(f\"Unexpected options: {', '.join(options.keys())}\")\n\n[docs]    def export_item(self, item):\n        raise NotImplementedError\n\n[docs]    def serialize_field(self, field, name, value):\n        serializer = field.get(\"serializer\", lambda x: x)\n        return serializer(value)\n\n[docs]    def start_exporting(self):\n        pass\n\n[docs]    def finish_exporting(self):\n        pass\n\n    def _get_serialized_fields(self, item, default_value=None, include_empty=None):\n        \"\"\"Return the fields to export as an iterable of tuples\n        (name, serialized_value)\n        \"\"\"\n        item = ItemAdapter(item)\n\n        if include_empty is None:\n            include_empty = self.export_empty_fields\n\n        if self.fields_to_export is None:\n            if include_empty:\n                field_iter = item.field_names()\n            else:\n                field_iter = item.keys()\n        elif isinstance(self.fields_to_export, Mapping):\n            if include_empty:\n                field_iter = self.fields_to_export.items()\n            else:\n                field_iter = (\n                    (x, y) for x, y in self.fields_to_export.items() if x in item\n                )\n        else:\n            if include_empty:\n                field_iter = self.fields_to_export\n            else:\n                field_iter = (x for x in self.fields_to_export if x in item)\n\n        for field_name in field_iter:\n            if isinstance(field_name, str):\n                item_field, output_field = field_name, field_name\n            else:\n                item_field, output_field = field_name\n            if item_field in item:\n                field_meta = item.get_field_meta(item_field)\n                value = self.serialize_field(field_meta, output_field, item[item_field])\n            else:\n                value = default_value\n\n            yield output_field, value\n\n\n[docs]class JsonLinesItemExporter(BaseItemExporter):\n    def __init__(self, file, **kwargs):\n        super().__init__(dont_fail=True, **kwargs)\n        self.file = file\n        self._kwargs.setdefault(\"ensure_ascii\", not self.encoding)\n        self.encoder = ScrapyJSONEncoder(**self._kwargs)\n\n    def export_item(self, item):\n        itemdict = dict(self._get_serialized_fields(item))\n        data = self.encoder.encode(itemdict) + \"\\n\"\n        self.file.write(to_bytes(data, self.encoding))\n\n\n[docs]class JsonItemExporter(BaseItemExporter):\n    def __init__(self, file, **kwargs):\n        super().__init__(dont_fail=True, **kwargs)\n        self.file = file\n        # there is a small difference between the behaviour or JsonItemExporter.indent\n        # and ScrapyJSONEncoder.indent. ScrapyJSONEncoder.indent=None is needed to prevent\n        # the addition of newlines everywhere\n        json_indent = (\n            self.indent if self.indent is not None and self.indent > 0 else None\n        )\n        self._kwargs.setdefault(\"indent\", json_indent)\n        self._kwargs.setdefault(\"ensure_ascii\", not self.encoding)\n        self.encoder = ScrapyJSONEncoder(**self._kwargs)\n        self.first_item = True\n\n    def _beautify_newline(self):\n        if self.indent is not None:\n            self.file.write(b\"\\n\")\n\n    def _add_comma_after_first(self):\n        if self.first_item:\n            self.first_item = False\n        else:\n            self.file.write(b\",\")\n            self._beautify_newline()\n\n    def start_exporting(self):\n        self.file.write(b\"[\")\n        self._beautify_newline()\n\n    def finish_exporting(self):\n        self._beautify_newline()\n        self.file.write(b\"]\")\n\n    def export_item(self, item):\n        itemdict = dict(self._get_serialized_fields(item))\n        data = to_bytes(self.encoder.encode(itemdict), self.encoding)\n        self._add_comma_after_first()\n        self.file.write(data)\n\n\n[docs]class XmlItemExporter(BaseItemExporter):\n    def __init__(self, file, **kwargs):\n        self.item_element = kwargs.pop(\"item_element\", \"item\")\n        self.root_element = kwargs.pop(\"root_element\", \"items\")\n        super().__init__(**kwargs)\n        if not self.encoding:\n            self.encoding = \"utf-8\"\n        self.xg = XMLGenerator(file, encoding=self.encoding)\n\n    def _beautify_newline(self, new_item=False):\n        if self.indent is not None and (self.indent > 0 or new_item):\n            self.xg.characters(\"\\n\")\n\n    def _beautify_indent(self, depth=1):\n        if self.indent:\n            self.xg.characters(\" \" * self.indent * depth)\n\n    def start_exporting(self):\n        self.xg.startDocument()\n        self.xg.startElement(self.root_element, {})\n        self._beautify_newline(new_item=True)\n\n    def export_item(self, item):\n        self._beautify_indent(depth=1)\n        self.xg.startElement(self.item_element, {})\n        self._beautify_newline()\n        for name, value in self._get_serialized_fields(item, default_value=\"\"):\n            self._export_xml_field(name, value, depth=2)\n        self._beautify_indent(depth=1)\n        self.xg.endElement(self.item_element)\n        self._beautify_newline(new_item=True)\n\n    def finish_exporting(self):\n        self.xg.endElement(self.root_element)\n        self.xg.endDocument()\n\n    def _export_xml_field(self, name, serialized_value, depth):\n        self._beautify_indent(depth=depth)\n        self.xg.startElement(name, {})\n        if hasattr(serialized_value, \"items\"):\n            self._beautify_newline()\n            for subname, value in serialized_value.items():\n                self._export_xml_field(subname, value, depth=depth + 1)\n            self._beautify_indent(depth=depth)\n        elif is_listlike(serialized_value):\n            self._beautify_newline()\n            for value in serialized_value:\n                self._export_xml_field(\"value\", value, depth=depth + 1)\n            self._beautify_indent(depth=depth)\n        elif isinstance(serialized_value, str):\n            self.xg.characters(serialized_value)\n        else:\n            self.xg.characters(str(serialized_value))\n        self.xg.endElement(name)\n        self._beautify_newline()\n\n\n[docs]class CsvItemExporter(BaseItemExporter):\n    def __init__(\n        self,\n        file,\n        include_headers_line=True,\n        join_multivalued=\",\",\n        errors=None,\n        **kwargs,\n    ):\n        super().__init__(dont_fail=True, **kwargs)\n        if not self.encoding:\n            self.encoding = \"utf-8\"\n        self.include_headers_line = include_headers_line\n        self.stream = io.TextIOWrapper(\n            file,\n            line_buffering=False,\n            write_through=True,\n            encoding=self.encoding,\n            newline=\"\",  # Windows needs this https://github.com/scrapy/scrapy/issues/3034\n            errors=errors,\n        )\n        self.csv_writer = csv.writer(self.stream, **self._kwargs)\n        self._headers_not_written = True\n        self._join_multivalued = join_multivalued\n\n    def serialize_field(self, field, name, value):\n        serializer = field.get(\"serializer\", self._join_if_needed)\n        return serializer(value)\n\n    def _join_if_needed(self, value):\n        if isinstance(value, (list, tuple)):\n            try:\n                return self._join_multivalued.join(value)\n            except TypeError:  # list in value may not contain strings\n                pass\n        return value\n\n    def export_item(self, item):\n        if self._headers_not_written:\n            self._headers_not_written = False\n            self._write_headers_and_set_fields_to_export(item)\n\n        fields = self._get_serialized_fields(item, default_value=\"\", include_empty=True)\n        values = list(self._build_row(x for _, x in fields))\n        self.csv_writer.writerow(values)\n\n    def finish_exporting(self):\n        self.stream.detach()  # Avoid closing the wrapped file.\n\n    def _build_row(self, values):\n        for s in values:\n            try:\n                yield to_unicode(s, self.encoding)\n            except TypeError:\n                yield s\n\n    def _write_headers_and_set_fields_to_export(self, item):\n        if self.include_headers_line:\n            if not self.fields_to_export:\n                # use declared field names, or keys if the item is a dict\n                self.fields_to_export = ItemAdapter(item).field_names()\n            if isinstance(self.fields_to_export, Mapping):\n                fields = self.fields_to_export.values()\n            else:\n                fields = self.fields_to_export\n            row = list(self._build_row(fields))\n            self.csv_writer.writerow(row)\n\n\n[docs]class PickleItemExporter(BaseItemExporter):\n    def __init__(self, file, protocol=4, **kwargs):\n        super().__init__(**kwargs)\n        self.file = file\n        self.protocol = protocol\n\n    def export_item(self, item):\n        d = dict(self._get_serialized_fields(item))\n        pickle.dump(d, self.file, self.protocol)\n\n\n[docs]class MarshalItemExporter(BaseItemExporter):\n    \"\"\"Exports items in a Python-specific binary format (see\n    :mod:`marshal`).\n\n    :param file: The file-like object to use for exporting the data. Its\n                 ``write`` method should accept :class:`bytes` (a disk file\n                 opened in binary mode, a :class:`~io.BytesIO` object, etc)\n    \"\"\"\n\n    def __init__(self, file, **kwargs):\n        super().__init__(**kwargs)\n        self.file = file\n\n    def export_item(self, item):\n        marshal.dump(dict(self._get_serialized_fields(item)), self.file)\n\n\n[docs]class PprintItemExporter(BaseItemExporter):\n    def __init__(self, file, **kwargs):\n        super().__init__(**kwargs)\n        self.file = file\n\n    def export_item(self, item):\n        itemdict = dict(self._get_serialized_fields(item))\n        self.file.write(to_bytes(pprint.pformat(itemdict) + \"\\n\"))\n\n\n[docs]class PythonItemExporter(BaseItemExporter):\n    \"\"\"This is a base class for item exporters that extends\n    :class:`BaseItemExporter` with support for nested items.\n\n    It serializes items to built-in Python types, so that any serialization\n    library (e.g. :mod:`json` or msgpack_) can be used on top of it.\n\n    .. _msgpack: https://pypi.org/project/msgpack/\n    \"\"\"\n\n    def _configure(self, options, dont_fail=False):\n        super()._configure(options, dont_fail)\n        if not self.encoding:\n            self.encoding = \"utf-8\"\n\n    def serialize_field(self, field, name, value):\n        serializer = field.get(\"serializer\", self._serialize_value)\n        return serializer(value)\n\n    def _serialize_value(self, value):\n        if isinstance(value, Item):\n            return self.export_item(value)\n        if is_item(value):\n            return dict(self._serialize_item(value))\n        if is_listlike(value):\n            return [self._serialize_value(v) for v in value]\n        if isinstance(value, (str, bytes)):\n            return to_unicode(value, encoding=self.encoding)\n        return value\n\n    def _serialize_item(self, item):\n        for key, value in ItemAdapter(item).items():\n            yield key, self._serialize_value(value)\n\n    def export_item(self, item):\n        result = dict(self._get_serialized_fields(item))\n        return result\n</pre>"], "links": [{"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.BaseItemExporter"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.BaseItemExporter.export_item"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.BaseItemExporter.serialize_field"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.BaseItemExporter.start_exporting"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.BaseItemExporter.finish_exporting"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.JsonLinesItemExporter"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.JsonItemExporter"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.XmlItemExporter"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.CsvItemExporter"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.PickleItemExporter"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.MarshalItemExporter"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.PprintItemExporter"}, {"text": "[docs]", "href": "../../topics/exporters.html#scrapy.exporters.PythonItemExporter"}], "timestamp": "2023-10-12T21:20:26.632923", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/_modules/scrapy/core/scheduler.html", "content": {"sections": [], "paragraphs": []}, "code_blocks": ["\nfrom __future__ import annotations\n\nimport json\nimport logging\nfrom abc import abstractmethod\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, Optional, Type, TypeVar, cast\n\nfrom twisted.internet.defer import Deferred\n\nfrom scrapy.crawler import Crawler\nfrom scrapy.dupefilters import BaseDupeFilter\nfrom scrapy.http.request import Request\nfrom scrapy.spiders import Spider\nfrom scrapy.statscollectors import StatsCollector\nfrom scrapy.utils.job import job_dir\nfrom scrapy.utils.misc import create_instance, load_object\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseSchedulerMeta(type):\n    \"\"\"\n    Metaclass to check scheduler classes against the necessary interface\n    \"\"\"\n\n    def __instancecheck__(cls, instance: Any) -> bool:\n        return cls.__subclasscheck__(type(instance))\n\n    def __subclasscheck__(cls, subclass: type) -> bool:\n        return (\n            hasattr(subclass, \"has_pending_requests\")\n            and callable(subclass.has_pending_requests)\n            and hasattr(subclass, \"enqueue_request\")\n            and callable(subclass.enqueue_request)\n            and hasattr(subclass, \"next_request\")\n            and callable(subclass.next_request)\n        )\n\n\n[docs]class BaseScheduler(metaclass=BaseSchedulerMeta):\n    \"\"\"\n    The scheduler component is responsible for storing requests received from\n    the engine, and feeding them back upon request (also to the engine).\n\n    The original sources of said requests are:\n\n    * Spider: ``start_requests`` method, requests created for URLs in the ``start_urls`` attribute, request callbacks\n    * Spider middleware: ``process_spider_output`` and ``process_spider_exception`` methods\n    * Downloader middleware: ``process_request``, ``process_response`` and ``process_exception`` methods\n\n    The order in which the scheduler returns its stored requests (via the ``next_request`` method)\n    plays a great part in determining the order in which those requests are downloaded.\n\n    The methods defined in this class constitute the minimal interface that the Scrapy engine will interact with.\n    \"\"\"\n\n[docs]    @classmethod\n    def from_crawler(cls, crawler: Crawler) -> Self:\n        \"\"\"\n        Factory method which receives the current :class:`~scrapy.crawler.Crawler` object as argument.\n        \"\"\"\n        return cls()\n\n[docs]    def open(self, spider: Spider) -> Optional[Deferred]:\n        \"\"\"\n        Called when the spider is opened by the engine. It receives the spider\n        instance as argument and it's useful to execute initialization code.\n\n        :param spider: the spider object for the current crawl\n        :type spider: :class:`~scrapy.spiders.Spider`\n        \"\"\"\n        pass\n\n[docs]    def close(self, reason: str) -> Optional[Deferred]:\n        \"\"\"\n        Called when the spider is closed by the engine. It receives the reason why the crawl\n        finished as argument and it's useful to execute cleaning code.\n\n        :param reason: a string which describes the reason why the spider was closed\n        :type reason: :class:`str`\n        \"\"\"\n        pass\n\n[docs]    @abstractmethod\n    def has_pending_requests(self) -> bool:\n        \"\"\"\n        ``True`` if the scheduler has enqueued requests, ``False`` otherwise\n        \"\"\"\n        raise NotImplementedError()\n\n[docs]    @abstractmethod\n    def enqueue_request(self, request: Request) -> bool:\n        \"\"\"\n        Process a request received by the engine.\n\n        Return ``True`` if the request is stored correctly, ``False`` otherwise.\n\n        If ``False``, the engine will fire a ``request_dropped`` signal, and\n        will not make further attempts to schedule the request at a later time.\n        For reference, the default Scrapy scheduler returns ``False`` when the\n        request is rejected by the dupefilter.\n        \"\"\"\n        raise NotImplementedError()\n\n[docs]    @abstractmethod\n    def next_request(self) -> Optional[Request]:\n        \"\"\"\n        Return the next :class:`~scrapy.http.Request` to be processed, or ``None``\n        to indicate that there are no requests to be considered ready at the moment.\n\n        Returning ``None`` implies that no request from the scheduler will be sent\n        to the downloader in the current reactor cycle. The engine will continue\n        calling ``next_request`` until ``has_pending_requests`` is ``False``.\n        \"\"\"\n        raise NotImplementedError()\n\n\nSchedulerTV = TypeVar(\"SchedulerTV\", bound=\"Scheduler\")\n\n\n[docs]class Scheduler(BaseScheduler):\n    \"\"\"\n    Default Scrapy scheduler. This implementation also handles duplication\n    filtering via the :setting:`dupefilter <DUPEFILTER_CLASS>`.\n\n    This scheduler stores requests into several priority queues (defined by the\n    :setting:`SCHEDULER_PRIORITY_QUEUE` setting). In turn, said priority queues\n    are backed by either memory or disk based queues (respectively defined by the\n    :setting:`SCHEDULER_MEMORY_QUEUE` and :setting:`SCHEDULER_DISK_QUEUE` settings).\n\n    Request prioritization is almost entirely delegated to the priority queue. The only\n    prioritization performed by this scheduler is using the disk-based queue if present\n    (i.e. if the :setting:`JOBDIR` setting is defined) and falling back to the memory-based\n    queue if a serialization error occurs. If the disk queue is not present, the memory one\n    is used directly.\n\n    :param dupefilter: An object responsible for checking and filtering duplicate requests.\n                       The value for the :setting:`DUPEFILTER_CLASS` setting is used by default.\n    :type dupefilter: :class:`scrapy.dupefilters.BaseDupeFilter` instance or similar:\n                      any class that implements the `BaseDupeFilter` interface\n\n    :param jobdir: The path of a directory to be used for persisting the crawl's state.\n                   The value for the :setting:`JOBDIR` setting is used by default.\n                   See :ref:`topics-jobs`.\n    :type jobdir: :class:`str` or ``None``\n\n    :param dqclass: A class to be used as persistent request queue.\n                    The value for the :setting:`SCHEDULER_DISK_QUEUE` setting is used by default.\n    :type dqclass: class\n\n    :param mqclass: A class to be used as non-persistent request queue.\n                    The value for the :setting:`SCHEDULER_MEMORY_QUEUE` setting is used by default.\n    :type mqclass: class\n\n    :param logunser: A boolean that indicates whether or not unserializable requests should be logged.\n                     The value for the :setting:`SCHEDULER_DEBUG` setting is used by default.\n    :type logunser: bool\n\n    :param stats: A stats collector object to record stats about the request scheduling process.\n                  The value for the :setting:`STATS_CLASS` setting is used by default.\n    :type stats: :class:`scrapy.statscollectors.StatsCollector` instance or similar:\n                 any class that implements the `StatsCollector` interface\n\n    :param pqclass: A class to be used as priority queue for requests.\n                    The value for the :setting:`SCHEDULER_PRIORITY_QUEUE` setting is used by default.\n    :type pqclass: class\n\n    :param crawler: The crawler object corresponding to the current crawl.\n    :type crawler: :class:`scrapy.crawler.Crawler`\n    \"\"\"\n\n    def __init__(\n        self,\n        dupefilter: BaseDupeFilter,\n        jobdir: Optional[str] = None,\n        dqclass=None,\n        mqclass=None,\n        logunser: bool = False,\n        stats: Optional[StatsCollector] = None,\n        pqclass=None,\n        crawler: Optional[Crawler] = None,\n    ):\n        self.df: BaseDupeFilter = dupefilter\n        self.dqdir: Optional[str] = self._dqdir(jobdir)\n        self.pqclass = pqclass\n        self.dqclass = dqclass\n        self.mqclass = mqclass\n        self.logunser: bool = logunser\n        self.stats: Optional[StatsCollector] = stats\n        self.crawler: Optional[Crawler] = crawler\n\n[docs]    @classmethod\n    def from_crawler(cls: Type[SchedulerTV], crawler: Crawler) -> SchedulerTV:\n        \"\"\"\n        Factory method, initializes the scheduler with arguments taken from the crawl settings\n        \"\"\"\n        dupefilter_cls = load_object(crawler.settings[\"DUPEFILTER_CLASS\"])\n        return cls(\n            dupefilter=create_instance(dupefilter_cls, crawler.settings, crawler),\n            jobdir=job_dir(crawler.settings),\n            dqclass=load_object(crawler.settings[\"SCHEDULER_DISK_QUEUE\"]),\n            mqclass=load_object(crawler.settings[\"SCHEDULER_MEMORY_QUEUE\"]),\n            logunser=crawler.settings.getbool(\"SCHEDULER_DEBUG\"),\n            stats=crawler.stats,\n            pqclass=load_object(crawler.settings[\"SCHEDULER_PRIORITY_QUEUE\"]),\n            crawler=crawler,\n        )\n\n[docs]    def has_pending_requests(self) -> bool:\n        return len(self) > 0\n\n[docs]    def open(self, spider: Spider) -> Optional[Deferred]:\n        \"\"\"\n        (1) initialize the memory queue\n        (2) initialize the disk queue if the ``jobdir`` attribute is a valid directory\n        (3) return the result of the dupefilter's ``open`` method\n        \"\"\"\n        self.spider = spider\n        self.mqs = self._mq()\n        self.dqs = self._dq() if self.dqdir else None\n        return self.df.open()\n\n[docs]    def close(self, reason: str) -> Optional[Deferred]:\n        \"\"\"\n        (1) dump pending requests to disk if there is a disk queue\n        (2) return the result of the dupefilter's ``close`` method\n        \"\"\"\n        if self.dqs is not None:\n            state = self.dqs.close()\n            assert isinstance(self.dqdir, str)\n            self._write_dqs_state(self.dqdir, state)\n        return self.df.close(reason)\n\n[docs]    def enqueue_request(self, request: Request) -> bool:\n        \"\"\"\n        Unless the received request is filtered out by the Dupefilter, attempt to push\n        it into the disk queue, falling back to pushing it into the memory queue.\n\n        Increment the appropriate stats, such as: ``scheduler/enqueued``,\n        ``scheduler/enqueued/disk``, ``scheduler/enqueued/memory``.\n\n        Return ``True`` if the request was stored successfully, ``False`` otherwise.\n        \"\"\"\n        if not request.dont_filter and self.df.request_seen(request):\n            self.df.log(request, self.spider)\n            return False\n        dqok = self._dqpush(request)\n        assert self.stats is not None\n        if dqok:\n            self.stats.inc_value(\"scheduler/enqueued/disk\", spider=self.spider)\n        else:\n            self._mqpush(request)\n            self.stats.inc_value(\"scheduler/enqueued/memory\", spider=self.spider)\n        self.stats.inc_value(\"scheduler/enqueued\", spider=self.spider)\n        return True\n\n[docs]    def next_request(self) -> Optional[Request]:\n        \"\"\"\n        Return a :class:`~scrapy.http.Request` object from the memory queue,\n        falling back to the disk queue if the memory queue is empty.\n        Return ``None`` if there are no more enqueued requests.\n\n        Increment the appropriate stats, such as: ``scheduler/dequeued``,\n        ``scheduler/dequeued/disk``, ``scheduler/dequeued/memory``.\n        \"\"\"\n        request: Optional[Request] = self.mqs.pop()\n        assert self.stats is not None\n        if request is not None:\n            self.stats.inc_value(\"scheduler/dequeued/memory\", spider=self.spider)\n        else:\n            request = self._dqpop()\n            if request is not None:\n                self.stats.inc_value(\"scheduler/dequeued/disk\", spider=self.spider)\n        if request is not None:\n            self.stats.inc_value(\"scheduler/dequeued\", spider=self.spider)\n        return request\n\n[docs]    def __len__(self) -> int:\n        \"\"\"\n        Return the total amount of enqueued requests\n        \"\"\"\n        return len(self.dqs) + len(self.mqs) if self.dqs is not None else len(self.mqs)\n\n    def _dqpush(self, request: Request) -> bool:\n        if self.dqs is None:\n            return False\n        try:\n            self.dqs.push(request)\n        except ValueError as e:  # non serializable request\n            if self.logunser:\n                msg = (\n                    \"Unable to serialize request: %(request)s - reason:\"\n                    \" %(reason)s - no more unserializable requests will be\"\n                    \" logged (stats being collected)\"\n                )\n                logger.warning(\n                    msg,\n                    {\"request\": request, \"reason\": e},\n                    exc_info=True,\n                    extra={\"spider\": self.spider},\n                )\n                self.logunser = False\n            assert self.stats is not None\n            self.stats.inc_value(\"scheduler/unserializable\", spider=self.spider)\n            return False\n        else:\n            return True\n\n    def _mqpush(self, request: Request) -> None:\n        self.mqs.push(request)\n\n    def _dqpop(self) -> Optional[Request]:\n        if self.dqs is not None:\n            return self.dqs.pop()\n        return None\n\n    def _mq(self):\n        \"\"\"Create a new priority queue instance, with in-memory storage\"\"\"\n        return create_instance(\n            self.pqclass,\n            settings=None,\n            crawler=self.crawler,\n            downstream_queue_cls=self.mqclass,\n            key=\"\",\n        )\n\n    def _dq(self):\n        \"\"\"Create a new priority queue instance, with disk storage\"\"\"\n        assert self.dqdir\n        state = self._read_dqs_state(self.dqdir)\n        q = create_instance(\n            self.pqclass,\n            settings=None,\n            crawler=self.crawler,\n            downstream_queue_cls=self.dqclass,\n            key=self.dqdir,\n            startprios=state,\n        )\n        if q:\n            logger.info(\n                \"Resuming crawl (%(queuesize)d requests scheduled)\",\n                {\"queuesize\": len(q)},\n                extra={\"spider\": self.spider},\n            )\n        return q\n\n    def _dqdir(self, jobdir: Optional[str]) -> Optional[str]:\n        \"\"\"Return a folder name to keep disk queue state at\"\"\"\n        if jobdir is not None:\n            dqdir = Path(jobdir, \"requests.queue\")\n            if not dqdir.exists():\n                dqdir.mkdir(parents=True)\n            return str(dqdir)\n        return None\n\n    def _read_dqs_state(self, dqdir: str) -> list:\n        path = Path(dqdir, \"active.json\")\n        if not path.exists():\n            return []\n        with path.open(encoding=\"utf-8\") as f:\n            return cast(list, json.load(f))\n\n    def _write_dqs_state(self, dqdir: str, state: list) -> None:\n        with Path(dqdir, \"active.json\").open(\"w\", encoding=\"utf-8\") as f:\n            json.dump(state, f)\n</pre>"], "links": [{"text": "[docs]", "href": "../../../topics/scheduler.html#scrapy.core.scheduler.BaseScheduler"}, {"text": "[docs]", "href": "../../../topics/scheduler.html#scrapy.core.scheduler.BaseScheduler.from_crawler"}, {"text": "[docs]", "href": "../../../topics/scheduler.html#scrapy.core.scheduler.BaseScheduler.open"}, {"text": "[docs]", "href": "../../../topics/scheduler.html#scrapy.core.scheduler.BaseScheduler.close"}, {"text": "[docs]", "href": "../../../topics/scheduler.html#scrapy.core.scheduler.BaseScheduler.has_pending_requests"}, {"text": "[docs]", "href": "../../../topics/scheduler.html#scrapy.core.scheduler.BaseScheduler.enqueue_request"}, {"text": "[docs]", "href": "../../../topics/scheduler.html#scrapy.core.scheduler.BaseScheduler.next_request"}, {"text": "[docs]", "href": "../../../topics/scheduler.html#scrapy.core.scheduler.Scheduler"}, {"text": "[docs]", "href": "../../../topics/scheduler.html#scrapy.core.scheduler.Scheduler.from_crawler"}, {"text": "[docs]", "href": "../../../topics/scheduler.html#scrapy.core.scheduler.Scheduler.has_pending_requests"}, {"text": "[docs]", "href": "../../../topics/scheduler.html#scrapy.core.scheduler.Scheduler.open"}, {"text": "[docs]", "href": "../../../topics/scheduler.html#scrapy.core.scheduler.Scheduler.close"}, {"text": "[docs]", "href": "../../../topics/scheduler.html#scrapy.core.scheduler.Scheduler.enqueue_request"}, {"text": "[docs]", "href": "../../../topics/scheduler.html#scrapy.core.scheduler.Scheduler.next_request"}, {"text": "[docs]", "href": "../../../topics/scheduler.html#scrapy.core.scheduler.Scheduler.__len__"}], "timestamp": "2023-10-12T21:20:31.118581", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"},
{"url": "https://docs.scrapy.org/en/latest/_modules/scrapy/spiderloader.html", "content": {"sections": [], "paragraphs": []}, "code_blocks": ["\nfrom __future__ import annotations\n\nimport traceback\nimport warnings\nfrom collections import defaultdict\nfrom types import ModuleType\nfrom typing import TYPE_CHECKING, DefaultDict, Dict, List, Tuple, Type\n\nfrom zope.interface import implementer\n\nfrom scrapy import Request, Spider\nfrom scrapy.interfaces import ISpiderLoader\nfrom scrapy.settings import BaseSettings\nfrom scrapy.utils.misc import walk_modules\nfrom scrapy.utils.spider import iter_spider_classes\n\nif TYPE_CHECKING:\n    # typing.Self requires Python 3.11\n    from typing_extensions import Self\n\n\n[docs]@implementer(ISpiderLoader)\nclass SpiderLoader:\n    \"\"\"\n    SpiderLoader is a class which locates and loads spiders\n    in a Scrapy project.\n    \"\"\"\n\n    def __init__(self, settings: BaseSettings):\n        self.spider_modules: List[str] = settings.getlist(\"SPIDER_MODULES\")\n        self.warn_only: bool = settings.getbool(\"SPIDER_LOADER_WARN_ONLY\")\n        self._spiders: Dict[str, Type[Spider]] = {}\n        self._found: DefaultDict[str, List[Tuple[str, str]]] = defaultdict(list)\n        self._load_all_spiders()\n\n    def _check_name_duplicates(self) -> None:\n        dupes = []\n        for name, locations in self._found.items():\n            dupes.extend(\n                [\n                    f\"  {cls} named {name!r} (in {mod})\"\n                    for mod, cls in locations\n                    if len(locations) > 1\n                ]\n            )\n\n        if dupes:\n            dupes_string = \"\\n\\n\".join(dupes)\n            warnings.warn(\n                \"There are several spiders with the same name:\\n\\n\"\n                f\"{dupes_string}\\n\\n  This can cause unexpected behavior.\",\n                category=UserWarning,\n            )\n\n    def _load_spiders(self, module: ModuleType) -> None:\n        for spcls in iter_spider_classes(module):\n            self._found[spcls.name].append((module.__name__, spcls.__name__))\n            self._spiders[spcls.name] = spcls\n\n    def _load_all_spiders(self) -> None:\n        for name in self.spider_modules:\n            try:\n                for module in walk_modules(name):\n                    self._load_spiders(module)\n            except ImportError:\n                if self.warn_only:\n                    warnings.warn(\n                        f\"\\n{traceback.format_exc()}Could not load spiders \"\n                        f\"from module '{name}'. \"\n                        \"See above traceback for details.\",\n                        category=RuntimeWarning,\n                    )\n                else:\n                    raise\n        self._check_name_duplicates()\n\n[docs]    @classmethod\n    def from_settings(cls, settings: BaseSettings) -> Self:\n        return cls(settings)\n\n[docs]    def load(self, spider_name: str) -> Type[Spider]:\n        \"\"\"\n        Return the Spider class for the given spider name. If the spider\n        name is not found, raise a KeyError.\n        \"\"\"\n        try:\n            return self._spiders[spider_name]\n        except KeyError:\n            raise KeyError(f\"Spider not found: {spider_name}\")\n\n[docs]    def find_by_request(self, request: Request) -> List[str]:\n        \"\"\"\n        Return the list of spider names that can handle the given request.\n        \"\"\"\n        return [\n            name for name, cls in self._spiders.items() if cls.handles_request(request)\n        ]\n\n[docs]    def list(self) -> List[str]:\n        \"\"\"\n        Return a list with the names of all spiders available in the project.\n        \"\"\"\n        return list(self._spiders.keys())\n</pre>"], "links": [{"text": "[docs]", "href": "../../topics/api.html#scrapy.spiderloader.SpiderLoader"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.spiderloader.SpiderLoader.from_settings"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.spiderloader.SpiderLoader.load"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.spiderloader.SpiderLoader.find_by_request"}, {"text": "[docs]", "href": "../../topics/api.html#scrapy.spiderloader.SpiderLoader.list"}], "timestamp": "2023-10-12T21:20:35.349619", "version": "2.11", "revision": "197781e3", "last_updated": "2023-09-24"}
]